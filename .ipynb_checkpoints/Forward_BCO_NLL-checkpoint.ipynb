{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "medieval-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pybullet --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caring-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch_optimizer as th_optim\n",
    "import pybullet_envs\n",
    "\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch.distributions import Normal, Independent\n",
    "from torch.nn import Parameter, functional as F\n",
    "\n",
    "import collections\n",
    "from utils.models import *\n",
    "from utils.utils import *\n",
    "from utils.dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "norman-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_simple_bco(records, records2=None, env_name=None, s=100, off=0.8):\n",
    "    ypoints = np.array(records)\n",
    "    plt.plot(ypoints)\n",
    "    if records2 is not None:\n",
    "        ypoints = np.array(records2)\n",
    "        plt.plot(ypoints, linestyle = 'dotted')\n",
    "    else:\n",
    "        ypoints[1:] = ypoints[1:] #- (np.random.rand(ypoints[1:].shape[0])-0.25)*2*s + (np.random.rand(ypoints[1:].shape[0])-off)*4*s\n",
    "    plt.title(env_name)\n",
    "    plt.xlabel('steps (10e3)')\n",
    "    plt.ylabel('reward')\n",
    "    plt.legend([\"Forward matching\", \"BC from observation\"], loc =\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "egyptian-miracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000000, device='cpu'):\n",
    "        self.capacity = capacity\n",
    "        self.device = device\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def append(self, episode_step):\n",
    "        self.buffer.append(episode_step)\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        # Note: replace=False makes random.choice O(n)\n",
    "        indexes = np.random.choice(len(self.buffer), sample_size, replace=True)\n",
    "        samples = [self.buffer[idx] for idx in indexes]\n",
    "        return self._unpack(samples)\n",
    "\n",
    "    def _unpack(self, samples):\n",
    "        states, actions, rewards, dones, next_states = [], [], [], [], []\n",
    "        for episode_step in samples:\n",
    "            states.append(episode_step['s'])\n",
    "            actions.append(episode_step['a'])\n",
    "            next_states.append(episode_step['s_prime'])\n",
    "\n",
    "        states = torch.FloatTensor(np.array(states, copy=False)).to(self.device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states, copy=False)).to(self.device)\n",
    "        actions = torch.LongTensor(np.array(actions, copy=False)).to(self.device)\n",
    "        return states, actions, next_states\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "known-albania",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnll = (\\n            mbrl.util.math.gaussian_nll(pred_mean, pred_logvar, target, reduce=False)\\n            .mean((1, 2))  # average over batch and target dimension\\n            .sum()\\n        )  # sum over ensemble dimension\\n        nll += 0.01 * (self.max_logvar.sum() - self.min_logvar.sum())\\n\\ndef gaussian_nll(\\n    pred_mean: torch.Tensor,\\n    pred_logvar: torch.Tensor,\\n    target: torch.Tensor,\\n    reduce: bool = True,\\n) -> torch.Tensor:\\n    \\n    l2 = F.mse_loss(pred_mean, target, reduction=\"none\")\\n    inv_var = (-pred_logvar).exp()\\n    losses = l2 * inv_var + pred_logvar\\n    if reduce:\\n        return losses.sum(dim=1).mean()\\n    return losses\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_noise_to_weights(model):\n",
    "    print('add noise to weight')\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param.add_(torch.randn(param.size()) * 0.1)\n",
    "            \n",
    "class NNet(nn.Module):\n",
    "    def __init__(self, D_in, D_out, \n",
    "                 n_hidden=64,\n",
    "                 do_rate=0.1,\n",
    "                 activation_in='relu'):\n",
    "        super(NNet, self).__init__()\n",
    "        if activation_in == 'relu':\n",
    "            mid_act = torch.nn.ReLU()\n",
    "        elif activation_in == 'tanh':\n",
    "            mid_act = torch.nn.Tanh()\n",
    "        elif activation_in == 'sigmoid':\n",
    "            mid_act = torch.nn.Sigmoid()\n",
    "\n",
    "        \n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(D_in, n_hidden),\n",
    "            mid_act,\n",
    "            torch.nn.Linear(n_hidden, n_hidden),\n",
    "            #torch.nn.Dropout(0.1),\n",
    "            mid_act,\n",
    "\n",
    "            torch.nn.Linear(n_hidden, n_hidden),\n",
    "            #torch.nn.Dropout(0.1),\n",
    "            mid_act,\n",
    "\n",
    "            #torch.nn.utils.spectral_norm(torch.nn.Linear(n_hidden, D_out, bias=True)),\n",
    "            torch.nn.Linear(n_hidden, D_out, bias=True),\n",
    "            #torch.nn.Dropout(0.1),\n",
    "        )\n",
    "\n",
    "    def forward(self, s, a):\n",
    "        x = torch.cat((s,a), dim=1)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "def kl_divergence(z, mu, std):\n",
    "        # https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed\n",
    "        # --------------------------\n",
    "        # Monte carlo KL divergence\n",
    "        # --------------------------\n",
    "        # 1. define the first two probabilities (in this case Normal for both)\n",
    "        p = torch.distributions.Normal(mu, torch.ones_like(std)*0.06)\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "        # 2. get the probabilities from the equation\n",
    "        log_qzx = q.log_prob(z)\n",
    "        log_pz = p.log_prob(z)\n",
    "\n",
    "        # kl\n",
    "        kl = (log_qzx - log_pz)\n",
    "        kl = kl.sum(-1)\n",
    "        return kl\n",
    "    \n",
    "from torch.distributions import Independent, Normal\n",
    "\n",
    "class DRILEnsemble(nn.Module):\n",
    "      # https://github.com/Kaixhin/imitation-learning/blob/795e8b216dde1a4995a093d490b03e6e0119a313/models.py#L49\n",
    "    def __init__(self, state_size, action_size, hidden_size, activation_function='tanh', log_std_dev_init=-5., dropout=0):\n",
    "        super().__init__()\n",
    "        #self.actors = self._create_fcnn(state_size, hidden_size, output_size=action_size, activation_function=activation_function, dropout=dropout, final_gain=0.01)\n",
    "        n_ensemble = 1\n",
    "        self.actors = []\n",
    "        self.log_std_devs = []\n",
    "        for m in range(n_ensemble):\n",
    "            self.actors.append(self._create_fcnn(state_size, \n",
    "                                                 hidden_size, \n",
    "                                                 output_size=action_size, \n",
    "                                                 activation_function=activation_function, \n",
    "                                                 dropout=dropout, \n",
    "                                                 final_gain=0.01))\n",
    "            self.log_std_devs.append(Parameter(torch.full((action_size, ), log_std_dev_init, dtype=torch.float32)))\n",
    "\n",
    "    def forward(self, state, en_id):\n",
    "        mean = self.actors[en_id](state)\n",
    "        policy = Independent(Normal(mean, self.log_std_devs[en_id].exp()), 1)\n",
    "        return policy\n",
    "    \n",
    "    def _create_fcnn(self, input_size, hidden_size, output_size, activation_function, dropout=0, final_gain=1.0):\n",
    "        #assert activation_function in ACTIVATION_FUNCTIONS.keys()\n",
    "        ACTIVATION_FUNCTIONS = {'relu': nn.ReLU, 'sigmoid': nn.Sigmoid, 'tanh': nn.Tanh}\n",
    "        assert activation_function in ACTIVATION_FUNCTIONS.keys()\n",
    "        network_dims, layers = (input_size, hidden_size, hidden_size), []\n",
    "\n",
    "        for l in range(len(network_dims) - 1):\n",
    "            layer = nn.Linear(network_dims[l], network_dims[l + 1])\n",
    "            nn.init.orthogonal_(layer.weight, gain=nn.init.calculate_gain(activation_function))\n",
    "            nn.init.constant_(layer.bias, 0)\n",
    "            layers.append(layer)\n",
    "            if dropout > 0: layers.append(nn.Dropout(p=dropout))\n",
    "            layers.append(ACTIVATION_FUNCTIONS[activation_function]())\n",
    "\n",
    "        final_layer = nn.Linear(network_dims[-1], output_size)\n",
    "        nn.init.orthogonal_(final_layer.weight, gain=final_gain)\n",
    "        nn.init.constant_(final_layer.bias, 0)\n",
    "        layers.append(final_layer)\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "      # Calculates the log probability of an action a with the policy π(·|s) given state s\n",
    "    def log_prob(self, state, action, en_id):\n",
    "        return self.forward(state, en_id).log_prob(action)\n",
    "\n",
    "    def _get_action_uncertainty(self, state, action):\n",
    "        ensemble_policies = []\n",
    "        for eid in range(len(self.actors)):  # Perform Monte-Carlo dropout for an implicit ensemble\n",
    "            ensemble_policies.append(self.log_prob(state, action, eid).exp())\n",
    "        return torch.stack(ensemble_policies).var(dim=0)\n",
    "\n",
    "      # Set uncertainty threshold at the 98th quantile of uncertainty costs calculated over the expert data\n",
    "    def set_uncertainty_threshold(self, expert_state, expert_action):\n",
    "        self.q = torch.quantile(self._get_action_uncertainty(expert_state, expert_action), 0.9).item()\n",
    "\n",
    "    def predict_reward(self, state, action):\n",
    "        # Calculate (raw) uncertainty cost\n",
    "        uncertainty_cost = self._get_action_uncertainty(state, action)\n",
    "        # Calculate clipped uncertainty cost\n",
    "        neg_idxs = uncertainty_cost.less_equal(self.q)\n",
    "        uncertainty_cost[neg_idxs] = -1\n",
    "        uncertainty_cost[~neg_idxs] = 1\n",
    "        return -uncertainty_cost\n",
    "\n",
    "    \n",
    "# Performs a behavioural cloning update\n",
    "def supervised_NLL_update(agent, expert_dataloader, agent_optimiser, batch_size):\n",
    "    #expert_dataloader = DataLoader(expert_trajectories, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=4)\n",
    "    for expert_transition in expert_dataloader:\n",
    "        expert_state, expert_action = expert_transition['states'], expert_transition['actions']\n",
    "        agent_optimiser.zero_grad(set_to_none=True)\n",
    "        behavioural_cloning_loss = -agent.log_prob(expert_state, expert_action).mean()  # Maximum likelihood objective\n",
    "        behavioural_cloning_loss.backward()\n",
    "        agent_optimiser.step()\n",
    "    return behavioural_cloning_loss\n",
    "\n",
    "\"\"\"\n",
    "nll = (\n",
    "            mbrl.util.math.gaussian_nll(pred_mean, pred_logvar, target, reduce=False)\n",
    "            .mean((1, 2))  # average over batch and target dimension\n",
    "            .sum()\n",
    "        )  # sum over ensemble dimension\n",
    "        nll += 0.01 * (self.max_logvar.sum() - self.min_logvar.sum())\n",
    "\n",
    "def gaussian_nll(\n",
    "    pred_mean: torch.Tensor,\n",
    "    pred_logvar: torch.Tensor,\n",
    "    target: torch.Tensor,\n",
    "    reduce: bool = True,\n",
    ") -> torch.Tensor:\n",
    "    \n",
    "    l2 = F.mse_loss(pred_mean, target, reduction=\"none\")\n",
    "    inv_var = (-pred_logvar).exp()\n",
    "    losses = l2 * inv_var + pred_logvar\n",
    "    if reduce:\n",
    "        return losses.sum(dim=1).mean()\n",
    "    return losses\n",
    "\"\"\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "overall-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_model_training(transitions, inv_model, ep_num=100):\n",
    "    inv_dataset = transition_dataset(transitions)\n",
    "    #inv_dataset_list.append(inv_dataset)\n",
    "    #inv_dataset_final = ConcatDataset(inv_dataset_list)\n",
    "    inv_loader = DataLoader(inv_dataset, batch_size=1024, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "    #print('-- training: ' + str(m+1) + ' of ' + str(inv_model.n_ensemble) + ' NNs --')\n",
    "    print('dynamic model training...')\n",
    "    \n",
    "    for en_id in range(len(inv_model.actors)):\n",
    "        add_noise_to_weights(inv_model.actors[en_id])\n",
    "        #inv_opt = optim.Adam(inv_model.parameters(), lr=1e-3, weight_decay=0.0001)\n",
    "        #\"\"\"\n",
    "        inv_opt_yogi = th_optim.Yogi(\n",
    "            inv_model.actors[en_id].parameters(),\n",
    "            lr= 1e-2,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-3,\n",
    "            initial_accumulator=1e-6,\n",
    "            weight_decay=0,\n",
    "        )\n",
    "\n",
    "        inv_opt = th_optim.Lookahead(inv_opt_yogi,  alpha=0.5)#k=5\n",
    "        #\"\"\"\n",
    "\n",
    "        for epoch in range(ep_num): \n",
    "            running_loss = 0\n",
    "            for i, data in enumerate(inv_loader):\n",
    "                s, a, s_prime = data\n",
    "                inv_opt.zero_grad()\n",
    "\n",
    "                inv_input = torch.cat((s,a), dim=1).float()\n",
    "\n",
    "                #print(inv_input[0])\n",
    "                #print(inv_model._get_action_uncertainty(inv_input, s_prime))\n",
    "                #print(inv_input[0].shape)\n",
    "                #print(inv_model._get_action_uncertainty(inv_input, s_prime).shape)\n",
    "                loss = -inv_model.log_prob(inv_input, s_prime, en_id).mean()  # Maximum likelihood objective\n",
    "                #loss = -custom_loglikehood(inv_model.actors[0](inv_input), inv_model.log_std_devs[0].exp(), s_prime)\n",
    "\n",
    "                loss.backward()\n",
    "                running_loss += loss.item()\n",
    "                if i%100 == 99:\n",
    "                    running_loss = 0\n",
    "                inv_opt.step()\n",
    "            if epoch%20==0:\n",
    "                print('Epoch:%d Batch:%d Loss:%.5f'%(epoch, i+1, loss))\n",
    "    print('Done!')\n",
    "    \n",
    "    inv_model.set_uncertainty_threshold(inv_input, s_prime)#(inv_dataset.x, inv_dataset.y)\n",
    "    \n",
    "    print('threshold:', inv_model.q)\n",
    "    return inv_model\n",
    "\n",
    "def train_bc(trajs, policy, dynamics,  ep_num=50, sample_itr=500, batch_size=1024):\n",
    "    \n",
    "    bc_dataset = imitation_dataset(trajs)\n",
    "    bc_loader = DataLoader(bc_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    add_noise_to_weights(policy)\n",
    "    \n",
    "    print('Learning policy....')\n",
    "    #bc_opt = optim.Adam(policy.parameters(), lr=1e-3, weight_decay=0.0001)\n",
    "    #\"\"\"\n",
    "    bc_opt_yogi = th_optim.Yogi(\n",
    "        policy.parameters(),\n",
    "        lr= 1e-2,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-3,\n",
    "        initial_accumulator=1e-6,\n",
    "        weight_decay=0,\n",
    "    )\n",
    "\n",
    "    bc_opt = th_optim.Lookahead(bc_opt_yogi,  alpha=0.5)#k=5,\n",
    "    #\"\"\"\n",
    "    bc_loss = nn.MSELoss()\n",
    "    # bc_loss = nn.L1Loss()\n",
    "    \n",
    "    err_sample_size = 256\n",
    "    \n",
    "    #if err_behaviors.__len__() < err_sample_size:\n",
    "    #    err_sample_size = err_behaviors.__len__()\n",
    "    #if err_behaviors.__len__() > 0:\n",
    "    #    err_s, err_a, err_sprime = err_behaviors.sample(err_sample_size)\n",
    "    \n",
    "    for epoch in range(ep_num):  \n",
    "        running_loss = 0\n",
    "        for i, data in enumerate(bc_loader):\n",
    "            s, s_prime = data\n",
    "            bc_opt.zero_grad()\n",
    "            \n",
    "            ex_dim=5\n",
    "            s = s.expand(ex_dim,s.shape[0],s.shape[1]).reshape(ex_dim*s.shape[0],s.shape[1]) #s.repeat(30,1)\n",
    "            s_prime = s_prime.expand(ex_dim,s_prime.shape[0],s_prime.shape[1]).reshape(ex_dim*s_prime.shape[0],s_prime.shape[1]) #s_prime.repeat(30,1)\n",
    "            \n",
    "            #\"\"\"\n",
    "            #a_pred = policy.reparam_forward(s.float())\n",
    "            try:\n",
    "                a_mu, a_sigma = policy(s.float())\n",
    "                #a_pred = Normal(loc=a_mu, scale=a_sigma+1e-6).rsample(sample_shape=[10])\n",
    "                #a_pred = a_pred.reshape(10*a_mu.shape[0], a_mu.shape[1])\n",
    "                a_pred = Normal(loc=a_mu, scale=a_sigma+0.03).rsample()\n",
    "                a_lar_pred = Normal(loc=a_mu, scale=a_sigma+0.3).rsample()\n",
    "                #print(a_pred.shape)\n",
    "                \n",
    "                #err_a_mu, err_a_sigma = policy(err_s.float())\n",
    "            except:\n",
    "                a_pred = policy.reparam_forward(s.float())\n",
    "            #\"\"\"\n",
    "            #print(torch.cat((s, a_pred), dim=1).shape)\n",
    "            \n",
    "            err_loss_func = nn.GaussianNLLLoss()\n",
    "            inv_input = torch.cat((s,a_pred), dim=1).float()\n",
    "            #loss = -inv_model.log_prob(inv_input, s_prime, en_id=0).mean()  # Maximum likelihood objective\n",
    "            loss = 0\n",
    "            err_loss = 0\n",
    "            \n",
    "            for m in range(len(dynamics.actors)):\n",
    "                #print(policy.log_prob(s.float(), a_pred).shape)\n",
    "                #print(inv_model.log_prob(inv_input, s_prime, en_id=m).shape)\n",
    "                #loss += torch.dot(policy.log_prob(s.float(), a_pred), inv_model.log_prob(inv_input, s_prime, en_id=m)).mean()\n",
    "                #print(policy.log_prob(s.float(), a_pred)[:20])\n",
    "                #print(inv_model.log_prob(inv_input, s_prime, en_id=m)[:20])\n",
    "                \n",
    "                loss += -inv_model.log_prob(inv_input, s_prime, en_id=m).mean()# + 1e-7\n",
    "                loss += -policy.log_prob(s.float(), a_pred).mean() - policy.log_prob(s.float(), a_lar_pred).mean()* 0.05\n",
    "                #if err_behaviors.__len__()!=0:\n",
    "                #    err_loss -= err_loss_func(err_a_mu, err_a, err_a_sigma, eps=1e-6) \n",
    "            #loss_uc = inv_model._get_action_uncertainty(inv_input, s_prime).mean()\n",
    "            #print(loss_uc, inv_model.q)\n",
    "            #loss_kl = kl_divergence(a_pred, a_mu, a_sigma).mean()\n",
    "            #loss += loss_kl\n",
    "            \n",
    "            #loss += err_loss\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            if i%20 == 19:\n",
    "                running_loss = 0\n",
    "            bc_opt.step()\n",
    "        if epoch%10==0:\n",
    "            print('Epoch:%d Batch:%d Loss:%.3f'%(epoch, i+1, loss))\n",
    "\n",
    "    print('Done!')\n",
    "    return policy\n",
    "\n",
    "\n",
    "def load_demos(DEMO_DIR):\n",
    "    \"\"\"load demonstrations\"\"\"\n",
    "    try:\n",
    "        trajstrajs = np.load(\"experts/states_expert_walker_.npy\")[:10]\n",
    "    except:\n",
    "        with open(DEMO_DIR, 'rb') as f:\n",
    "            trajs = pickle.load(f)\n",
    "    demos = []\n",
    "    for t_id, traj in enumerate(trajs):\n",
    "        demo =[]\n",
    "        #print(t_id)\n",
    "        for item in traj:    \n",
    "            obs = item['observation']\n",
    "            #obs = list(obs)\n",
    "            #print(obs)\n",
    "            demo.append(obs)\n",
    "        #print(np.array(demo).shape)\n",
    "        demos.append(np.array(demo))\n",
    "\n",
    "    print(np.array(demos).shape)\n",
    "    demos = demos[:10]\n",
    "    return demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "right-sacramento",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_inv_samples(env, policy, num_samples, env_type, use_policy, max_steps):\n",
    "    count = 0\n",
    "    transitions = []\n",
    "    s = env.reset()\n",
    "    t = 0\n",
    "    r = 0\n",
    "    rs = []\n",
    "    err_pair = None\n",
    "    \n",
    "    while count < num_samples:\n",
    "        \n",
    "        if env_type == 'continuous':\n",
    "            if use_policy:\n",
    "                try:\n",
    "                    mean, sigma = policy(torch.tensor([s]).float())\n",
    "                    #print(mean, sigma)\n",
    "                    pi = Normal(loc=mean, scale=sigma+1e-7)\n",
    "                    a = pi.sample().detach().numpy()[0]\n",
    "                    #print(a)\n",
    "                    #a = select_action_continuous(s, policy)\n",
    "                except:\n",
    "                    print(mean, sigma)\n",
    "                    a = policy.reparam_forward(torch.tensor([s]).float(), tau=10e-2).detach().numpy()[0]\n",
    "                    #pi, sigma, mu = policy(torch.tensor([s]).float())\n",
    "                    #a = policy.mdn_sample(pi, sigma, mu).detach().numpy()[0]\n",
    "            else:\n",
    "                a = env.action_space.sample()\n",
    "        else:\n",
    "            a = select_action_discrete(s, policy)\n",
    "            \n",
    "        a = np.clip(a, -1, 1)\n",
    "        \n",
    "        s_prime, reward, done, _ = env.step(a)\n",
    "        \n",
    "            #print(a)\n",
    "        transitions.append([s, a, s_prime])\n",
    "        count += 1\n",
    "        t += 1\n",
    "        r += reward\n",
    "        #print(t)\n",
    "        if done == True or t>(max_steps-1) or count == (num_samples-1):\n",
    "            if done==True and (t<(max_steps-1) or count != (num_samples-1)):\n",
    "                err_pair={\"s\":s, \"a\":a, \"s_prime\":s_prime}\n",
    "            rs.append(r)\n",
    "            print(\"reward:\", r, \"setps:\", t, \"count:\", count)\n",
    "            s = env.reset()\n",
    "            t = 0\n",
    "            r = 0\n",
    "            break\n",
    "        else:\n",
    "            s = s_prime\n",
    "    print(\"avg rewards:\",np.mean(np.array(rs)))\n",
    "    return transitions, np.mean(np.array(rs)), count, err_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-enhancement",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# start Walker2DBulletEnv-v0 training ###################\n",
      "(49, 1000, 22)\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf] [-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf]\n",
      "[1. 1. 1. 1. 1. 1.] [-1. -1. -1. -1. -1. -1.]\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 13.612366641615518 setps: 9 count: 9\n",
      "avg rewards: 13.612366641615518\n",
      "Done! (9, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:91: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:1 Loss:434405.90625\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 15.979032434444525 setps: 11 count: 11\n",
      "avg rewards: 15.979032434444525\n",
      "Done! (20, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:542746.12500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 21 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 14.130870393406076 setps: 10 count: 10\n",
      "avg rewards: 14.130870393406076\n",
      "Done! (30, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:615525.43750\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 31 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 18.669600738589363 setps: 19 count: 19\n",
      "avg rewards: 18.669600738589363\n",
      "Done! (49, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:788425.25000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 50 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 12.537137566346793 setps: 10 count: 10\n",
      "avg rewards: 12.537137566346793\n",
      "Done! (59, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:860240.68750\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 60 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 20.983999046466487 setps: 19 count: 19\n",
      "avg rewards: 20.983999046466487\n",
      "Done! (78, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1050453.37500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 79 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 16.50595132143062 setps: 14 count: 14\n",
      "avg rewards: 16.50595132143062\n",
      "Done! (92, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1015697.56250\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 93 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 14.302710170543287 setps: 11 count: 11\n",
      "avg rewards: 14.302710170543287\n",
      "Done! (103, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1078579.87500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 104 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 22.77872056429769 setps: 22 count: 22\n",
      "avg rewards: 22.77872056429769\n",
      "Done! (125, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1249574.12500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 126 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 13.55962494687701 setps: 11 count: 11\n",
      "avg rewards: 13.55962494687701\n",
      "Done! (136, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1044051.43750\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 137 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 21.112317685608282 setps: 17 count: 17\n",
      "avg rewards: 21.112317685608282\n",
      "Done! (153, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1179783.62500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 154 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 22.987958626993354 setps: 23 count: 23\n",
      "avg rewards: 22.987958626993354\n",
      "Done! (176, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1220274.12500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 177 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 15.775270355938117 setps: 14 count: 14\n",
      "avg rewards: 15.775270355938117\n",
      "Done! (190, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1341841.37500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 191 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 22.72204447431286 setps: 21 count: 21\n",
      "avg rewards: 22.72204447431286\n",
      "Done! (211, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1138483.62500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 212 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 28.121835709999143 setps: 31 count: 31\n",
      "avg rewards: 28.121835709999143\n",
      "Done! (242, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1333182.75000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 243 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 17.4859841881218 setps: 13 count: 13\n",
      "avg rewards: 17.4859841881218\n",
      "Done! (255, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1254704.87500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 256 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 21.956135166872993 setps: 20 count: 20\n",
      "avg rewards: 21.956135166872993\n",
      "Done! (275, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1275230.12500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 276 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 18.700343586238162 setps: 16 count: 16\n",
      "avg rewards: 18.700343586238162\n",
      "Done! (291, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1487614.00000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 292 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 17.602037226998075 setps: 16 count: 16\n",
      "avg rewards: 17.602037226998075\n",
      "Done! (307, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1601977.37500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 308 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 16.612084158926155 setps: 13 count: 13\n",
      "avg rewards: 16.612084158926155\n",
      "Done! (320, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1463092.87500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 321 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 21.83146399106336 setps: 21 count: 21\n",
      "avg rewards: 21.83146399106336\n",
      "Done! (341, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1428754.62500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 342 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 15.37157861517917 setps: 10 count: 10\n",
      "avg rewards: 15.37157861517917\n",
      "Done! (351, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1552004.00000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 352 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 14.817820578589453 setps: 10 count: 10\n",
      "avg rewards: 14.817820578589453\n",
      "Done! (361, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1783002.62500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 362 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 23.440631950121315 setps: 24 count: 24\n",
      "avg rewards: 23.440631950121315\n",
      "Done! (385, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1830076.62500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 386 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 23.54868436737161 setps: 23 count: 23\n",
      "avg rewards: 23.54868436737161\n",
      "Done! (408, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1754452.75000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 409 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 18.88502148178959 setps: 17 count: 17\n",
      "avg rewards: 18.88502148178959\n",
      "Done! (425, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:1 Loss:1541804.75000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 426 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 15.715828248830805 setps: 13 count: 13\n",
      "avg rewards: 15.715828248830805\n",
      "Done! (438, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1721391.62500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 439 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 15.105577748261567 setps: 10 count: 10\n",
      "avg rewards: 15.105577748261567\n",
      "Done! (448, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1692705.75000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 449 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 17.149385631272157 setps: 17 count: 17\n",
      "avg rewards: 17.149385631272157\n",
      "Done! (465, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1783343.37500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 466 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 16.3025287117227 setps: 12 count: 12\n",
      "avg rewards: 16.3025287117227\n",
      "Done! (477, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:2203960.00000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 478 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 20.935026204536555 setps: 21 count: 21\n",
      "avg rewards: 20.935026204536555\n",
      "Done! (498, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:2353128.75000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 499 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 30.513668347043865 setps: 28 count: 28\n",
      "avg rewards: 30.513668347043865\n",
      "Done! (526, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:2048197.12500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 527 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 11.599477476179889 setps: 9 count: 9\n",
      "avg rewards: 11.599477476179889\n",
      "Done! (535, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:2051637.62500\n",
      "Epoch:20 Batch:1 Loss:1568754.62500\n",
      "Epoch:40 Batch:1 Loss:1482645.62500\n",
      "Epoch:60 Batch:1 Loss:1429105.87500\n",
      "Epoch:80 Batch:1 Loss:1360233.00000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "add noise to weight\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:1266859.500\n",
      "Epoch:10 Batch:10 Loss:1270149.000\n",
      "Epoch:20 Batch:10 Loss:1261785.125\n",
      "Epoch:30 Batch:10 Loss:1267065.125\n",
      "Epoch:40 Batch:10 Loss:1280760.125\n",
      "Epoch:50 Batch:10 Loss:1249689.250\n",
      "Epoch:60 Batch:10 Loss:1254264.125\n",
      "Epoch:70 Batch:10 Loss:1259728.125\n",
      "Epoch:80 Batch:10 Loss:1259904.750\n",
      "Epoch:90 Batch:10 Loss:1260608.375\n",
      "Done!\n",
      "######## STEP 536 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 11.919460353069008 setps: 7 count: 7\n",
      "avg rewards: 11.919460353069008\n",
      "Done! (542, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1933558.62500\n",
      "Epoch:20 Batch:1 Loss:1289299.25000\n",
      "Epoch:40 Batch:1 Loss:1243948.12500\n",
      "Epoch:60 Batch:1 Loss:1169309.37500\n",
      "Epoch:80 Batch:1 Loss:1089766.00000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "add noise to weight\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:1053065.000\n",
      "Epoch:10 Batch:10 Loss:1052693.000\n",
      "Epoch:20 Batch:10 Loss:1063756.375\n",
      "Epoch:30 Batch:10 Loss:1057915.625\n",
      "Epoch:40 Batch:10 Loss:1055758.000\n",
      "Epoch:50 Batch:10 Loss:1065070.000\n",
      "Epoch:60 Batch:10 Loss:1058818.000\n",
      "Epoch:70 Batch:10 Loss:1056287.750\n",
      "Epoch:80 Batch:10 Loss:1066240.500\n",
      "Epoch:90 Batch:10 Loss:1061125.375\n",
      "Done!\n",
      "######## STEP 543 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 37.20932151520829 setps: 34 count: 34\n",
      "avg rewards: 37.20932151520829\n",
      "Done! (576, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1708696.87500\n",
      "Epoch:20 Batch:1 Loss:1123679.12500\n",
      "Epoch:40 Batch:1 Loss:1034576.68750\n",
      "Epoch:60 Batch:1 Loss:978623.75000\n",
      "Epoch:80 Batch:1 Loss:946555.25000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "add noise to weight\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:881093.125\n",
      "Epoch:10 Batch:10 Loss:879253.250\n",
      "Epoch:20 Batch:10 Loss:891091.000\n",
      "Epoch:30 Batch:10 Loss:883101.562\n",
      "Epoch:40 Batch:10 Loss:881982.375\n",
      "Epoch:50 Batch:10 Loss:884528.875\n",
      "Epoch:60 Batch:10 Loss:888372.812\n",
      "Epoch:70 Batch:10 Loss:884412.500\n",
      "Epoch:80 Batch:10 Loss:880969.062\n",
      "Epoch:90 Batch:10 Loss:878105.312\n",
      "Done!\n",
      "######## STEP 577 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 7.219135691149858 setps: 11 count: 11\n",
      "avg rewards: 7.219135691149858\n",
      "Done! (587, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1403699.62500\n",
      "Epoch:20 Batch:1 Loss:906106.12500\n",
      "Epoch:40 Batch:1 Loss:861096.43750\n",
      "Epoch:60 Batch:1 Loss:837886.62500\n",
      "Epoch:80 Batch:1 Loss:775988.18750\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "add noise to weight\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:752679.750\n",
      "Epoch:10 Batch:10 Loss:743449.875\n",
      "Epoch:20 Batch:10 Loss:747742.500\n",
      "Epoch:30 Batch:10 Loss:751150.250\n",
      "Epoch:40 Batch:10 Loss:746279.062\n",
      "Epoch:50 Batch:10 Loss:743521.375\n",
      "Epoch:60 Batch:10 Loss:744732.188\n",
      "Epoch:70 Batch:10 Loss:746031.250\n",
      "Epoch:80 Batch:10 Loss:745131.562\n",
      "Epoch:90 Batch:10 Loss:738410.500\n",
      "Done!\n",
      "######## STEP 588 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 24.194851414064757 setps: 58 count: 58\n",
      "avg rewards: 24.194851414064757\n",
      "Done! (645, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1180012.87500\n",
      "Epoch:20 Batch:1 Loss:774197.56250\n",
      "Epoch:40 Batch:1 Loss:733840.68750\n",
      "Epoch:60 Batch:1 Loss:679748.18750\n",
      "Epoch:80 Batch:1 Loss:625317.87500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "add noise to weight\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:617975.438\n",
      "Epoch:10 Batch:10 Loss:619892.312\n",
      "Epoch:20 Batch:10 Loss:621760.438\n",
      "Epoch:30 Batch:10 Loss:617719.438\n",
      "Epoch:40 Batch:10 Loss:615602.688\n",
      "Epoch:50 Batch:10 Loss:619485.562\n",
      "Epoch:60 Batch:10 Loss:611236.750\n",
      "Epoch:70 Batch:10 Loss:622143.188\n",
      "Epoch:80 Batch:10 Loss:622594.375\n",
      "Epoch:90 Batch:10 Loss:621483.188\n",
      "Done!\n",
      "######## STEP 646 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 11.491359578147238 setps: 11 count: 11\n",
      "avg rewards: 11.491359578147238\n",
      "Done! (656, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1228233.62500\n",
      "Epoch:20 Batch:1 Loss:670586.31250\n",
      "Epoch:40 Batch:1 Loss:611538.75000\n",
      "Epoch:60 Batch:1 Loss:582543.12500\n",
      "Epoch:80 Batch:1 Loss:562221.87500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "add noise to weight\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:542369.812\n",
      "Epoch:10 Batch:10 Loss:542390.500\n",
      "Epoch:20 Batch:10 Loss:541153.188\n",
      "Epoch:30 Batch:10 Loss:538869.375\n",
      "Epoch:40 Batch:10 Loss:543833.625\n",
      "Epoch:50 Batch:10 Loss:539514.938\n",
      "Epoch:60 Batch:10 Loss:544657.250\n",
      "Epoch:70 Batch:10 Loss:535698.438\n",
      "Epoch:80 Batch:10 Loss:541114.188\n",
      "Epoch:90 Batch:10 Loss:545221.500\n",
      "Done!\n",
      "######## STEP 657 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 3.082610626927635 setps: 11 count: 11\n",
      "avg rewards: 3.082610626927635\n",
      "Done! (667, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:957951.50000\n",
      "Epoch:20 Batch:1 Loss:590523.50000\n",
      "Epoch:40 Batch:1 Loss:548648.93750\n",
      "Epoch:60 Batch:1 Loss:518033.28125\n",
      "Epoch:80 Batch:1 Loss:482967.21875\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "add noise to weight\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:470782.812\n",
      "Epoch:10 Batch:10 Loss:470570.938\n",
      "Epoch:20 Batch:10 Loss:472070.562\n",
      "Epoch:30 Batch:10 Loss:473751.969\n",
      "Epoch:40 Batch:10 Loss:472243.781\n",
      "Epoch:50 Batch:10 Loss:472327.281\n",
      "Epoch:60 Batch:10 Loss:471403.250\n",
      "Epoch:70 Batch:10 Loss:462964.062\n",
      "Epoch:80 Batch:10 Loss:471829.000\n",
      "Epoch:90 Batch:10 Loss:469117.000\n",
      "Done!\n",
      "######## STEP 668 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 5.660785336863773 setps: 8 count: 8\n",
      "avg rewards: 5.660785336863773\n",
      "Done! (675, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:1 Loss:1170015.75000\n",
      "Epoch:20 Batch:1 Loss:517812.62500\n",
      "Epoch:40 Batch:1 Loss:496163.65625\n",
      "Epoch:60 Batch:1 Loss:479524.46875\n",
      "Epoch:80 Batch:1 Loss:441812.06250\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "add noise to weight\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:418237.188\n",
      "Epoch:10 Batch:10 Loss:420002.906\n",
      "Epoch:20 Batch:10 Loss:416347.281\n",
      "Epoch:30 Batch:10 Loss:419548.969\n",
      "Epoch:40 Batch:10 Loss:412386.188\n",
      "Epoch:50 Batch:10 Loss:416318.844\n",
      "Epoch:60 Batch:10 Loss:418946.000\n",
      "Epoch:70 Batch:10 Loss:418847.594\n",
      "Epoch:80 Batch:10 Loss:421186.531\n",
      "Epoch:90 Batch:10 Loss:419211.562\n",
      "Done!\n",
      "######## STEP 676 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 5.928162276605144 setps: 9 count: 9\n",
      "avg rewards: 5.928162276605144\n",
      "Done! (684, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:992983.93750\n",
      "Epoch:20 Batch:1 Loss:458597.37500\n",
      "Epoch:40 Batch:1 Loss:436861.28125\n",
      "Epoch:60 Batch:1 Loss:420358.18750\n",
      "Epoch:80 Batch:1 Loss:387638.09375\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "add noise to weight\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:376450.438\n",
      "Epoch:10 Batch:10 Loss:373703.375\n",
      "Epoch:20 Batch:10 Loss:376251.312\n",
      "Epoch:30 Batch:10 Loss:371927.875\n",
      "Epoch:40 Batch:10 Loss:373473.500\n",
      "Epoch:50 Batch:10 Loss:375935.906\n",
      "Epoch:60 Batch:10 Loss:378854.844\n",
      "Epoch:70 Batch:10 Loss:376392.250\n",
      "Epoch:80 Batch:10 Loss:378142.750\n",
      "Epoch:90 Batch:10 Loss:375554.531\n",
      "Done!\n",
      "######## STEP 685 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 3.352766942809103 setps: 10 count: 10\n",
      "avg rewards: 3.352766942809103\n",
      "Done! (694, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:889769.50000\n",
      "Epoch:20 Batch:1 Loss:416211.78125\n",
      "Epoch:40 Batch:1 Loss:398561.65625\n",
      "Epoch:60 Batch:1 Loss:370511.90625\n",
      "Epoch:80 Batch:1 Loss:354433.71875\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "add noise to weight\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:344695.875\n",
      "Epoch:10 Batch:10 Loss:346632.938\n",
      "Epoch:20 Batch:10 Loss:345023.469\n",
      "Epoch:30 Batch:10 Loss:345094.312\n",
      "Epoch:40 Batch:10 Loss:342398.344\n",
      "Epoch:50 Batch:10 Loss:345645.656\n",
      "Epoch:60 Batch:10 Loss:343755.000\n",
      "Epoch:70 Batch:10 Loss:341191.031\n",
      "Epoch:80 Batch:10 Loss:344768.031\n",
      "Epoch:90 Batch:10 Loss:343566.188\n",
      "Done!\n",
      "######## STEP 695 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 5.275425946475297 setps: 10 count: 10\n",
      "avg rewards: 5.275425946475297\n",
      "Done! (704, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:898362.18750\n",
      "Epoch:20 Batch:1 Loss:409898.28125\n",
      "Epoch:40 Batch:1 Loss:361594.56250\n",
      "Epoch:60 Batch:1 Loss:343233.96875\n",
      "Epoch:80 Batch:1 Loss:331370.12500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "add noise to weight\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:323389.469\n",
      "Epoch:10 Batch:10 Loss:321373.938\n",
      "Epoch:20 Batch:10 Loss:321765.062\n",
      "Epoch:30 Batch:10 Loss:323871.406\n",
      "Epoch:40 Batch:10 Loss:323203.062\n",
      "Epoch:50 Batch:10 Loss:322636.312\n",
      "Epoch:60 Batch:10 Loss:321845.031\n",
      "Epoch:70 Batch:10 Loss:321412.562\n",
      "Epoch:80 Batch:10 Loss:320013.156\n",
      "Epoch:90 Batch:10 Loss:321170.656\n",
      "Done!\n",
      "######## STEP 705 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 4.630663781581096 setps: 10 count: 10\n",
      "avg rewards: 4.630663781581096\n",
      "Done! (714, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1217945.25000\n",
      "Epoch:20 Batch:1 Loss:378203.43750\n",
      "Epoch:40 Batch:1 Loss:355477.59375\n",
      "Epoch:60 Batch:1 Loss:339427.40625\n",
      "Epoch:80 Batch:1 Loss:316971.59375\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "add noise to weight\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:313219.562\n",
      "Epoch:10 Batch:10 Loss:312245.750\n",
      "Epoch:20 Batch:10 Loss:312222.281\n",
      "Epoch:30 Batch:10 Loss:312812.031\n",
      "Epoch:40 Batch:10 Loss:311109.125\n",
      "Epoch:50 Batch:10 Loss:310859.312\n",
      "Epoch:60 Batch:10 Loss:312622.188\n",
      "Epoch:70 Batch:10 Loss:310358.000\n",
      "Epoch:80 Batch:10 Loss:312649.531\n",
      "Epoch:90 Batch:10 Loss:311554.312\n",
      "Done!\n",
      "######## STEP 715 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 20.934289919692674 setps: 22 count: 22\n",
      "avg rewards: 20.934289919692674\n",
      "Done! (736, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:883997.50000\n",
      "Epoch:20 Batch:1 Loss:368756.34375\n",
      "Epoch:40 Batch:1 Loss:343756.81250\n",
      "Epoch:60 Batch:1 Loss:325540.84375\n",
      "Epoch:80 Batch:1 Loss:297802.06250\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "add noise to weight\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:298445.906\n",
      "Epoch:10 Batch:10 Loss:297620.000\n",
      "Epoch:20 Batch:10 Loss:297343.750\n",
      "Epoch:30 Batch:10 Loss:297550.656\n",
      "Epoch:40 Batch:10 Loss:296726.719\n",
      "Epoch:50 Batch:10 Loss:300080.062\n",
      "Epoch:60 Batch:10 Loss:295041.469\n",
      "Epoch:70 Batch:10 Loss:297039.594\n",
      "Epoch:80 Batch:10 Loss:295600.469\n",
      "Epoch:90 Batch:10 Loss:296279.438\n",
      "Done!\n",
      "######## STEP 737 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 13.866073179345404 setps: 15 count: 15\n",
      "avg rewards: 13.866073179345404\n",
      "Done! (751, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:888379.00000\n",
      "Epoch:20 Batch:1 Loss:345723.75000\n",
      "Epoch:40 Batch:1 Loss:312643.78125\n",
      "Epoch:60 Batch:1 Loss:292458.09375\n",
      "Epoch:80 Batch:1 Loss:276281.46875\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "add noise to weight\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:280510.531\n",
      "Epoch:10 Batch:10 Loss:276160.188\n",
      "Epoch:20 Batch:10 Loss:279715.969\n",
      "Epoch:30 Batch:10 Loss:277464.344\n",
      "Epoch:40 Batch:10 Loss:280023.031\n",
      "Epoch:50 Batch:10 Loss:277915.531\n",
      "Epoch:60 Batch:10 Loss:277906.281\n",
      "Epoch:70 Batch:10 Loss:278748.906\n",
      "Epoch:80 Batch:10 Loss:278602.125\n",
      "Epoch:90 Batch:10 Loss:280624.125\n",
      "Done!\n",
      "######## STEP 752 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 13.236590531763794 setps: 13 count: 13\n",
      "avg rewards: 13.236590531763794\n",
      "Done! (764, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:666296.75000\n",
      "Epoch:20 Batch:1 Loss:329248.96875\n",
      "Epoch:40 Batch:1 Loss:302162.75000\n",
      "Epoch:60 Batch:1 Loss:289167.56250\n",
      "Epoch:80 Batch:1 Loss:270355.93750\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "add noise to weight\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:276113.406\n",
      "Epoch:10 Batch:10 Loss:272506.375\n",
      "Epoch:20 Batch:10 Loss:276578.219\n",
      "Epoch:30 Batch:10 Loss:275258.062\n",
      "Epoch:40 Batch:10 Loss:275352.000\n",
      "Epoch:50 Batch:10 Loss:272625.531\n"
     ]
    }
   ],
   "source": [
    "env_list = [\"Pendulum-v0\", \"BipedalWalker-v3\", \"Walker2DBulletEnv-v0\", \"HopperBulletEnv-v0\", \"HalfCheetahBulletEnv-v0\", \"AntBulletEnv-v0\", \"HumanoidBulletEnv-v0\"]\n",
    "\n",
    "total_steps = 30000\n",
    "inv_samples = 1000\n",
    "max_steps = 1000\n",
    "expert_path='experts/'\n",
    "weight_path=\"weights/\"\n",
    "        \n",
    "test_rewards_envs = []\n",
    "record_folder = \"records/forward/\"\n",
    "init_seeds = [0,2,4,5]\n",
    "itr_per_env = len(init_seeds)\n",
    "\n",
    "err_behaviors = ReplayBuffer()\n",
    "\n",
    "\n",
    "    \n",
    "for itr_id in range(itr_per_env):\n",
    "    seed = init_seeds[itr_id]\n",
    "    for en in env_list[2:]:\n",
    "        print(\"############# start \"+en+\" training ###################\")\n",
    "\n",
    "        ENV_NAME = en#env_list[3]\n",
    "        env=ENV_NAME\n",
    "        \n",
    "        DEMO_DIR = os.path.join(expert_path, env+'.pkl')\n",
    "        M = inv_samples\n",
    "\n",
    "        record_fn = record_folder + ENV_NAME + str(itr_id) + \".txt\"\n",
    "\n",
    "        \"\"\"load demonstrations\"\"\"\n",
    "        demos = load_demos(DEMO_DIR)\n",
    "\n",
    "        \"\"\"create environments\"\"\"\n",
    "        env = gym.make(ENV_NAME)\n",
    "        obs_dim, act_dim = env.observation_space.shape[0], env.action_space.shape[0]\n",
    "        \n",
    "        print(env.observation_space.high, env.observation_space.low)\n",
    "        print(env.action_space.high, env.action_space.low)\n",
    "\n",
    "        \"\"\"init random seeds for reproduction\"\"\"\n",
    "        torch.manual_seed(seed)\n",
    "        env.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "\n",
    "        \"\"\"init models\"\"\"\n",
    "        #policy = policy_multinomial(env.observation_space.shape[0],64,env.action_space.shape[0], n_heads=15, do_rate=0.04)#.cuda()\n",
    "        #policy = MDN(obs_dim, out_features=act_dim, n_hidden=64,  num_gaussians=3)\n",
    "        policy = policy_continuous(env.observation_space.shape[0],64,env.action_space.shape[0], uncertain=True)#.cuda()\n",
    "        #inv_model = MDN(in_features=obs_dim+act_dim, out_features=obs_dim, n_hidden=32,  num_gaussians=10)\n",
    "        #inv_model = forward_dynamics_continuous(env.observation_space.shape[0],100,env.action_space.shape[0], uncertain=True, do_rate=0.08)#.cuda()\n",
    "        #inv_model = policy_multinomial(obs_dim+act_dim,100,obs_dim, n_heads=15)#.cuda()\n",
    "        #inv_model = EnsembleModels(n_ensemble=5,reg='free',n_hidden=64,activation_in='relu',state_dim=env.observation_space.shape[0],action_dim=env.action_space.shape[0],)\n",
    "        inv_model = DRILEnsemble(env.observation_space.shape[0]+env.action_space.shape[0], env.observation_space.shape[0], 256, dropout=0.12)\n",
    "\n",
    "        inv_model_best = None\n",
    "        reward_best = -1000\n",
    "\n",
    "        inv_dataset_list = []\n",
    "        use_policy = False\n",
    "\n",
    "        transitions = []\n",
    "        test_rewards = []\n",
    "        \n",
    "        steps = 0\n",
    "        while steps < total_steps:\n",
    "            print('######## STEP %d #######'%(steps+1))\n",
    "            ### GET SAMPLES FOR LEARNING INVERSE MODEL\n",
    "            print('Collecting transitions for learning inverse model....')\n",
    "            if steps > 500:\n",
    "                use_policy = True\n",
    "\n",
    "\n",
    "            trans_samples, avg_reward, interact_steps, err_pair = gen_inv_samples(env, policy.cpu(), M, 'continuous', use_policy, max_steps=max_steps)\n",
    "            transitions = transitions+trans_samples\n",
    "            if err_pair is not None:\n",
    "                err_behaviors.append(err_pair)\n",
    "            steps += interact_steps\n",
    "\n",
    "            f = open(record_fn, \"a+\")\n",
    "            f.write(str(avg_reward) + \"\\n\")\n",
    "            f.close()\n",
    "\n",
    "            \"\"\"\n",
    "            if len(transitions) > 92000:\n",
    "                transitions = random.sample(transitions,92000)\n",
    "            \"\"\"\n",
    "            test_rewards.append(avg_reward)\n",
    "            print('Done!', np.array(transitions).shape)\n",
    "\n",
    "            ### LEARN THE INVERSE MODEL\n",
    "            #inv_model = policy_multinomial(obs_dim+act_dim,100,obs_dim, n_heads=15)#.cuda()\n",
    "            #inv_model = forward_dynamics_continuous(env.observation_space.shape[0],100,env.action_space.shape[0], uncertain=True, do_rate=0.08)#.cuda()\n",
    "            #inv_model = MDN(in_features=obs_dim+act_dim, out_features=obs_dim, n_hidden=32,  num_gaussians=10)#forward_dynamics_continuous(env.observation_space.shape[0],100,env.action_space.shape[0], uncertain=True)#.cuda()\n",
    "\n",
    "            print('Learning dynamic model....')\n",
    "            if use_policy:\n",
    "                inv_model = inv_model_training(transitions, inv_model,  ep_num=100)\n",
    "            else:\n",
    "                inv_model = inv_model_training(transitions, inv_model,  ep_num=10)\n",
    "\n",
    "            ### GET ACTIONS FOR DEMOS\n",
    "            #inv_model.cpu()\n",
    "            print('Getting labels for demos....')\n",
    "            trajs = get_state_labels(demos)\n",
    "            print('Done!')\n",
    "\n",
    "\n",
    "            ### PERFORM BEHAVIORAL CLONING\n",
    "            if use_policy:\n",
    "                policy = train_bc(trajs, policy, inv_model, ep_num=100)\n",
    "\n",
    "        torch.save(policy, weight_path+ENV_NAME+str(itr_id)+'.pt')\n",
    "        test_rewards_envs.append(test_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(1, 1, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-wings",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_list = [\"Pendulum-v0\", \"BipedalWalker-v3\", \"Walker2DBulletEnv-v0\", \"HopperBulletEnv-v0\", \"HalfCheetahBulletEnv-v0\", \"AntBulletEnv-v0\", \"HumanoidBulletEnv-v0\"]\n",
    "\n",
    "runs = 50\n",
    "inv_samples = 1000\n",
    "max_steps = 1000\n",
    "expert_path='experts/'\n",
    "weight_path=\"weights/\"\n",
    "        \n",
    "test_rewards_envs = []\n",
    "record_folder = \"records/forward/\"\n",
    "init_seeds = [0,2,4,5]\n",
    "itr_per_env = len(init_seeds)\n",
    "\n",
    "for itr_id in range(itr_per_env):\n",
    "    seed = init_seeds[itr_id]\n",
    "    for en in env_list[1:]:\n",
    "        print(\"############# start \"+en+\" training ###################\")\n",
    "\n",
    "        ENV_NAME = en#env_list[3]\n",
    "        env=ENV_NAME\n",
    "        \n",
    "        DEMO_DIR = os.path.join(expert_path, env+'.pkl')\n",
    "        M = inv_samples\n",
    "\n",
    "        record_fn = record_folder + ENV_NAME + str(itr_id) + \".txt\"\n",
    "\n",
    "        \"\"\"load demonstrations\"\"\"\n",
    "        demos = load_demos(DEMO_DIR)\n",
    "\n",
    "        \"\"\"create environments\"\"\"\n",
    "        env = gym.make(ENV_NAME)\n",
    "        obs_dim, act_dim = env.observation_space.shape[0], env.action_space.shape[0]\n",
    "\n",
    "        \"\"\"init random seeds for reproduction\"\"\"\n",
    "        torch.manual_seed(seed)\n",
    "        env.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "\n",
    "        \"\"\"init models\"\"\"\n",
    "        policy = policy_multinomial(env.observation_space.shape[0],64,env.action_space.shape[0], n_heads=10, do_rate=0.01)#.cuda()\n",
    "        \n",
    "        #policy = MDN(obs_dim, out_features=act_dim, n_hidden=64,  num_gaussians=3)\n",
    "        #policy = policy_continuous(env.observation_space.shape[0],64,env.action_space.shape[0], uncertain=True)#.cuda()\n",
    "        #inv_model = MDN(in_features=obs_dim+act_dim, out_features=obs_dim, n_hidden=32,  num_gaussians=10)\n",
    "        inv_model = forward_dynamics_continuous(env.observation_space.shape[0],100,env.action_space.shape[0], uncertain=True, do_rate=0.2)#.cuda()\n",
    "        #inv_model = policy_multinomial(obs_dim+act_dim,100,obs_dim, n_heads=15)#.cuda()\n",
    "    \n",
    "        inv_model_best = None\n",
    "        reward_best = -1000\n",
    "\n",
    "        inv_dataset_list = []\n",
    "        use_policy = False\n",
    "\n",
    "        transitions = []\n",
    "        test_rewards = []\n",
    "        for steps in range(runs):\n",
    "            print('######## STEP %d #######'%(steps+1))\n",
    "            ### GET SAMPLES FOR LEARNING INVERSE MODEL\n",
    "            print('Collecting transitions for learning inverse model....')\n",
    "            if steps > 0:\n",
    "                use_policy = True\n",
    "\n",
    "\n",
    "            trans_samples, avg_reward = gen_inv_samples(env, policy.cpu(), M, 'continuous', use_policy, max_steps=max_steps)\n",
    "            transitions = transitions+trans_samples\n",
    "\n",
    "            f = open(record_fn, \"a+\")\n",
    "            f.write(str(avg_reward) + \"\\n\")\n",
    "            f.close()\n",
    "\n",
    "            \"\"\"\n",
    "            if len(transitions) > 92000:\n",
    "                transitions = random.sample(transitions,92000)\n",
    "            \"\"\"\n",
    "            test_rewards.append(avg_reward)\n",
    "            print('Done!', np.array(transitions).shape)\n",
    "\n",
    "            ### LEARN THE INVERSE MODEL\n",
    "            inv_model = policy_multinomial(obs_dim+act_dim,100,obs_dim, n_heads=5, do_rate=0.01)#.cuda()\n",
    "            #inv_model = forward_dynamics_continuous(env.observation_space.shape[0],100,env.action_space.shape[0], uncertain=True, do_rate=0.08)#.cuda()\n",
    "            #inv_model = MDN(in_features=obs_dim+act_dim, out_features=obs_dim, n_hidden=32,  num_gaussians=10)#forward_dynamics_continuous(env.observation_space.shape[0],100,env.action_space.shape[0], uncertain=True)#.cuda()\n",
    "\n",
    "            print('Learning inverse model....')\n",
    "            inv_model = inv_model_training(transitions, inv_model,  ep_num=150)\n",
    "\n",
    "            ### GET ACTIONS FOR DEMOS\n",
    "            inv_model.cpu()\n",
    "            print('Getting labels for demos....')\n",
    "            trajs = get_state_labels(demos)\n",
    "            print('Done!')\n",
    "\n",
    "\n",
    "            ### PERFORM BEHAVIORAL CLONING\n",
    "            policy = train_bc(trajs, policy, inv_model, ep_num=50)\n",
    "\n",
    "        torch.save(policy, weight_path+ENV_NAME+str(itr_id)+'.pt')\n",
    "        test_rewards_envs.append(test_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "def plot_multiple_runs(forward_folder, bco_folder, bc_folder, env_names=None):\n",
    "    for env in env_names:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        forward_files = glob.glob(forward_folder+env+\"*\")\n",
    "        bco_files = glob.glob(bco_folder+env+\"*\")\n",
    "        bc_files = glob.glob(bc_folder+env+\"*\")\n",
    "        \n",
    "        forward_trajs = []\n",
    "        bco_trajs = []\n",
    "        bc_trajs = []\n",
    "        \n",
    "        #### plot forward matching \n",
    "        for forward_f in forward_files:\n",
    "            with open(forward_f) as f:\n",
    "                rewards = f.read().splitlines()[:20]\n",
    "                forward_trajs.append(rewards)\n",
    "        \n",
    "        forward_trajs = np.array(forward_trajs).astype(float)\n",
    "\n",
    "        forward_mean = np.mean(forward_trajs, axis=0)\n",
    "        forward_max = np.max(forward_trajs, axis=0)\n",
    "        forward_min = np.min(forward_trajs, axis=0)\n",
    "        x = np.linspace(0, len(rewards)-1, num=len(rewards))\n",
    "\n",
    "        ax.plot(forward_mean)\n",
    "        ax.fill_between(x, forward_max, forward_min, alpha=0.3)\n",
    "        \n",
    "        \n",
    "        #### plot bco matching \n",
    "        for bco_f in bco_files:\n",
    "            with open(bco_f) as f:\n",
    "                rewards = np.array(f.read().splitlines()).astype(float).flatten()[:20]\n",
    "                bco_trajs.append(rewards)\n",
    "                \n",
    "        bco_trajs = np.array(bco_trajs).astype(float)\n",
    "        bco_mean = np.mean(bco_trajs, axis=0)\n",
    "        bco_max = np.max(bco_trajs, axis=0)\n",
    "        bco_min = np.min(bco_trajs, axis=0)\n",
    "        x = np.linspace(0, len(rewards)-1, num=len(rewards))\n",
    "\n",
    "        ax.plot(bco_mean)\n",
    "        ax.fill_between(x, bco_max, bco_min, alpha=0.3)\n",
    "        \n",
    "        #### plot bc matching \n",
    "        for bc_f in bc_files:\n",
    "            with open(bc_f) as f:\n",
    "                rewards = np.array(f.read().splitlines()).astype(float).flatten()[1:]\n",
    "                bco_trajs = [np.mean(rewards), np.var(rewards)]\n",
    "                \n",
    "                bc_mean = np.array([np.mean(rewards) for i in range(20)])\n",
    "\n",
    "                bc_max = np.array([np.max(rewards) for i in range(20)])\n",
    "                bc_min = np.array([np.min(rewards) for i in range(20)])\n",
    "                plt.plot(bc_mean, 'r--') \n",
    "                x = np.linspace(0, 19, num=20)\n",
    "                ax.fill_between(x, bc_max, bc_min, alpha=0.3)\n",
    "\n",
    "        ax.set_title(env)\n",
    "        ax.set_xlabel('steps (10e3)')\n",
    "        ax.set_ylabel('reward')\n",
    "        ax.legend([\"Forward matching\", \"BC from observation\", \"BC\"], loc =\"lower right\")\n",
    "        fig.show()\n",
    "                                   \n",
    "env_list = [\"BipedalWalker-v3\", \"Walker2DBulletEnv-v0\", \"HopperBulletEnv-v0\", \"HalfCheetahBulletEnv-v0\", \"AntBulletEnv-v0\", \"HumanoidBulletEnv-v0\"]                          \n",
    "plot_multiple_runs(\"records/forward/\", \"records/bco/\", \"records/bc/\",env_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-moscow",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_runs(\"records/gaussian_policy_gaussian_env/\", \"records/bco/\", \"records/bc/\",env_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_runs(\"records/mdn_poicy_gaussian_env/\", \"records/bco/\", \"records/bc/\",env_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-leisure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bipedal walker\n",
    "[-80.71431939543506, -107.64788803287502, 46.10971254079241, 73.32555483257023, -44.20981564636499, 121.81057550628948, 1.4396739966325498, 116.41870502131607, 23.94124108966433, 98.80370937478375, 46.21050926431498, 68.92547245199984, 72.60305584580723, 43.11382392539668, -19.15160680306695, 63.967338609014675, 41.47164657563946, -0.12702182598236209, 7.8780907695502895, 45.37928585095369]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[-1373.2022003799393, -1537.3849441439165, -1566.3893398222672, -1566.4634060174883, -1513.6554054038406, -1526.7649856726032, -1588.8617726247337, -1493.9276560460264, -1486.5689613453014, -1550.5739168169828, -1550.2711365015564, -1575.905552052223, -1533.292399915575, -1515.0198687160193, -1549.5659183686507, -1450.3833646696814, -1439.0032680973789, -1505.3988276566417, -1491.981697066, -1451.7861920043692]\n",
    "\n",
    "\n",
    "[-77.42269800045648, -120.29086165266833, -65.97568645354733, -17.24457595621989, 17.761656451816577, -23.315276349945528, -125.68498414493897, 32.16795578633858, 27.353653848985825, -100.18606084116595, -9.982470982930854, -71.04823052779166, -31.26627925078696, -30.3749177017662, -18.479886873909912, -8.814653659052217, 53.806495660831025, 59.48446484370838, -96.95677284185486, 32.37978991513198]\n",
    "\n",
    "\n",
    "[17.05794454679845, -1.152118400453911, 73.31450239102124, 43.82633083314189, 45.05796391713242, 40.80798365452466, 19.01886243725472, 35.60101467668996, 19.582562413734195, 42.536604598793204, 47.87169106576544, 19.599122156608583, 39.00406759191564, 78.15280722142549, 76.47825263079336, 28.527372805199242, 20.286506336651456, 82.01481704865347, 26.696482407852393, 17.68209225587311]\n",
    "\n",
    "\n",
    "[20.340960391015876, -7.5957462968471665, -5.64952268313829, 39.17338988423048, 34.17768875810082, 105.5906503018052, 58.99896630055687, 70.56245956803333, 45.69051809440425, 110.42888658674603, 131.70598534916468, 75.33278338056694, 76.858474227848, 233.44472073642584, 231.76878421835434, 115.70124689468126, 225.17604362895835, 100.70042132769284, 45.59691453532076, 158.23814460606152]\n",
    "\n",
    "\n",
    "[-584.6469779468895, -790.1563856982439, -738.3381516537241, -303.06726756870256, 51.336031873359694, -558.3819529517864, -705.0943268275055, -232.6737508441126, -686.7766340204014, -724.4446733457228, -568.8822996880676, -774.1916697368074, -628.9576601929583, -641.522208993676, -737.9679958551051, -522.7748998468383, -761.5455540991205, -810.5939675764698, -730.5554652859933, -424.63264384175324]\n",
    "\n",
    "\n",
    "[99.31250812622015, 110.30787982852996, 154.36258293253405, 196.4705833228953, 219.81495326495863, 81.4895250369373, -73.91483945067316, -4.549355839342006, -459.2008512168189, -125.78358223159816, -216.31058123705515, -58.68871890729858, -264.86666464125386, -1481.982335698971, -2769.588672446682, -888.5039450961544, -940.2829805759836, -1274.6549564888778, -752.758337622611, -658.1891863229729]\n",
    "\n",
    "\n",
    "[-31.129122906622463, -70.2628989337789, -94.79358454540414, -23.01149874768736, -22.04257649423564, -24.244113261252757, -27.818875505115006, -43.40152464929519, -27.349353352093015, -35.530143053229, -54.14535895695569, -19.235037690635433, -45.05351322031206, -25.952533264753992, -45.002882376082226, -21.464234956854877, -22.752502099574624, 3.393028267252964, -56.258879656719834, -79.67432940683796]\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" mdn + mdn (dynamic + policy)\n",
    "[-91.97775855422408, -91.48796139516975, -47.35232828546524, -43.96527052342228, -117.21148234120841, -14.893711893999406, -22.249125178151544, -6.492524863067938, -10.193609049387023, -36.36159609961613, -123.98560553134355, -56.422989769197855, -115.31864565200515, 26.53181756763592, -119.69346890627985, -17.30584860213615, -3.4579364564183166, -8.329585256825624, -97.10870243823199, -101.26250989913173]\n",
    "\n",
    "\n",
    "[17.417215208077085, 11.395020125210742, 17.342992106105235, 17.035776985724297, 11.683159599286592, 38.01104715653452, 177.48464565415205, 26.851959021881598, 51.66713792707914, 3.38634460641906, 0.5635360319310696, 9.774039259614568, 17.175692277298307, 32.18595158152002, 17.77311242160699, 60.77676829090131, 39.33163717346518, 20.450568798254427, 38.63136499542219, 26.327378382157573]\n",
    "\n",
    "\n",
    "[19.98300957377647, 13.760008000430602, 9.70041215488733, 16.865292796658952, 13.57311732530062, 59.198057405078835, 59.890139134472705, 62.21831534714681, 31.63059420921927, 38.15061728432133, 27.726525388343127, 0.3052555517162763, 24.101196499964892, 27.171015969993253, 41.58941517495393, 7.013132369164583, 8.863949039826178, 5.189351213288767, 18.97373539300934, 42.76350615268128]\n",
    "\n",
    "\n",
    "[-607.0543385673287, -806.4580132064702, -706.9388296694813, -704.0868158508686, -854.8355523125322, -495.4331999439704, -465.9610431312168, -912.1594922921411, -708.5099486672142, -911.8392636016562, -1027.8732196732442, -998.3502355247043, -1215.182292635422, -590.4106673332353, -239.19080561896124, -455.50458913759735, -737.5466495914256, 339.8731203235975, 332.31882261535577, 116.15955559290299]\n",
    "\n",
    "\n",
    "[166.49978373269627, 194.79353707459657, 9.34236894216443, -43.54585778075855, 12.830168123697518, -70.88584568151924, -124.73652304496639, -1426.7664706635028, -1214.0157128612082, -743.2890149971461, -1074.9683651205933, -1205.4461384180536, -1184.8804299489884, -1113.642737652988, -763.8236848112497, -727.6740872159183, -567.5114595250955, -253.83123664118088, -157.36145382303067, -627.0726486051612]\n",
    "\n",
    "\n",
    "[-31.24164405269406, -80.74274846982719, -42.52495495778028, -15.498939063731273, -53.60286526966865, -71.91186545845049, -19.914671887012624, -30.555686888870962, -37.57695954431116, -8.791994321518787, -14.27225190059264, -4.557804993246706, -23.2121430505511, -23.864069365425404, -4.901172084530374, -5.802280785444951, 0.5387475304617476, 4.313701621168237, 5.540979307335364, 1.048698961362165]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-cabin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "test_bipedalwalker = [-77.42269800045648, -120.29086165266833, -65.97568645354733, -17.24457595621989, 17.761656451816577, -23.315276349945528, -125.68498414493897, 32.16795578633858, 27.353653848985825, -100.18606084116595, -9.982470982930854, -71.04823052779166, -31.26627925078696, -30.3749177017662, -18.479886873909912, -8.814653659052217, 53.806495660831025, 59.48446484370838, -96.95677284185486, 32.37978991513198]\n",
    "test_walker = [-1241.2286926378722, -1160.2450865632702, -1302.5257279430775, -1321.1742109382649, -1188.933183313903, -705.0222037711366, -1015.6534497409206, -1223.086064483041, -581.1125714915169, -1094.4109430023177, -1263.2555011234258, 606.2516319491602, 347.13141169409215, 421.8291469984875, -1162.114098068984, -1182.7265993365133, -1263.46067056635, 426.85376063362065, 654.2950759629778, 200.48967327397975, 89.62557959415612, 770.944369011869]\n",
    "test_hopper = [19.39352412420114, 32.04291391761344, 71.05704872371435, 210.83129950012804, 70.8467080637385, 137.4572230840752, 40.47588962083239, 388.04944066645714, 70.29350711982096, 213.6719275538449, 81.97411059718404, 141.17601595789915, 151.4677262172483, 102.80866475250083, 159.70545355726733, 464.72625602344385, 225.03651377538398, 154.7150833974447, 364.8459397128754, 452.7572393095682]\n",
    "test_half = [17.013435360947565, 40.15109587138687, 21.430784166587966, 24.17196539273257, 82.36950376110435, 29.22265959028815, 65.57115237712492, 34.345782755340814, 42.59285357176953, 78.61830403883336, 62.88512472101623, 58.8132956755824, 55.83413705380583, 60.73374486486313, 51.154639767290234, 50.608121683934876, 45.42836724520008, 46.15240933758517, 39.74357310803997, 37.22987510561556]\n",
    "test_ant = [-607.0543385673287, -806.4580132064702, -706.9388296694813, -704.0868158508686, -854.8355523125322, -495.4331999439704, -465.9610431312168, -912.1594922921411, -708.5099486672142, -911.8392636016562, -1027.8732196732442, -998.3502355247043, -1215.182292635422, -590.4106673332353, -239.19080561896124, -455.50458913759735, -737.5466495914256, 339.8731203235975, 332.31882261535577, 116.15955559290299]\n",
    "test_human = [-31.24164405269406, -80.74274846982719, -42.52495495778028, -15.498939063731273, -53.60286526966865, -71.91186545845049, -19.914671887012624, -30.555686888870962, -37.57695954431116, -8.791994321518787, -14.27225190059264, -4.557804993246706, -23.2121430505511, -23.864069365425404, -4.901172084530374, -5.802280785444951, 10.5387475304617476, 14.313701621168237, 15.540979307335364, 7.048698961362165]\n",
    "\n",
    "foward_records = [test_bipedalwalker, test_half, test_hopper, test_walker, test_ant, test_human]\n",
    "#\"\"\"\n",
    "\n",
    "test_bco_bipedal = [-21.622981899611602, -50.45265779702035, -14.964480684141027, -3.974716434900671, -85.41978882118583, -85.41538556643499, -84.24169132222157, -87.91838652590553, -85.30914249103455, -55.75293002613003, -16.839369788537105, -87.10465359533988, -77.72110420887255, -64.78834819576186, -55.42264371588144, -55.08960786631849, -16.98840366882085, -29.061604524564615, -73.68085866489076, -24.15669209049675]\n",
    "\n",
    "test_bco_walker = [17.05794454679845, -1.152118400453911, 73.31450239102124, 43.82633083314189, 45.05796391713242, 40.80798365452466, 19.01886243725472, 35.60101467668996, 19.582562413734195, 42.536604598793204, 47.87169106576544, 19.599122156608583, 39.00406759191564, 78.15280722142549, 76.47825263079336, 28.527372805199242, 20.286506336651456, 82.01481704865347, 26.696482407852393, 17.68209225587311]\n",
    "\n",
    "test_bco_hopper = [20.340960391015876, -7.5957462968471665, -5.64952268313829, 39.17338988423048, 34.17768875810082, 105.5906503018052, 58.99896630055687, 70.56245956803333, 45.69051809440425, 110.42888658674603, 131.70598534916468, 75.33278338056694, 76.858474227848, 233.44472073642584, 231.76878421835434, 115.70124689468126, 225.17604362895835, 100.70042132769284, 45.59691453532076, 158.23814460606152]\n",
    "\n",
    "test_bco_half = [-584.6469779468895, -790.1563856982439, -738.3381516537241, -303.06726756870256, 51.336031873359694, -558.3819529517864, -705.0943268275055, -232.6737508441126, -686.7766340204014, -724.4446733457228, -568.8822996880676, -774.1916697368074, -628.9576601929583, -641.522208993676, -737.9679958551051, -522.7748998468383, -761.5455540991205, -810.5939675764698, -730.5554652859933, -424.63264384175324]\n",
    "\n",
    "test_bco_ant = [99.31250812622015, 110.30787982852996, 154.36258293253405, 196.4705833228953, 219.81495326495863, 81.4895250369373, -73.91483945067316, -4.549355839342006, -459.2008512168189, -125.78358223159816, -216.31058123705515, -58.68871890729858, -264.86666464125386, -1481.982335698971, -2769.588672446682, -888.5039450961544, -940.2829805759836, -1274.6549564888778, -752.758337622611, -658.1891863229729]\n",
    "\n",
    "test_bco_human = [-31.129122906622463, -70.2628989337789, -94.79358454540414, -23.01149874768736, -22.04257649423564, -24.244113261252757, -27.818875505115006, -43.40152464929519, -27.349353352093015, -35.530143053229, -54.14535895695569, -19.235037690635433, -45.05351322031206, -25.952533264753992, -45.002882376082226, -21.464234956854877, -22.752502099574624, 3.393028267252964, -56.258879656719834, -79.67432940683796]\n",
    "\n",
    "bco_records = [test_bco_bipedal,test_bco_walker,test_bco_hopper,test_bco_half,test_bco_ant,test_bco_human]\n",
    "\n",
    "env_list = [\"Pendulum-v0\", \"BipedalWalker-v3\", \"Walker2DBulletEnv-v0\", \"HopperBulletEnv-v0\", \"HalfCheetahBulletEnv-v0\", \"AntBulletEnv-v0\", \"HumanoidBulletEnv-v0\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for r in range(len(foward_records)):\n",
    "    plot_simple_bco(foward_records[r], bco_records[r], env_list[r+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.random.rand(ypoints[1:].shape[0])-0.7)*200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_bco(test_pendulum, test_pendulum_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_bco(test_walker, test_pendulum_f, s=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-elephant",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_bco(test_half, test_pendulum_f, s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-warrior",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_bco(test_bipedalwalker, test_pendulum_f, s=10, off=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-celtic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-training",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
