{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "medieval-waste",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pybullet in /home/uenian/anaconda3/envs/torch/lib/python3.9/site-packages (3.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pybullet --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caring-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch_optimizer as th_optim\n",
    "import pybullet_envs\n",
    "\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch.distributions import Normal, Independent\n",
    "from torch.nn import Parameter, functional as F\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "import collections\n",
    "from utils.models import *\n",
    "from utils.utils import *\n",
    "from utils.dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "norman-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_simple_bco(records, records2=None, env_name=None, s=100, off=0.8):\n",
    "    ypoints = np.array(records)\n",
    "    plt.plot(ypoints)\n",
    "    if records2 is not None:\n",
    "        ypoints = np.array(records2)\n",
    "        plt.plot(ypoints, linestyle = 'dotted')\n",
    "    else:\n",
    "        ypoints[1:] = ypoints[1:] #- (np.random.rand(ypoints[1:].shape[0])-0.25)*2*s + (np.random.rand(ypoints[1:].shape[0])-off)*4*s\n",
    "    plt.title(env_name)\n",
    "    plt.xlabel('steps (10e3)')\n",
    "    plt.ylabel('reward')\n",
    "    plt.legend([\"Forward matching\", \"BC from observation\"], loc =\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "egyptian-miracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000000, device='cpu'):\n",
    "        self.capacity = capacity\n",
    "        self.device = device\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def append(self, episode_step):\n",
    "        self.buffer.append(episode_step)\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        # Note: replace=False makes random.choice O(n)\n",
    "        indexes = np.random.choice(len(self.buffer), sample_size, replace=True)\n",
    "        samples = [self.buffer[idx] for idx in indexes]\n",
    "        return self._unpack(samples)\n",
    "\n",
    "    def _unpack(self, samples):\n",
    "        states, actions, rewards, dones, next_states = [], [], [], [], []\n",
    "        for episode_step in samples:\n",
    "            states.append(episode_step['s'])\n",
    "            actions.append(episode_step['a'])\n",
    "            next_states.append(episode_step['s_prime'])\n",
    "\n",
    "        states = torch.FloatTensor(np.array(states, copy=False)).to(self.device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states, copy=False)).to(self.device)\n",
    "        actions = torch.LongTensor(np.array(actions, copy=False)).to(self.device)\n",
    "        return states, actions, next_states\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "known-albania",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnll = (\\n            mbrl.util.math.gaussian_nll(pred_mean, pred_logvar, target, reduce=False)\\n            .mean((1, 2))  # average over batch and target dimension\\n            .sum()\\n        )  # sum over ensemble dimension\\n        nll += 0.01 * (self.max_logvar.sum() - self.min_logvar.sum())\\n\\ndef gaussian_nll(\\n    pred_mean: torch.Tensor,\\n    pred_logvar: torch.Tensor,\\n    target: torch.Tensor,\\n    reduce: bool = True,\\n) -> torch.Tensor:\\n    \\n    l2 = F.mse_loss(pred_mean, target, reduction=\"none\")\\n    inv_var = (-pred_logvar).exp()\\n    losses = l2 * inv_var + pred_logvar\\n    if reduce:\\n        return losses.sum(dim=1).mean()\\n    return losses\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_noise_to_weights(model):\n",
    "    print('add noise to weight')\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param.add_(torch.randn(param.size()) * 0.1)\n",
    "            \n",
    "class NNet(nn.Module):\n",
    "    def __init__(self, D_in, D_out, \n",
    "                 n_hidden=64,\n",
    "                 do_rate=0.1,\n",
    "                 activation_in='relu'):\n",
    "        super(NNet, self).__init__()\n",
    "        if activation_in == 'relu':\n",
    "            mid_act = torch.nn.ReLU()\n",
    "        elif activation_in == 'tanh':\n",
    "            mid_act = torch.nn.Tanh()\n",
    "        elif activation_in == 'sigmoid':\n",
    "            mid_act = torch.nn.Sigmoid()\n",
    "\n",
    "        \n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(D_in, n_hidden),\n",
    "            mid_act,\n",
    "            torch.nn.Linear(n_hidden, n_hidden),\n",
    "            #torch.nn.Dropout(0.1),\n",
    "            mid_act,\n",
    "\n",
    "            torch.nn.Linear(n_hidden, n_hidden),\n",
    "            #torch.nn.Dropout(0.1),\n",
    "            mid_act,\n",
    "\n",
    "            #torch.nn.utils.spectral_norm(torch.nn.Linear(n_hidden, D_out, bias=True)),\n",
    "            torch.nn.Linear(n_hidden, D_out, bias=True),\n",
    "            #torch.nn.Dropout(0.1),\n",
    "        )\n",
    "\n",
    "    def forward(self, s, a):\n",
    "        x = torch.cat((s,a), dim=1)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "def kl_divergence(z, mu, std):\n",
    "        # https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed\n",
    "        # --------------------------\n",
    "        # Monte carlo KL divergence\n",
    "        # --------------------------\n",
    "        # 1. define the first two probabilities (in this case Normal for both)\n",
    "        p = torch.distributions.Normal(mu, torch.ones_like(std)*0.06)\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "        # 2. get the probabilities from the equation\n",
    "        log_qzx = q.log_prob(z)\n",
    "        log_pz = p.log_prob(z)\n",
    "\n",
    "        # kl\n",
    "        kl = (log_qzx - log_pz)\n",
    "        kl = kl.sum(-1)\n",
    "        return kl\n",
    "    \n",
    "from torch.distributions import Independent, Normal\n",
    "\n",
    "class DRILEnsemble(nn.Module):\n",
    "      # https://github.com/Kaixhin/imitation-learning/blob/795e8b216dde1a4995a093d490b03e6e0119a313/models.py#L49\n",
    "    def __init__(self, state_size, action_size, hidden_size, activation_function='tanh', log_std_dev_init=-5., dropout=0):\n",
    "        super().__init__()\n",
    "        #self.actors = self._create_fcnn(state_size, hidden_size, output_size=action_size, activation_function=activation_function, dropout=dropout, final_gain=0.01)\n",
    "        n_ensemble = 1\n",
    "        self.actors = []\n",
    "        self.log_std_devs = []\n",
    "        for m in range(n_ensemble):\n",
    "            self.actors.append(self._create_fcnn(state_size, \n",
    "                                                 hidden_size, \n",
    "                                                 output_size=action_size, \n",
    "                                                 activation_function=activation_function, \n",
    "                                                 dropout=dropout, \n",
    "                                                 final_gain=0.01))\n",
    "            self.log_std_devs.append(Parameter(torch.full((action_size, ), log_std_dev_init, dtype=torch.float32)))\n",
    "\n",
    "    def forward(self, state, en_id):\n",
    "        mean = self.actors[en_id](state)\n",
    "        policy = Independent(Normal(mean, self.log_std_devs[en_id].exp()), 1)\n",
    "        return policy\n",
    "    \n",
    "    def _create_fcnn(self, input_size, hidden_size, output_size, activation_function, dropout=0, final_gain=1.0):\n",
    "        #assert activation_function in ACTIVATION_FUNCTIONS.keys()\n",
    "        ACTIVATION_FUNCTIONS = {'relu': nn.ReLU, 'sigmoid': nn.Sigmoid, 'tanh': nn.Tanh}\n",
    "        assert activation_function in ACTIVATION_FUNCTIONS.keys()\n",
    "        network_dims, layers = (input_size, hidden_size, hidden_size), []\n",
    "\n",
    "        for l in range(len(network_dims) - 1):\n",
    "            layer = nn.Linear(network_dims[l], network_dims[l + 1])\n",
    "            nn.init.orthogonal_(layer.weight, gain=nn.init.calculate_gain(activation_function))\n",
    "            nn.init.constant_(layer.bias, 0)\n",
    "            layers.append(layer)\n",
    "            if dropout > 0: layers.append(nn.Dropout(p=dropout))\n",
    "            layers.append(ACTIVATION_FUNCTIONS[activation_function]())\n",
    "\n",
    "        final_layer = nn.Linear(network_dims[-1], output_size)\n",
    "        nn.init.orthogonal_(final_layer.weight, gain=final_gain)\n",
    "        nn.init.constant_(final_layer.bias, 0)\n",
    "        layers.append(final_layer)\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "      # Calculates the log probability of an action a with the policy π(·|s) given state s\n",
    "    def log_prob(self, state, action, en_id):\n",
    "        return self.forward(state, en_id).log_prob(action)\n",
    "\n",
    "    def _get_action_uncertainty(self, state, action):\n",
    "        ensemble_policies = []\n",
    "        for eid in range(len(self.actors)):  # Perform Monte-Carlo dropout for an implicit ensemble\n",
    "            ensemble_policies.append(self.log_prob(state, action, eid).exp())\n",
    "        return torch.stack(ensemble_policies).var(dim=0)\n",
    "\n",
    "      # Set uncertainty threshold at the 98th quantile of uncertainty costs calculated over the expert data\n",
    "    def set_uncertainty_threshold(self, expert_state, expert_action):\n",
    "        self.q = torch.quantile(self._get_action_uncertainty(expert_state, expert_action), 0.9).item()\n",
    "\n",
    "    def predict_reward(self, state, action):\n",
    "        # Calculate (raw) uncertainty cost\n",
    "        uncertainty_cost = self._get_action_uncertainty(state, action)\n",
    "        # Calculate clipped uncertainty cost\n",
    "        neg_idxs = uncertainty_cost.less_equal(self.q)\n",
    "        uncertainty_cost[neg_idxs] = -1\n",
    "        uncertainty_cost[~neg_idxs] = 1\n",
    "        return -uncertainty_cost\n",
    "\n",
    "    \n",
    "# Performs a behavioural cloning update\n",
    "def supervised_NLL_update(agent, expert_dataloader, agent_optimiser, batch_size):\n",
    "    #expert_dataloader = DataLoader(expert_trajectories, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=4)\n",
    "    for expert_transition in expert_dataloader:\n",
    "        expert_state, expert_action = expert_transition['states'], expert_transition['actions']\n",
    "        agent_optimiser.zero_grad(set_to_none=True)\n",
    "        behavioural_cloning_loss = -agent.log_prob(expert_state, expert_action).mean()  # Maximum likelihood objective\n",
    "        behavioural_cloning_loss.backward()\n",
    "        agent_optimiser.step()\n",
    "    return behavioural_cloning_loss\n",
    "\n",
    "\"\"\"\n",
    "nll = (\n",
    "            mbrl.util.math.gaussian_nll(pred_mean, pred_logvar, target, reduce=False)\n",
    "            .mean((1, 2))  # average over batch and target dimension\n",
    "            .sum()\n",
    "        )  # sum over ensemble dimension\n",
    "        nll += 0.01 * (self.max_logvar.sum() - self.min_logvar.sum())\n",
    "\n",
    "def gaussian_nll(\n",
    "    pred_mean: torch.Tensor,\n",
    "    pred_logvar: torch.Tensor,\n",
    "    target: torch.Tensor,\n",
    "    reduce: bool = True,\n",
    ") -> torch.Tensor:\n",
    "    \n",
    "    l2 = F.mse_loss(pred_mean, target, reduction=\"none\")\n",
    "    inv_var = (-pred_logvar).exp()\n",
    "    losses = l2 * inv_var + pred_logvar\n",
    "    if reduce:\n",
    "        return losses.sum(dim=1).mean()\n",
    "    return losses\n",
    "\"\"\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "overall-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_model_training(transitions, inv_model, ep_num=100):\n",
    "    inv_dataset = transition_dataset(transitions)\n",
    "    #inv_dataset_list.append(inv_dataset)\n",
    "    #inv_dataset_final = ConcatDataset(inv_dataset_list)\n",
    "    inv_loader = DataLoader(inv_dataset, batch_size=1024, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "    #print('-- training: ' + str(m+1) + ' of ' + str(inv_model.n_ensemble) + ' NNs --')\n",
    "    print('dynamic model training...')\n",
    "    \n",
    "    for en_id in range(len(inv_model.actors)):\n",
    "        add_noise_to_weights(inv_model.actors[en_id])\n",
    "        #inv_opt = optim.Adam(inv_model.parameters(), lr=1e-3, weight_decay=0.0001)\n",
    "        #\"\"\"\n",
    "        inv_opt_yogi = th_optim.Yogi(\n",
    "            inv_model.actors[en_id].parameters(),\n",
    "            lr= 1e-2,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-3,\n",
    "            initial_accumulator=1e-6,\n",
    "            weight_decay=0,\n",
    "        )\n",
    "\n",
    "        inv_opt = th_optim.Lookahead(inv_opt_yogi,  alpha=0.5)#k=5\n",
    "        #\"\"\"\n",
    "\n",
    "        for epoch in range(ep_num): \n",
    "            running_loss = 0\n",
    "            for i, data in enumerate(inv_loader):\n",
    "                s, a, s_prime = data\n",
    "                \n",
    "                inv_opt.zero_grad()\n",
    "\n",
    "                inv_input = torch.cat((s,a), dim=1).float()\n",
    "\n",
    "                #print(inv_input[0])\n",
    "                #print(inv_model._get_action_uncertainty(inv_input, s_prime))\n",
    "                #print(inv_input[0].shape)\n",
    "                #print(inv_model._get_action_uncertainty(inv_input, s_prime).shape)\n",
    "                loss = -inv_model.log_prob(inv_input, s_prime, en_id).mean()  # Maximum likelihood objective\n",
    "                #loss = -custom_loglikehood(inv_model.actors[0](inv_input), inv_model.log_std_devs[0].exp(), s_prime)\n",
    "\n",
    "                loss.backward()\n",
    "                running_loss += loss.item()\n",
    "                if i%100 == 99:\n",
    "                    running_loss = 0\n",
    "                inv_opt.step()\n",
    "            if epoch%20==0:\n",
    "                print('Epoch:%d Batch:%d Loss:%.5f'%(epoch, i+1, loss))\n",
    "    print('Done!')\n",
    "    \n",
    "    inv_model.set_uncertainty_threshold(inv_input, s_prime)#(inv_dataset.x, inv_dataset.y)\n",
    "    \n",
    "    print('threshold:', inv_model.q)\n",
    "    return inv_model\n",
    "\n",
    "def train_bc(trajs, policy, dynamics,  ep_num=50, sample_itr=500, batch_size=1024):\n",
    "    \n",
    "    bc_dataset = imitation_dataset(trajs)\n",
    "    bc_loader = DataLoader(bc_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    add_noise_to_weights(policy)\n",
    "    \n",
    "    print('Learning policy....')\n",
    "    #bc_opt = optim.Adam(policy.parameters(), lr=1e-3, weight_decay=0.0001)\n",
    "    #\"\"\"\n",
    "    bc_opt_yogi = th_optim.Yogi(\n",
    "        policy.parameters(),\n",
    "        lr= 1e-2,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-3,\n",
    "        initial_accumulator=1e-6,\n",
    "        weight_decay=0,\n",
    "    )\n",
    "\n",
    "    bc_opt = th_optim.Lookahead(bc_opt_yogi,  alpha=0.5)#k=5,\n",
    "    #\"\"\"\n",
    "    bc_loss = nn.MSELoss()\n",
    "    # bc_loss = nn.L1Loss()\n",
    "    \n",
    "    err_sample_size = 256\n",
    "    \n",
    "    #if err_behaviors.__len__() < err_sample_size:\n",
    "    #    err_sample_size = err_behaviors.__len__()\n",
    "    #if err_behaviors.__len__() > 0:\n",
    "    #    err_s, err_a, err_sprime = err_behaviors.sample(err_sample_size)\n",
    "    \n",
    "    for epoch in range(ep_num):  \n",
    "        running_loss = 0\n",
    "        for i, data in enumerate(bc_loader):\n",
    "            s, s_prime = data\n",
    "            bc_opt.zero_grad()\n",
    "            #print('1', s.shape, s_prime.shape)\n",
    "            \n",
    "            ex_dim=5\n",
    "            s = s.expand(ex_dim,s.shape[0],s.shape[1]).reshape(ex_dim*s.shape[0],s.shape[1]) #s.repeat(30,1)\n",
    "            s_prime = s_prime.expand(ex_dim,s_prime.shape[0],s_prime.shape[1]).reshape(ex_dim*s_prime.shape[0],s_prime.shape[1]) #s_prime.repeat(30,1)\n",
    "            \n",
    "            #print(s.shape, s_prime.shape)\n",
    "            #\"\"\"\n",
    "            #a_pred = policy.reparam_forward(s.float())\n",
    "            try:\n",
    "                a_mu, a_sigma = policy(s.float())\n",
    "                #a_pred = Normal(loc=a_mu, scale=a_sigma+1e-6).rsample(sample_shape=[10])\n",
    "                #a_pred = a_pred.reshape(10*a_mu.shape[0], a_mu.shape[1])\n",
    "                a_pred = Normal(loc=a_mu, scale=a_sigma+0.03).rsample()\n",
    "                a_lar_pred = Normal(loc=a_mu, scale=a_sigma+0.3).rsample()\n",
    "                #print(a_pred.shape)\n",
    "                \n",
    "                #err_a_mu, err_a_sigma = policy(err_s.float())\n",
    "            except:\n",
    "                a_pred = policy.reparam_forward(s.float())\n",
    "            #\"\"\"\n",
    "            #print(torch.cat((s, a_pred), dim=1).shape)\n",
    "            \n",
    "            err_loss_func = nn.GaussianNLLLoss()\n",
    "            inv_input = torch.cat((s,a_pred), dim=1).float()\n",
    "            #loss = -inv_model.log_prob(inv_input, s_prime, en_id=0).mean()  # Maximum likelihood objective\n",
    "            loss = 0\n",
    "            err_loss = 0\n",
    "            \n",
    "            for m in range(len(dynamics.actors)):\n",
    "                #print(policy.log_prob(s.float(), a_pred).shape)\n",
    "                #print(inv_model.log_prob(inv_input, s_prime, en_id=m).shape)\n",
    "                #loss += torch.dot(policy.log_prob(s.float(), a_pred), inv_model.log_prob(inv_input, s_prime, en_id=m)).mean()\n",
    "                #print(policy.log_prob(s.float(), a_pred)[:20])\n",
    "                #print(inv_model.log_prob(inv_input, s_prime, en_id=m)[:20])\n",
    "                \n",
    "                loss += -inv_model.log_prob(inv_input, s_prime, en_id=m).mean()# + 1e-7\n",
    "                loss += -policy.log_prob(s.float(), a_pred).mean() - policy.log_prob(s.float(), a_lar_pred).mean()* 0.05\n",
    "                #if err_behaviors.__len__()!=0:\n",
    "                #    err_loss -= err_loss_func(err_a_mu, err_a, err_a_sigma, eps=1e-6) \n",
    "            #loss_uc = inv_model._get_action_uncertainty(inv_input, s_prime).mean()\n",
    "            #print(loss_uc, inv_model.q)\n",
    "            #loss_kl = kl_divergence(a_pred, a_mu, a_sigma).mean()\n",
    "            #loss += loss_kl\n",
    "            \n",
    "            #loss += err_loss\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            if i%20 == 19:\n",
    "                running_loss = 0\n",
    "            bc_opt.step()\n",
    "        if epoch%10==0:\n",
    "            print('Epoch:%d Batch:%d Loss:%.3f'%(epoch, i+1, loss))\n",
    "\n",
    "    print('Done!')\n",
    "    return policy\n",
    "\n",
    "\n",
    "def load_demos(DEMO_DIR):\n",
    "    \"\"\"load demonstrations\"\"\"\n",
    "    try:\n",
    "        trajstrajs = np.load(\"experts/states_expert_walker_.npy\")[:10]\n",
    "    except:\n",
    "        with open(DEMO_DIR, 'rb') as f:\n",
    "            trajs = pickle.load(f)\n",
    "    demos = []\n",
    "    for t_id, traj in enumerate(trajs):\n",
    "        demo =[]\n",
    "        #print(t_id)\n",
    "        for item in traj:    \n",
    "            obs = item['observation']\n",
    "            #obs = list(obs)\n",
    "            #print(obs)\n",
    "            demo.append(obs)\n",
    "        #print(np.array(demo).shape)\n",
    "        demos.append(np.array(demo))\n",
    "\n",
    "    print(np.array(demos).shape)\n",
    "    demos = demos[:10]\n",
    "    return demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "right-sacramento",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_inv_samples(env, policy, num_samples, env_type, use_policy, max_steps, use_vecnorm):\n",
    "    count = 0\n",
    "    transitions = []\n",
    "    s = env.reset()\n",
    "    t = 0\n",
    "    r = 0\n",
    "    rs = []\n",
    "    err_pair = None\n",
    "    \n",
    "    while count < num_samples:\n",
    "        \n",
    "        if env_type == 'continuous':\n",
    "            if use_policy:\n",
    "                try:\n",
    "                    mean, sigma = policy(torch.tensor([s]).float())\n",
    "                    #print(mean, sigma)\n",
    "                    pi = Normal(loc=mean, scale=sigma+1e-7)\n",
    "                    a = pi.sample().detach().numpy()[0]\n",
    "                    #print(a)\n",
    "                    #a = select_action_continuous(s, policy)\n",
    "                except:\n",
    "                    print(mean, sigma)\n",
    "                    a = policy.reparam_forward(torch.tensor([s]).float(), tau=10e-2).detach().numpy()[0]\n",
    "                    #pi, sigma, mu = policy(torch.tensor([s]).float())\n",
    "                    #a = policy.mdn_sample(pi, sigma, mu).detach().numpy()[0]\n",
    "            else:\n",
    "                a = env.action_space.sample()\n",
    "        else:\n",
    "            a = select_action_discrete(s, policy)\n",
    "            \n",
    "        a = np.clip(a, -1, 1)\n",
    "        \n",
    "        if len(a.shape) > 1:\n",
    "            act = a[0]\n",
    "        else:\n",
    "            act = a\n",
    "        \n",
    "        if use_vecnorm and len(a.shape) > 1:\n",
    "            s_prime, reward, done, _ = env.step(a)\n",
    "        elif use_vecnorm and len(a.shape) == 1:\n",
    "            s_prime, reward, done, _ = env.step([a])\n",
    "        else:\n",
    "            s_prime, reward, done, _ = env.step(a)\n",
    "            \n",
    "    \n",
    "        if len(s_prime.shape)>1:\n",
    "            s_prime = s_prime[0]\n",
    "            \n",
    "        if len(s.shape)>1:\n",
    "            s = s[0]\n",
    "            \n",
    "        \n",
    "        transitions.append([s, act, s_prime])\n",
    "        count += 1\n",
    "        t += 1\n",
    "        r += reward\n",
    "        #print(t)\n",
    "        if done == True or t>(max_steps-1) or count == (num_samples-1):\n",
    "            if done==True and (t<(max_steps-1) or count != (num_samples-1)):\n",
    "                err_pair={\"s\":s, \"a\":a, \"s_prime\":s_prime}\n",
    "            rs.append(r)\n",
    "            print(\"reward:\", r, \"setps:\", t, \"count:\", count)\n",
    "            s = env.reset()\n",
    "            t = 0\n",
    "            r = 0\n",
    "            break\n",
    "        else:\n",
    "            s = s_prime\n",
    "    print(\"avg rewards:\",np.mean(np.array(rs)))\n",
    "    return transitions, np.mean(np.array(rs)), count, err_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bc58b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vecnorm_env(env_name, stats_path=None):\n",
    "    use_vecnorm = False\n",
    "    try:\n",
    "        # Load the saved statistics\n",
    "        env = gym.make(env_name)\n",
    "        env = Monitor(env)\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        env = VecNormalize.load(stats_path, env)\n",
    "        use_vecnorm = True\n",
    "    except:\n",
    "        env = gym.make(env_name)\n",
    "        \n",
    "    return env, use_vecnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7b8622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# start Walker2DBulletEnv-v0 training ###################\n",
      "(50, 1000, 1, 22)\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf] [-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf]\n",
      "[1. 1. 1. 1. 1. 1.] [-1. -1. -1. -1. -1. -1.]\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [13.612367] setps: 9 count: 9\n",
      "avg rewards: 13.612367\n",
      "Done! (9, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2633179/2778355092.py:101: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  print('Done!', np.array(transitions).shape)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:1 Loss:2474913.50000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [16.545662] setps: 12 count: 12\n",
      "avg rewards: 16.545662\n",
      "Done! (21, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1110759.25000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 22 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [18.100552] setps: 17 count: 17\n",
      "avg rewards: 18.100552\n",
      "Done! (38, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1028696.00000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 39 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [14.72739] setps: 11 count: 11\n",
      "avg rewards: 14.72739\n",
      "Done! (49, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1062315.00000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 50 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [13.768438] setps: 10 count: 10\n",
      "avg rewards: 13.768438\n",
      "Done! (59, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1108132.75000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 60 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [19.642797] setps: 17 count: 17\n",
      "avg rewards: 19.642797\n",
      "Done! (76, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1273202.75000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 77 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [17.5229] setps: 16 count: 16\n",
      "avg rewards: 17.5229\n",
      "Done! (92, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1267389.50000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 93 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [16.147028] setps: 15 count: 15\n",
      "avg rewards: 16.147028\n",
      "Done! (107, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1593978.87500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 108 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [14.429008] setps: 11 count: 11\n",
      "avg rewards: 14.429008\n",
      "Done! (118, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1292959.62500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 119 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [13.281215] setps: 11 count: 11\n",
      "avg rewards: 13.281215\n",
      "Done! (129, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1633932.50000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 130 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [20.578737] setps: 18 count: 18\n",
      "avg rewards: 20.578737\n",
      "Done! (147, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1562067.75000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 148 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [13.650224] setps: 9 count: 9\n",
      "avg rewards: 13.650224\n",
      "Done! (156, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1537645.62500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 157 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [21.982] setps: 27 count: 27\n",
      "avg rewards: 21.982\n",
      "Done! (183, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1759843.50000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 184 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [13.489965] setps: 9 count: 9\n",
      "avg rewards: 13.489965\n",
      "Done! (192, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1674295.00000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 193 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [23.948784] setps: 20 count: 20\n",
      "avg rewards: 23.948784\n",
      "Done! (212, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1880153.37500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 213 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [25.958609] setps: 29 count: 29\n",
      "avg rewards: 25.958609\n",
      "Done! (241, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1604842.37500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 242 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [18.842575] setps: 14 count: 14\n",
      "avg rewards: 18.842575\n",
      "Done! (255, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1655844.87500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 256 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [18.079544] setps: 16 count: 16\n",
      "avg rewards: 18.079544\n",
      "Done! (271, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1691837.75000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 272 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [14.68013] setps: 10 count: 10\n",
      "avg rewards: 14.68013\n",
      "Done! (281, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1778115.50000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 282 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [15.055781] setps: 10 count: 10\n",
      "avg rewards: 15.055781\n",
      "Done! (291, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1827642.25000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 292 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [18.673796] setps: 16 count: 16\n",
      "avg rewards: 18.673796\n",
      "Done! (307, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:2008383.00000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 308 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [16.134092] setps: 12 count: 12\n",
      "avg rewards: 16.134092\n",
      "Done! (319, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:2070941.87500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 320 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [20.234457] setps: 20 count: 20\n",
      "avg rewards: 20.234457\n",
      "Done! (339, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1787389.00000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 340 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [13.961297] setps: 10 count: 10\n",
      "avg rewards: 13.961297\n",
      "Done! (349, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:2161203.00000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 350 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [16.044485] setps: 13 count: 13\n",
      "avg rewards: 16.044485\n",
      "Done! (362, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:2104336.25000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 363 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [17.267416] setps: 13 count: 13\n",
      "avg rewards: 17.267416\n",
      "Done! (375, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1966274.37500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 376 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [24.181074] setps: 27 count: 27\n",
      "avg rewards: 24.181074\n",
      "Done! (402, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:1 Loss:2157127.25000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 403 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [16.716988] setps: 12 count: 12\n",
      "avg rewards: 16.716988\n",
      "Done! (414, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:2214927.25000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 415 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [18.100534] setps: 13 count: 13\n",
      "avg rewards: 18.100534\n",
      "Done! (427, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:2194881.00000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 428 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [12.761863] setps: 10 count: 10\n",
      "avg rewards: 12.761863\n",
      "Done! (437, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:2123140.50000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 438 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [15.153992] setps: 10 count: 10\n",
      "avg rewards: 15.153992\n",
      "Done! (447, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:2324837.75000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 448 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [17.300417] setps: 18 count: 18\n",
      "avg rewards: 17.300417\n",
      "Done! (465, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:2271558.00000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 466 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [15.897924] setps: 12 count: 12\n",
      "avg rewards: 15.897924\n",
      "Done! (477, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:2180970.25000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 478 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [22.22137] setps: 21 count: 21\n",
      "avg rewards: 22.22137\n",
      "Done! (498, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:2804142.25000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 499 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [29.838406] setps: 28 count: 28\n",
      "avg rewards: 29.838406\n",
      "Done! (526, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:2518319.00000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "######## STEP 527 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [16.3344] setps: 19 count: 19\n",
      "avg rewards: 16.3344\n",
      "Done! (545, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:2411745.25000\n",
      "Epoch:20 Batch:1 Loss:1796913.00000\n",
      "Epoch:40 Batch:1 Loss:1644584.12500\n",
      "Epoch:60 Batch:1 Loss:1612743.37500\n",
      "Epoch:80 Batch:1 Loss:1472675.87500\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "add noise to weight\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:5803807.500\n",
      "Epoch:10 Batch:10 Loss:4931838.000\n",
      "Epoch:20 Batch:10 Loss:4911598.500\n",
      "Epoch:30 Batch:10 Loss:4872585.000\n",
      "Epoch:40 Batch:10 Loss:4774764.500\n",
      "Epoch:50 Batch:10 Loss:4772830.000\n",
      "Epoch:60 Batch:10 Loss:4817082.000\n",
      "Epoch:70 Batch:10 Loss:4795684.500\n",
      "Epoch:80 Batch:10 Loss:4833648.000\n",
      "Epoch:90 Batch:10 Loss:4819744.000\n",
      "Done!\n",
      "######## STEP 546 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [7.0747514] setps: 11 count: 11\n",
      "avg rewards: 7.0747514\n",
      "Done! (556, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1981847.50000\n",
      "Epoch:20 Batch:1 Loss:1461862.87500\n",
      "Epoch:40 Batch:1 Loss:1340266.12500\n",
      "Epoch:60 Batch:1 Loss:1251720.00000\n",
      "Epoch:80 Batch:1 Loss:1177570.25000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "add noise to weight\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:4275302.500\n",
      "Epoch:10 Batch:10 Loss:3985258.750\n",
      "Epoch:20 Batch:10 Loss:3973938.750\n",
      "Epoch:30 Batch:10 Loss:3984539.250\n",
      "Epoch:40 Batch:10 Loss:3971391.500\n",
      "Epoch:50 Batch:10 Loss:3909183.000\n",
      "Epoch:60 Batch:10 Loss:4002204.750\n",
      "Epoch:70 Batch:10 Loss:3898319.750\n",
      "Epoch:80 Batch:10 Loss:3912815.500\n",
      "Epoch:90 Batch:10 Loss:3884757.500\n",
      "Done!\n",
      "######## STEP 557 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [11.017547] setps: 10 count: 10\n",
      "avg rewards: 11.017547\n",
      "Done! (566, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1809869.62500\n",
      "Epoch:20 Batch:1 Loss:1168922.50000\n",
      "Epoch:40 Batch:1 Loss:1100566.12500\n",
      "Epoch:60 Batch:1 Loss:1029610.62500\n",
      "Epoch:80 Batch:1 Loss:963954.06250\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "add noise to weight\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:3607300.000\n",
      "Epoch:10 Batch:10 Loss:3473129.250\n",
      "Epoch:20 Batch:10 Loss:3345896.750\n",
      "Epoch:30 Batch:10 Loss:3331944.000\n",
      "Epoch:40 Batch:10 Loss:3272613.750\n",
      "Epoch:50 Batch:10 Loss:3307003.250\n",
      "Epoch:60 Batch:10 Loss:3275732.750\n",
      "Epoch:70 Batch:10 Loss:3230635.750\n",
      "Epoch:80 Batch:10 Loss:3275576.000\n",
      "Epoch:90 Batch:10 Loss:3239984.750\n",
      "Done!\n",
      "######## STEP 567 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [7.4375973] setps: 13 count: 13\n",
      "avg rewards: 7.4375973\n",
      "Done! (579, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1730475.37500\n",
      "Epoch:20 Batch:1 Loss:964999.93750\n",
      "Epoch:40 Batch:1 Loss:883077.75000\n",
      "Epoch:60 Batch:1 Loss:860861.12500\n",
      "Epoch:80 Batch:1 Loss:811131.00000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "add noise to weight\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:3317991.250\n",
      "Epoch:10 Batch:10 Loss:2966201.000\n",
      "Epoch:20 Batch:10 Loss:2957986.750\n",
      "Epoch:30 Batch:10 Loss:2855511.000\n",
      "Epoch:40 Batch:10 Loss:2899455.750\n",
      "Epoch:50 Batch:10 Loss:2981962.500\n",
      "Epoch:60 Batch:10 Loss:2861318.250\n",
      "Epoch:70 Batch:10 Loss:2857428.250\n",
      "Epoch:80 Batch:10 Loss:2825287.000\n",
      "Epoch:90 Batch:10 Loss:2908984.750\n",
      "Done!\n",
      "######## STEP 580 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [9.010835] setps: 11 count: 11\n",
      "avg rewards: 9.010835\n",
      "Done! (590, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1303814.37500\n",
      "Epoch:20 Batch:1 Loss:825698.18750\n",
      "Epoch:40 Batch:1 Loss:771348.81250\n",
      "Epoch:60 Batch:1 Loss:704479.37500\n",
      "Epoch:80 Batch:1 Loss:656026.25000\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "add noise to weight\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:2498540.750\n",
      "Epoch:10 Batch:10 Loss:2280133.250\n",
      "Epoch:20 Batch:10 Loss:2249626.500\n",
      "Epoch:30 Batch:10 Loss:2269121.500\n",
      "Epoch:40 Batch:10 Loss:2276118.750\n",
      "Epoch:50 Batch:10 Loss:2233835.500\n",
      "Epoch:60 Batch:10 Loss:2268904.250\n",
      "Epoch:70 Batch:10 Loss:2262315.750\n",
      "Epoch:80 Batch:10 Loss:2256917.000\n",
      "Epoch:90 Batch:10 Loss:2210010.000\n",
      "Done!\n",
      "######## STEP 591 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [9.035225] setps: 15 count: 15\n",
      "avg rewards: 9.035225\n",
      "Done! (605, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1144199.87500\n",
      "Epoch:20 Batch:1 Loss:689848.25000\n",
      "Epoch:40 Batch:1 Loss:643646.93750\n",
      "Epoch:60 Batch:1 Loss:609697.87500\n",
      "Epoch:80 Batch:1 Loss:578992.18750\n",
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "add noise to weight\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:2096102.875\n",
      "Epoch:10 Batch:10 Loss:1967785.250\n",
      "Epoch:20 Batch:10 Loss:1964295.125\n",
      "Epoch:30 Batch:10 Loss:1965477.375\n",
      "Epoch:40 Batch:10 Loss:1942334.500\n",
      "Epoch:50 Batch:10 Loss:1954038.125\n",
      "Epoch:60 Batch:10 Loss:1961418.750\n",
      "Epoch:70 Batch:10 Loss:1935831.375\n",
      "Epoch:80 Batch:10 Loss:1917136.375\n",
      "Epoch:90 Batch:10 Loss:1911515.500\n",
      "Done!\n",
      "######## STEP 606 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: [10.821704] setps: 11 count: 11\n",
      "avg rewards: 10.821704\n",
      "Done! (616, 3)\n",
      "Learning dynamic model....\n",
      "dynamic model training...\n",
      "add noise to weight\n",
      "Epoch:0 Batch:1 Loss:1089872.25000\n",
      "Epoch:20 Batch:1 Loss:611784.12500\n",
      "Epoch:40 Batch:1 Loss:573250.81250\n",
      "Epoch:60 Batch:1 Loss:530050.43750\n",
      "Epoch:80 Batch:1 Loss:503052.40625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "threshold: nan\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "add noise to weight\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:1875835.875\n",
      "Epoch:10 Batch:10 Loss:1788470.125\n",
      "Epoch:20 Batch:10 Loss:1724928.750\n",
      "Epoch:30 Batch:10 Loss:1726935.250\n",
      "Epoch:40 Batch:10 Loss:1721138.125\n",
      "Epoch:50 Batch:10 Loss:1761345.750\n",
      "Epoch:60 Batch:10 Loss:1707267.125\n",
      "Epoch:70 Batch:10 Loss:1654303.000\n",
      "Epoch:80 Batch:10 Loss:1689991.125\n"
     ]
    }
   ],
   "source": [
    "ENV_LIST = [\"Pendulum-v0\", \"BipedalWalker-v3\", \"Walker2DBulletEnv-v0\", \"HopperBulletEnv-v0\", \"HalfCheetahBulletEnv-v0\", \"AntBulletEnv-v0\", \"HumanoidBulletEnv-v0\"]\n",
    "\n",
    "EXP_PTH = \"experts/\"\n",
    "DEMO_PTH = \"experts/expert_demo/\"\n",
    "VECNORM_PTH = \"experts/expert_env/vec_normalize_\"\n",
    "WEIGHT_PTH=\"weights/\"\n",
    "\n",
    "VECTER_ENV=True\n",
    "\n",
    "total_steps = 30000\n",
    "inv_samples = 1000\n",
    "max_steps = 1000\n",
    "        \n",
    "test_rewards_envs = []\n",
    "record_folder = \"records/forward/\"\n",
    "init_seeds = [0,2,4,5]\n",
    "itr_per_env = len(init_seeds)\n",
    "\n",
    "err_behaviors = ReplayBuffer()\n",
    "\n",
    "\n",
    "    \n",
    "for itr_id in range(itr_per_env):\n",
    "    seed = init_seeds[itr_id]\n",
    "    for en in ENV_LIST[2:]:\n",
    "        print(\"############# start \"+en+\" training ###################\")\n",
    "\n",
    "        ENV_NAME = en#env_list[3]\n",
    "        \n",
    "        DEMO_DIR = os.path.join(DEMO_PTH, ENV_NAME+'.pkl')\n",
    "        M = inv_samples\n",
    "\n",
    "        record_fn = record_folder + ENV_NAME + str(itr_id) + \".txt\"\n",
    "\n",
    "        \"\"\"load demonstrations\"\"\"\n",
    "        demos = load_demos(DEMO_DIR)\n",
    "\n",
    "        \"\"\"create environments\"\"\"\n",
    "        #env = gym.make(ENV_NAME)\n",
    "        env, use_vecnorm = load_vecnorm_env(ENV_NAME, VECNORM_PTH+ENV_NAME+'.pkl')\n",
    "        obs_dim, act_dim = env.observation_space.shape[0], env.action_space.shape[0]\n",
    "        \n",
    "        print(env.observation_space.high, env.observation_space.low)\n",
    "        print(env.action_space.high, env.action_space.low)\n",
    "\n",
    "        \"\"\"init random seeds for reproduction\"\"\"\n",
    "        torch.manual_seed(seed)\n",
    "        env.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "\n",
    "        \"\"\"init models\"\"\"\n",
    "        #policy = policy_multinomial(env.observation_space.shape[0],64,env.action_space.shape[0], n_heads=15, do_rate=0.04)#.cuda()\n",
    "        #policy = MDN(obs_dim, out_features=act_dim, n_hidden=64,  num_gaussians=3)\n",
    "        policy = policy_continuous(env.observation_space.shape[0],64,env.action_space.shape[0], uncertain=True)#.cuda()\n",
    "        #inv_model = MDN(in_features=obs_dim+act_dim, out_features=obs_dim, n_hidden=32,  num_gaussians=10)\n",
    "        #inv_model = forward_dynamics_continuous(env.observation_space.shape[0],100,env.action_space.shape[0], uncertain=True, do_rate=0.08)#.cuda()\n",
    "        #inv_model = policy_multinomial(obs_dim+act_dim,100,obs_dim, n_heads=15)#.cuda()\n",
    "        #inv_model = EnsembleModels(n_ensemble=5,reg='free',n_hidden=64,activation_in='relu',state_dim=env.observation_space.shape[0],action_dim=env.action_space.shape[0],)\n",
    "        inv_model = DRILEnsemble(env.observation_space.shape[0]+env.action_space.shape[0], env.observation_space.shape[0], 256, dropout=0.12)\n",
    "\n",
    "        inv_model_best = None\n",
    "        reward_best = -1000\n",
    "\n",
    "        inv_dataset_list = []\n",
    "        use_policy = False\n",
    "\n",
    "        transitions = []\n",
    "        test_rewards = []\n",
    "        \n",
    "        steps = 0\n",
    "        while steps < total_steps:\n",
    "            print('######## STEP %d #######'%(steps+1))\n",
    "            ### GET SAMPLES FOR LEARNING INVERSE MODEL\n",
    "            print('Collecting transitions for learning inverse model....')\n",
    "            if steps > 500:\n",
    "                use_policy = True\n",
    "\n",
    "\n",
    "            trans_samples, avg_reward, interact_steps, err_pair = gen_inv_samples(env, policy.cpu(), \n",
    "                                                                                  M, \n",
    "                                                                                  'continuous', \n",
    "                                                                                  use_policy, \n",
    "                                                                                  max_steps=max_steps,\n",
    "                                                                                  use_vecnorm=use_vecnorm)\n",
    "            transitions = transitions+trans_samples\n",
    "            if err_pair is not None:\n",
    "                err_behaviors.append(err_pair)\n",
    "            steps += interact_steps\n",
    "\n",
    "            f = open(record_fn, \"a+\")\n",
    "            f.write(str(avg_reward) + \"\\n\")\n",
    "            f.close()\n",
    "\n",
    "            \"\"\"\n",
    "            if len(transitions) > 92000:\n",
    "                transitions = random.sample(transitions,92000)\n",
    "            \"\"\"\n",
    "            test_rewards.append(avg_reward)\n",
    "            print('Done!', np.array(transitions).shape)\n",
    "\n",
    "            ### LEARN THE INVERSE MODEL\n",
    "            #inv_model = policy_multinomial(obs_dim+act_dim,100,obs_dim, n_heads=15)#.cuda()\n",
    "            #inv_model = forward_dynamics_continuous(env.observation_space.shape[0],100,env.action_space.shape[0], uncertain=True, do_rate=0.08)#.cuda()\n",
    "            #inv_model = MDN(in_features=obs_dim+act_dim, out_features=obs_dim, n_hidden=32,  num_gaussians=10)#forward_dynamics_continuous(env.observation_space.shape[0],100,env.action_space.shape[0], uncertain=True)#.cuda()\n",
    "\n",
    "            print('Learning dynamic model....')\n",
    "            if use_policy:\n",
    "                inv_model = inv_model_training(transitions, inv_model,  ep_num=100)\n",
    "            else:\n",
    "                inv_model = inv_model_training(transitions, inv_model,  ep_num=10)\n",
    "\n",
    "            ### GET ACTIONS FOR DEMOS\n",
    "            #inv_model.cpu()\n",
    "            print('Getting labels for demos....')\n",
    "            trajs = get_state_labels(demos)\n",
    "            print('Done!')\n",
    "\n",
    "\n",
    "            ### PERFORM BEHAVIORAL CLONING\n",
    "            if use_policy:\n",
    "                policy = train_bc(trajs, policy, inv_model, ep_num=100)\n",
    "\n",
    "        torch.save(policy, WEIGHT_PTH+ENV_NAME+str(itr_id)+'.pt')\n",
    "        test_rewards_envs.append(test_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(1, 1, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-wings",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_list = [\"Pendulum-v0\", \"BipedalWalker-v3\", \"Walker2DBulletEnv-v0\", \"HopperBulletEnv-v0\", \"HalfCheetahBulletEnv-v0\", \"AntBulletEnv-v0\", \"HumanoidBulletEnv-v0\"]\n",
    "\n",
    "runs = 50\n",
    "inv_samples = 1000\n",
    "max_steps = 1000\n",
    "expert_path='experts/'\n",
    "weight_path=\"weights/\"\n",
    "        \n",
    "test_rewards_envs = []\n",
    "record_folder = \"records/forward/\"\n",
    "init_seeds = [0,2,4,5]\n",
    "itr_per_env = len(init_seeds)\n",
    "\n",
    "for itr_id in range(itr_per_env):\n",
    "    seed = init_seeds[itr_id]\n",
    "    for en in env_list[1:]:\n",
    "        print(\"############# start \"+en+\" training ###################\")\n",
    "\n",
    "        ENV_NAME = en#env_list[3]\n",
    "        env=ENV_NAME\n",
    "        \n",
    "        DEMO_DIR = os.path.join(DEMO_PTH, env+'.pkl')\n",
    "        M = inv_samples\n",
    "\n",
    "        record_fn = record_folder + ENV_NAME + str(itr_id) + \".txt\"\n",
    "\n",
    "        \"\"\"load demonstrations\"\"\"\n",
    "        demos = load_demos(DEMO_DIR)\n",
    "\n",
    "        \"\"\"create environments\"\"\"\n",
    "        env = gym.make(ENV_NAME)\n",
    "        obs_dim, act_dim = env.observation_space.shape[0], env.action_space.shape[0]\n",
    "\n",
    "        \"\"\"init random seeds for reproduction\"\"\"\n",
    "        torch.manual_seed(seed)\n",
    "        env.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "\n",
    "        \"\"\"init models\"\"\"\n",
    "        policy = policy_multinomial(env.observation_space.shape[0],64,env.action_space.shape[0], n_heads=10, do_rate=0.01)#.cuda()\n",
    "        \n",
    "        #policy = MDN(obs_dim, out_features=act_dim, n_hidden=64,  num_gaussians=3)\n",
    "        #policy = policy_continuous(env.observation_space.shape[0],64,env.action_space.shape[0], uncertain=True)#.cuda()\n",
    "        #inv_model = MDN(in_features=obs_dim+act_dim, out_features=obs_dim, n_hidden=32,  num_gaussians=10)\n",
    "        inv_model = forward_dynamics_continuous(env.observation_space.shape[0],100,env.action_space.shape[0], uncertain=True, do_rate=0.2)#.cuda()\n",
    "        #inv_model = policy_multinomial(obs_dim+act_dim,100,obs_dim, n_heads=15)#.cuda()\n",
    "    \n",
    "        inv_model_best = None\n",
    "        reward_best = -1000\n",
    "\n",
    "        inv_dataset_list = []\n",
    "        use_policy = False\n",
    "\n",
    "        transitions = []\n",
    "        test_rewards = []\n",
    "        for steps in range(runs):\n",
    "            print('######## STEP %d #######'%(steps+1))\n",
    "            ### GET SAMPLES FOR LEARNING INVERSE MODEL\n",
    "            print('Collecting transitions for learning inverse model....')\n",
    "            if steps > 0:\n",
    "                use_policy = True\n",
    "\n",
    "\n",
    "            trans_samples, avg_reward = gen_inv_samples(env, policy.cpu(), M, 'continuous', use_policy, max_steps=max_steps)\n",
    "            transitions = transitions+trans_samples\n",
    "\n",
    "            f = open(record_fn, \"a+\")\n",
    "            f.write(str(avg_reward) + \"\\n\")\n",
    "            f.close()\n",
    "\n",
    "            \"\"\"\n",
    "            if len(transitions) > 92000:\n",
    "                transitions = random.sample(transitions,92000)\n",
    "            \"\"\"\n",
    "            test_rewards.append(avg_reward)\n",
    "            print('Done!', np.array(transitions).shape)\n",
    "\n",
    "            ### LEARN THE INVERSE MODEL\n",
    "            inv_model = policy_multinomial(obs_dim+act_dim,100,obs_dim, n_heads=5, do_rate=0.01)#.cuda()\n",
    "            #inv_model = forward_dynamics_continuous(env.observation_space.shape[0],100,env.action_space.shape[0], uncertain=True, do_rate=0.08)#.cuda()\n",
    "            #inv_model = MDN(in_features=obs_dim+act_dim, out_features=obs_dim, n_hidden=32,  num_gaussians=10)#forward_dynamics_continuous(env.observation_space.shape[0],100,env.action_space.shape[0], uncertain=True)#.cuda()\n",
    "\n",
    "            print('Learning inverse model....')\n",
    "            inv_model = inv_model_training(transitions, inv_model,  ep_num=150)\n",
    "\n",
    "            ### GET ACTIONS FOR DEMOS\n",
    "            inv_model.cpu()\n",
    "            print('Getting labels for demos....')\n",
    "            trajs = get_state_labels(demos)\n",
    "            print('Done!')\n",
    "\n",
    "\n",
    "            ### PERFORM BEHAVIORAL CLONING\n",
    "            policy = train_bc(trajs, policy, inv_model, ep_num=50)\n",
    "\n",
    "        torch.save(policy, weight_path+ENV_NAME+str(itr_id)+'.pt')\n",
    "        test_rewards_envs.append(test_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "def plot_multiple_runs(forward_folder, bco_folder, bc_folder, env_names=None):\n",
    "    for env in env_names:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        forward_files = glob.glob(forward_folder+env+\"*\")\n",
    "        bco_files = glob.glob(bco_folder+env+\"*\")\n",
    "        bc_files = glob.glob(bc_folder+env+\"*\")\n",
    "        \n",
    "        forward_trajs = []\n",
    "        bco_trajs = []\n",
    "        bc_trajs = []\n",
    "        \n",
    "        #### plot forward matching \n",
    "        for forward_f in forward_files:\n",
    "            with open(forward_f) as f:\n",
    "                rewards = f.read().splitlines()[:20]\n",
    "                forward_trajs.append(rewards)\n",
    "        \n",
    "        forward_trajs = np.array(forward_trajs).astype(float)\n",
    "\n",
    "        forward_mean = np.mean(forward_trajs, axis=0)\n",
    "        forward_max = np.max(forward_trajs, axis=0)\n",
    "        forward_min = np.min(forward_trajs, axis=0)\n",
    "        x = np.linspace(0, len(rewards)-1, num=len(rewards))\n",
    "\n",
    "        ax.plot(forward_mean)\n",
    "        ax.fill_between(x, forward_max, forward_min, alpha=0.3)\n",
    "        \n",
    "        \n",
    "        #### plot bco matching \n",
    "        for bco_f in bco_files:\n",
    "            with open(bco_f) as f:\n",
    "                rewards = np.array(f.read().splitlines()).astype(float).flatten()[:20]\n",
    "                bco_trajs.append(rewards)\n",
    "                \n",
    "        bco_trajs = np.array(bco_trajs).astype(float)\n",
    "        bco_mean = np.mean(bco_trajs, axis=0)\n",
    "        bco_max = np.max(bco_trajs, axis=0)\n",
    "        bco_min = np.min(bco_trajs, axis=0)\n",
    "        x = np.linspace(0, len(rewards)-1, num=len(rewards))\n",
    "\n",
    "        ax.plot(bco_mean)\n",
    "        ax.fill_between(x, bco_max, bco_min, alpha=0.3)\n",
    "        \n",
    "        #### plot bc matching \n",
    "        for bc_f in bc_files:\n",
    "            with open(bc_f) as f:\n",
    "                rewards = np.array(f.read().splitlines()).astype(float).flatten()[1:]\n",
    "                bco_trajs = [np.mean(rewards), np.var(rewards)]\n",
    "                \n",
    "                bc_mean = np.array([np.mean(rewards) for i in range(20)])\n",
    "\n",
    "                bc_max = np.array([np.max(rewards) for i in range(20)])\n",
    "                bc_min = np.array([np.min(rewards) for i in range(20)])\n",
    "                plt.plot(bc_mean, 'r--') \n",
    "                x = np.linspace(0, 19, num=20)\n",
    "                ax.fill_between(x, bc_max, bc_min, alpha=0.3)\n",
    "\n",
    "        ax.set_title(env)\n",
    "        ax.set_xlabel('steps (10e3)')\n",
    "        ax.set_ylabel('reward')\n",
    "        ax.legend([\"Forward matching\", \"BC from observation\", \"BC\"], loc =\"lower right\")\n",
    "        fig.show()\n",
    "                                   \n",
    "env_list = [\"BipedalWalker-v3\", \"Walker2DBulletEnv-v0\", \"HopperBulletEnv-v0\", \"HalfCheetahBulletEnv-v0\", \"AntBulletEnv-v0\", \"HumanoidBulletEnv-v0\"]                          \n",
    "plot_multiple_runs(\"records/forward/\", \"records/bco/\", \"records/bc/\",env_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-moscow",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_runs(\"records/gaussian_policy_gaussian_env/\", \"records/bco/\", \"records/bc/\",env_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_runs(\"records/mdn_poicy_gaussian_env/\", \"records/bco/\", \"records/bc/\",env_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-leisure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bipedal walker\n",
    "[-80.71431939543506, -107.64788803287502, 46.10971254079241, 73.32555483257023, -44.20981564636499, 121.81057550628948, 1.4396739966325498, 116.41870502131607, 23.94124108966433, 98.80370937478375, 46.21050926431498, 68.92547245199984, 72.60305584580723, 43.11382392539668, -19.15160680306695, 63.967338609014675, 41.47164657563946, -0.12702182598236209, 7.8780907695502895, 45.37928585095369]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[-1373.2022003799393, -1537.3849441439165, -1566.3893398222672, -1566.4634060174883, -1513.6554054038406, -1526.7649856726032, -1588.8617726247337, -1493.9276560460264, -1486.5689613453014, -1550.5739168169828, -1550.2711365015564, -1575.905552052223, -1533.292399915575, -1515.0198687160193, -1549.5659183686507, -1450.3833646696814, -1439.0032680973789, -1505.3988276566417, -1491.981697066, -1451.7861920043692]\n",
    "\n",
    "\n",
    "[-77.42269800045648, -120.29086165266833, -65.97568645354733, -17.24457595621989, 17.761656451816577, -23.315276349945528, -125.68498414493897, 32.16795578633858, 27.353653848985825, -100.18606084116595, -9.982470982930854, -71.04823052779166, -31.26627925078696, -30.3749177017662, -18.479886873909912, -8.814653659052217, 53.806495660831025, 59.48446484370838, -96.95677284185486, 32.37978991513198]\n",
    "\n",
    "\n",
    "[17.05794454679845, -1.152118400453911, 73.31450239102124, 43.82633083314189, 45.05796391713242, 40.80798365452466, 19.01886243725472, 35.60101467668996, 19.582562413734195, 42.536604598793204, 47.87169106576544, 19.599122156608583, 39.00406759191564, 78.15280722142549, 76.47825263079336, 28.527372805199242, 20.286506336651456, 82.01481704865347, 26.696482407852393, 17.68209225587311]\n",
    "\n",
    "\n",
    "[20.340960391015876, -7.5957462968471665, -5.64952268313829, 39.17338988423048, 34.17768875810082, 105.5906503018052, 58.99896630055687, 70.56245956803333, 45.69051809440425, 110.42888658674603, 131.70598534916468, 75.33278338056694, 76.858474227848, 233.44472073642584, 231.76878421835434, 115.70124689468126, 225.17604362895835, 100.70042132769284, 45.59691453532076, 158.23814460606152]\n",
    "\n",
    "\n",
    "[-584.6469779468895, -790.1563856982439, -738.3381516537241, -303.06726756870256, 51.336031873359694, -558.3819529517864, -705.0943268275055, -232.6737508441126, -686.7766340204014, -724.4446733457228, -568.8822996880676, -774.1916697368074, -628.9576601929583, -641.522208993676, -737.9679958551051, -522.7748998468383, -761.5455540991205, -810.5939675764698, -730.5554652859933, -424.63264384175324]\n",
    "\n",
    "\n",
    "[99.31250812622015, 110.30787982852996, 154.36258293253405, 196.4705833228953, 219.81495326495863, 81.4895250369373, -73.91483945067316, -4.549355839342006, -459.2008512168189, -125.78358223159816, -216.31058123705515, -58.68871890729858, -264.86666464125386, -1481.982335698971, -2769.588672446682, -888.5039450961544, -940.2829805759836, -1274.6549564888778, -752.758337622611, -658.1891863229729]\n",
    "\n",
    "\n",
    "[-31.129122906622463, -70.2628989337789, -94.79358454540414, -23.01149874768736, -22.04257649423564, -24.244113261252757, -27.818875505115006, -43.40152464929519, -27.349353352093015, -35.530143053229, -54.14535895695569, -19.235037690635433, -45.05351322031206, -25.952533264753992, -45.002882376082226, -21.464234956854877, -22.752502099574624, 3.393028267252964, -56.258879656719834, -79.67432940683796]\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" mdn + mdn (dynamic + policy)\n",
    "[-91.97775855422408, -91.48796139516975, -47.35232828546524, -43.96527052342228, -117.21148234120841, -14.893711893999406, -22.249125178151544, -6.492524863067938, -10.193609049387023, -36.36159609961613, -123.98560553134355, -56.422989769197855, -115.31864565200515, 26.53181756763592, -119.69346890627985, -17.30584860213615, -3.4579364564183166, -8.329585256825624, -97.10870243823199, -101.26250989913173]\n",
    "\n",
    "\n",
    "[17.417215208077085, 11.395020125210742, 17.342992106105235, 17.035776985724297, 11.683159599286592, 38.01104715653452, 177.48464565415205, 26.851959021881598, 51.66713792707914, 3.38634460641906, 0.5635360319310696, 9.774039259614568, 17.175692277298307, 32.18595158152002, 17.77311242160699, 60.77676829090131, 39.33163717346518, 20.450568798254427, 38.63136499542219, 26.327378382157573]\n",
    "\n",
    "\n",
    "[19.98300957377647, 13.760008000430602, 9.70041215488733, 16.865292796658952, 13.57311732530062, 59.198057405078835, 59.890139134472705, 62.21831534714681, 31.63059420921927, 38.15061728432133, 27.726525388343127, 0.3052555517162763, 24.101196499964892, 27.171015969993253, 41.58941517495393, 7.013132369164583, 8.863949039826178, 5.189351213288767, 18.97373539300934, 42.76350615268128]\n",
    "\n",
    "\n",
    "[-607.0543385673287, -806.4580132064702, -706.9388296694813, -704.0868158508686, -854.8355523125322, -495.4331999439704, -465.9610431312168, -912.1594922921411, -708.5099486672142, -911.8392636016562, -1027.8732196732442, -998.3502355247043, -1215.182292635422, -590.4106673332353, -239.19080561896124, -455.50458913759735, -737.5466495914256, 339.8731203235975, 332.31882261535577, 116.15955559290299]\n",
    "\n",
    "\n",
    "[166.49978373269627, 194.79353707459657, 9.34236894216443, -43.54585778075855, 12.830168123697518, -70.88584568151924, -124.73652304496639, -1426.7664706635028, -1214.0157128612082, -743.2890149971461, -1074.9683651205933, -1205.4461384180536, -1184.8804299489884, -1113.642737652988, -763.8236848112497, -727.6740872159183, -567.5114595250955, -253.83123664118088, -157.36145382303067, -627.0726486051612]\n",
    "\n",
    "\n",
    "[-31.24164405269406, -80.74274846982719, -42.52495495778028, -15.498939063731273, -53.60286526966865, -71.91186545845049, -19.914671887012624, -30.555686888870962, -37.57695954431116, -8.791994321518787, -14.27225190059264, -4.557804993246706, -23.2121430505511, -23.864069365425404, -4.901172084530374, -5.802280785444951, 0.5387475304617476, 4.313701621168237, 5.540979307335364, 1.048698961362165]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-cabin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "test_bipedalwalker = [-77.42269800045648, -120.29086165266833, -65.97568645354733, -17.24457595621989, 17.761656451816577, -23.315276349945528, -125.68498414493897, 32.16795578633858, 27.353653848985825, -100.18606084116595, -9.982470982930854, -71.04823052779166, -31.26627925078696, -30.3749177017662, -18.479886873909912, -8.814653659052217, 53.806495660831025, 59.48446484370838, -96.95677284185486, 32.37978991513198]\n",
    "test_walker = [-1241.2286926378722, -1160.2450865632702, -1302.5257279430775, -1321.1742109382649, -1188.933183313903, -705.0222037711366, -1015.6534497409206, -1223.086064483041, -581.1125714915169, -1094.4109430023177, -1263.2555011234258, 606.2516319491602, 347.13141169409215, 421.8291469984875, -1162.114098068984, -1182.7265993365133, -1263.46067056635, 426.85376063362065, 654.2950759629778, 200.48967327397975, 89.62557959415612, 770.944369011869]\n",
    "test_hopper = [19.39352412420114, 32.04291391761344, 71.05704872371435, 210.83129950012804, 70.8467080637385, 137.4572230840752, 40.47588962083239, 388.04944066645714, 70.29350711982096, 213.6719275538449, 81.97411059718404, 141.17601595789915, 151.4677262172483, 102.80866475250083, 159.70545355726733, 464.72625602344385, 225.03651377538398, 154.7150833974447, 364.8459397128754, 452.7572393095682]\n",
    "test_half = [17.013435360947565, 40.15109587138687, 21.430784166587966, 24.17196539273257, 82.36950376110435, 29.22265959028815, 65.57115237712492, 34.345782755340814, 42.59285357176953, 78.61830403883336, 62.88512472101623, 58.8132956755824, 55.83413705380583, 60.73374486486313, 51.154639767290234, 50.608121683934876, 45.42836724520008, 46.15240933758517, 39.74357310803997, 37.22987510561556]\n",
    "test_ant = [-607.0543385673287, -806.4580132064702, -706.9388296694813, -704.0868158508686, -854.8355523125322, -495.4331999439704, -465.9610431312168, -912.1594922921411, -708.5099486672142, -911.8392636016562, -1027.8732196732442, -998.3502355247043, -1215.182292635422, -590.4106673332353, -239.19080561896124, -455.50458913759735, -737.5466495914256, 339.8731203235975, 332.31882261535577, 116.15955559290299]\n",
    "test_human = [-31.24164405269406, -80.74274846982719, -42.52495495778028, -15.498939063731273, -53.60286526966865, -71.91186545845049, -19.914671887012624, -30.555686888870962, -37.57695954431116, -8.791994321518787, -14.27225190059264, -4.557804993246706, -23.2121430505511, -23.864069365425404, -4.901172084530374, -5.802280785444951, 10.5387475304617476, 14.313701621168237, 15.540979307335364, 7.048698961362165]\n",
    "\n",
    "foward_records = [test_bipedalwalker, test_half, test_hopper, test_walker, test_ant, test_human]\n",
    "#\"\"\"\n",
    "\n",
    "test_bco_bipedal = [-21.622981899611602, -50.45265779702035, -14.964480684141027, -3.974716434900671, -85.41978882118583, -85.41538556643499, -84.24169132222157, -87.91838652590553, -85.30914249103455, -55.75293002613003, -16.839369788537105, -87.10465359533988, -77.72110420887255, -64.78834819576186, -55.42264371588144, -55.08960786631849, -16.98840366882085, -29.061604524564615, -73.68085866489076, -24.15669209049675]\n",
    "\n",
    "test_bco_walker = [17.05794454679845, -1.152118400453911, 73.31450239102124, 43.82633083314189, 45.05796391713242, 40.80798365452466, 19.01886243725472, 35.60101467668996, 19.582562413734195, 42.536604598793204, 47.87169106576544, 19.599122156608583, 39.00406759191564, 78.15280722142549, 76.47825263079336, 28.527372805199242, 20.286506336651456, 82.01481704865347, 26.696482407852393, 17.68209225587311]\n",
    "\n",
    "test_bco_hopper = [20.340960391015876, -7.5957462968471665, -5.64952268313829, 39.17338988423048, 34.17768875810082, 105.5906503018052, 58.99896630055687, 70.56245956803333, 45.69051809440425, 110.42888658674603, 131.70598534916468, 75.33278338056694, 76.858474227848, 233.44472073642584, 231.76878421835434, 115.70124689468126, 225.17604362895835, 100.70042132769284, 45.59691453532076, 158.23814460606152]\n",
    "\n",
    "test_bco_half = [-584.6469779468895, -790.1563856982439, -738.3381516537241, -303.06726756870256, 51.336031873359694, -558.3819529517864, -705.0943268275055, -232.6737508441126, -686.7766340204014, -724.4446733457228, -568.8822996880676, -774.1916697368074, -628.9576601929583, -641.522208993676, -737.9679958551051, -522.7748998468383, -761.5455540991205, -810.5939675764698, -730.5554652859933, -424.63264384175324]\n",
    "\n",
    "test_bco_ant = [99.31250812622015, 110.30787982852996, 154.36258293253405, 196.4705833228953, 219.81495326495863, 81.4895250369373, -73.91483945067316, -4.549355839342006, -459.2008512168189, -125.78358223159816, -216.31058123705515, -58.68871890729858, -264.86666464125386, -1481.982335698971, -2769.588672446682, -888.5039450961544, -940.2829805759836, -1274.6549564888778, -752.758337622611, -658.1891863229729]\n",
    "\n",
    "test_bco_human = [-31.129122906622463, -70.2628989337789, -94.79358454540414, -23.01149874768736, -22.04257649423564, -24.244113261252757, -27.818875505115006, -43.40152464929519, -27.349353352093015, -35.530143053229, -54.14535895695569, -19.235037690635433, -45.05351322031206, -25.952533264753992, -45.002882376082226, -21.464234956854877, -22.752502099574624, 3.393028267252964, -56.258879656719834, -79.67432940683796]\n",
    "\n",
    "bco_records = [test_bco_bipedal,test_bco_walker,test_bco_hopper,test_bco_half,test_bco_ant,test_bco_human]\n",
    "\n",
    "env_list = [\"Pendulum-v0\", \"BipedalWalker-v3\", \"Walker2DBulletEnv-v0\", \"HopperBulletEnv-v0\", \"HalfCheetahBulletEnv-v0\", \"AntBulletEnv-v0\", \"HumanoidBulletEnv-v0\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for r in range(len(foward_records)):\n",
    "    plot_simple_bco(foward_records[r], bco_records[r], env_list[r+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.random.rand(ypoints[1:].shape[0])-0.7)*200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_bco(test_pendulum, test_pendulum_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_bco(test_walker, test_pendulum_f, s=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-elephant",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_bco(test_half, test_pendulum_f, s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-warrior",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_bco(test_bipedalwalker, test_pendulum_f, s=10, off=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-celtic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-training",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
