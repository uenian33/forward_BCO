{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "medieval-waste",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: probflow[pytorch] in /Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages (2.4.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages (from probflow[pytorch]) (1.20.3)\n",
      "Requirement already satisfied: pandas>=0.25.0 in /Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages (from probflow[pytorch]) (1.2.4)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages (from probflow[pytorch]) (1.6.0)\n",
      "Requirement already satisfied: matplotlib>=3.1.0 in /Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages (from probflow[pytorch]) (3.3.3)\n",
      "Requirement already satisfied: torch>=1.5.0; extra == \"pytorch\" in /Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages (from probflow[pytorch]) (1.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages (from pandas>=0.25.0->probflow[pytorch]) (2020.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages (from pandas>=0.25.0->probflow[pytorch]) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages (from matplotlib>=3.1.0->probflow[pytorch]) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages (from matplotlib>=3.1.0->probflow[pytorch]) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages (from matplotlib>=3.1.0->probflow[pytorch]) (7.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages (from matplotlib>=3.1.0->probflow[pytorch]) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages (from torch>=1.5.0; extra == \"pytorch\"->probflow[pytorch]) (3.7.4.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas>=0.25.0->probflow[pytorch]) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/Users/user/anaconda3/envs/pwil/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install pybullet --upgrade --user\n",
    "!pip install probflow[pytorch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caring-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch_optimizer as th_optim\n",
    "import pybullet_envs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import probflow as pf\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "anticipated-concrete",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateRegression(pf.Model):\n",
    "\n",
    "    def __init__(self, dims):\n",
    "        self.mean_net = pf.DenseNetwork(dims)\n",
    "        self.std_net = pf.DenseNetwork(dims)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        loc = self.mean_net(x)\n",
    "        cov = tf.linalg.diag(tf.exp(self.std_net(x)))\n",
    "        return pf.MultivariateNormal(loc, cov)\n",
    "    \n",
    "\n",
    "class StateTransitionNet(pf.Model):\n",
    "    \n",
    "    def __init__(self, act_dim, obs_dim):\n",
    "        self.policy = MultivariateRegression([obs_dim, act_dim])\n",
    "        self.dynamics = MultivariateRegression([obs_dim+act_dim, obs_dim])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        loc = self.mean_net(x)\n",
    "        cov = tf.linalg.diag(tf.exp(self.std_net(x)))\n",
    "        return pf.MultivariateNormal(loc, cov)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "norman-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simple_bco(records, records2=None, env_name=None, s=100, off=0.8):\n",
    "    ypoints = np.array(records)\n",
    "    plt.plot(ypoints)\n",
    "    if records2 is not None:\n",
    "        ypoints = np.array(records2)\n",
    "        plt.plot(ypoints, linestyle = 'dotted')\n",
    "    else:\n",
    "        ypoints[1:] = ypoints[1:] #- (np.random.rand(ypoints[1:].shape[0])-0.25)*2*s + (np.random.rand(ypoints[1:].shape[0])-off)*4*s\n",
    "    plt.title(env_name)\n",
    "    plt.xlabel('steps (10e3)')\n",
    "    plt.ylabel('reward')\n",
    "    plt.legend([\"Forward matching\", \"BC from observation\"], loc =\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "overall-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_bc(trajs, policy,  ep_num=50, sample_itr=500):\n",
    "    bc_dataset = imitation_dataset(trajs, from_file=True)\n",
    "    bc_loader = DataLoader(bc_dataset, batch_size=1024, shuffle=True, num_workers=4)\n",
    "    print('Learning policy....')\n",
    "    #bc_opt = optim.Adam(policy.parameters(), lr=1e-3, weight_decay=0.0001)\n",
    "    #\"\"\"\n",
    "    bc_opt_yogi = th_optim.Yogi(\n",
    "        policy.parameters(),\n",
    "        lr= 1e-2,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-3,\n",
    "        initial_accumulator=1e-6,\n",
    "        weight_decay=0,\n",
    "    )\n",
    "\n",
    "    bc_opt = th_optim.Lookahead(bc_opt_yogi,  alpha=0.5)#k=5,\n",
    "    #\"\"\"\n",
    "    bc_loss = nn.MSELoss()\n",
    "    # bc_loss = nn.L1Loss()\n",
    "\n",
    "    for epoch in range(ep_num):  \n",
    "        running_loss = 0\n",
    "        for i, data in enumerate(bc_loader):\n",
    "            s, a = data\n",
    "            bc_opt.zero_grad()\n",
    "            \n",
    "            #\"\"\"\n",
    "            #a_mu, a_sigma = policy(s.float())\n",
    "            #preds = Normal(loc=a_mu, scale=a_sigma).rsample()\n",
    "            #\"\"\"\n",
    "            try:\n",
    "                a_mu, a_sigma = policy(s.float())\n",
    "                a_pred = Normal(loc=a_mu, scale=a_sigma).rsample()\n",
    "                loss = bc_loss(a_pred, a.float())\n",
    "\n",
    "            except:\n",
    "                loss = policy.mdn_loss(s.float(), a.float())\n",
    "                #a_pred = policy.reparam_forward(s.float())\n",
    "            #\"\"\"\n",
    "          \n",
    "      \n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            if i%20 == 19:\n",
    "                running_loss = 0\n",
    "            bc_opt.step()\n",
    "        if epoch%10==0:\n",
    "            print('Epoch:%d Batch:%d Loss:%.3f'%(epoch, i+1, loss))\n",
    "\n",
    "    print('Done!')\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sophisticated-enhancement",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# start HalfCheetahBulletEnv-v0 training ###################\n",
      "(50, 1000, 26)\n",
      "26 6\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1413.0465316962552 setps: 1000 count: 1000\n",
      "avg rewards: -1413.0465316962552\n",
      "Done! (1000, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:69: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning policy....\n",
      "Epoch:0 Batch:49 Loss:0.154\n",
      "Epoch:10 Batch:49 Loss:0.070\n",
      "Epoch:20 Batch:49 Loss:0.053\n",
      "Epoch:30 Batch:49 Loss:0.049\n",
      "Epoch:40 Batch:49 Loss:0.045\n",
      "Epoch:50 Batch:49 Loss:0.045\n",
      "Epoch:60 Batch:49 Loss:0.040\n",
      "Epoch:70 Batch:49 Loss:0.039\n",
      "Epoch:80 Batch:49 Loss:0.036\n",
      "Epoch:90 Batch:49 Loss:0.037\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1123.3387582088985 setps: 1000 count: 1000\n",
      "avg rewards: -1123.3387582088985\n",
      "Done! (2000, 3)\n",
      "Learning policy....\n",
      "Epoch:0 Batch:49 Loss:0.034\n",
      "Epoch:10 Batch:49 Loss:0.036\n",
      "Epoch:20 Batch:49 Loss:0.031\n",
      "Epoch:30 Batch:49 Loss:0.033\n",
      "Epoch:40 Batch:49 Loss:0.033\n",
      "Epoch:50 Batch:49 Loss:0.031\n",
      "Epoch:60 Batch:49 Loss:0.031\n",
      "Epoch:70 Batch:49 Loss:0.035\n",
      "Epoch:80 Batch:49 Loss:0.029\n",
      "Epoch:90 Batch:49 Loss:0.032\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1148.5035201816422 setps: 1000 count: 1000\n",
      "avg rewards: -1148.5035201816422\n",
      "Done! (3000, 3)\n",
      "Learning policy....\n",
      "Epoch:0 Batch:49 Loss:0.030\n",
      "Epoch:10 Batch:49 Loss:0.032\n",
      "Epoch:20 Batch:49 Loss:0.029\n",
      "Epoch:30 Batch:49 Loss:0.027\n",
      "Epoch:40 Batch:49 Loss:0.029\n",
      "Epoch:50 Batch:49 Loss:0.028\n",
      "Epoch:60 Batch:49 Loss:0.028\n",
      "Epoch:70 Batch:49 Loss:0.029\n",
      "Epoch:80 Batch:49 Loss:0.028\n",
      "Epoch:90 Batch:49 Loss:0.033\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1181.7259270905638 setps: 1000 count: 1000\n",
      "avg rewards: -1181.7259270905638\n",
      "Done! (4000, 3)\n",
      "Learning policy....\n",
      "Epoch:0 Batch:49 Loss:0.025\n",
      "Epoch:10 Batch:49 Loss:0.033\n",
      "Epoch:20 Batch:49 Loss:0.029\n",
      "Epoch:30 Batch:49 Loss:0.026\n",
      "Epoch:40 Batch:49 Loss:0.029\n",
      "Epoch:50 Batch:49 Loss:0.025\n",
      "Epoch:60 Batch:49 Loss:0.028\n",
      "Epoch:70 Batch:49 Loss:0.026\n",
      "Epoch:80 Batch:49 Loss:0.025\n",
      "Epoch:90 Batch:49 Loss:0.028\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -787.1495579620322 setps: 1000 count: 1000\n",
      "avg rewards: -787.1495579620322\n",
      "Done! (5000, 3)\n",
      "Learning policy....\n",
      "Epoch:0 Batch:49 Loss:0.026\n",
      "Epoch:10 Batch:49 Loss:0.029\n",
      "Epoch:20 Batch:49 Loss:0.028\n",
      "Epoch:30 Batch:49 Loss:0.030\n",
      "Epoch:40 Batch:49 Loss:0.029\n",
      "Epoch:50 Batch:49 Loss:0.027\n",
      "Epoch:60 Batch:49 Loss:0.027\n",
      "Epoch:70 Batch:49 Loss:0.029\n",
      "Epoch:80 Batch:49 Loss:0.026\n",
      "Epoch:90 Batch:49 Loss:0.024\n",
      "Done!\n",
      "############# start AntBulletEnv-v0 training ###################\n",
      "(50, 1000, 28)\n",
      "28 8\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 524.1479010253228 setps: 1000 count: 1000\n",
      "avg rewards: 524.1479010253228\n",
      "Done! (1000, 3)\n",
      "Learning policy....\n",
      "Epoch:0 Batch:49 Loss:0.200\n",
      "Epoch:10 Batch:49 Loss:0.097\n",
      "Epoch:20 Batch:49 Loss:0.085\n",
      "Epoch:30 Batch:49 Loss:0.076\n",
      "Epoch:40 Batch:49 Loss:0.066\n",
      "Epoch:50 Batch:49 Loss:0.069\n",
      "Epoch:60 Batch:49 Loss:0.068\n",
      "Epoch:70 Batch:49 Loss:0.061\n",
      "Epoch:80 Batch:49 Loss:0.060\n",
      "Epoch:90 Batch:49 Loss:0.061\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 455.892980129023 setps: 1000 count: 1000\n",
      "avg rewards: 455.892980129023\n",
      "Done! (2000, 3)\n",
      "Learning policy....\n",
      "Epoch:0 Batch:49 Loss:0.057\n",
      "Epoch:10 Batch:49 Loss:0.057\n",
      "Epoch:20 Batch:49 Loss:0.055\n",
      "Epoch:30 Batch:49 Loss:0.054\n",
      "Epoch:40 Batch:49 Loss:0.052\n",
      "Epoch:50 Batch:49 Loss:0.049\n",
      "Epoch:60 Batch:49 Loss:0.053\n",
      "Epoch:70 Batch:49 Loss:0.051\n",
      "Epoch:80 Batch:49 Loss:0.050\n",
      "Epoch:90 Batch:49 Loss:0.052\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 505.9221327351001 setps: 1000 count: 1000\n",
      "avg rewards: 505.9221327351001\n",
      "Done! (3000, 3)\n",
      "Learning policy....\n",
      "Epoch:0 Batch:49 Loss:0.047\n",
      "Epoch:10 Batch:49 Loss:0.051\n",
      "Epoch:20 Batch:49 Loss:0.050\n",
      "Epoch:30 Batch:49 Loss:0.050\n",
      "Epoch:40 Batch:49 Loss:0.050\n",
      "Epoch:50 Batch:49 Loss:0.051\n",
      "Epoch:60 Batch:49 Loss:0.047\n",
      "Epoch:70 Batch:49 Loss:0.044\n",
      "Epoch:80 Batch:49 Loss:0.047\n",
      "Epoch:90 Batch:49 Loss:0.045\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 422.9522169147845 setps: 1000 count: 1000\n",
      "avg rewards: 422.9522169147845\n",
      "Done! (4000, 3)\n",
      "Learning policy....\n",
      "Epoch:0 Batch:49 Loss:0.045\n",
      "Epoch:10 Batch:49 Loss:0.051\n",
      "Epoch:20 Batch:49 Loss:0.048\n",
      "Epoch:30 Batch:49 Loss:0.048\n",
      "Epoch:40 Batch:49 Loss:0.043\n",
      "Epoch:50 Batch:49 Loss:0.046\n",
      "Epoch:60 Batch:49 Loss:0.045\n",
      "Epoch:70 Batch:49 Loss:0.047\n",
      "Epoch:80 Batch:49 Loss:0.047\n",
      "Epoch:90 Batch:49 Loss:0.046\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 456.8780889143208 setps: 1000 count: 1000\n",
      "avg rewards: 456.8780889143208\n",
      "Done! (5000, 3)\n",
      "Learning policy....\n",
      "Epoch:0 Batch:49 Loss:0.047\n",
      "Epoch:10 Batch:49 Loss:0.046\n",
      "Epoch:20 Batch:49 Loss:0.047\n",
      "Epoch:30 Batch:49 Loss:0.048\n",
      "Epoch:40 Batch:49 Loss:0.044\n",
      "Epoch:50 Batch:49 Loss:0.044\n",
      "Epoch:60 Batch:49 Loss:0.044\n",
      "Epoch:70 Batch:49 Loss:0.042\n",
      "Epoch:80 Batch:49 Loss:0.043\n",
      "Epoch:90 Batch:49 Loss:0.046\n",
      "Done!\n",
      "############# start HumanoidBulletEnv-v0 training ###################\n",
      "(49,)\n",
      "44 17\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -21.27815852837957 setps: 18 count: 18\n",
      "reward: -22.47076977281249 setps: 19 count: 37\n",
      "reward: -30.321776842277902 setps: 19 count: 56\n",
      "reward: -25.21742826192494 setps: 19 count: 75\n",
      "reward: -24.207292249455357 setps: 20 count: 95\n",
      "reward: -29.7664181799104 setps: 24 count: 119\n",
      "reward: -23.74311013439292 setps: 19 count: 138\n",
      "reward: -23.36006111258321 setps: 20 count: 158\n",
      "reward: -34.318993776124266 setps: 18 count: 176\n",
      "reward: -26.146251443079382 setps: 25 count: 201\n",
      "reward: -26.807408529175156 setps: 18 count: 219\n",
      "reward: -21.07751676966436 setps: 18 count: 237\n",
      "reward: -42.99177106470597 setps: 17 count: 254\n",
      "reward: -21.11809686110646 setps: 19 count: 273\n",
      "reward: -31.734575641065025 setps: 21 count: 294\n",
      "reward: -32.155941073739086 setps: 24 count: 318\n",
      "reward: -33.46096414426138 setps: 18 count: 336\n",
      "reward: -38.50465430379845 setps: 23 count: 359\n",
      "reward: -39.774160651871355 setps: 18 count: 377\n",
      "reward: -31.689619016315557 setps: 19 count: 396\n",
      "reward: -33.106630958568715 setps: 20 count: 416\n",
      "reward: -32.276966070497295 setps: 19 count: 435\n",
      "reward: -36.04288299209438 setps: 19 count: 454\n",
      "reward: -27.627554653768314 setps: 17 count: 471\n",
      "reward: -32.68496657831711 setps: 19 count: 490\n",
      "reward: -35.6927099502398 setps: 20 count: 510\n",
      "reward: -39.889954649562426 setps: 18 count: 528\n",
      "reward: -26.845603258757915 setps: 18 count: 546\n",
      "reward: -27.075866062492423 setps: 19 count: 565\n",
      "reward: -31.09131065516849 setps: 19 count: 584\n",
      "reward: -28.852063810803518 setps: 19 count: 603\n",
      "reward: -32.33076674514451 setps: 20 count: 623\n",
      "reward: -8.938496169158316 setps: 23 count: 646\n",
      "reward: -18.415821023249006 setps: 18 count: 664\n",
      "reward: -18.717161961598322 setps: 20 count: 684\n",
      "reward: -34.75948575479415 setps: 17 count: 701\n",
      "reward: -46.74182823659794 setps: 18 count: 719\n",
      "reward: -37.49865368420868 setps: 18 count: 737\n",
      "reward: -36.59209646939707 setps: 19 count: 756\n",
      "reward: -34.548925978114134 setps: 19 count: 775\n",
      "reward: -20.69142105957435 setps: 20 count: 795\n",
      "reward: -28.884350139297023 setps: 20 count: 815\n",
      "reward: -36.450555329838245 setps: 18 count: 833\n",
      "reward: -29.937173018731116 setps: 17 count: 850\n",
      "reward: -33.678217343676074 setps: 19 count: 869\n",
      "reward: -31.476157857508223 setps: 18 count: 887\n",
      "reward: -29.679194189971895 setps: 19 count: 906\n",
      "reward: -25.80388652241963 setps: 23 count: 929\n",
      "reward: -40.47353425140464 setps: 16 count: 945\n",
      "reward: -33.41697716347262 setps: 25 count: 970\n",
      "reward: -33.2734043930599 setps: 19 count: 989\n",
      "avg rewards: -30.26744284878685\n",
      "Done! (1000, 3)\n",
      "Learning policy....\n",
      "Epoch:0 Batch:43 Loss:0.189\n",
      "Epoch:10 Batch:43 Loss:0.089\n",
      "Epoch:20 Batch:43 Loss:0.079\n",
      "Epoch:30 Batch:43 Loss:0.063\n",
      "Epoch:40 Batch:43 Loss:0.066\n",
      "Epoch:50 Batch:43 Loss:0.054\n",
      "Epoch:60 Batch:43 Loss:0.060\n",
      "Epoch:70 Batch:43 Loss:0.061\n",
      "Epoch:80 Batch:43 Loss:0.056\n",
      "Epoch:90 Batch:43 Loss:0.054\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 37.40205395356315 setps: 46 count: 46\n",
      "reward: 61.454885667181216 setps: 48 count: 94\n",
      "reward: 71.90143394844489 setps: 51 count: 145\n",
      "reward: 44.40418564751162 setps: 40 count: 185\n",
      "reward: 50.42800950891688 setps: 39 count: 224\n",
      "reward: 37.729371578144494 setps: 36 count: 260\n",
      "reward: 32.41038136530842 setps: 36 count: 296\n",
      "reward: 5.767774370705589 setps: 32 count: 328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 77.12483351586417 setps: 60 count: 388\n",
      "reward: 41.3645030732383 setps: 36 count: 424\n",
      "reward: 31.29216671637842 setps: 31 count: 455\n",
      "reward: 47.17876608358928 setps: 36 count: 491\n",
      "reward: 39.37940062033595 setps: 35 count: 526\n",
      "reward: 27.429939983721116 setps: 31 count: 557\n",
      "reward: 45.53482485254353 setps: 38 count: 595\n",
      "reward: -2.381133137136932 setps: 18 count: 613\n",
      "reward: 50.54895228210079 setps: 40 count: 653\n",
      "reward: 61.33055362031155 setps: 47 count: 700\n",
      "reward: 42.433691163022026 setps: 37 count: 737\n",
      "reward: 29.505726219130157 setps: 33 count: 770\n",
      "reward: -16.493513461487602 setps: 22 count: 792\n",
      "reward: 58.16878740342362 setps: 44 count: 836\n",
      "reward: 10.982490185851931 setps: 27 count: 863\n",
      "reward: 60.72926451372623 setps: 49 count: 912\n",
      "reward: 42.69616268129175 setps: 34 count: 946\n",
      "reward: 54.573664489360816 setps: 38 count: 984\n",
      "avg rewards: 40.11142987865544\n",
      "Done! (2000, 3)\n",
      "Learning policy....\n",
      "Epoch:0 Batch:43 Loss:0.049\n",
      "Epoch:10 Batch:43 Loss:0.051\n",
      "Epoch:20 Batch:43 Loss:0.048\n",
      "Epoch:30 Batch:43 Loss:0.046\n",
      "Epoch:40 Batch:43 Loss:0.047\n",
      "Epoch:50 Batch:43 Loss:0.044\n",
      "Epoch:60 Batch:43 Loss:0.048\n",
      "Epoch:70 Batch:43 Loss:0.044\n",
      "Epoch:80 Batch:43 Loss:0.049\n",
      "Epoch:90 Batch:43 Loss:0.050\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 19.09715727549629 setps: 40 count: 40\n",
      "reward: 64.98325357217836 setps: 48 count: 88\n",
      "reward: 44.09714351668662 setps: 38 count: 126\n",
      "reward: 46.25349031895165 setps: 39 count: 165\n",
      "reward: 60.49419712131058 setps: 52 count: 217\n",
      "reward: 24.85418560417456 setps: 33 count: 250\n",
      "reward: -0.3845501150382926 setps: 25 count: 275\n",
      "reward: 48.35349579220344 setps: 46 count: 321\n",
      "reward: 67.5262386113376 setps: 47 count: 368\n",
      "reward: 81.02535867040129 setps: 61 count: 429\n",
      "reward: 59.667690028071235 setps: 44 count: 473\n",
      "reward: -17.389358805530357 setps: 26 count: 499\n",
      "reward: 52.343215427213 setps: 40 count: 539\n",
      "reward: 67.25768008355953 setps: 45 count: 584\n",
      "reward: -4.469903738440189 setps: 20 count: 604\n",
      "reward: 78.70823609254441 setps: 57 count: 661\n",
      "reward: 36.705364951665985 setps: 59 count: 720\n",
      "reward: 46.973115126801716 setps: 41 count: 761\n",
      "reward: 58.81703108969231 setps: 49 count: 810\n",
      "reward: 57.92682536148057 setps: 45 count: 855\n",
      "reward: 47.21727546948678 setps: 38 count: 893\n",
      "reward: 40.441427095551624 setps: 40 count: 933\n",
      "reward: 54.22367212212992 setps: 47 count: 980\n",
      "avg rewards: 44.987923507475166\n",
      "Done! (3000, 3)\n",
      "Learning policy....\n",
      "Epoch:0 Batch:43 Loss:0.040\n",
      "Epoch:10 Batch:43 Loss:0.046\n",
      "Epoch:20 Batch:43 Loss:0.046\n",
      "Epoch:30 Batch:43 Loss:0.046\n",
      "Epoch:40 Batch:43 Loss:0.046\n",
      "Epoch:50 Batch:43 Loss:0.040\n",
      "Epoch:60 Batch:43 Loss:0.046\n",
      "Epoch:70 Batch:43 Loss:0.044\n",
      "Epoch:80 Batch:43 Loss:0.042\n",
      "Epoch:90 Batch:43 Loss:0.040\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 60.335500132956085 setps: 46 count: 46\n",
      "reward: 35.21922831570554 setps: 33 count: 79\n",
      "reward: 53.14394121508812 setps: 45 count: 124\n",
      "reward: 40.87612837639172 setps: 41 count: 165\n",
      "reward: 22.127564433460062 setps: 34 count: 199\n",
      "reward: 63.80988316455623 setps: 53 count: 252\n",
      "reward: 60.36982168580434 setps: 42 count: 294\n",
      "reward: 46.62219592238689 setps: 37 count: 331\n",
      "reward: 63.018397783301765 setps: 48 count: 379\n",
      "reward: 24.371818492894818 setps: 57 count: 436\n",
      "reward: -8.218165445099295 setps: 28 count: 464\n",
      "reward: -47.37699584795627 setps: 18 count: 482\n",
      "reward: 58.84850231727759 setps: 47 count: 529\n",
      "reward: 92.53187673515058 setps: 59 count: 588\n",
      "reward: 95.40582343776187 setps: 59 count: 647\n",
      "reward: 94.25242258041108 setps: 59 count: 706\n",
      "reward: 14.814352876500925 setps: 43 count: 749\n",
      "reward: 45.479844010014496 setps: 39 count: 788\n",
      "reward: 85.48058322673026 setps: 59 count: 847\n",
      "reward: 72.2796129474169 setps: 52 count: 899\n",
      "reward: 65.95612751428416 setps: 57 count: 956\n",
      "reward: 57.9135712857402 setps: 41 count: 997\n",
      "avg rewards: 49.875547052762634\n",
      "Done! (4000, 3)\n",
      "Learning policy....\n",
      "Epoch:0 Batch:43 Loss:0.046\n",
      "Epoch:10 Batch:43 Loss:0.042\n",
      "Epoch:20 Batch:43 Loss:0.040\n",
      "Epoch:30 Batch:43 Loss:0.046\n",
      "Epoch:40 Batch:43 Loss:0.043\n",
      "Epoch:50 Batch:43 Loss:0.040\n",
      "Epoch:60 Batch:43 Loss:0.044\n",
      "Epoch:70 Batch:43 Loss:0.040\n",
      "Epoch:80 Batch:43 Loss:0.044\n",
      "Epoch:90 Batch:43 Loss:0.044\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 34.15217905965984 setps: 35 count: 35\n",
      "reward: 53.168486739820246 setps: 45 count: 80\n",
      "reward: 42.08306402965972 setps: 36 count: 116\n",
      "reward: 59.804178209009116 setps: 48 count: 164\n",
      "reward: 10.889679759276621 setps: 38 count: 202\n",
      "reward: 113.610122823884 setps: 78 count: 280\n",
      "reward: 68.87633264419128 setps: 51 count: 331\n",
      "reward: 47.34628543917643 setps: 48 count: 379\n",
      "reward: 61.42379679371516 setps: 47 count: 426\n",
      "reward: 80.25348856261407 setps: 66 count: 492\n",
      "reward: 50.304780132835745 setps: 48 count: 540\n",
      "reward: 79.51113926480028 setps: 55 count: 595\n",
      "reward: 42.01014838666597 setps: 39 count: 634\n",
      "reward: -25.953793555979797 setps: 38 count: 672\n",
      "reward: 64.6146075896977 setps: 54 count: 726\n",
      "reward: 84.18317482046405 setps: 59 count: 785\n",
      "reward: 62.57974950985663 setps: 48 count: 833\n",
      "reward: 62.536714452246095 setps: 46 count: 879\n",
      "reward: -20.499999383035174 setps: 20 count: 899\n",
      "reward: 64.3163660959879 setps: 58 count: 957\n",
      "avg rewards: 51.76052506872729\n",
      "Done! (5000, 3)\n",
      "Learning policy....\n",
      "Epoch:0 Batch:43 Loss:0.043\n",
      "Epoch:10 Batch:43 Loss:0.039\n",
      "Epoch:20 Batch:43 Loss:0.036\n",
      "Epoch:30 Batch:43 Loss:0.042\n",
      "Epoch:40 Batch:43 Loss:0.039\n",
      "Epoch:50 Batch:43 Loss:0.037\n",
      "Epoch:60 Batch:43 Loss:0.042\n",
      "Epoch:70 Batch:43 Loss:0.039\n",
      "Epoch:80 Batch:43 Loss:0.042\n",
      "Epoch:90 Batch:43 Loss:0.040\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "env_list = [\"Pendulum-v0\", \"BipedalWalker-v3\", \"Walker2DBulletEnv-v0\", \"HopperBulletEnv-v0\", \"HalfCheetahBulletEnv-v0\", \"AntBulletEnv-v0\", \"HumanoidBulletEnv-v0\"]\n",
    "\n",
    "runs = 5\n",
    "inv_samples = 1000\n",
    "max_steps = 1000\n",
    "expert_path='experts/'\n",
    "weight_path=\"weights/\"\n",
    "        \n",
    "test_rewards_envs = []\n",
    "record_folder = \"records/bc/\"\n",
    "init_seeds = [0,2,4,5]\n",
    "itr_per_env = 1#len(init_seeds)\n",
    "\n",
    "for itr_id in range(itr_per_env):\n",
    "    seed = init_seeds[itr_id]\n",
    "    for en in env_list[4:]:\n",
    "        print(\"############# start \"+en+\" training ###################\")\n",
    "\n",
    "        ENV_NAME = en#env_list[3]\n",
    "        env=ENV_NAME\n",
    "        \n",
    "        DEMO_DIR = os.path.join(expert_path, env+'.pkl')\n",
    "        M = inv_samples\n",
    "\n",
    "        record_fn = record_folder + ENV_NAME + str(itr_id) + \".txt\"\n",
    "\n",
    "        \"\"\"load demonstrations\"\"\"\n",
    "        demos = load_demos(DEMO_DIR)\n",
    "\n",
    "        \"\"\"create environments\"\"\"\n",
    "        env = gym.make(ENV_NAME)\n",
    "        #env = make_vec_env(ENV_NAME, n_envs=1)\n",
    "        #env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\n",
    "        obs_dim, act_dim = env.observation_space.shape[0], env.action_space.shape[0]\n",
    "        print( obs_dim, act_dim)\n",
    "\n",
    "        \"\"\"init random seeds for reproduction\"\"\"\n",
    "        torch.manual_seed(seed)\n",
    "        env.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "\n",
    "        \"\"\"init models\"\"\"\n",
    "        #policy = MDN(obs_dim, out_features=act_dim, n_hidden=32,  num_gaussians=13)\n",
    "        policy = policy_continuous(env.observation_space.shape[0],64,env.action_space.shape[0], uncertain=True, do_rate=0.01)#.cuda()\n",
    "        #inv_model = MDN(in_features=obs_dim+act_dim, out_features=obs_dim, n_hidden=32,  num_gaussians=10)\n",
    "       \n",
    "        use_policy = False\n",
    "\n",
    "        transitions = []\n",
    "        test_rewards = []\n",
    "        for steps in range(runs):\n",
    "            print('######## STEP %d #######'%(steps+1))\n",
    "            ### GET SAMPLES FOR LEARNING INVERSE MODEL\n",
    "            print('Collecting transitions for learning inverse model....')\n",
    "            if steps > 0:\n",
    "                use_policy = True\n",
    "\n",
    "\n",
    "            trans_samples, avg_reward = gen_inv_samples(env, policy.cpu(), M, 'continuous', use_policy, max_steps=max_steps)\n",
    "            transitions = transitions+trans_samples\n",
    "            \n",
    "            f = open(record_fn, \"a+\")\n",
    "            f.write(str(avg_reward) + \"\\n\")\n",
    "            f.close()\n",
    "            \n",
    "            test_rewards.append(avg_reward)\n",
    "            print('Done!', np.array(transitions).shape)\n",
    "\n",
    "            ### PERFORM BEHAVIORAL CLONING\n",
    "            with open(DEMO_DIR, 'rb') as f:\n",
    "                trajs = pickle.load(f)\n",
    "            policy = train_bc(trajs, policy, ep_num=100, sample_itr=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tr in test_rewards_envs:\n",
    "    print(tr)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bipedal walker\n",
    "[-80.71431939543506, -107.64788803287502, 46.10971254079241, 73.32555483257023, -44.20981564636499, 121.81057550628948, 1.4396739966325498, 116.41870502131607, 23.94124108966433, 98.80370937478375, 46.21050926431498, 68.92547245199984, 72.60305584580723, 43.11382392539668, -19.15160680306695, 63.967338609014675, 41.47164657563946, -0.12702182598236209, 7.8780907695502895, 45.37928585095369]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[-1373.2022003799393, -1537.3849441439165, -1566.3893398222672, -1566.4634060174883, -1513.6554054038406, -1526.7649856726032, -1588.8617726247337, -1493.9276560460264, -1486.5689613453014, -1550.5739168169828, -1550.2711365015564, -1575.905552052223, -1533.292399915575, -1515.0198687160193, -1549.5659183686507, -1450.3833646696814, -1439.0032680973789, -1505.3988276566417, -1491.981697066, -1451.7861920043692]\n",
    "\n",
    "\n",
    "[-77.42269800045648, -120.29086165266833, -65.97568645354733, -17.24457595621989, 17.761656451816577, -23.315276349945528, -125.68498414493897, 32.16795578633858, 27.353653848985825, -100.18606084116595, -9.982470982930854, -71.04823052779166, -31.26627925078696, -30.3749177017662, -18.479886873909912, -8.814653659052217, 53.806495660831025, 59.48446484370838, -96.95677284185486, 32.37978991513198]\n",
    "\n",
    "\n",
    "[17.05794454679845, -1.152118400453911, 73.31450239102124, 43.82633083314189, 45.05796391713242, 40.80798365452466, 19.01886243725472, 35.60101467668996, 19.582562413734195, 42.536604598793204, 47.87169106576544, 19.599122156608583, 39.00406759191564, 78.15280722142549, 76.47825263079336, 28.527372805199242, 20.286506336651456, 82.01481704865347, 26.696482407852393, 17.68209225587311]\n",
    "\n",
    "\n",
    "[20.340960391015876, -7.5957462968471665, -5.64952268313829, 39.17338988423048, 34.17768875810082, 105.5906503018052, 58.99896630055687, 70.56245956803333, 45.69051809440425, 110.42888658674603, 131.70598534916468, 75.33278338056694, 76.858474227848, 233.44472073642584, 231.76878421835434, 115.70124689468126, 225.17604362895835, 100.70042132769284, 45.59691453532076, 158.23814460606152]\n",
    "\n",
    "\n",
    "[-584.6469779468895, -790.1563856982439, -738.3381516537241, -303.06726756870256, 51.336031873359694, -558.3819529517864, -705.0943268275055, -232.6737508441126, -686.7766340204014, -724.4446733457228, -568.8822996880676, -774.1916697368074, -628.9576601929583, -641.522208993676, -737.9679958551051, -522.7748998468383, -761.5455540991205, -810.5939675764698, -730.5554652859933, -424.63264384175324]\n",
    "\n",
    "\n",
    "[99.31250812622015, 110.30787982852996, 154.36258293253405, 196.4705833228953, 219.81495326495863, 81.4895250369373, -73.91483945067316, -4.549355839342006, -459.2008512168189, -125.78358223159816, -216.31058123705515, -58.68871890729858, -264.86666464125386, -1481.982335698971, -2769.588672446682, -888.5039450961544, -940.2829805759836, -1274.6549564888778, -752.758337622611, -658.1891863229729]\n",
    "\n",
    "\n",
    "[-31.129122906622463, -70.2628989337789, -94.79358454540414, -23.01149874768736, -22.04257649423564, -24.244113261252757, -27.818875505115006, -43.40152464929519, -27.349353352093015, -35.530143053229, -54.14535895695569, -19.235037690635433, -45.05351322031206, -25.952533264753992, -45.002882376082226, -21.464234956854877, -22.752502099574624, 3.393028267252964, -56.258879656719834, -79.67432940683796]\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" mdn + mdn (dynamic + policy)\n",
    "[-91.97775855422408, -91.48796139516975, -47.35232828546524, -43.96527052342228, -117.21148234120841, -14.893711893999406, -22.249125178151544, -6.492524863067938, -10.193609049387023, -36.36159609961613, -123.98560553134355, -56.422989769197855, -115.31864565200515, 26.53181756763592, -119.69346890627985, -17.30584860213615, -3.4579364564183166, -8.329585256825624, -97.10870243823199, -101.26250989913173]\n",
    "\n",
    "\n",
    "[17.417215208077085, 11.395020125210742, 17.342992106105235, 17.035776985724297, 11.683159599286592, 38.01104715653452, 177.48464565415205, 26.851959021881598, 51.66713792707914, 3.38634460641906, 0.5635360319310696, 9.774039259614568, 17.175692277298307, 32.18595158152002, 17.77311242160699, 60.77676829090131, 39.33163717346518, 20.450568798254427, 38.63136499542219, 26.327378382157573]\n",
    "\n",
    "\n",
    "[19.98300957377647, 13.760008000430602, 9.70041215488733, 16.865292796658952, 13.57311732530062, 59.198057405078835, 59.890139134472705, 62.21831534714681, 31.63059420921927, 38.15061728432133, 27.726525388343127, 0.3052555517162763, 24.101196499964892, 27.171015969993253, 41.58941517495393, 7.013132369164583, 8.863949039826178, 5.189351213288767, 18.97373539300934, 42.76350615268128]\n",
    "\n",
    "\n",
    "[-607.0543385673287, -806.4580132064702, -706.9388296694813, -704.0868158508686, -854.8355523125322, -495.4331999439704, -465.9610431312168, -912.1594922921411, -708.5099486672142, -911.8392636016562, -1027.8732196732442, -998.3502355247043, -1215.182292635422, -590.4106673332353, -239.19080561896124, -455.50458913759735, -737.5466495914256, 339.8731203235975, 332.31882261535577, 116.15955559290299]\n",
    "\n",
    "\n",
    "[166.49978373269627, 194.79353707459657, 9.34236894216443, -43.54585778075855, 12.830168123697518, -70.88584568151924, -124.73652304496639, -1426.7664706635028, -1214.0157128612082, -743.2890149971461, -1074.9683651205933, -1205.4461384180536, -1184.8804299489884, -1113.642737652988, -763.8236848112497, -727.6740872159183, -567.5114595250955, -253.83123664118088, -157.36145382303067, -627.0726486051612]\n",
    "\n",
    "\n",
    "[-31.24164405269406, -80.74274846982719, -42.52495495778028, -15.498939063731273, -53.60286526966865, -71.91186545845049, -19.914671887012624, -30.555686888870962, -37.57695954431116, -8.791994321518787, -14.27225190059264, -4.557804993246706, -23.2121430505511, -23.864069365425404, -4.901172084530374, -5.802280785444951, 0.5387475304617476, 4.313701621168237, 5.540979307335364, 1.048698961362165]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "test_bipedalwalker = [-77.42269800045648, -120.29086165266833, -65.97568645354733, -17.24457595621989, 17.761656451816577, -23.315276349945528, -125.68498414493897, 32.16795578633858, 27.353653848985825, -100.18606084116595, -9.982470982930854, -71.04823052779166, -31.26627925078696, -30.3749177017662, -18.479886873909912, -8.814653659052217, 53.806495660831025, 59.48446484370838, -96.95677284185486, 32.37978991513198]\n",
    "test_walker = [-1241.2286926378722, -1160.2450865632702, -1302.5257279430775, -1321.1742109382649, -1188.933183313903, -705.0222037711366, -1015.6534497409206, -1223.086064483041, -581.1125714915169, -1094.4109430023177, -1263.2555011234258, 606.2516319491602, 347.13141169409215, 421.8291469984875, -1162.114098068984, -1182.7265993365133, -1263.46067056635, 426.85376063362065, 654.2950759629778, 200.48967327397975, 89.62557959415612, 770.944369011869]\n",
    "test_hopper = [19.39352412420114, 32.04291391761344, 71.05704872371435, 210.83129950012804, 70.8467080637385, 137.4572230840752, 40.47588962083239, 388.04944066645714, 70.29350711982096, 213.6719275538449, 81.97411059718404, 141.17601595789915, 151.4677262172483, 102.80866475250083, 159.70545355726733, 464.72625602344385, 225.03651377538398, 154.7150833974447, 364.8459397128754, 452.7572393095682]\n",
    "test_half = [17.013435360947565, 40.15109587138687, 21.430784166587966, 24.17196539273257, 82.36950376110435, 29.22265959028815, 65.57115237712492, 34.345782755340814, 42.59285357176953, 78.61830403883336, 62.88512472101623, 58.8132956755824, 55.83413705380583, 60.73374486486313, 51.154639767290234, 50.608121683934876, 45.42836724520008, 46.15240933758517, 39.74357310803997, 37.22987510561556]\n",
    "test_ant = [-607.0543385673287, -806.4580132064702, -706.9388296694813, -704.0868158508686, -854.8355523125322, -495.4331999439704, -465.9610431312168, -912.1594922921411, -708.5099486672142, -911.8392636016562, -1027.8732196732442, -998.3502355247043, -1215.182292635422, -590.4106673332353, -239.19080561896124, -455.50458913759735, -737.5466495914256, 339.8731203235975, 332.31882261535577, 116.15955559290299]\n",
    "test_human = [-31.24164405269406, -80.74274846982719, -42.52495495778028, -15.498939063731273, -53.60286526966865, -71.91186545845049, -19.914671887012624, -30.555686888870962, -37.57695954431116, -8.791994321518787, -14.27225190059264, -4.557804993246706, -23.2121430505511, -23.864069365425404, -4.901172084530374, -5.802280785444951, 10.5387475304617476, 14.313701621168237, 15.540979307335364, 7.048698961362165]\n",
    "\n",
    "foward_records = [test_bipedalwalker, test_half, test_hopper, test_walker, test_ant, test_human]\n",
    "#\"\"\"\n",
    "\n",
    "test_bco_bipedal = [-21.622981899611602, -50.45265779702035, -14.964480684141027, -3.974716434900671, -85.41978882118583, -85.41538556643499, -84.24169132222157, -87.91838652590553, -85.30914249103455, -55.75293002613003, -16.839369788537105, -87.10465359533988, -77.72110420887255, -64.78834819576186, -55.42264371588144, -55.08960786631849, -16.98840366882085, -29.061604524564615, -73.68085866489076, -24.15669209049675]\n",
    "\n",
    "test_bco_walker = [17.05794454679845, -1.152118400453911, 73.31450239102124, 43.82633083314189, 45.05796391713242, 40.80798365452466, 19.01886243725472, 35.60101467668996, 19.582562413734195, 42.536604598793204, 47.87169106576544, 19.599122156608583, 39.00406759191564, 78.15280722142549, 76.47825263079336, 28.527372805199242, 20.286506336651456, 82.01481704865347, 26.696482407852393, 17.68209225587311]\n",
    "\n",
    "test_bco_hopper = [20.340960391015876, -7.5957462968471665, -5.64952268313829, 39.17338988423048, 34.17768875810082, 105.5906503018052, 58.99896630055687, 70.56245956803333, 45.69051809440425, 110.42888658674603, 131.70598534916468, 75.33278338056694, 76.858474227848, 233.44472073642584, 231.76878421835434, 115.70124689468126, 225.17604362895835, 100.70042132769284, 45.59691453532076, 158.23814460606152]\n",
    "\n",
    "test_bco_half = [-584.6469779468895, -790.1563856982439, -738.3381516537241, -303.06726756870256, 51.336031873359694, -558.3819529517864, -705.0943268275055, -232.6737508441126, -686.7766340204014, -724.4446733457228, -568.8822996880676, -774.1916697368074, -628.9576601929583, -641.522208993676, -737.9679958551051, -522.7748998468383, -761.5455540991205, -810.5939675764698, -730.5554652859933, -424.63264384175324]\n",
    "\n",
    "test_bco_ant = [99.31250812622015, 110.30787982852996, 154.36258293253405, 196.4705833228953, 219.81495326495863, 81.4895250369373, -73.91483945067316, -4.549355839342006, -459.2008512168189, -125.78358223159816, -216.31058123705515, -58.68871890729858, -264.86666464125386, -1481.982335698971, -2769.588672446682, -888.5039450961544, -940.2829805759836, -1274.6549564888778, -752.758337622611, -658.1891863229729]\n",
    "\n",
    "test_bco_human = [-31.129122906622463, -70.2628989337789, -94.79358454540414, -23.01149874768736, -22.04257649423564, -24.244113261252757, -27.818875505115006, -43.40152464929519, -27.349353352093015, -35.530143053229, -54.14535895695569, -19.235037690635433, -45.05351322031206, -25.952533264753992, -45.002882376082226, -21.464234956854877, -22.752502099574624, 3.393028267252964, -56.258879656719834, -79.67432940683796]\n",
    "\n",
    "bco_records = [test_bco_bipedal,test_bco_walker,test_bco_hopper,test_bco_half,test_bco_ant,test_bco_human]\n",
    "\n",
    "env_list = [\"Pendulum-v0\", \"BipedalWalker-v3\", \"Walker2DBulletEnv-v0\", \"HopperBulletEnv-v0\", \"HalfCheetahBulletEnv-v0\", \"AntBulletEnv-v0\", \"HumanoidBulletEnv-v0\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for r in range(len(foward_records)):\n",
    "    plot_simple_bco(foward_records[r], bco_records[r], env_list[r+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.random.rand(ypoints[1:].shape[0])-0.7)*200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_bco(test_pendulum, test_pendulum_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_bco(test_walker, test_pendulum_f, s=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-banner",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_bco(test_half, test_pendulum_f, s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_bco(test_bipedalwalker, test_pendulum_f, s=10, off=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-dubai",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-rouge",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
