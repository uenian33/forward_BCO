{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "medieval-waste",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pybullet in /Users/user/.local/lib/python3.7/site-packages (3.1.7)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/Users/user/anaconda3/envs/pwil/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pybullet --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caring-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch_optimizer as th_optim\n",
    "import pybullet_envs\n",
    "\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch.distributions import Normal\n",
    "from models import *\n",
    "from utils import *\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "norman-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_simple_bco(records, records2=None, env_name=None, s=100, off=0.8):\n",
    "    ypoints = np.array(records)\n",
    "    plt.plot(ypoints)\n",
    "    if records2 is not None:\n",
    "        ypoints = np.array(records2)\n",
    "        plt.plot(ypoints, linestyle = 'dotted')\n",
    "    else:\n",
    "        ypoints[1:] = ypoints[1:] #- (np.random.rand(ypoints[1:].shape[0])-0.25)*2*s + (np.random.rand(ypoints[1:].shape[0])-off)*4*s\n",
    "    plt.title(env_name)\n",
    "    plt.xlabel('steps (10e3)')\n",
    "    plt.ylabel('reward')\n",
    "    plt.legend([\"Forward matching\", \"BC from observation\"], loc =\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "overall-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_model_training(transitions, inv_model, ep_num=100):\n",
    "    inv_dataset = transition_dataset(transitions)\n",
    "    #inv_dataset_list.append(inv_dataset)\n",
    "    #inv_dataset_final = ConcatDataset(inv_dataset_list)\n",
    "    inv_loader = DataLoader(inv_dataset, batch_size=1024, shuffle=True, num_workers=4)\n",
    "\n",
    "    #inv_opt = optim.Adam(inv_model.parameters(), lr=1e-3, weight_decay=0.0001)\n",
    "    #\"\"\"\n",
    "    inv_opt_yogi = th_optim.Yogi(\n",
    "        inv_model.parameters(),\n",
    "        lr= 1e-2,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-3,\n",
    "        initial_accumulator=1e-6,\n",
    "        weight_decay=0,\n",
    "    )\n",
    "\n",
    "    inv_opt = th_optim.Lookahead(inv_opt_yogi,  alpha=0.5)#k=5\n",
    "    #\"\"\"\n",
    "    inv_loss = nn.MSELoss()\n",
    "    #inv_loss = nn.L1Loss()\n",
    "\n",
    "    for epoch in range(ep_num): \n",
    "        running_loss = 0\n",
    "        for i, data in enumerate(inv_loader):\n",
    "            s, a, s_prime = data\n",
    "            inv_opt.zero_grad()\n",
    "            \"\"\"\n",
    "            a_pred = inv_model(s.float(), s_prime.float())\n",
    "            \"\"\"\n",
    "            #print(s.shape, a.shape, s_prime.shape, torch.cat((s.float(), a.float()), dim=1).shape)\n",
    "            try:\n",
    "                inputs = torch.cat((s.float(), a.float()), dim=1)\n",
    "                pred = inv_model.reparam_forward(inputs)\n",
    "            except:\n",
    "                sprime_m, sprime_v = inv_model(s.float(), a.float())\n",
    "                pred = Normal(sprime_m, sprime_v).rsample()\n",
    "            loss = inv_loss(pred, s_prime.float())\n",
    "            #loss2 = inv_model.mdn_loss(inputs, s_prime.float())\n",
    "            #loss = loss1 #+ loss2\n",
    "            #loss = inv_loss(a_pred, a.float())\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "            if i%100 == 99:\n",
    "                running_loss = 0\n",
    "            inv_opt.step()\n",
    "        if epoch%20==0:\n",
    "            print('Epoch:%d Batch:%d Loss:%.5f'%(epoch, i+1, loss))\n",
    "    print('Done!')\n",
    "    return inv_model\n",
    "\n",
    "def train_bc(trajs, policy, dynamics,  ep_num=50, sample_itr=500):\n",
    "    enable_dropout(dynamics)\n",
    "    \n",
    "    bc_dataset = imitation_dataset(trajs)\n",
    "    bc_loader = DataLoader(bc_dataset, batch_size=1024, shuffle=True, num_workers=4)\n",
    "    print('Learning policy....')\n",
    "    #bc_opt = optim.Adam(policy.parameters(), lr=1e-3, weight_decay=0.0001)\n",
    "    #\"\"\"\n",
    "    bc_opt_yogi = th_optim.Yogi(\n",
    "        policy.parameters(),\n",
    "        lr= 1e-2,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-3,\n",
    "        initial_accumulator=1e-6,\n",
    "        weight_decay=0,\n",
    "    )\n",
    "\n",
    "    bc_opt = th_optim.Lookahead(bc_opt_yogi,  alpha=0.5)#k=5,\n",
    "    #\"\"\"\n",
    "    bc_loss = nn.MSELoss()\n",
    "    # bc_loss = nn.L1Loss()\n",
    "\n",
    "    for epoch in range(ep_num):  \n",
    "        running_loss = 0\n",
    "        for i, data in enumerate(bc_loader):\n",
    "            s, s_prime = data\n",
    "            bc_opt.zero_grad()\n",
    "            \n",
    "            #\"\"\"\n",
    "            #a_pred = policy.reparam_forward(s.float())\n",
    "            try:\n",
    "                a_mu, a_sigma = policy(s.float())\n",
    "                a_pred = Normal(loc=a_mu, scale=a_sigma).rsample()\n",
    "            except:\n",
    "                a_pred = policy.reparam_forward(s.float())\n",
    "            #\"\"\"\n",
    "            #print(torch.cat((s, a_pred), dim=1).shape)\n",
    "            try:\n",
    "                preds = dynamics.reparam_forward(torch.cat((s, a_pred), dim=1)) \n",
    "            except:\n",
    "                d_m, d_v = dynamics(s, a_pred) \n",
    "                preds = Normal(d_m, d_v).rsample()\n",
    "            loss = bc_loss(preds, s_prime)\n",
    "            #print(loss, loss.shape, preds.shape, new_gts.shape)\n",
    "            \n",
    "            #for sid in range(sample_itr):\n",
    "                #\"\"\"\n",
    "                #a_mu, a_sigma = policy(s.float())\n",
    "                #a_pred = Normal(loc=a_mu, scale=a_sigma).rsample()\n",
    "                #\"\"\"\n",
    "                #s_m, s_v = dynamics(s, a_pred) \n",
    "                #print(pred.shape, s.shape, s_prime.shape)\n",
    "                #print(pred[0])\n",
    "                #loss = loss + bc_loss(preds[sid], s_prime)\n",
    "                \n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            if i%20 == 19:\n",
    "                running_loss = 0\n",
    "            bc_opt.step()\n",
    "        if epoch%10==0:\n",
    "            print('Epoch:%d Batch:%d Loss:%.3f'%(epoch, i+1, loss))\n",
    "\n",
    "    print('Done!')\n",
    "    return policy\n",
    "\n",
    "\n",
    "def load_demos(DEMO_DIR):\n",
    "    \"\"\"load demonstrations\"\"\"\n",
    "    try:\n",
    "        trajstrajs = np.load(\"experts/states_expert_walker_.npy\")[:10]\n",
    "    except:\n",
    "        with open(DEMO_DIR, 'rb') as f:\n",
    "            trajs = pickle.load(f)\n",
    "    demos = []\n",
    "    for t_id, traj in enumerate(trajs):\n",
    "        demo =[]\n",
    "        #print(t_id)\n",
    "        for item in traj:    \n",
    "            obs = item['observation']\n",
    "            #obs = list(obs)\n",
    "            #print(obs)\n",
    "            demo.append(obs)\n",
    "        #print(np.array(demo).shape)\n",
    "        demos.append(np.array(demo))\n",
    "\n",
    "    print(np.array(demos).shape)\n",
    "    demos = demos[:10]\n",
    "    return demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sophisticated-enhancement",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# start BipedalWalker-v3 training ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:138: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49,)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -103.19613867788638 setps: 100 count: 100\n",
      "reward: -104.89378569665428 setps: 60 count: 160\n",
      "reward: -102.36979020965472 setps: 111 count: 271\n",
      "reward: -122.54565536505669 setps: 115 count: 386\n",
      "avg rewards: -108.25134248731301\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:78: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/Users/user/Documents/rl/practical_IL/state_prediction_rl/bco/BCO/dataset.py:33: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.x = np.array([s, a]).flatten()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:1 Loss:1.31611\n",
      "Epoch:20 Batch:1 Loss:0.16875\n",
      "Epoch:40 Batch:1 Loss:0.14042\n",
      "Epoch:60 Batch:1 Loss:0.11249\n",
      "Epoch:80 Batch:1 Loss:0.09480\n",
      "Epoch:100 Batch:1 Loss:0.08098\n",
      "Epoch:120 Batch:1 Loss:0.07282\n",
      "Epoch:140 Batch:1 Loss:0.06927\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.128\n",
      "Epoch:10 Batch:9 Loss:0.122\n",
      "Epoch:20 Batch:9 Loss:0.111\n",
      "Epoch:30 Batch:9 Loss:0.109\n",
      "Epoch:40 Batch:9 Loss:0.103\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -124.63664194877819 setps: 122 count: 122\n",
      "reward: -33.652918032096075 setps: 800 count: 922\n",
      "avg rewards: -79.14477999043713\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:1.25785\n",
      "Epoch:20 Batch:2 Loss:0.10733\n",
      "Epoch:40 Batch:2 Loss:0.08053\n",
      "Epoch:60 Batch:2 Loss:0.06250\n",
      "Epoch:80 Batch:2 Loss:0.05433\n",
      "Epoch:100 Batch:2 Loss:0.05032\n",
      "Epoch:120 Batch:2 Loss:0.04783\n",
      "Epoch:140 Batch:2 Loss:0.04286\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.095\n",
      "Epoch:10 Batch:9 Loss:0.086\n",
      "Epoch:20 Batch:9 Loss:0.086\n",
      "Epoch:30 Batch:9 Loss:0.082\n",
      "Epoch:40 Batch:9 Loss:0.086\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -108.9738522739311 setps: 35 count: 35\n",
      "reward: -113.33518387778228 setps: 45 count: 80\n",
      "reward: -112.27260589527091 setps: 40 count: 120\n",
      "reward: -110.74987799884566 setps: 57 count: 177\n",
      "reward: -109.54826427920722 setps: 43 count: 220\n",
      "reward: -109.98353671845234 setps: 36 count: 256\n",
      "reward: -112.87875097584477 setps: 45 count: 301\n",
      "reward: -114.1355162600788 setps: 46 count: 347\n",
      "avg rewards: -111.48469853492664\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:1.04806\n",
      "Epoch:20 Batch:3 Loss:0.08510\n",
      "Epoch:40 Batch:3 Loss:0.06022\n",
      "Epoch:60 Batch:3 Loss:0.04728\n",
      "Epoch:80 Batch:3 Loss:0.04145\n",
      "Epoch:100 Batch:3 Loss:0.03703\n",
      "Epoch:120 Batch:3 Loss:0.03456\n",
      "Epoch:140 Batch:3 Loss:0.03366\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.081\n",
      "Epoch:10 Batch:9 Loss:0.075\n",
      "Epoch:20 Batch:9 Loss:0.079\n",
      "Epoch:30 Batch:9 Loss:0.074\n",
      "Epoch:40 Batch:9 Loss:0.074\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -110.82877092650284 setps: 56 count: 56\n",
      "reward: -114.07752134517767 setps: 61 count: 117\n",
      "reward: -111.65954242454531 setps: 57 count: 174\n",
      "reward: -108.54465779174554 setps: 44 count: 218\n",
      "reward: -118.91345622632466 setps: 62 count: 280\n",
      "reward: -110.95921735867424 setps: 57 count: 337\n",
      "reward: -119.06863279041585 setps: 72 count: 409\n",
      "avg rewards: -113.435971266198\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.96930\n",
      "Epoch:20 Batch:4 Loss:0.07407\n",
      "Epoch:40 Batch:4 Loss:0.05618\n",
      "Epoch:60 Batch:4 Loss:0.04440\n",
      "Epoch:80 Batch:4 Loss:0.04254\n",
      "Epoch:100 Batch:4 Loss:0.03873\n",
      "Epoch:120 Batch:4 Loss:0.03775\n",
      "Epoch:140 Batch:4 Loss:0.03447\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.068\n",
      "Epoch:10 Batch:9 Loss:0.072\n",
      "Epoch:20 Batch:9 Loss:0.066\n",
      "Epoch:30 Batch:9 Loss:0.066\n",
      "Epoch:40 Batch:9 Loss:0.066\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -18.7180545048627 setps: 800 count: 800\n",
      "avg rewards: -18.7180545048627\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.88270\n",
      "Epoch:20 Batch:5 Loss:0.06210\n",
      "Epoch:40 Batch:5 Loss:0.05076\n",
      "Epoch:60 Batch:5 Loss:0.04228\n",
      "Epoch:80 Batch:5 Loss:0.03861\n",
      "Epoch:100 Batch:5 Loss:0.03589\n",
      "Epoch:120 Batch:5 Loss:0.03466\n",
      "Epoch:140 Batch:5 Loss:0.03245\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.062\n",
      "Epoch:10 Batch:9 Loss:0.067\n",
      "Epoch:20 Batch:9 Loss:0.063\n",
      "Epoch:30 Batch:9 Loss:0.063\n",
      "Epoch:40 Batch:9 Loss:0.064\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -115.50497000216754 setps: 55 count: 55\n",
      "reward: -109.24439298569722 setps: 50 count: 105\n",
      "reward: -114.1108023145025 setps: 53 count: 158\n",
      "reward: -114.95922994790847 setps: 52 count: 210\n",
      "reward: -115.41634773843735 setps: 58 count: 268\n",
      "reward: -116.03836012779983 setps: 58 count: 326\n",
      "reward: -113.50118164808303 setps: 51 count: 377\n",
      "reward: -114.60077379962243 setps: 58 count: 435\n",
      "reward: -113.2459607009751 setps: 51 count: 486\n",
      "reward: -108.97950823982247 setps: 49 count: 535\n",
      "reward: -108.94105360638599 setps: 51 count: 586\n",
      "reward: -119.38196545909034 setps: 57 count: 643\n",
      "reward: -114.38794407843737 setps: 58 count: 701\n",
      "reward: -121.09387745888034 setps: 67 count: 768\n",
      "reward: -120.91449949374547 setps: 59 count: 827\n",
      "reward: -114.5689350272386 setps: 58 count: 885\n",
      "reward: -112.54918281863816 setps: 54 count: 939\n",
      "reward: -112.01661469999638 setps: 53 count: 992\n",
      "avg rewards: -114.41420000819046\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.68145\n",
      "Epoch:20 Batch:6 Loss:0.06047\n",
      "Epoch:40 Batch:6 Loss:0.04602\n",
      "Epoch:60 Batch:6 Loss:0.03989\n",
      "Epoch:80 Batch:6 Loss:0.03698\n",
      "Epoch:100 Batch:6 Loss:0.03428\n",
      "Epoch:120 Batch:6 Loss:0.03354\n",
      "Epoch:140 Batch:6 Loss:0.03270\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.065\n",
      "Epoch:10 Batch:9 Loss:0.068\n",
      "Epoch:20 Batch:9 Loss:0.068\n",
      "Epoch:30 Batch:9 Loss:0.064\n",
      "Epoch:40 Batch:9 Loss:0.062\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -114.56909780461092 setps: 59 count: 59\n",
      "reward: -121.46678349803389 setps: 95 count: 154\n",
      "reward: -115.51259483106868 setps: 60 count: 214\n",
      "reward: -112.75164914843565 setps: 55 count: 269\n",
      "reward: -109.56127493400002 setps: 43 count: 312\n",
      "avg rewards: -114.77228004322983\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.90123\n",
      "Epoch:20 Batch:7 Loss:0.05873\n",
      "Epoch:40 Batch:7 Loss:0.04437\n",
      "Epoch:60 Batch:7 Loss:0.03727\n",
      "Epoch:80 Batch:7 Loss:0.03656\n",
      "Epoch:100 Batch:7 Loss:0.03215\n",
      "Epoch:120 Batch:7 Loss:0.03056\n",
      "Epoch:140 Batch:7 Loss:0.03055\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.063\n",
      "Epoch:10 Batch:9 Loss:0.065\n",
      "Epoch:20 Batch:9 Loss:0.060\n",
      "Epoch:30 Batch:9 Loss:0.064\n",
      "Epoch:40 Batch:9 Loss:0.060\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -111.6942530274801 setps: 60 count: 60\n",
      "reward: -14.476467674675073 setps: 800 count: 860\n",
      "avg rewards: -63.08536035107759\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.57370\n",
      "Epoch:20 Batch:8 Loss:0.05190\n",
      "Epoch:40 Batch:8 Loss:0.04127\n",
      "Epoch:60 Batch:8 Loss:0.03604\n",
      "Epoch:80 Batch:8 Loss:0.03271\n",
      "Epoch:100 Batch:8 Loss:0.03116\n",
      "Epoch:120 Batch:8 Loss:0.03013\n",
      "Epoch:140 Batch:8 Loss:0.02799\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.061\n",
      "Epoch:10 Batch:9 Loss:0.063\n",
      "Epoch:20 Batch:9 Loss:0.060\n",
      "Epoch:30 Batch:9 Loss:0.059\n",
      "Epoch:40 Batch:9 Loss:0.063\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -110.22781469564575 setps: 60 count: 60\n",
      "reward: -110.97958780076914 setps: 63 count: 123\n",
      "reward: -106.32081282498315 setps: 57 count: 180\n",
      "reward: -13.367036625093231 setps: 800 count: 980\n",
      "avg rewards: -85.22381298662282\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.34021\n",
      "Epoch:20 Batch:9 Loss:0.05112\n",
      "Epoch:40 Batch:9 Loss:0.04018\n",
      "Epoch:60 Batch:9 Loss:0.03574\n",
      "Epoch:80 Batch:9 Loss:0.03369\n",
      "Epoch:100 Batch:9 Loss:0.03120\n",
      "Epoch:120 Batch:9 Loss:0.02922\n",
      "Epoch:140 Batch:9 Loss:0.02909\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.059\n",
      "Epoch:10 Batch:9 Loss:0.061\n",
      "Epoch:20 Batch:9 Loss:0.058\n",
      "Epoch:30 Batch:9 Loss:0.055\n",
      "Epoch:40 Batch:9 Loss:0.059\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -107.55622073831347 setps: 55 count: 55\n",
      "reward: -115.81724541945383 setps: 57 count: 112\n",
      "reward: -109.67970178544118 setps: 49 count: 161\n",
      "reward: -112.76784189577153 setps: 63 count: 224\n",
      "reward: -111.37738384967608 setps: 65 count: 289\n",
      "reward: -109.70472018889586 setps: 61 count: 350\n",
      "reward: -109.16945272365027 setps: 51 count: 401\n",
      "reward: -106.67862223466486 setps: 52 count: 453\n",
      "reward: -106.43027479515722 setps: 52 count: 505\n",
      "reward: -108.40771009090977 setps: 58 count: 563\n",
      "avg rewards: -109.7589173721934\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:10 Loss:0.30300\n",
      "Epoch:20 Batch:10 Loss:0.04971\n",
      "Epoch:40 Batch:10 Loss:0.03900\n",
      "Epoch:60 Batch:10 Loss:0.03442\n",
      "Epoch:80 Batch:10 Loss:0.03213\n",
      "Epoch:100 Batch:10 Loss:0.03129\n",
      "Epoch:120 Batch:10 Loss:0.02885\n",
      "Epoch:140 Batch:10 Loss:0.02896\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.058\n",
      "Epoch:10 Batch:9 Loss:0.057\n",
      "Epoch:20 Batch:9 Loss:0.056\n",
      "Epoch:30 Batch:9 Loss:0.059\n",
      "Epoch:40 Batch:9 Loss:0.058\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -122.68223219709596 setps: 85 count: 85\n",
      "reward: -111.11971743888905 setps: 48 count: 133\n",
      "reward: -109.28898411481642 setps: 48 count: 181\n",
      "reward: -119.69879151639435 setps: 69 count: 250\n",
      "reward: -121.45125848980621 setps: 65 count: 315\n",
      "reward: -120.40150232761727 setps: 63 count: 378\n",
      "reward: -107.22516276257485 setps: 58 count: 436\n",
      "reward: -120.75119744300035 setps: 64 count: 500\n",
      "reward: -111.92170123736064 setps: 53 count: 553\n",
      "reward: -119.11269642802824 setps: 104 count: 657\n",
      "reward: -116.51405472230725 setps: 78 count: 735\n",
      "avg rewards: -116.37884533435367\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.30276\n",
      "Epoch:20 Batch:11 Loss:0.04677\n",
      "Epoch:40 Batch:11 Loss:0.03957\n",
      "Epoch:60 Batch:11 Loss:0.03557\n",
      "Epoch:80 Batch:11 Loss:0.03274\n",
      "Epoch:100 Batch:11 Loss:0.03071\n",
      "Epoch:120 Batch:11 Loss:0.02875\n",
      "Epoch:140 Batch:11 Loss:0.02815\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.054\n",
      "Epoch:10 Batch:9 Loss:0.056\n",
      "Epoch:20 Batch:9 Loss:0.055\n",
      "Epoch:30 Batch:9 Loss:0.055\n",
      "Epoch:40 Batch:9 Loss:0.055\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -116.39034762240635 setps: 55 count: 55\n",
      "reward: -110.63076291706227 setps: 51 count: 106\n",
      "reward: -108.65675922462158 setps: 48 count: 154\n",
      "reward: -119.14853766302454 setps: 65 count: 219\n",
      "reward: -109.8272624948329 setps: 48 count: 267\n",
      "reward: -121.36773244647372 setps: 85 count: 352\n",
      "reward: -96.77867467269239 setps: 126 count: 478\n",
      "reward: -110.89830936339187 setps: 52 count: 530\n",
      "reward: -110.96719409705823 setps: 48 count: 578\n",
      "reward: -120.12736331861218 setps: 64 count: 642\n",
      "reward: -109.81856764506114 setps: 55 count: 697\n",
      "reward: -122.39253196303548 setps: 77 count: 774\n",
      "reward: -121.46946260406945 setps: 76 count: 850\n",
      "reward: -117.90247094981869 setps: 63 count: 913\n",
      "avg rewards: -114.02685549872578\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.36778\n",
      "Epoch:20 Batch:12 Loss:0.04815\n",
      "Epoch:40 Batch:12 Loss:0.03849\n",
      "Epoch:60 Batch:12 Loss:0.03383\n",
      "Epoch:80 Batch:12 Loss:0.03187\n",
      "Epoch:100 Batch:12 Loss:0.02986\n",
      "Epoch:120 Batch:12 Loss:0.03009\n",
      "Epoch:140 Batch:12 Loss:0.02607\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.056\n",
      "Epoch:10 Batch:9 Loss:0.052\n",
      "Epoch:20 Batch:9 Loss:0.058\n",
      "Epoch:30 Batch:9 Loss:0.053\n",
      "Epoch:40 Batch:9 Loss:0.051\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -113.66518467624424 setps: 52 count: 52\n",
      "reward: -119.41212931882652 setps: 71 count: 123\n",
      "reward: -117.57032862225051 setps: 65 count: 188\n",
      "reward: -113.74415258463037 setps: 64 count: 252\n",
      "reward: -112.76652145831038 setps: 50 count: 302\n",
      "reward: -120.98734861713338 setps: 71 count: 373\n",
      "reward: -120.94941498269017 setps: 81 count: 454\n",
      "reward: -110.6909803697411 setps: 49 count: 503\n",
      "reward: -108.57764202910661 setps: 46 count: 549\n",
      "reward: -116.71759406997698 setps: 66 count: 615\n",
      "reward: -111.01295200661322 setps: 49 count: 664\n",
      "reward: -124.98943448777125 setps: 86 count: 750\n",
      "reward: -112.9396318005994 setps: 56 count: 806\n",
      "reward: -119.61916933748748 setps: 63 count: 869\n",
      "reward: -110.47547195651693 setps: 48 count: 917\n",
      "reward: -110.48848456098263 setps: 48 count: 965\n",
      "avg rewards: -115.28790255493007\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.29632\n",
      "Epoch:20 Batch:13 Loss:0.04505\n",
      "Epoch:40 Batch:13 Loss:0.03741\n",
      "Epoch:60 Batch:13 Loss:0.03197\n",
      "Epoch:80 Batch:13 Loss:0.03230\n",
      "Epoch:100 Batch:13 Loss:0.02952\n",
      "Epoch:120 Batch:13 Loss:0.02930\n",
      "Epoch:140 Batch:13 Loss:0.02719\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.058\n",
      "Epoch:10 Batch:9 Loss:0.059\n",
      "Epoch:20 Batch:9 Loss:0.054\n",
      "Epoch:30 Batch:9 Loss:0.058\n",
      "Epoch:40 Batch:9 Loss:0.051\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -18.773012913489694 setps: 800 count: 800\n",
      "avg rewards: -18.773012913489694\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.19718\n",
      "Epoch:20 Batch:14 Loss:0.04339\n",
      "Epoch:40 Batch:14 Loss:0.03764\n",
      "Epoch:60 Batch:14 Loss:0.03389\n",
      "Epoch:80 Batch:14 Loss:0.03069\n",
      "Epoch:100 Batch:14 Loss:0.03084\n",
      "Epoch:120 Batch:14 Loss:0.02825\n",
      "Epoch:140 Batch:14 Loss:0.02695\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.054\n",
      "Epoch:10 Batch:9 Loss:0.057\n",
      "Epoch:20 Batch:9 Loss:0.054\n",
      "Epoch:30 Batch:9 Loss:0.052\n",
      "Epoch:40 Batch:9 Loss:0.049\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -96.33796301517776 setps: 139 count: 139\n",
      "reward: -17.553683176627207 setps: 800 count: 939\n",
      "avg rewards: -56.94582309590248\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.26670\n",
      "Epoch:20 Batch:15 Loss:0.04518\n",
      "Epoch:40 Batch:15 Loss:0.03746\n",
      "Epoch:60 Batch:15 Loss:0.03533\n",
      "Epoch:80 Batch:15 Loss:0.03176\n",
      "Epoch:100 Batch:15 Loss:0.03044\n",
      "Epoch:120 Batch:15 Loss:0.02791\n",
      "Epoch:140 Batch:15 Loss:0.02655\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.053\n",
      "Epoch:10 Batch:9 Loss:0.052\n",
      "Epoch:20 Batch:9 Loss:0.056\n",
      "Epoch:30 Batch:9 Loss:0.055\n",
      "Epoch:40 Batch:9 Loss:0.053\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 3.2181557928733566 setps: 800 count: 800\n",
      "avg rewards: 3.2181557928733566\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.25090\n",
      "Epoch:20 Batch:16 Loss:0.04415\n",
      "Epoch:40 Batch:16 Loss:0.03629\n",
      "Epoch:60 Batch:16 Loss:0.03345\n",
      "Epoch:80 Batch:16 Loss:0.02876\n",
      "Epoch:100 Batch:16 Loss:0.02947\n",
      "Epoch:120 Batch:16 Loss:0.02894\n",
      "Epoch:140 Batch:16 Loss:0.02617\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.052\n",
      "Epoch:10 Batch:9 Loss:0.050\n",
      "Epoch:20 Batch:9 Loss:0.048\n",
      "Epoch:30 Batch:9 Loss:0.053\n",
      "Epoch:40 Batch:9 Loss:0.050\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 30.405643147849442 setps: 800 count: 800\n",
      "avg rewards: 30.405643147849442\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.18828\n",
      "Epoch:20 Batch:17 Loss:0.04272\n",
      "Epoch:40 Batch:17 Loss:0.03510\n",
      "Epoch:60 Batch:17 Loss:0.03305\n",
      "Epoch:80 Batch:17 Loss:0.02944\n",
      "Epoch:100 Batch:17 Loss:0.02782\n",
      "Epoch:120 Batch:17 Loss:0.02936\n",
      "Epoch:140 Batch:17 Loss:0.02861\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.050\n",
      "Epoch:10 Batch:9 Loss:0.047\n",
      "Epoch:20 Batch:9 Loss:0.051\n",
      "Epoch:30 Batch:9 Loss:0.046\n",
      "Epoch:40 Batch:9 Loss:0.048\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -118.2263650259028 setps: 63 count: 63\n",
      "reward: 86.32633503491381 setps: 800 count: 863\n",
      "avg rewards: -15.950014995494492\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.16242\n",
      "Epoch:20 Batch:18 Loss:0.04270\n",
      "Epoch:40 Batch:18 Loss:0.03523\n",
      "Epoch:60 Batch:18 Loss:0.03380\n",
      "Epoch:80 Batch:18 Loss:0.03026\n",
      "Epoch:100 Batch:18 Loss:0.02906\n",
      "Epoch:120 Batch:18 Loss:0.02686\n",
      "Epoch:140 Batch:18 Loss:0.02770\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.045\n",
      "Epoch:10 Batch:9 Loss:0.047\n",
      "Epoch:20 Batch:9 Loss:0.042\n",
      "Epoch:30 Batch:9 Loss:0.044\n",
      "Epoch:40 Batch:9 Loss:0.046\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 96.9152429485352 setps: 800 count: 800\n",
      "reward: -87.54971627786072 setps: 132 count: 932\n",
      "reward: -119.87969567038492 setps: 65 count: 997\n",
      "avg rewards: -36.838056333236814\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.15833\n",
      "Epoch:20 Batch:19 Loss:0.04332\n",
      "Epoch:40 Batch:19 Loss:0.03419\n",
      "Epoch:60 Batch:19 Loss:0.03406\n",
      "Epoch:80 Batch:19 Loss:0.03142\n",
      "Epoch:100 Batch:19 Loss:0.02964\n",
      "Epoch:120 Batch:19 Loss:0.02890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:140 Batch:19 Loss:0.02795\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.042\n",
      "Epoch:10 Batch:9 Loss:0.046\n",
      "Epoch:20 Batch:9 Loss:0.039\n",
      "Epoch:30 Batch:9 Loss:0.043\n",
      "Epoch:40 Batch:9 Loss:0.041\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 174.0506602534159 setps: 800 count: 800\n",
      "avg rewards: 174.0506602534159\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.15896\n",
      "Epoch:20 Batch:20 Loss:0.04056\n",
      "Epoch:40 Batch:20 Loss:0.03471\n",
      "Epoch:60 Batch:20 Loss:0.03179\n",
      "Epoch:80 Batch:20 Loss:0.03023\n",
      "Epoch:100 Batch:20 Loss:0.02701\n",
      "Epoch:120 Batch:20 Loss:0.02757\n",
      "Epoch:140 Batch:20 Loss:0.02669\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.036\n",
      "Epoch:10 Batch:9 Loss:0.034\n",
      "Epoch:20 Batch:9 Loss:0.037\n",
      "Epoch:30 Batch:9 Loss:0.037\n",
      "Epoch:40 Batch:9 Loss:0.033\n",
      "Done!\n",
      "############# start Walker2DBulletEnv-v0 training ###################\n",
      "(49, 1000, 22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 13.612366641615518 setps: 9 count: 9\n",
      "reward: 15.333661909894728 setps: 11 count: 20\n",
      "reward: 14.06518672312959 setps: 10 count: 30\n",
      "reward: 22.825537883289506 setps: 26 count: 56\n",
      "reward: 20.030772349816107 setps: 20 count: 76\n",
      "reward: 16.999050246212573 setps: 16 count: 92\n",
      "reward: 15.545413191551052 setps: 14 count: 106\n",
      "reward: 16.226392472333103 setps: 13 count: 119\n",
      "reward: 14.878472657156816 setps: 18 count: 137\n",
      "reward: 17.093303792305232 setps: 15 count: 152\n",
      "reward: 25.38579114184395 setps: 28 count: 180\n",
      "reward: 15.62311114539043 setps: 12 count: 192\n",
      "reward: 25.67762851235602 setps: 21 count: 213\n",
      "reward: 27.122905671574703 setps: 28 count: 241\n",
      "reward: 16.43561664125446 setps: 12 count: 253\n",
      "reward: 19.666378375614293 setps: 19 count: 272\n",
      "reward: 13.895775478085852 setps: 9 count: 281\n",
      "reward: 15.099087875016266 setps: 10 count: 291\n",
      "reward: 17.484739339732915 setps: 15 count: 306\n",
      "reward: 16.458311988427884 setps: 13 count: 319\n",
      "reward: 24.631510851769416 setps: 23 count: 342\n",
      "reward: 17.27214320818748 setps: 13 count: 355\n",
      "reward: 14.835133212765506 setps: 11 count: 366\n",
      "reward: 15.280416195635917 setps: 11 count: 377\n",
      "reward: 24.255114293211953 setps: 25 count: 402\n",
      "reward: 16.724412600457438 setps: 12 count: 414\n",
      "reward: 17.03025952757744 setps: 13 count: 427\n",
      "reward: 14.916645828922626 setps: 11 count: 438\n",
      "reward: 14.978043961331423 setps: 10 count: 448\n",
      "reward: 16.548853618519203 setps: 17 count: 465\n",
      "reward: 15.808945510665946 setps: 12 count: 477\n",
      "reward: 19.61333122198557 setps: 19 count: 496\n",
      "reward: 17.831681481759002 setps: 15 count: 511\n",
      "reward: 19.90174298195634 setps: 19 count: 530\n",
      "reward: 18.793621826905294 setps: 15 count: 545\n",
      "reward: 13.583168764971196 setps: 9 count: 554\n",
      "reward: 15.982339450970178 setps: 12 count: 566\n",
      "reward: 13.074903366122454 setps: 8 count: 574\n",
      "reward: 17.861683710719806 setps: 13 count: 587\n",
      "reward: 14.566726174604263 setps: 13 count: 600\n",
      "reward: 15.696694286954878 setps: 13 count: 613\n",
      "reward: 19.669800819159715 setps: 15 count: 628\n",
      "reward: 16.03928580185311 setps: 11 count: 639\n",
      "reward: 15.914378718014632 setps: 11 count: 650\n",
      "reward: 13.102440853048757 setps: 7 count: 657\n",
      "reward: 17.618380022801286 setps: 16 count: 673\n",
      "reward: 19.715804762375775 setps: 15 count: 688\n",
      "reward: 17.94201492472348 setps: 13 count: 701\n",
      "reward: 12.085757447723882 setps: 8 count: 709\n",
      "reward: 15.273487867224322 setps: 11 count: 720\n",
      "reward: 17.834171069120927 setps: 18 count: 738\n",
      "reward: 18.715817689345567 setps: 17 count: 755\n",
      "reward: 14.797437266143971 setps: 12 count: 767\n",
      "reward: 12.104907780767828 setps: 8 count: 775\n",
      "reward: 16.28500750227686 setps: 12 count: 787\n",
      "reward: 26.443078579950086 setps: 33 count: 820\n",
      "reward: 21.864974904285916 setps: 21 count: 841\n",
      "reward: 16.940205392897767 setps: 13 count: 854\n",
      "reward: 15.434112157528578 setps: 12 count: 866\n",
      "reward: 12.747652808242128 setps: 8 count: 874\n",
      "reward: 24.63804462880071 setps: 39 count: 913\n",
      "reward: 13.530255509416747 setps: 8 count: 921\n",
      "reward: 19.585226617201993 setps: 22 count: 943\n",
      "reward: 21.90765792617749 setps: 24 count: 967\n",
      "reward: 17.693504198580918 setps: 15 count: 982\n",
      "reward: 14.033769243680581 setps: 8 count: 990\n",
      "reward: 13.813665551440499 setps: 10 count: 1000\n",
      "avg rewards: 17.378771882916087\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:78: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/Users/user/Documents/rl/practical_IL/state_prediction_rl/bco/BCO/dataset.py:33: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.x = np.array([s, a]).flatten()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:1 Loss:1.32712\n",
      "Epoch:20 Batch:1 Loss:0.12492\n",
      "Epoch:40 Batch:1 Loss:0.09037\n",
      "Epoch:60 Batch:1 Loss:0.06754\n",
      "Epoch:80 Batch:1 Loss:0.05568\n",
      "Epoch:100 Batch:1 Loss:0.05001\n",
      "Epoch:120 Batch:1 Loss:0.04576\n",
      "Epoch:140 Batch:1 Loss:0.04281\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.106\n",
      "Epoch:10 Batch:10 Loss:0.107\n",
      "Epoch:20 Batch:10 Loss:0.107\n",
      "Epoch:30 Batch:10 Loss:0.104\n",
      "Epoch:40 Batch:10 Loss:0.102\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 375.9841329031508 setps: 800 count: 800\n",
      "reward: 15.187196978322742 setps: 77 count: 877\n",
      "reward: 30.422413327236427 setps: 117 count: 994\n",
      "avg rewards: 140.53124773623665\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:1.21024\n",
      "Epoch:20 Batch:2 Loss:0.07685\n",
      "Epoch:40 Batch:2 Loss:0.04856\n",
      "Epoch:60 Batch:2 Loss:0.03600\n",
      "Epoch:80 Batch:2 Loss:0.03311\n",
      "Epoch:100 Batch:2 Loss:0.02788\n",
      "Epoch:120 Batch:2 Loss:0.02555\n",
      "Epoch:140 Batch:2 Loss:0.02286\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.100\n",
      "Epoch:10 Batch:10 Loss:0.096\n",
      "Epoch:20 Batch:10 Loss:0.097\n",
      "Epoch:30 Batch:10 Loss:0.098\n",
      "Epoch:40 Batch:10 Loss:0.096\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 19.259802156397193 setps: 82 count: 82\n",
      "reward: 34.41974431977722 setps: 116 count: 198\n",
      "reward: 21.860925211112768 setps: 83 count: 281\n",
      "reward: 24.998139286677183 setps: 95 count: 376\n",
      "reward: 44.81814672976617 setps: 135 count: 511\n",
      "reward: 23.859894813272682 setps: 88 count: 599\n",
      "avg rewards: 28.202775419500536\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:1.10560\n",
      "Epoch:20 Batch:3 Loss:0.05490\n",
      "Epoch:40 Batch:3 Loss:0.03533\n",
      "Epoch:60 Batch:3 Loss:0.02606\n",
      "Epoch:80 Batch:3 Loss:0.02194\n",
      "Epoch:100 Batch:3 Loss:0.01971\n",
      "Epoch:120 Batch:3 Loss:0.01888\n",
      "Epoch:140 Batch:3 Loss:0.01671\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.086\n",
      "Epoch:10 Batch:10 Loss:0.085\n",
      "Epoch:20 Batch:10 Loss:0.082\n",
      "Epoch:30 Batch:10 Loss:0.083\n",
      "Epoch:40 Batch:10 Loss:0.083\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 25.934525399388804 setps: 79 count: 79\n",
      "reward: 34.751195576413 setps: 113 count: 192\n",
      "reward: 115.98300713292997 setps: 273 count: 465\n",
      "reward: 24.882286603085245 setps: 82 count: 547\n",
      "avg rewards: 50.38775367795426\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.95748\n",
      "Epoch:20 Batch:4 Loss:0.03987\n",
      "Epoch:40 Batch:4 Loss:0.02905\n",
      "Epoch:60 Batch:4 Loss:0.01920\n",
      "Epoch:80 Batch:4 Loss:0.01633\n",
      "Epoch:100 Batch:4 Loss:0.01401\n",
      "Epoch:120 Batch:4 Loss:0.01363\n",
      "Epoch:140 Batch:4 Loss:0.01322\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.094\n",
      "Epoch:10 Batch:10 Loss:0.091\n",
      "Epoch:20 Batch:10 Loss:0.091\n",
      "Epoch:30 Batch:10 Loss:0.091\n",
      "Epoch:40 Batch:10 Loss:0.088\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 85.77958633592642 setps: 69 count: 69\n",
      "reward: 73.60387743353498 setps: 53 count: 122\n",
      "reward: 86.93248097242324 setps: 65 count: 187\n",
      "reward: 97.37444711187712 setps: 137 count: 324\n",
      "reward: 58.07038315393584 setps: 89 count: 413\n",
      "reward: 70.16855332591369 setps: 62 count: 475\n",
      "reward: 52.43491163726721 setps: 51 count: 526\n",
      "reward: 67.84969055691474 setps: 100 count: 626\n",
      "reward: 84.42635298633832 setps: 80 count: 706\n",
      "reward: 34.770561245513086 setps: 57 count: 763\n",
      "reward: 57.989373137924126 setps: 57 count: 820\n",
      "reward: 58.36348265032579 setps: 88 count: 908\n",
      "reward: 98.64305011095568 setps: 78 count: 986\n",
      "avg rewards: 71.26205774298847\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.65960\n",
      "Epoch:20 Batch:5 Loss:0.04156\n",
      "Epoch:40 Batch:5 Loss:0.02418\n",
      "Epoch:60 Batch:5 Loss:0.01702\n",
      "Epoch:80 Batch:5 Loss:0.01396\n",
      "Epoch:100 Batch:5 Loss:0.01335\n",
      "Epoch:120 Batch:5 Loss:0.01188\n",
      "Epoch:140 Batch:5 Loss:0.01183\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.069\n",
      "Epoch:10 Batch:10 Loss:0.068\n",
      "Epoch:20 Batch:10 Loss:0.068\n",
      "Epoch:30 Batch:10 Loss:0.067\n",
      "Epoch:40 Batch:10 Loss:0.068\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 73.72950331532482 setps: 120 count: 120\n",
      "reward: 148.2035549919004 setps: 228 count: 348\n",
      "reward: 41.92728269437796 setps: 72 count: 420\n",
      "reward: 38.536068201452146 setps: 67 count: 487\n",
      "reward: 63.28761798062478 setps: 70 count: 557\n",
      "reward: 53.05087428274564 setps: 90 count: 647\n",
      "reward: 74.33970326963029 setps: 95 count: 742\n",
      "reward: 87.78854643149533 setps: 139 count: 881\n",
      "avg rewards: 72.60789389594393\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.32856\n",
      "Epoch:20 Batch:6 Loss:0.03371\n",
      "Epoch:40 Batch:6 Loss:0.01934\n",
      "Epoch:60 Batch:6 Loss:0.01437\n",
      "Epoch:80 Batch:6 Loss:0.01309\n",
      "Epoch:100 Batch:6 Loss:0.01146\n",
      "Epoch:120 Batch:6 Loss:0.01003\n",
      "Epoch:140 Batch:6 Loss:0.01058\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.071\n",
      "Epoch:10 Batch:10 Loss:0.070\n",
      "Epoch:20 Batch:10 Loss:0.070\n",
      "Epoch:30 Batch:10 Loss:0.069\n",
      "Epoch:40 Batch:10 Loss:0.071\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 50.4745369383032 setps: 84 count: 84\n",
      "reward: 46.221127854629614 setps: 102 count: 186\n",
      "reward: 31.392628914893432 setps: 70 count: 256\n",
      "reward: 56.351828872629255 setps: 119 count: 375\n",
      "reward: 229.31642167793703 setps: 369 count: 744\n",
      "reward: 39.64337349463167 setps: 63 count: 807\n",
      "reward: 29.13528935637732 setps: 52 count: 859\n",
      "reward: 22.65352475420805 setps: 54 count: 913\n",
      "reward: 32.04139598229668 setps: 77 count: 990\n",
      "avg rewards: 59.69223642732292\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.69555\n",
      "Epoch:20 Batch:7 Loss:0.02759\n",
      "Epoch:40 Batch:7 Loss:0.01535\n",
      "Epoch:60 Batch:7 Loss:0.01307\n",
      "Epoch:80 Batch:7 Loss:0.01176\n",
      "Epoch:100 Batch:7 Loss:0.01035\n",
      "Epoch:120 Batch:7 Loss:0.00976\n",
      "Epoch:140 Batch:7 Loss:0.00875\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.072\n",
      "Epoch:10 Batch:10 Loss:0.069\n",
      "Epoch:20 Batch:10 Loss:0.070\n",
      "Epoch:30 Batch:10 Loss:0.069\n",
      "Epoch:40 Batch:10 Loss:0.070\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 22.023903097778383 setps: 20 count: 20\n",
      "reward: 27.445330567967908 setps: 25 count: 45\n",
      "reward: 27.348078066678138 setps: 30 count: 75\n",
      "reward: 21.313673563896742 setps: 30 count: 105\n",
      "reward: 22.586928186111617 setps: 23 count: 128\n",
      "reward: 37.92618229635119 setps: 34 count: 162\n",
      "reward: 37.434548577494574 setps: 29 count: 191\n",
      "reward: 25.042179401188335 setps: 19 count: 210\n",
      "reward: 26.650282367650654 setps: 22 count: 232\n",
      "reward: 31.396853589941745 setps: 30 count: 262\n",
      "reward: 29.12738898684038 setps: 28 count: 290\n",
      "reward: 32.481612437397416 setps: 28 count: 318\n",
      "reward: 30.285443366572146 setps: 22 count: 340\n",
      "reward: 20.620274021648218 setps: 23 count: 363\n",
      "reward: 29.17497544552316 setps: 21 count: 384\n",
      "reward: 23.32393290257605 setps: 23 count: 407\n",
      "reward: 26.44554552194604 setps: 24 count: 431\n",
      "reward: 32.11784626722656 setps: 28 count: 459\n",
      "reward: 23.958934997220062 setps: 24 count: 483\n",
      "reward: 21.454675958667938 setps: 23 count: 506\n",
      "reward: 22.208040332766537 setps: 20 count: 526\n",
      "reward: 25.364642954182642 setps: 24 count: 550\n",
      "reward: 23.021739335784513 setps: 26 count: 576\n",
      "reward: 23.84619986045291 setps: 24 count: 600\n",
      "reward: 22.010386013575772 setps: 20 count: 620\n",
      "reward: 26.122896933565787 setps: 25 count: 645\n",
      "reward: 25.019336940311767 setps: 20 count: 665\n",
      "reward: 30.63277506089798 setps: 27 count: 692\n",
      "reward: 22.538869629931288 setps: 20 count: 712\n",
      "reward: 22.22192545997823 setps: 27 count: 739\n",
      "reward: 28.872501389651728 setps: 24 count: 763\n",
      "reward: 24.040629893745066 setps: 20 count: 783\n",
      "reward: 30.398804119063428 setps: 27 count: 810\n",
      "reward: 24.115907346394668 setps: 24 count: 834\n",
      "reward: 28.03212507631397 setps: 25 count: 859\n",
      "reward: 21.479422402814084 setps: 27 count: 886\n",
      "reward: 24.723978051562153 setps: 20 count: 906\n",
      "reward: 24.998012803787425 setps: 22 count: 928\n",
      "reward: 27.472206018073482 setps: 22 count: 950\n",
      "reward: 23.323543350762332 setps: 22 count: 972\n",
      "reward: 24.441973302411498 setps: 18 count: 990\n",
      "avg rewards: 26.171817216992793\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:8 Loss:0.30544\n",
      "Epoch:20 Batch:8 Loss:0.02345\n",
      "Epoch:40 Batch:8 Loss:0.01611\n",
      "Epoch:60 Batch:8 Loss:0.01280\n",
      "Epoch:80 Batch:8 Loss:0.01202\n",
      "Epoch:100 Batch:8 Loss:0.01137\n",
      "Epoch:120 Batch:8 Loss:0.01130\n",
      "Epoch:140 Batch:8 Loss:0.00916\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.066\n",
      "Epoch:10 Batch:10 Loss:0.066\n",
      "Epoch:20 Batch:10 Loss:0.066\n",
      "Epoch:30 Batch:10 Loss:0.067\n",
      "Epoch:40 Batch:10 Loss:0.065\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 61.462925302129584 setps: 69 count: 69\n",
      "reward: 23.84416537100478 setps: 58 count: 127\n",
      "reward: 91.62902930819979 setps: 95 count: 222\n",
      "reward: 97.82001563863449 setps: 107 count: 329\n",
      "reward: 30.93459662456299 setps: 59 count: 388\n",
      "reward: 160.37960956001697 setps: 182 count: 570\n",
      "reward: 43.177038768735656 setps: 93 count: 663\n",
      "reward: 44.71365358854094 setps: 81 count: 744\n",
      "reward: 11.59284916643082 setps: 50 count: 794\n",
      "reward: 45.62265938555937 setps: 76 count: 870\n",
      "reward: 47.78354121338054 setps: 82 count: 952\n",
      "avg rewards: 59.90546217519963\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.25952\n",
      "Epoch:20 Batch:9 Loss:0.02538\n",
      "Epoch:40 Batch:9 Loss:0.01575\n",
      "Epoch:60 Batch:9 Loss:0.01375\n",
      "Epoch:80 Batch:9 Loss:0.01129\n",
      "Epoch:100 Batch:9 Loss:0.00988\n",
      "Epoch:120 Batch:9 Loss:0.01082\n",
      "Epoch:140 Batch:9 Loss:0.00934\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.070\n",
      "Epoch:10 Batch:10 Loss:0.069\n",
      "Epoch:20 Batch:10 Loss:0.068\n",
      "Epoch:30 Batch:10 Loss:0.068\n",
      "Epoch:40 Batch:10 Loss:0.068\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 560.1629834719405 setps: 800 count: 800\n",
      "reward: 85.8728630279147 setps: 67 count: 867\n",
      "reward: 24.102883478351586 setps: 64 count: 931\n",
      "avg rewards: 223.37957665940226\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.19981\n",
      "Epoch:20 Batch:10 Loss:0.02181\n",
      "Epoch:40 Batch:10 Loss:0.01478\n",
      "Epoch:60 Batch:10 Loss:0.01202\n",
      "Epoch:80 Batch:10 Loss:0.01034\n",
      "Epoch:100 Batch:10 Loss:0.00979\n",
      "Epoch:120 Batch:10 Loss:0.00947\n",
      "Epoch:140 Batch:10 Loss:0.00847\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.067\n",
      "Epoch:10 Batch:10 Loss:0.067\n",
      "Epoch:20 Batch:10 Loss:0.066\n",
      "Epoch:30 Batch:10 Loss:0.066\n",
      "Epoch:40 Batch:10 Loss:0.067\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 65.76888622792758 setps: 89 count: 89\n",
      "reward: 32.99120607213555 setps: 78 count: 167\n",
      "reward: 31.09445562440524 setps: 82 count: 249\n",
      "avg rewards: 43.28484930815612\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.28124\n",
      "Epoch:20 Batch:11 Loss:0.01895\n",
      "Epoch:40 Batch:11 Loss:0.01331\n",
      "Epoch:60 Batch:11 Loss:0.01110\n",
      "Epoch:80 Batch:11 Loss:0.00881\n",
      "Epoch:100 Batch:11 Loss:0.00888\n",
      "Epoch:120 Batch:11 Loss:0.00827\n",
      "Epoch:140 Batch:11 Loss:0.00841\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.066\n",
      "Epoch:10 Batch:10 Loss:0.064\n",
      "Epoch:20 Batch:10 Loss:0.063\n",
      "Epoch:30 Batch:10 Loss:0.063\n",
      "Epoch:40 Batch:10 Loss:0.062\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 116.61192913098932 setps: 106 count: 106\n",
      "reward: 57.10238981930596 setps: 93 count: 199\n",
      "reward: 27.96908733461896 setps: 59 count: 258\n",
      "reward: 47.68823103328906 setps: 69 count: 327\n",
      "reward: 126.79700027374928 setps: 117 count: 444\n",
      "reward: 54.816336945508375 setps: 97 count: 541\n",
      "reward: 76.9994776818334 setps: 67 count: 608\n",
      "reward: 69.53370540594771 setps: 51 count: 659\n",
      "reward: 99.07953738830254 setps: 89 count: 748\n",
      "reward: 67.46445094749654 setps: 57 count: 805\n",
      "reward: 72.57103786450318 setps: 60 count: 865\n",
      "reward: 70.0932062146676 setps: 54 count: 919\n",
      "reward: 39.02577180283405 setps: 66 count: 985\n",
      "avg rewards: 71.21170475715738\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.21794\n",
      "Epoch:20 Batch:12 Loss:0.01976\n",
      "Epoch:40 Batch:12 Loss:0.01428\n",
      "Epoch:60 Batch:12 Loss:0.00945\n",
      "Epoch:80 Batch:12 Loss:0.00967\n",
      "Epoch:100 Batch:12 Loss:0.00809\n",
      "Epoch:120 Batch:12 Loss:0.00828\n",
      "Epoch:140 Batch:12 Loss:0.00703\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.064\n",
      "Epoch:10 Batch:10 Loss:0.063\n",
      "Epoch:20 Batch:10 Loss:0.062\n",
      "Epoch:30 Batch:10 Loss:0.063\n",
      "Epoch:40 Batch:10 Loss:0.063\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 41.59999007929145 setps: 29 count: 29\n",
      "reward: 32.037224980641625 setps: 27 count: 56\n",
      "reward: 36.509250776453705 setps: 32 count: 88\n",
      "reward: 31.810034088791873 setps: 32 count: 120\n",
      "reward: 35.55920954042958 setps: 30 count: 150\n",
      "reward: 32.77140811149293 setps: 27 count: 177\n",
      "reward: 34.47109350272803 setps: 28 count: 205\n",
      "reward: 48.08511426293553 setps: 38 count: 243\n",
      "reward: 38.154454018830435 setps: 32 count: 275\n",
      "reward: 34.265471764651004 setps: 27 count: 302\n",
      "reward: 41.382590695797994 setps: 33 count: 335\n",
      "reward: 38.47172194623417 setps: 30 count: 365\n",
      "reward: 33.80776938981581 setps: 25 count: 390\n",
      "reward: 37.38365249365742 setps: 32 count: 422\n",
      "reward: 35.52355041106639 setps: 26 count: 448\n",
      "reward: 40.62684605892865 setps: 34 count: 482\n",
      "reward: 42.884759636291705 setps: 31 count: 513\n",
      "reward: 39.369026061809556 setps: 29 count: 542\n",
      "reward: 30.165322992480654 setps: 21 count: 563\n",
      "reward: 35.556334910803706 setps: 28 count: 591\n",
      "reward: 36.565236825123414 setps: 30 count: 621\n",
      "reward: 39.38103933946696 setps: 32 count: 653\n",
      "reward: 34.41320603436616 setps: 26 count: 679\n",
      "reward: 38.423570423391354 setps: 30 count: 709\n",
      "reward: 34.52961636801774 setps: 28 count: 737\n",
      "reward: 33.13757083972595 setps: 27 count: 764\n",
      "reward: 50.26701660979159 setps: 39 count: 803\n",
      "reward: 32.12816839560983 setps: 27 count: 830\n",
      "reward: 37.864141377764454 setps: 29 count: 859\n",
      "reward: 38.07775728776905 setps: 32 count: 891\n",
      "reward: 29.719568934652482 setps: 24 count: 915\n",
      "reward: 32.39862936479913 setps: 23 count: 938\n",
      "reward: 32.81229950412961 setps: 28 count: 966\n",
      "reward: 38.35767672427318 setps: 31 count: 997\n",
      "avg rewards: 36.720891875059216\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.13808\n",
      "Epoch:20 Batch:13 Loss:0.01716\n",
      "Epoch:40 Batch:13 Loss:0.01236\n",
      "Epoch:60 Batch:13 Loss:0.01041\n",
      "Epoch:80 Batch:13 Loss:0.01007\n",
      "Epoch:100 Batch:13 Loss:0.00998\n",
      "Epoch:120 Batch:13 Loss:0.00760\n",
      "Epoch:140 Batch:13 Loss:0.00855\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.060\n",
      "Epoch:10 Batch:10 Loss:0.058\n",
      "Epoch:20 Batch:10 Loss:0.058\n",
      "Epoch:30 Batch:10 Loss:0.060\n",
      "Epoch:40 Batch:10 Loss:0.058\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 65.82056821564916 setps: 128 count: 128\n",
      "reward: 344.2321837552307 setps: 473 count: 601\n",
      "reward: 54.83139494186617 setps: 86 count: 687\n",
      "reward: 17.961058700857404 setps: 48 count: 735\n",
      "reward: 87.7335368721033 setps: 86 count: 821\n",
      "reward: 97.66116475922462 setps: 154 count: 975\n",
      "avg rewards: 111.37331787415523\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.10760\n",
      "Epoch:20 Batch:14 Loss:0.01729\n",
      "Epoch:40 Batch:14 Loss:0.01202\n",
      "Epoch:60 Batch:14 Loss:0.00964\n",
      "Epoch:80 Batch:14 Loss:0.00820\n",
      "Epoch:100 Batch:14 Loss:0.00805\n",
      "Epoch:120 Batch:14 Loss:0.00819\n",
      "Epoch:140 Batch:14 Loss:0.00809\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.060\n",
      "Epoch:10 Batch:10 Loss:0.059\n",
      "Epoch:20 Batch:10 Loss:0.058\n",
      "Epoch:30 Batch:10 Loss:0.058\n",
      "Epoch:40 Batch:10 Loss:0.059\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 54.652906027434796 setps: 112 count: 112\n",
      "reward: 68.23495260910421 setps: 132 count: 244\n",
      "reward: 74.67077165572702 setps: 142 count: 386\n",
      "reward: 31.768694218632305 setps: 75 count: 461\n",
      "reward: 32.87136228517192 setps: 71 count: 532\n",
      "avg rewards: 52.43973735921405\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.10097\n",
      "Epoch:20 Batch:15 Loss:0.01658\n",
      "Epoch:40 Batch:15 Loss:0.01058\n",
      "Epoch:60 Batch:15 Loss:0.01035\n",
      "Epoch:80 Batch:15 Loss:0.00794\n",
      "Epoch:100 Batch:15 Loss:0.00789\n",
      "Epoch:120 Batch:15 Loss:0.00792\n",
      "Epoch:140 Batch:15 Loss:0.00787\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.063\n",
      "Epoch:10 Batch:10 Loss:0.062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 Batch:10 Loss:0.061\n",
      "Epoch:30 Batch:10 Loss:0.061\n",
      "Epoch:40 Batch:10 Loss:0.060\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 359.54110234711425 setps: 533 count: 533\n",
      "reward: 13.847522425389618 setps: 57 count: 590\n",
      "reward: 37.139444489347824 setps: 98 count: 688\n",
      "reward: 75.86657955641131 setps: 93 count: 781\n",
      "reward: 20.43738431338715 setps: 69 count: 850\n",
      "reward: 15.74871926790511 setps: 60 count: 910\n",
      "reward: 24.954280404873135 setps: 70 count: 980\n",
      "avg rewards: 78.21929040063262\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.07922\n",
      "Epoch:20 Batch:16 Loss:0.01550\n",
      "Epoch:40 Batch:16 Loss:0.01141\n",
      "Epoch:60 Batch:16 Loss:0.00875\n",
      "Epoch:80 Batch:16 Loss:0.00808\n",
      "Epoch:100 Batch:16 Loss:0.00723\n",
      "Epoch:120 Batch:16 Loss:0.00801\n",
      "Epoch:140 Batch:16 Loss:0.00712\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.060\n",
      "Epoch:10 Batch:10 Loss:0.057\n",
      "Epoch:20 Batch:10 Loss:0.056\n",
      "Epoch:30 Batch:10 Loss:0.057\n",
      "Epoch:40 Batch:10 Loss:0.054\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 19.153660155160473 setps: 61 count: 61\n",
      "reward: 19.408341394018496 setps: 61 count: 122\n",
      "reward: 17.083879515441375 setps: 60 count: 182\n",
      "reward: 18.652644945270737 setps: 68 count: 250\n",
      "reward: 19.508499292437055 setps: 66 count: 316\n",
      "reward: 17.696892236577696 setps: 58 count: 374\n",
      "reward: 22.569905946879583 setps: 66 count: 440\n",
      "reward: 15.09854680272692 setps: 60 count: 500\n",
      "reward: 15.492025373426433 setps: 50 count: 550\n",
      "reward: 25.007921697611167 setps: 78 count: 628\n",
      "reward: 22.391244539809115 setps: 68 count: 696\n",
      "reward: 16.84943557684747 setps: 61 count: 757\n",
      "reward: 17.46754397131007 setps: 63 count: 820\n",
      "reward: 16.95618541504518 setps: 58 count: 878\n",
      "reward: 14.806921190614233 setps: 49 count: 927\n",
      "reward: 17.20106584523164 setps: 58 count: 985\n",
      "avg rewards: 18.45904461865048\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.11356\n",
      "Epoch:20 Batch:17 Loss:0.01402\n",
      "Epoch:40 Batch:17 Loss:0.00992\n",
      "Epoch:60 Batch:17 Loss:0.00882\n",
      "Epoch:80 Batch:17 Loss:0.00741\n",
      "Epoch:100 Batch:17 Loss:0.00772\n",
      "Epoch:120 Batch:17 Loss:0.00728\n",
      "Epoch:140 Batch:17 Loss:0.00669\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.059\n",
      "Epoch:10 Batch:10 Loss:0.057\n",
      "Epoch:20 Batch:10 Loss:0.057\n",
      "Epoch:30 Batch:10 Loss:0.056\n",
      "Epoch:40 Batch:10 Loss:0.056\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 24.905223043676237 setps: 25 count: 25\n",
      "reward: 26.503039869751955 setps: 19 count: 44\n",
      "reward: 23.52205767626147 setps: 20 count: 64\n",
      "reward: 24.635743410071886 setps: 17 count: 81\n",
      "reward: 36.49327533074684 setps: 27 count: 108\n",
      "reward: 22.87456240419269 setps: 19 count: 127\n",
      "reward: 24.641897902417984 setps: 17 count: 144\n",
      "reward: 28.783080521202645 setps: 21 count: 165\n",
      "reward: 24.863182418374347 setps: 21 count: 186\n",
      "reward: 31.29210944207443 setps: 24 count: 210\n",
      "reward: 28.177975294140925 setps: 23 count: 233\n",
      "reward: 38.0675712366923 setps: 39 count: 272\n",
      "reward: 22.64485058562859 setps: 18 count: 290\n",
      "reward: 27.012231199681985 setps: 26 count: 316\n",
      "reward: 26.99623812993377 setps: 24 count: 340\n",
      "reward: 23.264564410672754 setps: 17 count: 357\n",
      "reward: 24.502962793414188 setps: 21 count: 378\n",
      "reward: 23.266051649767903 setps: 18 count: 396\n",
      "reward: 22.342981927060457 setps: 22 count: 418\n",
      "reward: 37.35219837043842 setps: 28 count: 446\n",
      "reward: 31.935924964011065 setps: 24 count: 470\n",
      "reward: 23.951537850165913 setps: 19 count: 489\n",
      "reward: 25.917570855462692 setps: 18 count: 507\n",
      "reward: 29.024414009047906 setps: 26 count: 533\n",
      "reward: 24.099702724054808 setps: 17 count: 550\n",
      "reward: 23.449565897451237 setps: 27 count: 577\n",
      "reward: 29.146396886814905 setps: 21 count: 598\n",
      "reward: 31.17963159145001 setps: 31 count: 629\n",
      "reward: 24.107819829844814 setps: 16 count: 645\n",
      "reward: 28.417293210506607 setps: 27 count: 672\n",
      "reward: 24.90376696671446 setps: 19 count: 691\n",
      "reward: 24.58250171718683 setps: 19 count: 710\n",
      "reward: 25.271660444152072 setps: 19 count: 729\n",
      "reward: 23.590201620596048 setps: 16 count: 745\n",
      "reward: 23.978133407967107 setps: 18 count: 763\n",
      "reward: 28.088613546548007 setps: 21 count: 784\n",
      "reward: 23.088522378203923 setps: 19 count: 803\n",
      "reward: 22.295809317583913 setps: 19 count: 822\n",
      "reward: 28.918480517028364 setps: 22 count: 844\n",
      "reward: 43.41996296685245 setps: 35 count: 879\n",
      "reward: 26.64205832089647 setps: 23 count: 902\n",
      "reward: 25.267752544074025 setps: 23 count: 925\n",
      "reward: 28.63807001945242 setps: 23 count: 948\n",
      "reward: 26.807786713383397 setps: 21 count: 969\n",
      "reward: 25.725207386838157 setps: 19 count: 988\n",
      "avg rewards: 26.99089296227754\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.07513\n",
      "Epoch:20 Batch:18 Loss:0.01686\n",
      "Epoch:40 Batch:18 Loss:0.01147\n",
      "Epoch:60 Batch:18 Loss:0.00790\n",
      "Epoch:80 Batch:18 Loss:0.00846\n",
      "Epoch:100 Batch:18 Loss:0.00734\n",
      "Epoch:120 Batch:18 Loss:0.00659\n",
      "Epoch:140 Batch:18 Loss:0.00712\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.054\n",
      "Epoch:10 Batch:10 Loss:0.052\n",
      "Epoch:20 Batch:10 Loss:0.053\n",
      "Epoch:30 Batch:10 Loss:0.052\n",
      "Epoch:40 Batch:10 Loss:0.053\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 81.6262498433134 setps: 136 count: 136\n",
      "reward: 44.3750322386055 setps: 44 count: 180\n",
      "reward: 41.38605420612583 setps: 92 count: 272\n",
      "reward: 37.2696506381486 setps: 57 count: 329\n",
      "reward: 27.265383632067827 setps: 66 count: 395\n",
      "avg rewards: 46.38447411165223\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.07695\n",
      "Epoch:20 Batch:19 Loss:0.01438\n",
      "Epoch:40 Batch:19 Loss:0.01075\n",
      "Epoch:60 Batch:19 Loss:0.00928\n",
      "Epoch:80 Batch:19 Loss:0.00835\n",
      "Epoch:100 Batch:19 Loss:0.00677\n",
      "Epoch:120 Batch:19 Loss:0.00658\n",
      "Epoch:140 Batch:19 Loss:0.00712\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.053\n",
      "Epoch:10 Batch:10 Loss:0.051\n",
      "Epoch:20 Batch:10 Loss:0.052\n",
      "Epoch:30 Batch:10 Loss:0.053\n",
      "Epoch:40 Batch:10 Loss:0.053\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 35.72541716392006 setps: 81 count: 81\n",
      "reward: 20.570201543300925 setps: 56 count: 137\n",
      "reward: 36.13045794133504 setps: 88 count: 225\n",
      "reward: 24.65769256781641 setps: 68 count: 293\n",
      "reward: 44.58285189971359 setps: 99 count: 392\n",
      "reward: 22.83413183739031 setps: 60 count: 452\n",
      "avg rewards: 30.75012549224606\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.06536\n",
      "Epoch:20 Batch:20 Loss:0.01289\n",
      "Epoch:40 Batch:20 Loss:0.01022\n",
      "Epoch:60 Batch:20 Loss:0.00795\n",
      "Epoch:80 Batch:20 Loss:0.00806\n",
      "Epoch:100 Batch:20 Loss:0.00711\n",
      "Epoch:120 Batch:20 Loss:0.00599\n",
      "Epoch:140 Batch:20 Loss:0.00604\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.057\n",
      "Epoch:10 Batch:10 Loss:0.055\n",
      "Epoch:20 Batch:10 Loss:0.055\n",
      "Epoch:30 Batch:10 Loss:0.055\n",
      "Epoch:40 Batch:10 Loss:0.055\n",
      "Done!\n",
      "############# start HopperBulletEnv-v0 training ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:138: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49,)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 18.135717426698825 setps: 8 count: 8\n",
      "reward: 20.31004078197002 setps: 10 count: 18\n",
      "reward: 21.438978731670066 setps: 15 count: 33\n",
      "reward: 17.765539309385346 setps: 9 count: 42\n",
      "reward: 18.332904579787286 setps: 11 count: 53\n",
      "reward: 21.875437221436005 setps: 19 count: 72\n",
      "reward: 21.506266662621055 setps: 15 count: 87\n",
      "reward: 19.376771460867893 setps: 11 count: 98\n",
      "reward: 24.13838943464216 setps: 12 count: 110\n",
      "reward: 20.692785486340288 setps: 17 count: 127\n",
      "reward: 18.940834932563305 setps: 11 count: 138\n",
      "reward: 21.166305673065654 setps: 14 count: 152\n",
      "reward: 21.23852567662252 setps: 19 count: 171\n",
      "reward: 21.796875095865108 setps: 11 count: 182\n",
      "reward: 20.010833770128375 setps: 12 count: 194\n",
      "reward: 20.04875340626022 setps: 10 count: 204\n",
      "reward: 18.056655500577474 setps: 8 count: 212\n",
      "reward: 17.39600966670987 setps: 7 count: 219\n",
      "reward: 22.168040633604683 setps: 15 count: 234\n",
      "reward: 22.489486426544318 setps: 12 count: 246\n",
      "reward: 16.921681424420967 setps: 7 count: 253\n",
      "reward: 20.304812986515756 setps: 9 count: 262\n",
      "reward: 19.91294782147306 setps: 15 count: 277\n",
      "reward: 25.1151405354336 setps: 17 count: 294\n",
      "reward: 17.640768596230192 setps: 7 count: 301\n",
      "reward: 21.37131197315757 setps: 23 count: 324\n",
      "reward: 23.107516491040585 setps: 18 count: 342\n",
      "reward: 17.87113537335681 setps: 18 count: 360\n",
      "reward: 21.333143469640344 setps: 14 count: 374\n",
      "reward: 16.236367013251584 setps: 6 count: 380\n",
      "reward: 18.379200361853872 setps: 14 count: 394\n",
      "reward: 18.160572214784043 setps: 14 count: 408\n",
      "reward: 28.397211807865823 setps: 15 count: 423\n",
      "reward: 42.62005108907906 setps: 40 count: 463\n",
      "reward: 18.12718621087551 setps: 10 count: 473\n",
      "reward: 22.348446159492592 setps: 16 count: 489\n",
      "reward: 22.58490416053828 setps: 13 count: 502\n",
      "reward: 18.85577770489035 setps: 7 count: 509\n",
      "reward: 24.706906872590476 setps: 14 count: 523\n",
      "reward: 23.529601036265372 setps: 16 count: 539\n",
      "reward: 20.90636560224084 setps: 10 count: 549\n",
      "reward: 21.022759772661086 setps: 9 count: 558\n",
      "reward: 18.993325167467994 setps: 10 count: 568\n",
      "reward: 15.589718839761916 setps: 6 count: 574\n",
      "reward: 24.23771513060492 setps: 13 count: 587\n",
      "reward: 18.721909652430618 setps: 15 count: 602\n",
      "reward: 21.95135470764362 setps: 11 count: 613\n",
      "reward: 18.423491722314793 setps: 12 count: 625\n",
      "reward: 26.554142017479176 setps: 27 count: 652\n",
      "reward: 21.519148920968295 setps: 11 count: 663\n",
      "reward: 21.72804832597467 setps: 8 count: 671\n",
      "reward: 18.013207625600625 setps: 8 count: 679\n",
      "reward: 19.974427477087012 setps: 10 count: 689\n",
      "reward: 21.006524462794182 setps: 11 count: 700\n",
      "reward: 17.406095495457702 setps: 10 count: 710\n",
      "reward: 18.114781990027403 setps: 8 count: 718\n",
      "reward: 20.886644943838476 setps: 10 count: 728\n",
      "reward: 19.1949897245373 setps: 13 count: 741\n",
      "reward: 20.460972389401288 setps: 9 count: 750\n",
      "reward: 27.29091283071757 setps: 18 count: 768\n",
      "reward: 23.599417462351266 setps: 20 count: 788\n",
      "reward: 19.10651244682958 setps: 9 count: 797\n",
      "reward: 23.42654081135261 setps: 14 count: 811\n",
      "reward: 22.88648002753762 setps: 11 count: 822\n",
      "reward: 14.441897365625485 setps: 14 count: 836\n",
      "reward: 23.193719969296946 setps: 11 count: 847\n",
      "reward: 17.056251911837897 setps: 9 count: 856\n",
      "reward: 18.224917638469197 setps: 11 count: 867\n",
      "reward: 21.66502618997329 setps: 16 count: 883\n",
      "reward: 19.407696980983015 setps: 8 count: 891\n",
      "reward: 6.465537419653269 setps: 27 count: 918\n",
      "reward: 19.796564671721715 setps: 8 count: 926\n",
      "reward: 22.738228704645007 setps: 18 count: 944\n",
      "reward: 19.708290001975545 setps: 10 count: 954\n",
      "reward: 22.50526024093852 setps: 24 count: 978\n",
      "reward: 20.741775295831026 setps: 10 count: 988\n",
      "reward: 12.698206946720893 setps: 7 count: 995\n",
      "avg rewards: 20.572320728115173\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:1.32462\n",
      "Epoch:20 Batch:1 Loss:0.16411\n",
      "Epoch:40 Batch:1 Loss:0.11441\n",
      "Epoch:60 Batch:1 Loss:0.07997\n",
      "Epoch:80 Batch:1 Loss:0.06757\n",
      "Epoch:100 Batch:1 Loss:0.06086\n",
      "Epoch:120 Batch:1 Loss:0.05716\n",
      "Epoch:140 Batch:1 Loss:0.05363\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.107\n",
      "Epoch:10 Batch:10 Loss:0.095\n",
      "Epoch:20 Batch:10 Loss:0.099\n",
      "Epoch:30 Batch:10 Loss:0.097\n",
      "Epoch:40 Batch:10 Loss:0.090\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 4.943292028462201 setps: 31 count: 31\n",
      "reward: 0.23479780848083376 setps: 29 count: 60\n",
      "reward: -2.2240698394976794 setps: 28 count: 88\n",
      "reward: 3.1034594789416596 setps: 29 count: 117\n",
      "reward: -0.9096665277058503 setps: 29 count: 146\n",
      "reward: 1.3778150060308687 setps: 28 count: 174\n",
      "reward: -0.18235334505880152 setps: 29 count: 203\n",
      "reward: 2.0654869557605693 setps: 30 count: 233\n",
      "reward: 0.1264681282176745 setps: 26 count: 259\n",
      "reward: 0.2473959011433169 setps: 28 count: 287\n",
      "reward: 0.7079077104775919 setps: 30 count: 317\n",
      "reward: -4.361058526684064 setps: 29 count: 346\n",
      "reward: -1.494840373021726 setps: 29 count: 375\n",
      "reward: 0.6739223351643879 setps: 29 count: 404\n",
      "reward: 0.49276388624420964 setps: 28 count: 432\n",
      "reward: 4.011274322168901 setps: 29 count: 461\n",
      "reward: 0.5743840598530361 setps: 27 count: 488\n",
      "reward: 2.374937098588269 setps: 28 count: 516\n",
      "reward: 1.4534287814634796 setps: 28 count: 544\n",
      "reward: -1.629747692885577 setps: 30 count: 574\n",
      "reward: -0.8668501850217591 setps: 28 count: 602\n",
      "reward: 0.21476027636672246 setps: 31 count: 633\n",
      "reward: -2.8801630525529625 setps: 30 count: 663\n",
      "reward: 0.37261201287037604 setps: 29 count: 692\n",
      "reward: 25.5667320597815 setps: 37 count: 729\n",
      "reward: -1.2829624767269716 setps: 29 count: 758\n",
      "reward: -2.1858049537899213 setps: 30 count: 788\n",
      "reward: 0.9442732896968664 setps: 27 count: 815\n",
      "reward: 8.369635412738715 setps: 31 count: 846\n",
      "reward: 0.9804132449498861 setps: 27 count: 873\n",
      "reward: 1.7159719458664755 setps: 28 count: 901\n",
      "reward: 1.908059396400493 setps: 30 count: 931\n",
      "reward: 2.1880047237442333 setps: 29 count: 960\n",
      "reward: -1.932986009871821 setps: 27 count: 987\n",
      "avg rewards: 1.3146262611939745\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:1.17814\n",
      "Epoch:20 Batch:2 Loss:0.09482\n",
      "Epoch:40 Batch:2 Loss:0.05216\n",
      "Epoch:60 Batch:2 Loss:0.04185\n",
      "Epoch:80 Batch:2 Loss:0.03849\n",
      "Epoch:100 Batch:2 Loss:0.03580\n",
      "Epoch:120 Batch:2 Loss:0.03319\n",
      "Epoch:140 Batch:2 Loss:0.02937\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.082\n",
      "Epoch:10 Batch:10 Loss:0.079\n",
      "Epoch:20 Batch:10 Loss:0.079\n",
      "Epoch:30 Batch:10 Loss:0.076\n",
      "Epoch:40 Batch:10 Loss:0.079\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 36.78664746903815 setps: 46 count: 46\n",
      "reward: 36.89982037438386 setps: 40 count: 86\n",
      "reward: 25.63476421617816 setps: 33 count: 119\n",
      "reward: 34.201912063630154 setps: 40 count: 159\n",
      "reward: 32.39234371807252 setps: 37 count: 196\n",
      "reward: 37.47527902717558 setps: 43 count: 239\n",
      "reward: 33.72886483127949 setps: 40 count: 279\n",
      "reward: 44.067857824600665 setps: 47 count: 326\n",
      "reward: 40.52722864754323 setps: 44 count: 370\n",
      "reward: 32.1900037576008 setps: 38 count: 408\n",
      "reward: 40.73042485527403 setps: 43 count: 451\n",
      "reward: 36.45443396583141 setps: 39 count: 490\n",
      "reward: 28.989744927642462 setps: 36 count: 526\n",
      "reward: 34.00403881781531 setps: 38 count: 564\n",
      "reward: 32.165018181466436 setps: 38 count: 602\n",
      "reward: 34.52595442249111 setps: 42 count: 644\n",
      "reward: 30.598022209724867 setps: 41 count: 685\n",
      "reward: 42.24027684325448 setps: 44 count: 729\n",
      "reward: 37.45296863613766 setps: 40 count: 769\n",
      "reward: 39.54467897602445 setps: 43 count: 812\n",
      "reward: 33.14024115285429 setps: 39 count: 851\n",
      "reward: 27.418076545839718 setps: 40 count: 891\n",
      "reward: 21.367762030276932 setps: 34 count: 925\n",
      "reward: 31.900670407369038 setps: 38 count: 963\n",
      "avg rewards: 34.351543079229366\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:1.10759\n",
      "Epoch:20 Batch:3 Loss:0.07757\n",
      "Epoch:40 Batch:3 Loss:0.04652\n",
      "Epoch:60 Batch:3 Loss:0.03509\n",
      "Epoch:80 Batch:3 Loss:0.03045\n",
      "Epoch:100 Batch:3 Loss:0.02890\n",
      "Epoch:120 Batch:3 Loss:0.02736\n",
      "Epoch:140 Batch:3 Loss:0.02447\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.077\n",
      "Epoch:10 Batch:10 Loss:0.084\n",
      "Epoch:20 Batch:10 Loss:0.092\n",
      "Epoch:30 Batch:10 Loss:0.077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:40 Batch:10 Loss:0.083\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 34.10366870642683 setps: 41 count: 41\n",
      "reward: 30.60268456241174 setps: 40 count: 81\n",
      "reward: 28.224104769995026 setps: 37 count: 118\n",
      "reward: 29.027081444366193 setps: 40 count: 158\n",
      "reward: 24.13212696707924 setps: 36 count: 194\n",
      "reward: 27.69461887561265 setps: 38 count: 232\n",
      "reward: 26.773687504997365 setps: 38 count: 270\n",
      "reward: 23.203705432198326 setps: 36 count: 306\n",
      "reward: 29.605055617273315 setps: 39 count: 345\n",
      "reward: 25.758780352769822 setps: 36 count: 381\n",
      "reward: 27.218632851462456 setps: 38 count: 419\n",
      "reward: 25.520118226837074 setps: 38 count: 457\n",
      "reward: 25.982585286711405 setps: 37 count: 494\n",
      "reward: 25.260012251109583 setps: 36 count: 530\n",
      "reward: 20.79731724492449 setps: 37 count: 567\n",
      "reward: 27.542844370413516 setps: 36 count: 603\n",
      "reward: 30.295848438261594 setps: 40 count: 643\n",
      "reward: 29.309138647657157 setps: 38 count: 681\n",
      "reward: 18.86012087092531 setps: 33 count: 714\n",
      "reward: 42.55987871348917 setps: 47 count: 761\n",
      "reward: 24.34914896654373 setps: 37 count: 798\n",
      "reward: 25.605224339995765 setps: 37 count: 835\n",
      "reward: 29.530710759932116 setps: 38 count: 873\n",
      "reward: 34.658422865378085 setps: 43 count: 916\n",
      "reward: 25.999129000376712 setps: 36 count: 952\n",
      "reward: 29.241290470368412 setps: 37 count: 989\n",
      "avg rewards: 27.76368990528912\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.90597\n",
      "Epoch:20 Batch:4 Loss:0.05393\n",
      "Epoch:40 Batch:4 Loss:0.03293\n",
      "Epoch:60 Batch:4 Loss:0.02712\n",
      "Epoch:80 Batch:4 Loss:0.02301\n",
      "Epoch:100 Batch:4 Loss:0.02317\n",
      "Epoch:120 Batch:4 Loss:0.02233\n",
      "Epoch:140 Batch:4 Loss:0.02089\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.075\n",
      "Epoch:10 Batch:10 Loss:0.075\n",
      "Epoch:20 Batch:10 Loss:0.070\n",
      "Epoch:30 Batch:10 Loss:0.075\n",
      "Epoch:40 Batch:10 Loss:0.072\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 29.85391669672099 setps: 39 count: 39\n",
      "reward: 31.991842686753078 setps: 39 count: 78\n",
      "reward: 38.17144892202777 setps: 43 count: 121\n",
      "reward: 30.59782308910653 setps: 37 count: 158\n",
      "reward: 25.93065335647843 setps: 35 count: 193\n",
      "reward: 18.005369517557845 setps: 33 count: 226\n",
      "reward: 28.38190102510271 setps: 36 count: 262\n",
      "reward: 27.326376967996477 setps: 35 count: 297\n",
      "reward: 31.788466978391803 setps: 39 count: 336\n",
      "reward: 38.236405198193104 setps: 45 count: 381\n",
      "reward: 27.770271715684792 setps: 37 count: 418\n",
      "reward: 32.747335954254964 setps: 40 count: 458\n",
      "reward: 34.8053177852591 setps: 40 count: 498\n",
      "reward: 25.649851751558888 setps: 35 count: 533\n",
      "reward: 28.62682848593613 setps: 37 count: 570\n",
      "reward: 23.877784033580976 setps: 33 count: 603\n",
      "reward: 26.92684489218518 setps: 34 count: 637\n",
      "reward: 38.849712752124454 setps: 44 count: 681\n",
      "reward: 32.71525313242163 setps: 39 count: 720\n",
      "reward: 24.950701577292786 setps: 34 count: 754\n",
      "reward: 41.62531078141036 setps: 48 count: 802\n",
      "reward: 35.70219231772354 setps: 42 count: 844\n",
      "reward: 26.21278788784403 setps: 35 count: 879\n",
      "reward: 22.454365089594646 setps: 34 count: 913\n",
      "reward: 25.511785365729878 setps: 34 count: 947\n",
      "reward: 35.47543496335566 setps: 41 count: 988\n",
      "avg rewards: 30.160999343241755\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.44650\n",
      "Epoch:20 Batch:5 Loss:0.04448\n",
      "Epoch:40 Batch:5 Loss:0.02719\n",
      "Epoch:60 Batch:5 Loss:0.02510\n",
      "Epoch:80 Batch:5 Loss:0.02240\n",
      "Epoch:100 Batch:5 Loss:0.02114\n",
      "Epoch:120 Batch:5 Loss:0.01863\n",
      "Epoch:140 Batch:5 Loss:0.02005\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.078\n",
      "Epoch:10 Batch:10 Loss:0.075\n",
      "Epoch:20 Batch:10 Loss:0.078\n",
      "Epoch:30 Batch:10 Loss:0.076\n",
      "Epoch:40 Batch:10 Loss:0.072\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 54.68765053475218 setps: 41 count: 41\n",
      "reward: 44.178758504061264 setps: 32 count: 73\n",
      "reward: 54.82776344981976 setps: 42 count: 115\n",
      "reward: 48.03922461233451 setps: 37 count: 152\n",
      "reward: 52.73206624678421 setps: 41 count: 193\n",
      "reward: 42.3116783645004 setps: 35 count: 228\n",
      "reward: 51.28796811804932 setps: 37 count: 265\n",
      "reward: 50.201600675116055 setps: 36 count: 301\n",
      "reward: 59.42872724439804 setps: 44 count: 345\n",
      "reward: 51.03702479505736 setps: 41 count: 386\n",
      "reward: 49.28035452675393 setps: 36 count: 422\n",
      "reward: 50.80344953205931 setps: 42 count: 464\n",
      "reward: 54.587033830278955 setps: 46 count: 510\n",
      "reward: 51.18173236433649 setps: 43 count: 553\n",
      "reward: 45.623615457020065 setps: 38 count: 591\n",
      "reward: 48.02155701295996 setps: 38 count: 629\n",
      "reward: 37.97395288858643 setps: 50 count: 679\n",
      "reward: 50.07818929444474 setps: 47 count: 726\n",
      "reward: 54.22814726491925 setps: 44 count: 770\n",
      "reward: 53.95559168588807 setps: 42 count: 812\n",
      "reward: 48.23988295833551 setps: 37 count: 849\n",
      "reward: 50.23344258530706 setps: 38 count: 887\n",
      "reward: 46.28230388448555 setps: 36 count: 923\n",
      "reward: 38.91432609101758 setps: 52 count: 975\n",
      "avg rewards: 49.50566841338608\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.45284\n",
      "Epoch:20 Batch:6 Loss:0.04022\n",
      "Epoch:40 Batch:6 Loss:0.02537\n",
      "Epoch:60 Batch:6 Loss:0.02093\n",
      "Epoch:80 Batch:6 Loss:0.01905\n",
      "Epoch:100 Batch:6 Loss:0.01779\n",
      "Epoch:120 Batch:6 Loss:0.01699\n",
      "Epoch:140 Batch:6 Loss:0.01594\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.074\n",
      "Epoch:10 Batch:10 Loss:0.079\n",
      "Epoch:20 Batch:10 Loss:0.073\n",
      "Epoch:30 Batch:10 Loss:0.067\n",
      "Epoch:40 Batch:10 Loss:0.071\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 25.973996460819038 setps: 15 count: 15\n",
      "reward: 28.213026573827662 setps: 14 count: 29\n",
      "reward: 31.201064581923127 setps: 16 count: 45\n",
      "reward: 33.927522057470924 setps: 19 count: 64\n",
      "reward: 27.927321710859538 setps: 15 count: 79\n",
      "reward: 32.177801331356754 setps: 17 count: 96\n",
      "reward: 26.722064526383477 setps: 13 count: 109\n",
      "reward: 32.47756067006121 setps: 18 count: 127\n",
      "reward: 28.853305041138082 setps: 15 count: 142\n",
      "reward: 28.294295644314847 setps: 15 count: 157\n",
      "reward: 26.89610410884343 setps: 13 count: 170\n",
      "reward: 25.19332957570732 setps: 10 count: 180\n",
      "reward: 39.50461079757078 setps: 22 count: 202\n",
      "reward: 30.98760698345286 setps: 15 count: 217\n",
      "reward: 34.07613743538387 setps: 19 count: 236\n",
      "reward: 29.314950236660657 setps: 15 count: 251\n",
      "reward: 26.73810002830869 setps: 13 count: 264\n",
      "reward: 21.312366685905726 setps: 13 count: 277\n",
      "reward: 24.15923524249665 setps: 15 count: 292\n",
      "reward: 24.941130339363006 setps: 10 count: 302\n",
      "reward: 32.204200540734746 setps: 16 count: 318\n",
      "reward: 31.93240895056806 setps: 17 count: 335\n",
      "reward: 29.552112275561374 setps: 14 count: 349\n",
      "reward: 30.676315796731796 setps: 17 count: 366\n",
      "reward: 26.121300548367437 setps: 12 count: 378\n",
      "reward: 28.323750960348114 setps: 14 count: 392\n",
      "reward: 24.642780417330506 setps: 14 count: 406\n",
      "reward: 31.027577365442994 setps: 17 count: 423\n",
      "reward: 24.00444852936926 setps: 13 count: 436\n",
      "reward: 26.707551761242208 setps: 13 count: 449\n",
      "reward: 31.180757642006213 setps: 16 count: 465\n",
      "reward: 23.32284341418126 setps: 13 count: 478\n",
      "reward: 29.671575258961823 setps: 14 count: 492\n",
      "reward: 31.461665005143733 setps: 15 count: 507\n",
      "reward: 24.960907044967463 setps: 13 count: 520\n",
      "reward: 28.81693762523064 setps: 14 count: 534\n",
      "reward: 26.261181101792317 setps: 14 count: 548\n",
      "reward: 34.15912134767859 setps: 19 count: 567\n",
      "reward: 33.870986018802796 setps: 18 count: 585\n",
      "reward: 27.486497730671545 setps: 13 count: 598\n",
      "reward: 27.12433110457205 setps: 13 count: 611\n",
      "reward: 25.048579062201316 setps: 12 count: 623\n",
      "reward: 30.21644824381219 setps: 16 count: 639\n",
      "reward: 29.542292059378816 setps: 16 count: 655\n",
      "reward: 24.950713171642565 setps: 12 count: 667\n",
      "reward: 28.989204888949462 setps: 18 count: 685\n",
      "reward: 32.14876276978903 setps: 17 count: 702\n",
      "reward: 27.733475922424983 setps: 14 count: 716\n",
      "reward: 34.70755723270122 setps: 20 count: 736\n",
      "reward: 29.582908555939508 setps: 15 count: 751\n",
      "reward: 30.07300505225721 setps: 15 count: 766\n",
      "reward: 26.393543044985563 setps: 14 count: 780\n",
      "reward: 23.041927237872734 setps: 14 count: 794\n",
      "reward: 35.150518052197 setps: 24 count: 818\n",
      "reward: 31.819833927560826 setps: 18 count: 836\n",
      "reward: 28.925234247595654 setps: 12 count: 848\n",
      "reward: 30.65397439072403 setps: 16 count: 864\n",
      "reward: 30.253776136839583 setps: 15 count: 879\n",
      "reward: 30.35931952660466 setps: 16 count: 895\n",
      "reward: 30.48519428680011 setps: 16 count: 911\n",
      "reward: 31.06736793179298 setps: 16 count: 927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 27.206128163056565 setps: 14 count: 941\n",
      "reward: 25.645772939454766 setps: 12 count: 953\n",
      "reward: 27.632008303633487 setps: 12 count: 965\n",
      "reward: 28.866916857195612 setps: 15 count: 980\n",
      "reward: 27.876706530169752 setps: 12 count: 992\n",
      "avg rewards: 28.9510901364717\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.65330\n",
      "Epoch:20 Batch:7 Loss:0.03522\n",
      "Epoch:40 Batch:7 Loss:0.02653\n",
      "Epoch:60 Batch:7 Loss:0.02142\n",
      "Epoch:80 Batch:7 Loss:0.02152\n",
      "Epoch:100 Batch:7 Loss:0.01887\n",
      "Epoch:120 Batch:7 Loss:0.01718\n",
      "Epoch:140 Batch:7 Loss:0.01863\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.068\n",
      "Epoch:10 Batch:10 Loss:0.064\n",
      "Epoch:20 Batch:10 Loss:0.060\n",
      "Epoch:30 Batch:10 Loss:0.063\n",
      "Epoch:40 Batch:10 Loss:0.063\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 59.22844533364987 setps: 58 count: 58\n",
      "reward: 72.18104235279168 setps: 61 count: 119\n",
      "reward: 126.38034946805419 setps: 99 count: 218\n",
      "reward: 86.51796654081264 setps: 96 count: 314\n",
      "reward: 47.590389457932886 setps: 36 count: 350\n",
      "reward: 46.66926093964285 setps: 33 count: 383\n",
      "reward: 49.467665714355824 setps: 36 count: 419\n",
      "reward: 37.756017002610314 setps: 35 count: 454\n",
      "reward: 36.379184148531934 setps: 60 count: 514\n",
      "reward: 45.7698731823737 setps: 42 count: 556\n",
      "reward: 57.837406650997586 setps: 66 count: 622\n",
      "reward: 48.223538023210125 setps: 38 count: 660\n",
      "reward: 79.14840805731588 setps: 68 count: 728\n",
      "reward: 48.273542933129654 setps: 42 count: 770\n",
      "reward: 47.101462948591504 setps: 43 count: 813\n",
      "reward: 62.84074177812873 setps: 75 count: 888\n",
      "reward: 57.87927612525964 setps: 43 count: 931\n",
      "reward: 73.5784740756484 setps: 56 count: 987\n",
      "avg rewards: 60.15683581850208\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.44371\n",
      "Epoch:20 Batch:8 Loss:0.03147\n",
      "Epoch:40 Batch:8 Loss:0.02541\n",
      "Epoch:60 Batch:8 Loss:0.02065\n",
      "Epoch:80 Batch:8 Loss:0.01850\n",
      "Epoch:100 Batch:8 Loss:0.02091\n",
      "Epoch:120 Batch:8 Loss:0.01805\n",
      "Epoch:140 Batch:8 Loss:0.01862\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.044\n",
      "Epoch:10 Batch:10 Loss:0.046\n",
      "Epoch:20 Batch:10 Loss:0.039\n",
      "Epoch:30 Batch:10 Loss:0.037\n",
      "Epoch:40 Batch:10 Loss:0.037\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 44.696173288537096 setps: 80 count: 80\n",
      "reward: 32.89813636174222 setps: 31 count: 111\n",
      "reward: 35.94204832912801 setps: 34 count: 145\n",
      "reward: 33.621087074083334 setps: 30 count: 175\n",
      "reward: 29.653693824959927 setps: 32 count: 207\n",
      "reward: 38.323326859324894 setps: 34 count: 241\n",
      "reward: 47.43087640155573 setps: 50 count: 291\n",
      "reward: 25.04064154759399 setps: 11 count: 302\n",
      "reward: 43.68957731901756 setps: 50 count: 352\n",
      "reward: 35.717870931206555 setps: 32 count: 384\n",
      "reward: 38.1602743252879 setps: 29 count: 413\n",
      "reward: 28.81865542581218 setps: 23 count: 436\n",
      "reward: 40.94869413920532 setps: 32 count: 468\n",
      "reward: 47.00070687733678 setps: 41 count: 509\n",
      "reward: 39.013401770891505 setps: 34 count: 543\n",
      "reward: 24.16417433395836 setps: 33 count: 576\n",
      "reward: 28.908819028540158 setps: 23 count: 599\n",
      "reward: 46.84005676532542 setps: 36 count: 635\n",
      "reward: 27.07635091144621 setps: 31 count: 666\n",
      "reward: 33.89887581709626 setps: 22 count: 688\n",
      "reward: 21.27065756250959 setps: 53 count: 741\n",
      "reward: 31.432254379882934 setps: 51 count: 792\n",
      "reward: 31.60704534625255 setps: 36 count: 828\n",
      "reward: 51.31551768502105 setps: 50 count: 878\n",
      "reward: 72.63277046993461 setps: 77 count: 955\n",
      "avg rewards: 37.20406747102601\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.25683\n",
      "Epoch:20 Batch:9 Loss:0.03170\n",
      "Epoch:40 Batch:9 Loss:0.02666\n",
      "Epoch:60 Batch:9 Loss:0.02427\n",
      "Epoch:80 Batch:9 Loss:0.01948\n",
      "Epoch:100 Batch:9 Loss:0.01949\n",
      "Epoch:120 Batch:9 Loss:0.01660\n",
      "Epoch:140 Batch:9 Loss:0.01699\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.034\n",
      "Epoch:10 Batch:10 Loss:0.036\n",
      "Epoch:20 Batch:10 Loss:0.038\n",
      "Epoch:30 Batch:10 Loss:0.036\n",
      "Epoch:40 Batch:10 Loss:0.037\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 25.743400557133995 setps: 33 count: 33\n",
      "reward: 36.288169817237936 setps: 24 count: 57\n",
      "reward: 30.919277542193594 setps: 20 count: 77\n",
      "reward: 29.15881230494415 setps: 19 count: 96\n",
      "reward: 33.26979600536725 setps: 22 count: 118\n",
      "reward: 35.221373184156256 setps: 23 count: 141\n",
      "reward: 32.40258182144462 setps: 21 count: 162\n",
      "reward: 31.326572997904442 setps: 21 count: 183\n",
      "reward: 31.30179978966772 setps: 23 count: 206\n",
      "reward: 30.055951159972754 setps: 46 count: 252\n",
      "reward: 33.42883778667892 setps: 22 count: 274\n",
      "reward: 30.151486247217694 setps: 20 count: 294\n",
      "reward: 25.652288584277265 setps: 18 count: 312\n",
      "reward: 32.02417394567311 setps: 21 count: 333\n",
      "reward: 33.310504819890774 setps: 21 count: 354\n",
      "reward: 33.460906253200676 setps: 22 count: 376\n",
      "reward: 30.40831689403421 setps: 20 count: 396\n",
      "reward: 33.447396085398225 setps: 22 count: 418\n",
      "reward: 39.631530212238424 setps: 26 count: 444\n",
      "reward: 35.2173791746056 setps: 23 count: 467\n",
      "reward: 32.01813218691677 setps: 21 count: 488\n",
      "reward: 32.078679826477305 setps: 20 count: 508\n",
      "reward: 33.27796312512073 setps: 22 count: 530\n",
      "reward: 31.47758076146565 setps: 20 count: 550\n",
      "reward: 30.949349346646343 setps: 19 count: 569\n",
      "reward: 26.503035350571736 setps: 12 count: 581\n",
      "reward: 36.94943097835493 setps: 25 count: 606\n",
      "reward: 33.296718409846655 setps: 21 count: 627\n",
      "reward: 37.91816336527263 setps: 23 count: 650\n",
      "reward: 30.765569189135572 setps: 16 count: 666\n",
      "reward: 31.036861901452358 setps: 20 count: 686\n",
      "reward: 32.188661455547845 setps: 21 count: 707\n",
      "reward: 32.5634331112873 setps: 21 count: 728\n",
      "reward: 30.962372031288396 setps: 20 count: 748\n",
      "reward: 35.1822875617363 setps: 22 count: 770\n",
      "reward: 32.52157798602274 setps: 20 count: 790\n",
      "reward: 32.14814651612978 setps: 21 count: 811\n",
      "reward: 33.60940023773437 setps: 22 count: 833\n",
      "reward: 31.532344829240166 setps: 20 count: 853\n",
      "reward: 36.414288564515296 setps: 24 count: 877\n",
      "reward: 37.33370333149505 setps: 23 count: 900\n",
      "reward: 32.21808878213923 setps: 17 count: 917\n",
      "reward: 28.811188639822646 setps: 19 count: 936\n",
      "reward: 26.59345210612082 setps: 18 count: 954\n",
      "reward: 38.55750881677232 setps: 24 count: 978\n",
      "reward: 31.29288600226137 setps: 20 count: 998\n",
      "avg rewards: 32.40481259992635\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.24705\n",
      "Epoch:20 Batch:10 Loss:0.03096\n",
      "Epoch:40 Batch:10 Loss:0.02432\n",
      "Epoch:60 Batch:10 Loss:0.02008\n",
      "Epoch:80 Batch:10 Loss:0.01842\n",
      "Epoch:100 Batch:10 Loss:0.01773\n",
      "Epoch:120 Batch:10 Loss:0.01793\n",
      "Epoch:140 Batch:10 Loss:0.01580\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.039\n",
      "Epoch:10 Batch:10 Loss:0.040\n",
      "Epoch:20 Batch:10 Loss:0.040\n",
      "Epoch:30 Batch:10 Loss:0.036\n",
      "Epoch:40 Batch:10 Loss:0.033\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 58.354688073864956 setps: 66 count: 66\n",
      "reward: 38.22469721066592 setps: 31 count: 97\n",
      "reward: 37.45624460808612 setps: 68 count: 165\n",
      "reward: 45.81050374427141 setps: 39 count: 204\n",
      "reward: 42.03748979655211 setps: 56 count: 260\n",
      "reward: 52.02187393992789 setps: 55 count: 315\n",
      "reward: 39.4988404960095 setps: 28 count: 343\n",
      "reward: 35.239295408403265 setps: 28 count: 371\n",
      "reward: 66.05181950381812 setps: 77 count: 448\n",
      "reward: 45.12943156358232 setps: 32 count: 480\n",
      "reward: 30.196338030707558 setps: 41 count: 521\n",
      "reward: 60.30611274921832 setps: 62 count: 583\n",
      "reward: 48.265725353058954 setps: 34 count: 617\n",
      "reward: 45.15677415090614 setps: 33 count: 650\n",
      "reward: 39.13975690030783 setps: 33 count: 683\n",
      "reward: 41.75763785231248 setps: 35 count: 718\n",
      "reward: 33.6212366190346 setps: 32 count: 750\n",
      "reward: 65.18221150556201 setps: 77 count: 827\n",
      "reward: 28.169360122799116 setps: 39 count: 866\n",
      "reward: 48.69639012565603 setps: 45 count: 911\n",
      "reward: 35.1643267326508 setps: 29 count: 940\n",
      "reward: 44.24642953687142 setps: 32 count: 972\n",
      "avg rewards: 44.53305381928486\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.30738\n",
      "Epoch:20 Batch:11 Loss:0.02885\n",
      "Epoch:40 Batch:11 Loss:0.02383\n",
      "Epoch:60 Batch:11 Loss:0.01889\n",
      "Epoch:80 Batch:11 Loss:0.01907\n",
      "Epoch:100 Batch:11 Loss:0.01652\n",
      "Epoch:120 Batch:11 Loss:0.01559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:140 Batch:11 Loss:0.01512\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.039\n",
      "Epoch:10 Batch:10 Loss:0.038\n",
      "Epoch:20 Batch:10 Loss:0.036\n",
      "Epoch:30 Batch:10 Loss:0.035\n",
      "Epoch:40 Batch:10 Loss:0.038\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 22.12629497799644 setps: 40 count: 40\n",
      "reward: 51.5639625120355 setps: 53 count: 93\n",
      "reward: 48.293323174856766 setps: 48 count: 141\n",
      "reward: 46.220223545530466 setps: 48 count: 189\n",
      "reward: 67.47569622447922 setps: 102 count: 291\n",
      "reward: 26.419209029055494 setps: 53 count: 344\n",
      "reward: 52.49901693011632 setps: 55 count: 399\n",
      "reward: 25.77930635802186 setps: 52 count: 451\n",
      "reward: 66.81107949690052 setps: 71 count: 522\n",
      "reward: 59.536189864210606 setps: 61 count: 583\n",
      "reward: 22.101356912261686 setps: 45 count: 628\n",
      "reward: 31.71263047886606 setps: 54 count: 682\n",
      "reward: 20.33675849421416 setps: 40 count: 722\n",
      "reward: 21.04849636676808 setps: 40 count: 762\n",
      "reward: 19.778734688628177 setps: 39 count: 801\n",
      "reward: 24.553651548988988 setps: 49 count: 850\n",
      "reward: 47.007952094275964 setps: 46 count: 896\n",
      "reward: 40.80717504264176 setps: 42 count: 938\n",
      "reward: 58.11955687018344 setps: 56 count: 994\n",
      "avg rewards: 39.58897971631745\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.22199\n",
      "Epoch:20 Batch:12 Loss:0.02668\n",
      "Epoch:40 Batch:12 Loss:0.02331\n",
      "Epoch:60 Batch:12 Loss:0.02098\n",
      "Epoch:80 Batch:12 Loss:0.01682\n",
      "Epoch:100 Batch:12 Loss:0.01539\n",
      "Epoch:120 Batch:12 Loss:0.01498\n",
      "Epoch:140 Batch:12 Loss:0.01606\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.033\n",
      "Epoch:10 Batch:10 Loss:0.037\n",
      "Epoch:20 Batch:10 Loss:0.030\n",
      "Epoch:30 Batch:10 Loss:0.032\n",
      "Epoch:40 Batch:10 Loss:0.031\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 26.809732188048656 setps: 12 count: 12\n",
      "reward: 29.786601126265303 setps: 15 count: 27\n",
      "reward: 31.219261708005796 setps: 14 count: 41\n",
      "reward: 30.7432972961542 setps: 18 count: 59\n",
      "reward: 30.20129621573433 setps: 36 count: 95\n",
      "reward: 29.147060293753746 setps: 16 count: 111\n",
      "reward: 30.162299701914893 setps: 14 count: 125\n",
      "reward: 27.096560420388414 setps: 12 count: 137\n",
      "reward: 26.12069807389198 setps: 12 count: 149\n",
      "reward: 17.914308832833193 setps: 32 count: 181\n",
      "reward: 27.71329598363372 setps: 14 count: 195\n",
      "reward: 26.40197750551015 setps: 12 count: 207\n",
      "reward: 31.799381896281556 setps: 18 count: 225\n",
      "reward: 16.42426285261608 setps: 29 count: 254\n",
      "reward: 27.53381692526309 setps: 12 count: 266\n",
      "reward: 27.39928176667454 setps: 17 count: 283\n",
      "reward: 26.2102027120447 setps: 10 count: 293\n",
      "reward: 29.814342162036336 setps: 14 count: 307\n",
      "reward: 28.522442727940508 setps: 17 count: 324\n",
      "reward: 25.62952680077579 setps: 12 count: 336\n",
      "reward: 27.232697369897505 setps: 16 count: 352\n",
      "reward: 25.543219181869073 setps: 29 count: 381\n",
      "reward: 17.739887075779546 setps: 29 count: 410\n",
      "reward: 32.17690951695113 setps: 16 count: 426\n",
      "reward: 30.040999424256736 setps: 14 count: 440\n",
      "reward: 30.467669585837577 setps: 14 count: 454\n",
      "reward: 29.235377627339037 setps: 14 count: 468\n",
      "reward: 31.87747306529782 setps: 23 count: 491\n",
      "reward: 27.93531408662093 setps: 14 count: 505\n",
      "reward: 29.04121113902802 setps: 13 count: 518\n",
      "reward: 30.73871031669696 setps: 14 count: 532\n",
      "reward: 26.33886167583987 setps: 13 count: 545\n",
      "reward: 31.240957785716457 setps: 19 count: 564\n",
      "reward: 19.41877820495574 setps: 29 count: 593\n",
      "reward: 30.433170471533952 setps: 21 count: 614\n",
      "reward: 29.989074515587703 setps: 35 count: 649\n",
      "reward: 29.094256304232115 setps: 19 count: 668\n",
      "reward: 31.881784264637094 setps: 19 count: 687\n",
      "reward: 19.97559588024451 setps: 27 count: 714\n",
      "reward: 18.76358119159704 setps: 37 count: 751\n",
      "reward: 26.05755801358173 setps: 13 count: 764\n",
      "reward: 31.631103388179326 setps: 15 count: 779\n",
      "reward: 27.483644045174884 setps: 13 count: 792\n",
      "reward: 16.86607128726318 setps: 29 count: 821\n",
      "reward: 18.267274428333625 setps: 25 count: 846\n",
      "reward: 32.75257437114924 setps: 16 count: 862\n",
      "reward: 31.397182932260336 setps: 22 count: 884\n",
      "reward: 28.53472747916676 setps: 12 count: 896\n",
      "reward: 27.874398876822667 setps: 12 count: 908\n",
      "reward: 26.728768864036827 setps: 12 count: 920\n",
      "reward: 23.35190442645544 setps: 35 count: 955\n",
      "reward: 20.03559098822151 setps: 32 count: 987\n",
      "reward: 28.51673884352931 setps: 13 count: 1000\n",
      "avg rewards: 27.08137195882755\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.16444\n",
      "Epoch:20 Batch:13 Loss:0.02738\n",
      "Epoch:40 Batch:13 Loss:0.02012\n",
      "Epoch:60 Batch:13 Loss:0.01699\n",
      "Epoch:80 Batch:13 Loss:0.01848\n",
      "Epoch:100 Batch:13 Loss:0.01614\n",
      "Epoch:120 Batch:13 Loss:0.01522\n",
      "Epoch:140 Batch:13 Loss:0.01574\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.030\n",
      "Epoch:10 Batch:10 Loss:0.032\n",
      "Epoch:20 Batch:10 Loss:0.031\n",
      "Epoch:30 Batch:10 Loss:0.030\n",
      "Epoch:40 Batch:10 Loss:0.034\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 32.30138381487341 setps: 47 count: 47\n",
      "reward: 17.258005741147645 setps: 36 count: 83\n",
      "reward: 39.22575215969119 setps: 40 count: 123\n",
      "reward: 27.49614216379705 setps: 37 count: 160\n",
      "reward: 20.268955967796497 setps: 40 count: 200\n",
      "reward: 24.74644890056661 setps: 39 count: 239\n",
      "reward: 17.570284135719707 setps: 37 count: 276\n",
      "reward: 21.039832768941416 setps: 36 count: 312\n",
      "reward: 39.326780029539066 setps: 32 count: 344\n",
      "reward: 36.26424092972447 setps: 40 count: 384\n",
      "reward: 19.257273178493783 setps: 36 count: 420\n",
      "reward: 19.275975565720003 setps: 34 count: 454\n",
      "reward: 19.51008664446126 setps: 39 count: 493\n",
      "reward: 21.046705804886003 setps: 30 count: 523\n",
      "reward: 24.296878673117316 setps: 46 count: 569\n",
      "reward: 38.07927477175399 setps: 29 count: 598\n",
      "reward: 47.121842918342736 setps: 45 count: 643\n",
      "reward: 17.18675813096634 setps: 39 count: 682\n",
      "reward: 19.15056052322761 setps: 35 count: 717\n",
      "reward: 17.24825025244063 setps: 36 count: 753\n",
      "reward: 23.126264389669814 setps: 47 count: 800\n",
      "reward: 20.230228074137987 setps: 39 count: 839\n",
      "reward: 40.12010765195155 setps: 34 count: 873\n",
      "reward: 39.255617931517186 setps: 33 count: 906\n",
      "reward: 26.07877267373987 setps: 39 count: 945\n",
      "reward: 21.465444127383673 setps: 43 count: 988\n",
      "avg rewards: 26.459533381677186\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.15380\n",
      "Epoch:20 Batch:14 Loss:0.02566\n",
      "Epoch:40 Batch:14 Loss:0.02180\n",
      "Epoch:60 Batch:14 Loss:0.01829\n",
      "Epoch:80 Batch:14 Loss:0.01761\n",
      "Epoch:100 Batch:14 Loss:0.01560\n",
      "Epoch:120 Batch:14 Loss:0.01494\n",
      "Epoch:140 Batch:14 Loss:0.01486\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.030\n",
      "Epoch:10 Batch:10 Loss:0.031\n",
      "Epoch:20 Batch:10 Loss:0.029\n",
      "Epoch:30 Batch:10 Loss:0.032\n",
      "Epoch:40 Batch:10 Loss:0.032\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 38.302412814262794 setps: 26 count: 26\n",
      "reward: 36.52529729301604 setps: 25 count: 51\n",
      "reward: 38.39749312324711 setps: 28 count: 79\n",
      "reward: 22.69194151555711 setps: 45 count: 124\n",
      "reward: 36.06105650470126 setps: 24 count: 148\n",
      "reward: 38.25431021300465 setps: 27 count: 175\n",
      "reward: 40.178729121196376 setps: 30 count: 205\n",
      "reward: 36.36233119541138 setps: 24 count: 229\n",
      "reward: 41.18599720461643 setps: 31 count: 260\n",
      "reward: 36.80656006397622 setps: 24 count: 284\n",
      "reward: 30.42538912710443 setps: 58 count: 342\n",
      "reward: 21.37887309212383 setps: 42 count: 384\n",
      "reward: 39.09789343624725 setps: 28 count: 412\n",
      "reward: 38.80195266030205 setps: 28 count: 440\n",
      "reward: 37.59756558632653 setps: 28 count: 468\n",
      "reward: 37.896381295706675 setps: 27 count: 495\n",
      "reward: 37.471182687100374 setps: 25 count: 520\n",
      "reward: 37.864226708136265 setps: 28 count: 548\n",
      "reward: 36.68379778636154 setps: 24 count: 572\n",
      "reward: 36.977035683895515 setps: 26 count: 598\n",
      "reward: 37.61042133045703 setps: 25 count: 623\n",
      "reward: 39.42306333762828 setps: 29 count: 652\n",
      "reward: 50.05991567113377 setps: 40 count: 692\n",
      "reward: 38.379778703895866 setps: 28 count: 720\n",
      "reward: 33.39593239859096 setps: 21 count: 741\n",
      "reward: 33.03439735370338 setps: 25 count: 766\n",
      "reward: 34.97817127438175 setps: 33 count: 799\n",
      "reward: 37.04648287851596 setps: 24 count: 823\n",
      "reward: 23.03992845837637 setps: 43 count: 866\n",
      "reward: 39.2932219351831 setps: 28 count: 894\n",
      "reward: 38.434002659638644 setps: 28 count: 922\n",
      "reward: 41.2625626885725 setps: 30 count: 952\n",
      "reward: 36.54212626017107 setps: 33 count: 985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg rewards: 36.4078918806831\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.14642\n",
      "Epoch:20 Batch:15 Loss:0.02378\n",
      "Epoch:40 Batch:15 Loss:0.01965\n",
      "Epoch:60 Batch:15 Loss:0.01700\n",
      "Epoch:80 Batch:15 Loss:0.01598\n",
      "Epoch:100 Batch:15 Loss:0.01497\n",
      "Epoch:120 Batch:15 Loss:0.01447\n",
      "Epoch:140 Batch:15 Loss:0.01315\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.032\n",
      "Epoch:10 Batch:10 Loss:0.029\n",
      "Epoch:20 Batch:10 Loss:0.031\n",
      "Epoch:30 Batch:10 Loss:0.031\n",
      "Epoch:40 Batch:10 Loss:0.032\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 36.864749989782275 setps: 29 count: 29\n",
      "reward: 36.202753465338894 setps: 27 count: 56\n",
      "reward: 36.52800329914899 setps: 29 count: 85\n",
      "reward: 17.733720498539334 setps: 38 count: 123\n",
      "reward: 28.994717618229338 setps: 32 count: 155\n",
      "reward: 36.7287658391986 setps: 30 count: 185\n",
      "reward: 19.893647136418444 setps: 36 count: 221\n",
      "reward: 34.35647299890843 setps: 25 count: 246\n",
      "reward: 17.601027609696022 setps: 33 count: 279\n",
      "reward: 21.04308800807485 setps: 39 count: 318\n",
      "reward: 39.65099356043066 setps: 33 count: 351\n",
      "reward: 36.43145538609097 setps: 28 count: 379\n",
      "reward: 36.321370294282666 setps: 28 count: 407\n",
      "reward: 43.04864347997063 setps: 42 count: 449\n",
      "reward: 26.832392730755974 setps: 38 count: 487\n",
      "reward: 24.068216304015365 setps: 32 count: 519\n",
      "reward: 20.125342999635905 setps: 43 count: 562\n",
      "reward: 37.90251708912111 setps: 31 count: 593\n",
      "reward: 21.69537192260177 setps: 30 count: 623\n",
      "reward: 27.091649109628634 setps: 46 count: 669\n",
      "reward: 18.13006185421982 setps: 40 count: 709\n",
      "reward: 23.52078504125675 setps: 36 count: 745\n",
      "reward: 18.30311784177029 setps: 34 count: 779\n",
      "reward: 33.04203218537005 setps: 31 count: 810\n",
      "reward: 30.47806370528415 setps: 34 count: 844\n",
      "reward: 24.088490736353556 setps: 27 count: 871\n",
      "reward: 33.28150658213708 setps: 25 count: 896\n",
      "reward: 18.190878513682385 setps: 44 count: 940\n",
      "reward: 38.24432390537113 setps: 30 count: 970\n",
      "avg rewards: 28.841177920872905\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.12651\n",
      "Epoch:20 Batch:16 Loss:0.02313\n",
      "Epoch:40 Batch:16 Loss:0.01997\n",
      "Epoch:60 Batch:16 Loss:0.01711\n",
      "Epoch:80 Batch:16 Loss:0.01517\n",
      "Epoch:100 Batch:16 Loss:0.01293\n",
      "Epoch:120 Batch:16 Loss:0.01423\n",
      "Epoch:140 Batch:16 Loss:0.01341\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.025\n",
      "Epoch:10 Batch:10 Loss:0.024\n",
      "Epoch:20 Batch:10 Loss:0.025\n",
      "Epoch:30 Batch:10 Loss:0.024\n",
      "Epoch:40 Batch:10 Loss:0.027\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 21.714602380634464 setps: 43 count: 43\n",
      "reward: 20.596166425844416 setps: 35 count: 78\n",
      "reward: 15.649035313632343 setps: 32 count: 110\n",
      "reward: 22.045733242208367 setps: 41 count: 151\n",
      "reward: 52.78877154017098 setps: 84 count: 235\n",
      "reward: 20.12075560132216 setps: 33 count: 268\n",
      "reward: 76.23972664998288 setps: 81 count: 349\n",
      "reward: 77.6524937295777 setps: 86 count: 435\n",
      "reward: 76.52462547014727 setps: 82 count: 517\n",
      "reward: 33.438892883308284 setps: 58 count: 575\n",
      "reward: 19.05971396284294 setps: 39 count: 614\n",
      "reward: 21.669501781724097 setps: 41 count: 655\n",
      "reward: 57.74752231306629 setps: 92 count: 747\n",
      "reward: 27.34505345646467 setps: 36 count: 783\n",
      "reward: 35.35049325173895 setps: 60 count: 843\n",
      "reward: 21.82256056210608 setps: 42 count: 885\n",
      "reward: 14.846823013485114 setps: 34 count: 919\n",
      "reward: 26.056832381925783 setps: 51 count: 970\n",
      "avg rewards: 35.59273910889905\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.14608\n",
      "Epoch:20 Batch:17 Loss:0.02218\n",
      "Epoch:40 Batch:17 Loss:0.01743\n",
      "Epoch:60 Batch:17 Loss:0.01593\n",
      "Epoch:80 Batch:17 Loss:0.01554\n",
      "Epoch:100 Batch:17 Loss:0.01515\n",
      "Epoch:120 Batch:17 Loss:0.01310\n",
      "Epoch:140 Batch:17 Loss:0.01259\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.028\n",
      "Epoch:10 Batch:10 Loss:0.026\n",
      "Epoch:20 Batch:10 Loss:0.027\n",
      "Epoch:30 Batch:10 Loss:0.028\n",
      "Epoch:40 Batch:10 Loss:0.027\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 21.547254039629472 setps: 41 count: 41\n",
      "reward: 52.990808899860724 setps: 51 count: 92\n",
      "reward: 42.62034430417697 setps: 44 count: 136\n",
      "reward: 51.81192198706122 setps: 52 count: 188\n",
      "reward: 54.71793384848713 setps: 51 count: 239\n",
      "reward: 51.868984908996204 setps: 53 count: 292\n",
      "reward: 35.594927337548874 setps: 41 count: 333\n",
      "reward: 35.68716034422833 setps: 43 count: 376\n",
      "reward: 60.077168914974 setps: 59 count: 435\n",
      "reward: 36.22273032582089 setps: 60 count: 495\n",
      "reward: 48.05057266276562 setps: 47 count: 542\n",
      "reward: 47.597753367880074 setps: 46 count: 588\n",
      "reward: 46.70915188599465 setps: 45 count: 633\n",
      "reward: 65.01010631816169 setps: 69 count: 702\n",
      "reward: 56.741898249187216 setps: 57 count: 759\n",
      "reward: 23.772221535115385 setps: 48 count: 807\n",
      "reward: 40.21958537423199 setps: 43 count: 850\n",
      "reward: 28.030751745519225 setps: 54 count: 904\n",
      "reward: 50.616810118789736 setps: 49 count: 953\n",
      "avg rewards: 44.73095190360154\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.13000\n",
      "Epoch:20 Batch:18 Loss:0.02165\n",
      "Epoch:40 Batch:18 Loss:0.01713\n",
      "Epoch:60 Batch:18 Loss:0.01505\n",
      "Epoch:80 Batch:18 Loss:0.01710\n",
      "Epoch:100 Batch:18 Loss:0.01240\n",
      "Epoch:120 Batch:18 Loss:0.01355\n",
      "Epoch:140 Batch:18 Loss:0.01211\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.027\n",
      "Epoch:10 Batch:10 Loss:0.027\n",
      "Epoch:20 Batch:10 Loss:0.024\n",
      "Epoch:30 Batch:10 Loss:0.025\n",
      "Epoch:40 Batch:10 Loss:0.027\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 36.2465540052508 setps: 35 count: 35\n",
      "reward: 21.501698737186967 setps: 39 count: 74\n",
      "reward: 20.622527636360605 setps: 37 count: 111\n",
      "reward: 40.372934367868574 setps: 32 count: 143\n",
      "reward: 18.962424026461665 setps: 40 count: 183\n",
      "reward: 38.649709420172435 setps: 35 count: 218\n",
      "reward: 45.02694657573446 setps: 40 count: 258\n",
      "reward: 21.275174520586724 setps: 40 count: 298\n",
      "reward: 34.88351542813616 setps: 28 count: 326\n",
      "reward: 25.495202795979278 setps: 32 count: 358\n",
      "reward: 20.886815423145897 setps: 42 count: 400\n",
      "reward: 18.68179201205349 setps: 34 count: 434\n",
      "reward: 21.063710998687025 setps: 33 count: 467\n",
      "reward: 22.657037874781235 setps: 13 count: 480\n",
      "reward: 24.993464906499135 setps: 36 count: 516\n",
      "reward: 25.714754481230923 setps: 34 count: 550\n",
      "reward: 20.84764024614269 setps: 40 count: 590\n",
      "reward: 21.344486025866352 setps: 31 count: 621\n",
      "reward: 27.660091634903797 setps: 33 count: 654\n",
      "reward: 31.192220958035602 setps: 44 count: 698\n",
      "reward: 26.513102302738126 setps: 42 count: 740\n",
      "reward: 17.90608561617992 setps: 38 count: 778\n",
      "reward: 26.823570379051667 setps: 48 count: 826\n",
      "reward: 23.046357784006965 setps: 45 count: 871\n",
      "reward: 18.90987096268364 setps: 39 count: 910\n",
      "reward: 14.299575551380986 setps: 45 count: 955\n",
      "reward: 45.08340117399785 setps: 43 count: 998\n",
      "avg rewards: 26.32076540167121\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.11829\n",
      "Epoch:20 Batch:19 Loss:0.02119\n",
      "Epoch:40 Batch:19 Loss:0.01631\n",
      "Epoch:60 Batch:19 Loss:0.01410\n",
      "Epoch:80 Batch:19 Loss:0.01282\n",
      "Epoch:100 Batch:19 Loss:0.01329\n",
      "Epoch:120 Batch:19 Loss:0.01347\n",
      "Epoch:140 Batch:19 Loss:0.01208\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.025\n",
      "Epoch:10 Batch:10 Loss:0.026\n",
      "Epoch:20 Batch:10 Loss:0.025\n",
      "Epoch:30 Batch:10 Loss:0.023\n",
      "Epoch:40 Batch:10 Loss:0.026\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 20.30727878524922 setps: 43 count: 43\n",
      "reward: 35.26075385571604 setps: 35 count: 78\n",
      "reward: 29.494310711832078 setps: 33 count: 111\n",
      "reward: 17.19156562645949 setps: 34 count: 145\n",
      "reward: 37.76358561594533 setps: 32 count: 177\n",
      "reward: 38.75247859741502 setps: 37 count: 214\n",
      "reward: 19.34329604529921 setps: 45 count: 259\n",
      "reward: 20.020497634363707 setps: 36 count: 295\n",
      "reward: 20.675037952596906 setps: 41 count: 336\n",
      "reward: 18.687883053968832 setps: 47 count: 383\n",
      "reward: 29.777610354239002 setps: 38 count: 421\n",
      "reward: 19.76870207551983 setps: 47 count: 468\n",
      "reward: 28.660071062752102 setps: 36 count: 504\n",
      "reward: 38.81958401589508 setps: 35 count: 539\n",
      "reward: 28.517391002127265 setps: 39 count: 578\n",
      "reward: 31.506881239160432 setps: 28 count: 606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 31.07114233845059 setps: 28 count: 634\n",
      "reward: 21.612283610014135 setps: 31 count: 665\n",
      "reward: 36.53926947592552 setps: 30 count: 695\n",
      "reward: 22.391397773679635 setps: 32 count: 727\n",
      "reward: 33.490889530719144 setps: 34 count: 761\n",
      "reward: 34.273674527059484 setps: 30 count: 791\n",
      "reward: 37.38782394293811 setps: 33 count: 824\n",
      "reward: 27.213183984313222 setps: 50 count: 874\n",
      "reward: 15.054731536925832 setps: 41 count: 915\n",
      "reward: 17.96840026828721 setps: 42 count: 957\n",
      "reward: 38.899102530578965 setps: 34 count: 991\n",
      "avg rewards: 27.79440100546042\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.10306\n",
      "Epoch:20 Batch:20 Loss:0.02208\n",
      "Epoch:40 Batch:20 Loss:0.01801\n",
      "Epoch:60 Batch:20 Loss:0.01346\n",
      "Epoch:80 Batch:20 Loss:0.01329\n",
      "Epoch:100 Batch:20 Loss:0.01067\n",
      "Epoch:120 Batch:20 Loss:0.01218\n",
      "Epoch:140 Batch:20 Loss:0.01332\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.030\n",
      "Epoch:10 Batch:10 Loss:0.030\n",
      "Epoch:20 Batch:10 Loss:0.030\n",
      "Epoch:30 Batch:10 Loss:0.032\n",
      "Epoch:40 Batch:10 Loss:0.030\n",
      "Done!\n",
      "############# start HalfCheetahBulletEnv-v0 training ###################\n",
      "(50, 1000, 26)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1128.1485240440795 setps: 800 count: 800\n",
      "avg rewards: -1128.1485240440795\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:1.41123\n",
      "Epoch:20 Batch:1 Loss:0.22886\n",
      "Epoch:40 Batch:1 Loss:0.17657\n",
      "Epoch:60 Batch:1 Loss:0.14142\n",
      "Epoch:80 Batch:1 Loss:0.11836\n",
      "Epoch:100 Batch:1 Loss:0.09964\n",
      "Epoch:120 Batch:1 Loss:0.08694\n",
      "Epoch:140 Batch:1 Loss:0.08165\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.260\n",
      "Epoch:10 Batch:10 Loss:0.259\n",
      "Epoch:20 Batch:10 Loss:0.256\n",
      "Epoch:30 Batch:10 Loss:0.252\n",
      "Epoch:40 Batch:10 Loss:0.253\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1131.0711871079022 setps: 800 count: 800\n",
      "avg rewards: -1131.0711871079022\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:1.26012\n",
      "Epoch:20 Batch:2 Loss:0.12087\n",
      "Epoch:40 Batch:2 Loss:0.08813\n",
      "Epoch:60 Batch:2 Loss:0.06644\n",
      "Epoch:80 Batch:2 Loss:0.05593\n",
      "Epoch:100 Batch:2 Loss:0.04710\n",
      "Epoch:120 Batch:2 Loss:0.04538\n",
      "Epoch:140 Batch:2 Loss:0.04239\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.223\n",
      "Epoch:10 Batch:10 Loss:0.216\n",
      "Epoch:20 Batch:10 Loss:0.215\n",
      "Epoch:30 Batch:10 Loss:0.219\n",
      "Epoch:40 Batch:10 Loss:0.216\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1251.4747997208096 setps: 800 count: 800\n",
      "avg rewards: -1251.4747997208096\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:1.12473\n",
      "Epoch:20 Batch:3 Loss:0.08473\n",
      "Epoch:40 Batch:3 Loss:0.05831\n",
      "Epoch:60 Batch:3 Loss:0.04420\n",
      "Epoch:80 Batch:3 Loss:0.03984\n",
      "Epoch:100 Batch:3 Loss:0.03485\n",
      "Epoch:120 Batch:3 Loss:0.03507\n",
      "Epoch:140 Batch:3 Loss:0.03146\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.200\n",
      "Epoch:10 Batch:10 Loss:0.196\n",
      "Epoch:20 Batch:10 Loss:0.194\n",
      "Epoch:30 Batch:10 Loss:0.192\n",
      "Epoch:40 Batch:10 Loss:0.192\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1005.2339373112471 setps: 800 count: 800\n",
      "avg rewards: -1005.2339373112471\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:1.12817\n",
      "Epoch:20 Batch:4 Loss:0.07103\n",
      "Epoch:40 Batch:4 Loss:0.05071\n",
      "Epoch:60 Batch:4 Loss:0.04108\n",
      "Epoch:80 Batch:4 Loss:0.03682\n",
      "Epoch:100 Batch:4 Loss:0.03335\n",
      "Epoch:120 Batch:4 Loss:0.03188\n",
      "Epoch:140 Batch:4 Loss:0.02849\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.185\n",
      "Epoch:10 Batch:10 Loss:0.177\n",
      "Epoch:20 Batch:10 Loss:0.173\n",
      "Epoch:30 Batch:10 Loss:0.168\n",
      "Epoch:40 Batch:10 Loss:0.169\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1047.3917442309814 setps: 800 count: 800\n",
      "avg rewards: -1047.3917442309814\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.92790\n",
      "Epoch:20 Batch:5 Loss:0.05690\n",
      "Epoch:40 Batch:5 Loss:0.04737\n",
      "Epoch:60 Batch:5 Loss:0.03592\n",
      "Epoch:80 Batch:5 Loss:0.03395\n",
      "Epoch:100 Batch:5 Loss:0.02957\n",
      "Epoch:120 Batch:5 Loss:0.02786\n",
      "Epoch:140 Batch:5 Loss:0.02750\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.117\n",
      "Epoch:10 Batch:10 Loss:0.113\n",
      "Epoch:20 Batch:10 Loss:0.112\n",
      "Epoch:30 Batch:10 Loss:0.112\n",
      "Epoch:40 Batch:10 Loss:0.114\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -955.5324005652723 setps: 800 count: 800\n",
      "avg rewards: -955.5324005652723\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.83970\n",
      "Epoch:20 Batch:6 Loss:0.05471\n",
      "Epoch:40 Batch:6 Loss:0.04099\n",
      "Epoch:60 Batch:6 Loss:0.03257\n",
      "Epoch:80 Batch:6 Loss:0.02894\n",
      "Epoch:100 Batch:6 Loss:0.02867\n",
      "Epoch:120 Batch:6 Loss:0.02549\n",
      "Epoch:140 Batch:6 Loss:0.02448\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.111\n",
      "Epoch:10 Batch:10 Loss:0.108\n",
      "Epoch:20 Batch:10 Loss:0.109\n",
      "Epoch:30 Batch:10 Loss:0.112\n",
      "Epoch:40 Batch:10 Loss:0.110\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1153.4917008322166 setps: 800 count: 800\n",
      "avg rewards: -1153.4917008322166\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:1.03647\n",
      "Epoch:20 Batch:7 Loss:0.05178\n",
      "Epoch:40 Batch:7 Loss:0.03582\n",
      "Epoch:60 Batch:7 Loss:0.03269\n",
      "Epoch:80 Batch:7 Loss:0.03081\n",
      "Epoch:100 Batch:7 Loss:0.02682\n",
      "Epoch:120 Batch:7 Loss:0.02585\n",
      "Epoch:140 Batch:7 Loss:0.02421\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.102\n",
      "Epoch:10 Batch:10 Loss:0.100\n",
      "Epoch:20 Batch:10 Loss:0.099\n",
      "Epoch:30 Batch:10 Loss:0.095\n",
      "Epoch:40 Batch:10 Loss:0.099\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1337.8074103612055 setps: 800 count: 800\n",
      "avg rewards: -1337.8074103612055\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.79489\n",
      "Epoch:20 Batch:8 Loss:0.04516\n",
      "Epoch:40 Batch:8 Loss:0.03430\n",
      "Epoch:60 Batch:8 Loss:0.02881\n",
      "Epoch:80 Batch:8 Loss:0.02629\n",
      "Epoch:100 Batch:8 Loss:0.02590\n",
      "Epoch:120 Batch:8 Loss:0.02393\n",
      "Epoch:140 Batch:8 Loss:0.02382\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.115\n",
      "Epoch:10 Batch:10 Loss:0.108\n",
      "Epoch:20 Batch:10 Loss:0.105\n",
      "Epoch:30 Batch:10 Loss:0.105\n",
      "Epoch:40 Batch:10 Loss:0.105\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1342.324352361483 setps: 800 count: 800\n",
      "avg rewards: -1342.324352361483\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.47994\n",
      "Epoch:20 Batch:9 Loss:0.04501\n",
      "Epoch:40 Batch:9 Loss:0.03359\n",
      "Epoch:60 Batch:9 Loss:0.02942\n",
      "Epoch:80 Batch:9 Loss:0.02683\n",
      "Epoch:100 Batch:9 Loss:0.02379\n",
      "Epoch:120 Batch:9 Loss:0.02301\n",
      "Epoch:140 Batch:9 Loss:0.02271\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.093\n",
      "Epoch:10 Batch:10 Loss:0.090\n",
      "Epoch:20 Batch:10 Loss:0.093\n",
      "Epoch:30 Batch:10 Loss:0.091\n",
      "Epoch:40 Batch:10 Loss:0.093\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1285.3233653292327 setps: 800 count: 800\n",
      "avg rewards: -1285.3233653292327\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.38723\n",
      "Epoch:20 Batch:10 Loss:0.04189\n",
      "Epoch:40 Batch:10 Loss:0.03087\n",
      "Epoch:60 Batch:10 Loss:0.02611\n",
      "Epoch:80 Batch:10 Loss:0.02359\n",
      "Epoch:100 Batch:10 Loss:0.02283\n",
      "Epoch:120 Batch:10 Loss:0.02163\n",
      "Epoch:140 Batch:10 Loss:0.02112\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.088\n",
      "Epoch:10 Batch:10 Loss:0.085\n",
      "Epoch:20 Batch:10 Loss:0.086\n",
      "Epoch:30 Batch:10 Loss:0.080\n",
      "Epoch:40 Batch:10 Loss:0.081\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1681.3688790743195 setps: 800 count: 800\n",
      "avg rewards: -1681.3688790743195\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.46820\n",
      "Epoch:20 Batch:11 Loss:0.03933\n",
      "Epoch:40 Batch:11 Loss:0.02534\n",
      "Epoch:60 Batch:11 Loss:0.02501\n",
      "Epoch:80 Batch:11 Loss:0.02039\n",
      "Epoch:100 Batch:11 Loss:0.02031\n",
      "Epoch:120 Batch:11 Loss:0.01971\n",
      "Epoch:140 Batch:11 Loss:0.01764\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:10 Loss:0.086\n",
      "Epoch:10 Batch:10 Loss:0.086\n",
      "Epoch:20 Batch:10 Loss:0.081\n",
      "Epoch:30 Batch:10 Loss:0.083\n",
      "Epoch:40 Batch:10 Loss:0.081\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1337.8664057871674 setps: 800 count: 800\n",
      "avg rewards: -1337.8664057871674\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.33652\n",
      "Epoch:20 Batch:12 Loss:0.03411\n",
      "Epoch:40 Batch:12 Loss:0.02438\n",
      "Epoch:60 Batch:12 Loss:0.02185\n",
      "Epoch:80 Batch:12 Loss:0.02023\n",
      "Epoch:100 Batch:12 Loss:0.01824\n",
      "Epoch:120 Batch:12 Loss:0.01942\n",
      "Epoch:140 Batch:12 Loss:0.01689\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.080\n",
      "Epoch:10 Batch:10 Loss:0.079\n",
      "Epoch:20 Batch:10 Loss:0.080\n",
      "Epoch:30 Batch:10 Loss:0.076\n",
      "Epoch:40 Batch:10 Loss:0.078\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1176.7620869666255 setps: 800 count: 800\n",
      "avg rewards: -1176.7620869666255\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.32488\n",
      "Epoch:20 Batch:13 Loss:0.03323\n",
      "Epoch:40 Batch:13 Loss:0.02885\n",
      "Epoch:60 Batch:13 Loss:0.02129\n",
      "Epoch:80 Batch:13 Loss:0.02154\n",
      "Epoch:100 Batch:13 Loss:0.01902\n",
      "Epoch:120 Batch:13 Loss:0.01888\n",
      "Epoch:140 Batch:13 Loss:0.01588\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.071\n",
      "Epoch:10 Batch:10 Loss:0.070\n",
      "Epoch:20 Batch:10 Loss:0.066\n",
      "Epoch:30 Batch:10 Loss:0.069\n",
      "Epoch:40 Batch:10 Loss:0.067\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1170.7223326221936 setps: 800 count: 800\n",
      "avg rewards: -1170.7223326221936\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.26876\n",
      "Epoch:20 Batch:14 Loss:0.03095\n",
      "Epoch:40 Batch:14 Loss:0.02723\n",
      "Epoch:60 Batch:14 Loss:0.02291\n",
      "Epoch:80 Batch:14 Loss:0.01887\n",
      "Epoch:100 Batch:14 Loss:0.01905\n",
      "Epoch:120 Batch:14 Loss:0.01884\n",
      "Epoch:140 Batch:14 Loss:0.01754\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.074\n",
      "Epoch:10 Batch:10 Loss:0.070\n",
      "Epoch:20 Batch:10 Loss:0.070\n",
      "Epoch:30 Batch:10 Loss:0.068\n",
      "Epoch:40 Batch:10 Loss:0.073\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -875.9630102841102 setps: 800 count: 800\n",
      "avg rewards: -875.9630102841102\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.24730\n",
      "Epoch:20 Batch:15 Loss:0.03078\n",
      "Epoch:40 Batch:15 Loss:0.02406\n",
      "Epoch:60 Batch:15 Loss:0.02273\n",
      "Epoch:80 Batch:15 Loss:0.02081\n",
      "Epoch:100 Batch:15 Loss:0.01887\n",
      "Epoch:120 Batch:15 Loss:0.01817\n",
      "Epoch:140 Batch:15 Loss:0.01701\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.054\n",
      "Epoch:10 Batch:10 Loss:0.054\n",
      "Epoch:20 Batch:10 Loss:0.051\n",
      "Epoch:30 Batch:10 Loss:0.052\n",
      "Epoch:40 Batch:10 Loss:0.050\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1218.2860042532416 setps: 800 count: 800\n",
      "avg rewards: -1218.2860042532416\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.24820\n",
      "Epoch:20 Batch:16 Loss:0.02887\n",
      "Epoch:40 Batch:16 Loss:0.02290\n",
      "Epoch:60 Batch:16 Loss:0.02137\n",
      "Epoch:80 Batch:16 Loss:0.01969\n",
      "Epoch:100 Batch:16 Loss:0.01792\n",
      "Epoch:120 Batch:16 Loss:0.01768\n",
      "Epoch:140 Batch:16 Loss:0.01649\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.052\n",
      "Epoch:10 Batch:10 Loss:0.055\n",
      "Epoch:20 Batch:10 Loss:0.049\n",
      "Epoch:30 Batch:10 Loss:0.050\n",
      "Epoch:40 Batch:10 Loss:0.051\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1231.4212141229054 setps: 800 count: 800\n",
      "avg rewards: -1231.4212141229054\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.25885\n",
      "Epoch:20 Batch:17 Loss:0.03639\n",
      "Epoch:40 Batch:17 Loss:0.02139\n",
      "Epoch:60 Batch:17 Loss:0.02348\n",
      "Epoch:80 Batch:17 Loss:0.01807\n",
      "Epoch:100 Batch:17 Loss:0.01677\n",
      "Epoch:120 Batch:17 Loss:0.01633\n",
      "Epoch:140 Batch:17 Loss:0.01698\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.054\n",
      "Epoch:10 Batch:10 Loss:0.051\n",
      "Epoch:20 Batch:10 Loss:0.054\n",
      "Epoch:30 Batch:10 Loss:0.053\n",
      "Epoch:40 Batch:10 Loss:0.051\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1326.0524567893476 setps: 800 count: 800\n",
      "avg rewards: -1326.0524567893476\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.19770\n",
      "Epoch:20 Batch:18 Loss:0.03135\n",
      "Epoch:40 Batch:18 Loss:0.02470\n",
      "Epoch:60 Batch:18 Loss:0.01905\n",
      "Epoch:80 Batch:18 Loss:0.01604\n",
      "Epoch:100 Batch:18 Loss:0.02002\n",
      "Epoch:120 Batch:18 Loss:0.01560\n",
      "Epoch:140 Batch:18 Loss:0.01383\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.053\n",
      "Epoch:10 Batch:10 Loss:0.053\n",
      "Epoch:20 Batch:10 Loss:0.050\n",
      "Epoch:30 Batch:10 Loss:0.051\n",
      "Epoch:40 Batch:10 Loss:0.052\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -996.5524203201005 setps: 800 count: 800\n",
      "avg rewards: -996.5524203201005\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.19844\n",
      "Epoch:20 Batch:19 Loss:0.02712\n",
      "Epoch:40 Batch:19 Loss:0.01815\n",
      "Epoch:60 Batch:19 Loss:0.01888\n",
      "Epoch:80 Batch:19 Loss:0.01635\n",
      "Epoch:100 Batch:19 Loss:0.01590\n",
      "Epoch:120 Batch:19 Loss:0.01467\n",
      "Epoch:140 Batch:19 Loss:0.01435\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.053\n",
      "Epoch:10 Batch:10 Loss:0.051\n",
      "Epoch:20 Batch:10 Loss:0.051\n",
      "Epoch:30 Batch:10 Loss:0.050\n",
      "Epoch:40 Batch:10 Loss:0.052\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -177.13222398596716 setps: 800 count: 800\n",
      "avg rewards: -177.13222398596716\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.19308\n",
      "Epoch:20 Batch:20 Loss:0.02810\n",
      "Epoch:40 Batch:20 Loss:0.02088\n",
      "Epoch:60 Batch:20 Loss:0.01963\n",
      "Epoch:80 Batch:20 Loss:0.01857\n",
      "Epoch:100 Batch:20 Loss:0.01697\n",
      "Epoch:120 Batch:20 Loss:0.01653\n",
      "Epoch:140 Batch:20 Loss:0.01462\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.048\n",
      "Epoch:10 Batch:10 Loss:0.046\n",
      "Epoch:20 Batch:10 Loss:0.047\n",
      "Epoch:30 Batch:10 Loss:0.046\n",
      "Epoch:40 Batch:10 Loss:0.048\n",
      "Done!\n",
      "############# start AntBulletEnv-v0 training ###################\n",
      "(50, 1000, 28)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 425.9359149051115 setps: 800 count: 800\n",
      "reward: 33.65539614537992 setps: 65 count: 865\n",
      "avg rewards: 229.7956555252457\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:1.38282\n",
      "Epoch:20 Batch:1 Loss:0.10477\n",
      "Epoch:40 Batch:1 Loss:0.06842\n",
      "Epoch:60 Batch:1 Loss:0.05789\n",
      "Epoch:80 Batch:1 Loss:0.05122\n",
      "Epoch:100 Batch:1 Loss:0.04376\n",
      "Epoch:120 Batch:1 Loss:0.03850\n",
      "Epoch:140 Batch:1 Loss:0.03320\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.307\n",
      "Epoch:10 Batch:10 Loss:0.302\n",
      "Epoch:20 Batch:10 Loss:0.302\n",
      "Epoch:30 Batch:10 Loss:0.305\n",
      "Epoch:40 Batch:10 Loss:0.297\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 214.3438336226115 setps: 800 count: 800\n",
      "avg rewards: 214.3438336226115\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:1.34412\n",
      "Epoch:20 Batch:2 Loss:0.07307\n",
      "Epoch:40 Batch:2 Loss:0.04311\n",
      "Epoch:60 Batch:2 Loss:0.03430\n",
      "Epoch:80 Batch:2 Loss:0.02970\n",
      "Epoch:100 Batch:2 Loss:0.02465\n",
      "Epoch:120 Batch:2 Loss:0.02173\n",
      "Epoch:140 Batch:2 Loss:0.01930\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.289\n",
      "Epoch:10 Batch:10 Loss:0.286\n",
      "Epoch:20 Batch:10 Loss:0.284\n",
      "Epoch:30 Batch:10 Loss:0.286\n",
      "Epoch:40 Batch:10 Loss:0.281\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 217.44654206249683 setps: 800 count: 800\n",
      "avg rewards: 217.44654206249683\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:1.30480\n",
      "Epoch:20 Batch:3 Loss:0.05364\n",
      "Epoch:40 Batch:3 Loss:0.03229\n",
      "Epoch:60 Batch:3 Loss:0.02531\n",
      "Epoch:80 Batch:3 Loss:0.02023\n",
      "Epoch:100 Batch:3 Loss:0.01760\n",
      "Epoch:120 Batch:3 Loss:0.01593\n",
      "Epoch:140 Batch:3 Loss:0.01401\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.307\n",
      "Epoch:10 Batch:10 Loss:0.296\n",
      "Epoch:20 Batch:10 Loss:0.290\n",
      "Epoch:30 Batch:10 Loss:0.286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:40 Batch:10 Loss:0.287\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 278.5140764536555 setps: 800 count: 800\n",
      "avg rewards: 278.5140764536555\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:1.12942\n",
      "Epoch:20 Batch:4 Loss:0.04047\n",
      "Epoch:40 Batch:4 Loss:0.02362\n",
      "Epoch:60 Batch:4 Loss:0.01803\n",
      "Epoch:80 Batch:4 Loss:0.01630\n",
      "Epoch:100 Batch:4 Loss:0.01455\n",
      "Epoch:120 Batch:4 Loss:0.01279\n",
      "Epoch:140 Batch:4 Loss:0.01259\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.266\n",
      "Epoch:10 Batch:10 Loss:0.248\n",
      "Epoch:20 Batch:10 Loss:0.249\n",
      "Epoch:30 Batch:10 Loss:0.245\n",
      "Epoch:40 Batch:10 Loss:0.245\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 243.61326357557397 setps: 800 count: 800\n",
      "avg rewards: 243.61326357557397\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.96386\n",
      "Epoch:20 Batch:5 Loss:0.03574\n",
      "Epoch:40 Batch:5 Loss:0.02352\n",
      "Epoch:60 Batch:5 Loss:0.01839\n",
      "Epoch:80 Batch:5 Loss:0.01516\n",
      "Epoch:100 Batch:5 Loss:0.01310\n",
      "Epoch:120 Batch:5 Loss:0.01284\n",
      "Epoch:140 Batch:5 Loss:0.01088\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.204\n",
      "Epoch:10 Batch:10 Loss:0.195\n",
      "Epoch:20 Batch:10 Loss:0.192\n",
      "Epoch:30 Batch:10 Loss:0.191\n",
      "Epoch:40 Batch:10 Loss:0.188\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 472.05862194579936 setps: 800 count: 800\n",
      "avg rewards: 472.05862194579936\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.63406\n",
      "Epoch:20 Batch:6 Loss:0.03073\n",
      "Epoch:40 Batch:6 Loss:0.02060\n",
      "Epoch:60 Batch:6 Loss:0.01587\n",
      "Epoch:80 Batch:6 Loss:0.01481\n",
      "Epoch:100 Batch:6 Loss:0.01257\n",
      "Epoch:120 Batch:6 Loss:0.01172\n",
      "Epoch:140 Batch:6 Loss:0.01027\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.210\n",
      "Epoch:10 Batch:10 Loss:0.204\n",
      "Epoch:20 Batch:10 Loss:0.206\n",
      "Epoch:30 Batch:10 Loss:0.200\n",
      "Epoch:40 Batch:10 Loss:0.202\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 308.10104962965244 setps: 800 count: 800\n",
      "avg rewards: 308.10104962965244\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.72960\n",
      "Epoch:20 Batch:7 Loss:0.02414\n",
      "Epoch:40 Batch:7 Loss:0.01824\n",
      "Epoch:60 Batch:7 Loss:0.01394\n",
      "Epoch:80 Batch:7 Loss:0.01279\n",
      "Epoch:100 Batch:7 Loss:0.01198\n",
      "Epoch:120 Batch:7 Loss:0.01123\n",
      "Epoch:140 Batch:7 Loss:0.01029\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.183\n",
      "Epoch:10 Batch:10 Loss:0.182\n",
      "Epoch:20 Batch:10 Loss:0.176\n",
      "Epoch:30 Batch:10 Loss:0.181\n",
      "Epoch:40 Batch:10 Loss:0.178\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 480.33412479175075 setps: 800 count: 800\n",
      "avg rewards: 480.33412479175075\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.56665\n",
      "Epoch:20 Batch:8 Loss:0.02445\n",
      "Epoch:40 Batch:8 Loss:0.01755\n",
      "Epoch:60 Batch:8 Loss:0.01405\n",
      "Epoch:80 Batch:8 Loss:0.01263\n",
      "Epoch:100 Batch:8 Loss:0.01026\n",
      "Epoch:120 Batch:8 Loss:0.01068\n",
      "Epoch:140 Batch:8 Loss:0.00983\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.182\n",
      "Epoch:10 Batch:10 Loss:0.182\n",
      "Epoch:20 Batch:10 Loss:0.180\n",
      "Epoch:30 Batch:10 Loss:0.179\n",
      "Epoch:40 Batch:10 Loss:0.176\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 443.04588261519507 setps: 800 count: 800\n",
      "avg rewards: 443.04588261519507\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.42404\n",
      "Epoch:20 Batch:9 Loss:0.02218\n",
      "Epoch:40 Batch:9 Loss:0.01534\n",
      "Epoch:60 Batch:9 Loss:0.01302\n",
      "Epoch:80 Batch:9 Loss:0.01222\n",
      "Epoch:100 Batch:9 Loss:0.01174\n",
      "Epoch:120 Batch:9 Loss:0.01064\n",
      "Epoch:140 Batch:9 Loss:0.00928\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.182\n",
      "Epoch:10 Batch:10 Loss:0.182\n",
      "Epoch:20 Batch:10 Loss:0.177\n",
      "Epoch:30 Batch:10 Loss:0.176\n",
      "Epoch:40 Batch:10 Loss:0.176\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 54.33861406983054 setps: 136 count: 136\n",
      "reward: 253.3052914534392 setps: 800 count: 936\n",
      "avg rewards: 153.82195276163486\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.38488\n",
      "Epoch:20 Batch:10 Loss:0.02167\n",
      "Epoch:40 Batch:10 Loss:0.01620\n",
      "Epoch:60 Batch:10 Loss:0.01341\n",
      "Epoch:80 Batch:10 Loss:0.01220\n",
      "Epoch:100 Batch:10 Loss:0.01156\n",
      "Epoch:120 Batch:10 Loss:0.01127\n",
      "Epoch:140 Batch:10 Loss:0.01030\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.115\n",
      "Epoch:10 Batch:10 Loss:0.111\n",
      "Epoch:20 Batch:10 Loss:0.112\n",
      "Epoch:30 Batch:10 Loss:0.111\n",
      "Epoch:40 Batch:10 Loss:0.110\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 7.128525892308972 setps: 162 count: 162\n",
      "reward: 29.350720756514043 setps: 239 count: 401\n",
      "reward: 11.00701135325508 setps: 187 count: 588\n",
      "avg rewards: 15.828752667359366\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.40366\n",
      "Epoch:20 Batch:11 Loss:0.02227\n",
      "Epoch:40 Batch:11 Loss:0.01620\n",
      "Epoch:60 Batch:11 Loss:0.01389\n",
      "Epoch:80 Batch:11 Loss:0.01350\n",
      "Epoch:100 Batch:11 Loss:0.01173\n",
      "Epoch:120 Batch:11 Loss:0.01220\n",
      "Epoch:140 Batch:11 Loss:0.01063\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.112\n",
      "Epoch:10 Batch:10 Loss:0.105\n",
      "Epoch:20 Batch:10 Loss:0.105\n",
      "Epoch:30 Batch:10 Loss:0.105\n",
      "Epoch:40 Batch:10 Loss:0.103\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 335.55043645820126 setps: 800 count: 800\n",
      "avg rewards: 335.55043645820126\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.45030\n",
      "Epoch:20 Batch:12 Loss:0.02270\n",
      "Epoch:40 Batch:12 Loss:0.01626\n",
      "Epoch:60 Batch:12 Loss:0.01427\n",
      "Epoch:80 Batch:12 Loss:0.01356\n",
      "Epoch:100 Batch:12 Loss:0.01275\n",
      "Epoch:120 Batch:12 Loss:0.01120\n",
      "Epoch:140 Batch:12 Loss:0.01025\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.114\n",
      "Epoch:10 Batch:10 Loss:0.109\n",
      "Epoch:20 Batch:10 Loss:0.104\n",
      "Epoch:30 Batch:10 Loss:0.106\n",
      "Epoch:40 Batch:10 Loss:0.106\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 273.70204460069044 setps: 800 count: 800\n",
      "avg rewards: 273.70204460069044\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.32004\n",
      "Epoch:20 Batch:13 Loss:0.02103\n",
      "Epoch:40 Batch:13 Loss:0.01595\n",
      "Epoch:60 Batch:13 Loss:0.01332\n",
      "Epoch:80 Batch:13 Loss:0.01146\n",
      "Epoch:100 Batch:13 Loss:0.01199\n",
      "Epoch:120 Batch:13 Loss:0.01097\n",
      "Epoch:140 Batch:13 Loss:0.00979\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.100\n",
      "Epoch:10 Batch:10 Loss:0.098\n",
      "Epoch:20 Batch:10 Loss:0.097\n",
      "Epoch:30 Batch:10 Loss:0.097\n",
      "Epoch:40 Batch:10 Loss:0.096\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 170.28784672149635 setps: 800 count: 800\n",
      "avg rewards: 170.28784672149635\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.27172\n",
      "Epoch:20 Batch:14 Loss:0.02163\n",
      "Epoch:40 Batch:14 Loss:0.01643\n",
      "Epoch:60 Batch:14 Loss:0.01351\n",
      "Epoch:80 Batch:14 Loss:0.01288\n",
      "Epoch:100 Batch:14 Loss:0.01150\n",
      "Epoch:120 Batch:14 Loss:0.00954\n",
      "Epoch:140 Batch:14 Loss:0.01068\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.109\n",
      "Epoch:10 Batch:10 Loss:0.103\n",
      "Epoch:20 Batch:10 Loss:0.101\n",
      "Epoch:30 Batch:10 Loss:0.100\n",
      "Epoch:40 Batch:10 Loss:0.099\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -136.4841075073956 setps: 800 count: 800\n",
      "reward: -18.348030656385536 setps: 123 count: 923\n",
      "avg rewards: -77.41606908189057\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.23841\n",
      "Epoch:20 Batch:15 Loss:0.02147\n",
      "Epoch:40 Batch:15 Loss:0.01647\n",
      "Epoch:60 Batch:15 Loss:0.01419\n",
      "Epoch:80 Batch:15 Loss:0.01276\n",
      "Epoch:100 Batch:15 Loss:0.01157\n",
      "Epoch:120 Batch:15 Loss:0.01117\n",
      "Epoch:140 Batch:15 Loss:0.00976\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.099\n",
      "Epoch:10 Batch:10 Loss:0.094\n",
      "Epoch:20 Batch:10 Loss:0.094\n",
      "Epoch:30 Batch:10 Loss:0.094\n",
      "Epoch:40 Batch:10 Loss:0.093\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 193.3222111081078 setps: 800 count: 800\n",
      "avg rewards: 193.3222111081078\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.23091\n",
      "Epoch:20 Batch:16 Loss:0.02100\n",
      "Epoch:40 Batch:16 Loss:0.01486\n",
      "Epoch:60 Batch:16 Loss:0.01448\n",
      "Epoch:80 Batch:16 Loss:0.01172\n",
      "Epoch:100 Batch:16 Loss:0.01156\n",
      "Epoch:120 Batch:16 Loss:0.01052\n",
      "Epoch:140 Batch:16 Loss:0.01104\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.088\n",
      "Epoch:10 Batch:10 Loss:0.087\n",
      "Epoch:20 Batch:10 Loss:0.088\n",
      "Epoch:30 Batch:10 Loss:0.086\n",
      "Epoch:40 Batch:10 Loss:0.088\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -116.96557782948513 setps: 800 count: 800\n",
      "avg rewards: -116.96557782948513\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.23553\n",
      "Epoch:20 Batch:17 Loss:0.02125\n",
      "Epoch:40 Batch:17 Loss:0.01579\n",
      "Epoch:60 Batch:17 Loss:0.01374\n",
      "Epoch:80 Batch:17 Loss:0.01277\n",
      "Epoch:100 Batch:17 Loss:0.01170\n",
      "Epoch:120 Batch:17 Loss:0.01166\n",
      "Epoch:140 Batch:17 Loss:0.00959\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.082\n",
      "Epoch:10 Batch:10 Loss:0.081\n",
      "Epoch:20 Batch:10 Loss:0.082\n",
      "Epoch:30 Batch:10 Loss:0.081\n",
      "Epoch:40 Batch:10 Loss:0.081\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -39.49493630858778 setps: 800 count: 800\n",
      "avg rewards: -39.49493630858778\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.21320\n",
      "Epoch:20 Batch:18 Loss:0.02141\n",
      "Epoch:40 Batch:18 Loss:0.01553\n",
      "Epoch:60 Batch:18 Loss:0.01422\n",
      "Epoch:80 Batch:18 Loss:0.01219\n",
      "Epoch:100 Batch:18 Loss:0.01192\n",
      "Epoch:120 Batch:18 Loss:0.01101\n",
      "Epoch:140 Batch:18 Loss:0.01109\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.080\n",
      "Epoch:10 Batch:10 Loss:0.081\n",
      "Epoch:20 Batch:10 Loss:0.081\n",
      "Epoch:30 Batch:10 Loss:0.080\n",
      "Epoch:40 Batch:10 Loss:0.079\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -241.10986299075145 setps: 800 count: 800\n",
      "avg rewards: -241.10986299075145\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.19080\n",
      "Epoch:20 Batch:19 Loss:0.01795\n",
      "Epoch:40 Batch:19 Loss:0.01472\n",
      "Epoch:60 Batch:19 Loss:0.01391\n",
      "Epoch:80 Batch:19 Loss:0.01215\n",
      "Epoch:100 Batch:19 Loss:0.01193\n",
      "Epoch:120 Batch:19 Loss:0.01140\n",
      "Epoch:140 Batch:19 Loss:0.01090\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.074\n",
      "Epoch:10 Batch:10 Loss:0.073\n",
      "Epoch:20 Batch:10 Loss:0.073\n",
      "Epoch:30 Batch:10 Loss:0.074\n",
      "Epoch:40 Batch:10 Loss:0.074\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -118.9352340591008 setps: 800 count: 800\n",
      "avg rewards: -118.9352340591008\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.19310\n",
      "Epoch:20 Batch:20 Loss:0.02172\n",
      "Epoch:40 Batch:20 Loss:0.01735\n",
      "Epoch:60 Batch:20 Loss:0.01420\n",
      "Epoch:80 Batch:20 Loss:0.01323\n",
      "Epoch:100 Batch:20 Loss:0.01251\n",
      "Epoch:120 Batch:20 Loss:0.01243\n",
      "Epoch:140 Batch:20 Loss:0.01071\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.070\n",
      "Epoch:10 Batch:10 Loss:0.068\n",
      "Epoch:20 Batch:10 Loss:0.069\n",
      "Epoch:30 Batch:10 Loss:0.068\n",
      "Epoch:40 Batch:10 Loss:0.069\n",
      "Done!\n",
      "############# start HumanoidBulletEnv-v0 training ###################\n",
      "(49,)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -21.27815852837957 setps: 18 count: 18\n",
      "reward: -22.47076977281249 setps: 19 count: 37\n",
      "reward: -30.321776842277902 setps: 19 count: 56\n",
      "reward: -25.21742826192494 setps: 19 count: 75\n",
      "reward: -24.207292249455357 setps: 20 count: 95\n",
      "reward: -29.7664181799104 setps: 24 count: 119\n",
      "reward: -23.74311013439292 setps: 19 count: 138\n",
      "reward: -23.36006111258321 setps: 20 count: 158\n",
      "reward: -34.318993776124266 setps: 18 count: 176\n",
      "reward: -26.146251443079382 setps: 25 count: 201\n",
      "reward: -26.807408529175156 setps: 18 count: 219\n",
      "reward: -21.07751676966436 setps: 18 count: 237\n",
      "reward: -42.99177106470597 setps: 17 count: 254\n",
      "reward: -21.11809686110646 setps: 19 count: 273\n",
      "reward: -31.734575641065025 setps: 21 count: 294\n",
      "reward: -32.155941073739086 setps: 24 count: 318\n",
      "reward: -33.46096414426138 setps: 18 count: 336\n",
      "reward: -38.50465430379845 setps: 23 count: 359\n",
      "reward: -39.774160651871355 setps: 18 count: 377\n",
      "reward: -31.689619016315557 setps: 19 count: 396\n",
      "reward: -33.106630958568715 setps: 20 count: 416\n",
      "reward: -32.276966070497295 setps: 19 count: 435\n",
      "reward: -36.04288299209438 setps: 19 count: 454\n",
      "reward: -27.627554653768314 setps: 17 count: 471\n",
      "reward: -32.68496657831711 setps: 19 count: 490\n",
      "reward: -35.6927099502398 setps: 20 count: 510\n",
      "reward: -39.889954649562426 setps: 18 count: 528\n",
      "reward: -26.845603258757915 setps: 18 count: 546\n",
      "reward: -27.075866062492423 setps: 19 count: 565\n",
      "reward: -31.09131065516849 setps: 19 count: 584\n",
      "reward: -28.852063810803518 setps: 19 count: 603\n",
      "reward: -32.33076674514451 setps: 20 count: 623\n",
      "reward: -8.938496169158316 setps: 23 count: 646\n",
      "reward: -18.415821023249006 setps: 18 count: 664\n",
      "reward: -18.717161961598322 setps: 20 count: 684\n",
      "reward: -34.75948575479415 setps: 17 count: 701\n",
      "reward: -46.74182823659794 setps: 18 count: 719\n",
      "reward: -37.49865368420868 setps: 18 count: 737\n",
      "reward: -36.59209646939707 setps: 19 count: 756\n",
      "reward: -34.548925978114134 setps: 19 count: 775\n",
      "reward: -20.69142105957435 setps: 20 count: 795\n",
      "reward: -28.884350139297023 setps: 20 count: 815\n",
      "reward: -36.450555329838245 setps: 18 count: 833\n",
      "reward: -29.937173018731116 setps: 17 count: 850\n",
      "reward: -33.678217343676074 setps: 19 count: 869\n",
      "reward: -31.476157857508223 setps: 18 count: 887\n",
      "reward: -29.679194189971895 setps: 19 count: 906\n",
      "reward: -25.80388652241963 setps: 23 count: 929\n",
      "reward: -40.47353425140464 setps: 16 count: 945\n",
      "reward: -33.41697716347262 setps: 25 count: 970\n",
      "reward: -33.2734043930599 setps: 19 count: 989\n",
      "avg rewards: -30.26744284878685\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:1.65203\n",
      "Epoch:20 Batch:1 Loss:0.56740\n",
      "Epoch:40 Batch:1 Loss:0.46714\n",
      "Epoch:60 Batch:1 Loss:0.39294\n",
      "Epoch:80 Batch:1 Loss:0.34124\n",
      "Epoch:100 Batch:1 Loss:0.30047\n",
      "Epoch:120 Batch:1 Loss:0.26792\n",
      "Epoch:140 Batch:1 Loss:0.24468\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.247\n",
      "Epoch:10 Batch:8 Loss:0.242\n",
      "Epoch:20 Batch:8 Loss:0.234\n",
      "Epoch:30 Batch:8 Loss:0.235\n",
      "Epoch:40 Batch:8 Loss:0.234\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 17.283859017975917 setps: 20 count: 20\n",
      "reward: 21.105271126664586 setps: 21 count: 41\n",
      "reward: 20.153002464739252 setps: 22 count: 63\n",
      "reward: 21.934872037233433 setps: 22 count: 85\n",
      "reward: 17.84935865811567 setps: 20 count: 105\n",
      "reward: 21.72356503868214 setps: 26 count: 131\n",
      "reward: 20.29934513692133 setps: 22 count: 153\n",
      "reward: 15.556590756912192 setps: 18 count: 171\n",
      "reward: 21.438030329548926 setps: 25 count: 196\n",
      "reward: 23.43532583920896 setps: 22 count: 218\n",
      "reward: 22.558606473398683 setps: 24 count: 242\n",
      "reward: 21.65368697406375 setps: 21 count: 263\n",
      "reward: 20.61581366380124 setps: 21 count: 284\n",
      "reward: 23.341798393486535 setps: 21 count: 305\n",
      "reward: 22.209815953001087 setps: 22 count: 327\n",
      "reward: 14.03831536034122 setps: 17 count: 344\n",
      "reward: 22.178845209853893 setps: 21 count: 365\n",
      "reward: 19.52037379604444 setps: 20 count: 385\n",
      "reward: 21.97263747168909 setps: 21 count: 406\n",
      "reward: 21.0113273153911 setps: 22 count: 428\n",
      "reward: 17.012871429351797 setps: 18 count: 446\n",
      "reward: 20.86418373450142 setps: 21 count: 467\n",
      "reward: 16.9391762996398 setps: 18 count: 485\n",
      "reward: 20.910618997270646 setps: 21 count: 506\n",
      "reward: 19.348306586657415 setps: 21 count: 527\n",
      "reward: 20.93216995232215 setps: 20 count: 547\n",
      "reward: 20.529312777532322 setps: 20 count: 567\n",
      "reward: 18.565839090559166 setps: 20 count: 587\n",
      "reward: 21.135144797859542 setps: 21 count: 608\n",
      "reward: 18.270090580775282 setps: 21 count: 629\n",
      "reward: 19.80431469089526 setps: 20 count: 649\n",
      "reward: 20.098923692727112 setps: 22 count: 671\n",
      "reward: 14.62234268950706 setps: 21 count: 692\n",
      "reward: 14.181189271701438 setps: 17 count: 709\n",
      "reward: 20.253574855385523 setps: 21 count: 730\n",
      "reward: 20.129655632110367 setps: 20 count: 750\n",
      "reward: 21.144197785606956 setps: 21 count: 771\n",
      "reward: 22.12271689772169 setps: 21 count: 792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 13.668143959355074 setps: 17 count: 809\n",
      "reward: 19.79312948161387 setps: 21 count: 830\n",
      "reward: 19.19280487193755 setps: 21 count: 851\n",
      "reward: 14.112841390646642 setps: 17 count: 868\n",
      "reward: 24.346062154811808 setps: 22 count: 890\n",
      "reward: 21.833997327132966 setps: 23 count: 913\n",
      "reward: 19.401853882485007 setps: 20 count: 933\n",
      "reward: 21.598215678443378 setps: 21 count: 954\n",
      "reward: 21.8644707813728 setps: 21 count: 975\n",
      "reward: 20.714926219214977 setps: 22 count: 997\n",
      "avg rewards: 19.85982326096276\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:1.43903\n",
      "Epoch:20 Batch:2 Loss:0.32208\n",
      "Epoch:40 Batch:2 Loss:0.25177\n",
      "Epoch:60 Batch:2 Loss:0.22149\n",
      "Epoch:80 Batch:2 Loss:0.18287\n",
      "Epoch:100 Batch:2 Loss:0.16493\n",
      "Epoch:120 Batch:2 Loss:0.15810\n",
      "Epoch:140 Batch:2 Loss:0.14049\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.218\n",
      "Epoch:10 Batch:8 Loss:0.216\n",
      "Epoch:20 Batch:8 Loss:0.217\n",
      "Epoch:30 Batch:8 Loss:0.216\n",
      "Epoch:40 Batch:8 Loss:0.215\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 40.16636490263482 setps: 32 count: 32\n",
      "reward: 23.67538354999561 setps: 20 count: 52\n",
      "reward: 43.84088937751366 setps: 35 count: 87\n",
      "reward: 51.152238984432195 setps: 34 count: 121\n",
      "reward: 46.95442137192878 setps: 36 count: 157\n",
      "reward: 38.92681527687673 setps: 36 count: 193\n",
      "reward: 53.305062335479306 setps: 36 count: 229\n",
      "reward: 41.135210449062285 setps: 33 count: 262\n",
      "reward: 46.531433727921105 setps: 36 count: 298\n",
      "reward: 45.91378470281779 setps: 34 count: 332\n",
      "reward: 30.758862695116836 setps: 27 count: 359\n",
      "reward: 21.404448817799857 setps: 20 count: 379\n",
      "reward: 14.327503354415242 setps: 17 count: 396\n",
      "reward: -2.9036602466236223 setps: 11 count: 407\n",
      "reward: 35.05254977989388 setps: 29 count: 436\n",
      "reward: 39.86104598233796 setps: 48 count: 484\n",
      "reward: 32.23342642487115 setps: 28 count: 512\n",
      "reward: 40.42104084272723 setps: 35 count: 547\n",
      "reward: 28.37700838296733 setps: 22 count: 569\n",
      "reward: 48.71019739616313 setps: 38 count: 607\n",
      "reward: 32.91850737445493 setps: 28 count: 635\n",
      "reward: 47.5771766317237 setps: 35 count: 670\n",
      "reward: 32.571389167154855 setps: 35 count: 705\n",
      "reward: 40.85537811776594 setps: 33 count: 738\n",
      "reward: 13.16629787823913 setps: 17 count: 755\n",
      "reward: 59.12195767895611 setps: 40 count: 795\n",
      "reward: 38.41563675763901 setps: 30 count: 825\n",
      "reward: 46.329794538007995 setps: 32 count: 857\n",
      "reward: 49.84514084534602 setps: 36 count: 893\n",
      "reward: 20.011861015886822 setps: 19 count: 912\n",
      "reward: 33.82538434589659 setps: 32 count: 944\n",
      "reward: 42.0459131138021 setps: 33 count: 977\n",
      "avg rewards: 36.766514549162636\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:1.32764\n",
      "Epoch:20 Batch:3 Loss:0.24436\n",
      "Epoch:40 Batch:3 Loss:0.19255\n",
      "Epoch:60 Batch:3 Loss:0.14674\n",
      "Epoch:80 Batch:3 Loss:0.13974\n",
      "Epoch:100 Batch:3 Loss:0.12154\n",
      "Epoch:120 Batch:3 Loss:0.11553\n",
      "Epoch:140 Batch:3 Loss:0.11723\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.153\n",
      "Epoch:10 Batch:8 Loss:0.152\n",
      "Epoch:20 Batch:8 Loss:0.152\n",
      "Epoch:30 Batch:8 Loss:0.152\n",
      "Epoch:40 Batch:8 Loss:0.151\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 50.67161279513385 setps: 41 count: 41\n",
      "reward: 46.7125121090372 setps: 37 count: 78\n",
      "reward: 29.577472925056654 setps: 29 count: 107\n",
      "reward: 33.939197525302006 setps: 29 count: 136\n",
      "reward: 72.95666487943379 setps: 52 count: 188\n",
      "reward: 28.56714310884854 setps: 26 count: 214\n",
      "reward: 43.35253182610467 setps: 39 count: 253\n",
      "reward: 35.07376197682169 setps: 36 count: 289\n",
      "reward: 29.2678713559726 setps: 26 count: 315\n",
      "reward: 39.89216503049975 setps: 37 count: 352\n",
      "reward: 13.429723857471256 setps: 18 count: 370\n",
      "reward: 71.97887888547265 setps: 58 count: 428\n",
      "reward: 46.57507202749403 setps: 50 count: 478\n",
      "reward: 33.16090648370301 setps: 42 count: 520\n",
      "reward: 14.335246651670605 setps: 17 count: 537\n",
      "reward: 19.77912479835941 setps: 20 count: 557\n",
      "reward: 48.300211142007896 setps: 51 count: 608\n",
      "reward: 47.44768021849742 setps: 39 count: 647\n",
      "reward: 28.512185126292753 setps: 39 count: 686\n",
      "reward: 31.406408921965337 setps: 34 count: 720\n",
      "reward: 13.891163350267746 setps: 17 count: 737\n",
      "reward: 25.110213478207886 setps: 24 count: 761\n",
      "reward: 68.90547552487115 setps: 53 count: 814\n",
      "reward: 46.04812676750589 setps: 48 count: 862\n",
      "reward: 56.26765710144681 setps: 43 count: 905\n",
      "reward: 23.443687378933827 setps: 21 count: 926\n",
      "reward: 77.65795763735514 setps: 50 count: 976\n",
      "avg rewards: 39.86150566236051\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:1.20872\n",
      "Epoch:20 Batch:4 Loss:0.21795\n",
      "Epoch:40 Batch:4 Loss:0.16108\n",
      "Epoch:60 Batch:4 Loss:0.12900\n",
      "Epoch:80 Batch:4 Loss:0.11564\n",
      "Epoch:100 Batch:4 Loss:0.10137\n",
      "Epoch:120 Batch:4 Loss:0.09380\n",
      "Epoch:140 Batch:4 Loss:0.10507\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.153\n",
      "Epoch:10 Batch:8 Loss:0.151\n",
      "Epoch:20 Batch:8 Loss:0.148\n",
      "Epoch:30 Batch:8 Loss:0.151\n",
      "Epoch:40 Batch:8 Loss:0.151\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 24.21760403587249 setps: 57 count: 57\n",
      "reward: 15.338636139770097 setps: 17 count: 74\n",
      "reward: 32.87275465524145 setps: 46 count: 120\n",
      "reward: 24.271354862506257 setps: 35 count: 155\n",
      "reward: 26.99706939757308 setps: 35 count: 190\n",
      "reward: 26.133745590593037 setps: 28 count: 218\n",
      "reward: 27.692891197206343 setps: 41 count: 259\n",
      "reward: 17.93027119912876 setps: 19 count: 278\n",
      "reward: 40.63981944812839 setps: 38 count: 316\n",
      "reward: 16.83555242397124 setps: 18 count: 334\n",
      "reward: 24.0007895909599 setps: 36 count: 370\n",
      "reward: 45.346374782947514 setps: 52 count: 422\n",
      "reward: 17.465158951895123 setps: 29 count: 451\n",
      "reward: 14.599711274824221 setps: 17 count: 468\n",
      "reward: 39.91254130363087 setps: 39 count: 507\n",
      "reward: 11.419021609070477 setps: 16 count: 523\n",
      "reward: 53.254016363363185 setps: 46 count: 569\n",
      "reward: 29.00811130388465 setps: 39 count: 608\n",
      "reward: 18.069548336529987 setps: 19 count: 627\n",
      "reward: 25.704841815239348 setps: 35 count: 662\n",
      "reward: 32.50676713857973 setps: 36 count: 698\n",
      "reward: 30.32817428075213 setps: 36 count: 734\n",
      "reward: 16.597750498088136 setps: 19 count: 753\n",
      "reward: 18.09320837555279 setps: 27 count: 780\n",
      "reward: 29.050895665818825 setps: 34 count: 814\n",
      "reward: 25.572431373315332 setps: 34 count: 848\n",
      "reward: 28.993875389274034 setps: 38 count: 886\n",
      "reward: 21.761044145154305 setps: 30 count: 916\n",
      "reward: 28.22966511447595 setps: 67 count: 983\n",
      "reward: -9.064285402283714 setps: 12 count: 995\n",
      "avg rewards: 25.125978028702132\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:1.19130\n",
      "Epoch:20 Batch:5 Loss:0.17427\n",
      "Epoch:40 Batch:5 Loss:0.12624\n",
      "Epoch:60 Batch:5 Loss:0.11190\n",
      "Epoch:80 Batch:5 Loss:0.10399\n",
      "Epoch:100 Batch:5 Loss:0.09527\n",
      "Epoch:120 Batch:5 Loss:0.08936\n",
      "Epoch:140 Batch:5 Loss:0.08391\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.147\n",
      "Epoch:10 Batch:8 Loss:0.140\n",
      "Epoch:20 Batch:8 Loss:0.140\n",
      "Epoch:30 Batch:8 Loss:0.138\n",
      "Epoch:40 Batch:8 Loss:0.139\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 35.78512958659849 setps: 26 count: 26\n",
      "reward: 44.54627329048381 setps: 32 count: 58\n",
      "reward: 48.56126656172999 setps: 30 count: 88\n",
      "reward: 35.86828353865421 setps: 27 count: 115\n",
      "reward: 37.51843184102326 setps: 26 count: 141\n",
      "reward: 33.72952617497213 setps: 25 count: 166\n",
      "reward: 42.39172754076135 setps: 29 count: 195\n",
      "reward: 44.74478309780388 setps: 31 count: 226\n",
      "reward: 41.59128100151283 setps: 32 count: 258\n",
      "reward: 46.31714608052862 setps: 32 count: 290\n",
      "reward: 36.771053733846934 setps: 25 count: 315\n",
      "reward: 44.80065046886594 setps: 32 count: 347\n",
      "reward: 50.789447264935 setps: 32 count: 379\n",
      "reward: 35.19111664226365 setps: 26 count: 405\n",
      "reward: 43.07261200188223 setps: 29 count: 434\n",
      "reward: 54.82845192584354 setps: 35 count: 469\n",
      "reward: 41.03329412998428 setps: 30 count: 499\n",
      "reward: 50.77783038009949 setps: 35 count: 534\n",
      "reward: 31.6144207885387 setps: 24 count: 558\n",
      "reward: 18.496276956230574 setps: 17 count: 575\n",
      "reward: 15.995866644305348 setps: 17 count: 592\n",
      "reward: 39.81127586016228 setps: 27 count: 619\n",
      "reward: 41.30046661609958 setps: 29 count: 648\n",
      "reward: 39.934604128499636 setps: 27 count: 675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 37.51761364810809 setps: 26 count: 701\n",
      "reward: 19.72883065846836 setps: 17 count: 718\n",
      "reward: 32.247530641993215 setps: 26 count: 744\n",
      "reward: 34.95919872420638 setps: 24 count: 768\n",
      "reward: 49.121387256037266 setps: 32 count: 800\n",
      "reward: 49.61701068910915 setps: 36 count: 836\n",
      "reward: 36.56255509494513 setps: 29 count: 865\n",
      "reward: 42.0771372215575 setps: 30 count: 895\n",
      "reward: 34.002358836015624 setps: 25 count: 920\n",
      "reward: 34.13383551342995 setps: 27 count: 947\n",
      "reward: 38.62783202327555 setps: 28 count: 975\n",
      "reward: 18.36596733979532 setps: 18 count: 993\n",
      "avg rewards: 38.40090205284909\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:1.12779\n",
      "Epoch:20 Batch:6 Loss:0.15912\n",
      "Epoch:40 Batch:6 Loss:0.11896\n",
      "Epoch:60 Batch:6 Loss:0.10000\n",
      "Epoch:80 Batch:6 Loss:0.08974\n",
      "Epoch:100 Batch:6 Loss:0.08374\n",
      "Epoch:120 Batch:6 Loss:0.08414\n",
      "Epoch:140 Batch:6 Loss:0.08243\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.140\n",
      "Epoch:10 Batch:8 Loss:0.138\n",
      "Epoch:20 Batch:8 Loss:0.136\n",
      "Epoch:30 Batch:8 Loss:0.135\n",
      "Epoch:40 Batch:8 Loss:0.135\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 28.62632174188184 setps: 27 count: 27\n",
      "reward: 41.07626475778817 setps: 41 count: 68\n",
      "reward: 36.04811577433429 setps: 33 count: 101\n",
      "reward: 33.46452636355679 setps: 31 count: 132\n",
      "reward: 70.47260279410838 setps: 58 count: 190\n",
      "reward: 37.52071889560321 setps: 35 count: 225\n",
      "reward: 47.740577797821615 setps: 41 count: 266\n",
      "reward: 26.572809123028122 setps: 32 count: 298\n",
      "reward: 37.07231680063122 setps: 34 count: 332\n",
      "reward: 41.93969109933095 setps: 37 count: 369\n",
      "reward: 46.66671844190278 setps: 37 count: 406\n",
      "reward: 36.2653538247905 setps: 32 count: 438\n",
      "reward: 38.597955941192055 setps: 35 count: 473\n",
      "reward: 13.107634452414638 setps: 17 count: 490\n",
      "reward: 23.169879434346505 setps: 26 count: 516\n",
      "reward: 33.71685081212927 setps: 33 count: 549\n",
      "reward: 63.84676573735778 setps: 49 count: 598\n",
      "reward: 42.29284443619109 setps: 47 count: 645\n",
      "reward: 18.590962497080913 setps: 19 count: 664\n",
      "reward: 19.56347466660664 setps: 20 count: 684\n",
      "reward: 32.20879016053077 setps: 33 count: 717\n",
      "reward: 71.01578210469307 setps: 58 count: 775\n",
      "reward: 45.751672939829575 setps: 37 count: 812\n",
      "reward: 26.655946551772647 setps: 26 count: 838\n",
      "reward: 30.62249440391025 setps: 31 count: 869\n",
      "reward: 26.070408898543974 setps: 27 count: 896\n",
      "reward: 30.73452867660817 setps: 31 count: 927\n",
      "reward: 39.84921329509379 setps: 37 count: 964\n",
      "avg rewards: 37.11647222939568\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:1.19653\n",
      "Epoch:20 Batch:7 Loss:0.13987\n",
      "Epoch:40 Batch:7 Loss:0.10844\n",
      "Epoch:60 Batch:7 Loss:0.09355\n",
      "Epoch:80 Batch:7 Loss:0.08461\n",
      "Epoch:100 Batch:7 Loss:0.07862\n",
      "Epoch:120 Batch:7 Loss:0.08032\n",
      "Epoch:140 Batch:7 Loss:0.07357\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.130\n",
      "Epoch:10 Batch:8 Loss:0.126\n",
      "Epoch:20 Batch:8 Loss:0.127\n",
      "Epoch:30 Batch:8 Loss:0.124\n",
      "Epoch:40 Batch:8 Loss:0.125\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 68.14328827049175 setps: 41 count: 41\n",
      "reward: 80.94295456605907 setps: 52 count: 93\n",
      "reward: 46.43839488997473 setps: 35 count: 128\n",
      "reward: 74.69791832068586 setps: 48 count: 176\n",
      "reward: 54.40067035202956 setps: 42 count: 218\n",
      "reward: 91.17580412697717 setps: 52 count: 270\n",
      "reward: 56.03771789854217 setps: 43 count: 313\n",
      "reward: 16.444398371188434 setps: 20 count: 333\n",
      "reward: 105.83086959149949 setps: 71 count: 404\n",
      "reward: 60.1126124679111 setps: 44 count: 448\n",
      "reward: 105.35250737472819 setps: 73 count: 521\n",
      "reward: 65.81493443870716 setps: 40 count: 561\n",
      "reward: 19.932346006247094 setps: 18 count: 579\n",
      "reward: 24.16900245635916 setps: 23 count: 602\n",
      "reward: 62.294160927220936 setps: 40 count: 642\n",
      "reward: 18.06040826166136 setps: 18 count: 660\n",
      "reward: 66.13645218948514 setps: 42 count: 702\n",
      "reward: 68.66647088673925 setps: 44 count: 746\n",
      "reward: 88.21255871539324 setps: 52 count: 798\n",
      "reward: 84.39029865095216 setps: 50 count: 848\n",
      "reward: 12.313219602277968 setps: 18 count: 866\n",
      "reward: 22.714747797051675 setps: 21 count: 887\n",
      "reward: 77.61115244511166 setps: 59 count: 946\n",
      "reward: 68.90830502050522 setps: 42 count: 988\n",
      "avg rewards: 59.95004973449165\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.97144\n",
      "Epoch:20 Batch:8 Loss:0.13328\n",
      "Epoch:40 Batch:8 Loss:0.10128\n",
      "Epoch:60 Batch:8 Loss:0.08497\n",
      "Epoch:80 Batch:8 Loss:0.08278\n",
      "Epoch:100 Batch:8 Loss:0.07445\n",
      "Epoch:120 Batch:8 Loss:0.07082\n",
      "Epoch:140 Batch:8 Loss:0.06947\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.130\n",
      "Epoch:10 Batch:8 Loss:0.128\n",
      "Epoch:20 Batch:8 Loss:0.129\n",
      "Epoch:30 Batch:8 Loss:0.128\n",
      "Epoch:40 Batch:8 Loss:0.126\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 27.545485324345652 setps: 27 count: 27\n",
      "reward: 21.840208069780786 setps: 21 count: 48\n",
      "reward: 20.672852804847935 setps: 24 count: 72\n",
      "reward: 15.042080219864145 setps: 18 count: 90\n",
      "reward: 28.505759004871653 setps: 30 count: 120\n",
      "reward: 26.516535590565766 setps: 25 count: 145\n",
      "reward: 35.98100363793345 setps: 32 count: 177\n",
      "reward: 33.739222053582495 setps: 29 count: 206\n",
      "reward: 20.81395035231544 setps: 20 count: 226\n",
      "reward: 18.327267515451243 setps: 19 count: 245\n",
      "reward: 29.11622891775623 setps: 27 count: 272\n",
      "reward: 29.006550127615625 setps: 29 count: 301\n",
      "reward: 54.12634875874938 setps: 51 count: 352\n",
      "reward: 29.037125044832646 setps: 26 count: 378\n",
      "reward: 22.938880783166685 setps: 25 count: 403\n",
      "reward: 20.005350293892846 setps: 23 count: 426\n",
      "reward: 27.07642093900358 setps: 26 count: 452\n",
      "reward: 24.083131239707296 setps: 24 count: 476\n",
      "reward: 32.73829073614033 setps: 31 count: 507\n",
      "reward: 22.385208095786222 setps: 19 count: 526\n",
      "reward: 23.682269318272297 setps: 25 count: 551\n",
      "reward: 39.91581691462635 setps: 36 count: 587\n",
      "reward: 21.141688911344687 setps: 24 count: 611\n",
      "reward: 20.044129926201997 setps: 22 count: 633\n",
      "reward: 24.86570206584292 setps: 24 count: 657\n",
      "reward: 19.74677984545415 setps: 19 count: 676\n",
      "reward: 26.024508836920727 setps: 25 count: 701\n",
      "reward: 28.02686652586999 setps: 26 count: 727\n",
      "reward: 17.89160836233641 setps: 18 count: 745\n",
      "reward: 24.6149531182833 setps: 25 count: 770\n",
      "reward: 23.420205628170518 setps: 24 count: 794\n",
      "reward: 25.095156135038994 setps: 25 count: 819\n",
      "reward: 31.797832228228806 setps: 29 count: 848\n",
      "reward: 18.96259663449455 setps: 20 count: 868\n",
      "reward: 19.414377855614294 setps: 19 count: 887\n",
      "reward: 25.320170864443938 setps: 26 count: 913\n",
      "reward: 29.515792756958398 setps: 28 count: 941\n",
      "reward: 26.54932477937545 setps: 27 count: 968\n",
      "reward: 23.317761946409874 setps: 26 count: 994\n",
      "avg rewards: 25.86783185036146\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.92831\n",
      "Epoch:20 Batch:9 Loss:0.12428\n",
      "Epoch:40 Batch:9 Loss:0.08787\n",
      "Epoch:60 Batch:9 Loss:0.08251\n",
      "Epoch:80 Batch:9 Loss:0.07681\n",
      "Epoch:100 Batch:9 Loss:0.07244\n",
      "Epoch:120 Batch:9 Loss:0.06905\n",
      "Epoch:140 Batch:9 Loss:0.06521\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.126\n",
      "Epoch:10 Batch:8 Loss:0.123\n",
      "Epoch:20 Batch:8 Loss:0.120\n",
      "Epoch:30 Batch:8 Loss:0.121\n",
      "Epoch:40 Batch:8 Loss:0.120\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 81.2546574530032 setps: 46 count: 46\n",
      "reward: 72.4157153863067 setps: 45 count: 91\n",
      "reward: 79.99470504842027 setps: 57 count: 148\n",
      "reward: 98.3947800312686 setps: 54 count: 202\n",
      "reward: 57.16202156111978 setps: 40 count: 242\n",
      "reward: 63.45212024324571 setps: 51 count: 293\n",
      "reward: 68.61710604929976 setps: 49 count: 342\n",
      "reward: 77.35707879599357 setps: 45 count: 387\n",
      "reward: 94.17701076604718 setps: 57 count: 444\n",
      "reward: 77.93194284130149 setps: 47 count: 491\n",
      "reward: 81.72808836399345 setps: 46 count: 537\n",
      "reward: 76.29669929088702 setps: 54 count: 591\n",
      "reward: 44.00851499525889 setps: 32 count: 623\n",
      "reward: 60.928367413992234 setps: 46 count: 669\n",
      "reward: 50.65332218059776 setps: 40 count: 709\n",
      "reward: 83.95208112752732 setps: 49 count: 758\n",
      "reward: 97.12422282790799 setps: 57 count: 815\n",
      "reward: 84.50518442196773 setps: 56 count: 871\n",
      "reward: 84.80021970979286 setps: 48 count: 919\n",
      "reward: 95.02433908445964 setps: 59 count: 978\n",
      "reward: 21.27758884112991 setps: 18 count: 996\n",
      "avg rewards: 73.85979840159624\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:10 Loss:0.63320\n",
      "Epoch:20 Batch:10 Loss:0.12161\n",
      "Epoch:40 Batch:10 Loss:0.09343\n",
      "Epoch:60 Batch:10 Loss:0.07567\n",
      "Epoch:80 Batch:10 Loss:0.06947\n",
      "Epoch:100 Batch:10 Loss:0.06280\n",
      "Epoch:120 Batch:10 Loss:0.06683\n",
      "Epoch:140 Batch:10 Loss:0.06243\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.103\n",
      "Epoch:10 Batch:8 Loss:0.101\n",
      "Epoch:20 Batch:8 Loss:0.101\n",
      "Epoch:30 Batch:8 Loss:0.098\n",
      "Epoch:40 Batch:8 Loss:0.100\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 42.14768606287689 setps: 34 count: 34\n",
      "reward: 34.36164792709897 setps: 29 count: 63\n",
      "reward: 41.15897121769086 setps: 32 count: 95\n",
      "reward: 14.990559335044235 setps: 17 count: 112\n",
      "reward: 34.04053708772553 setps: 30 count: 142\n",
      "reward: 35.98779635903193 setps: 30 count: 172\n",
      "reward: 31.507680355424235 setps: 30 count: 202\n",
      "reward: 17.76816448699683 setps: 18 count: 220\n",
      "reward: 48.01136760575463 setps: 34 count: 254\n",
      "reward: 34.28013770643156 setps: 32 count: 286\n",
      "reward: 38.48846896024042 setps: 32 count: 318\n",
      "reward: 32.3770924493001 setps: 28 count: 346\n",
      "reward: 35.087097484643174 setps: 32 count: 378\n",
      "reward: 37.04247086181568 setps: 31 count: 409\n",
      "reward: 14.53398959478218 setps: 17 count: 426\n",
      "reward: 35.67622097505081 setps: 33 count: 459\n",
      "reward: 37.364706585511165 setps: 33 count: 492\n",
      "reward: 33.163662896594914 setps: 28 count: 520\n",
      "reward: 50.4084608485602 setps: 37 count: 557\n",
      "reward: 40.458804037148354 setps: 32 count: 589\n",
      "reward: 52.463116277956566 setps: 39 count: 628\n",
      "reward: 48.44850432286767 setps: 35 count: 663\n",
      "reward: 44.425088673853324 setps: 32 count: 695\n",
      "reward: 38.02010670141899 setps: 34 count: 729\n",
      "reward: 24.108536532924337 setps: 21 count: 750\n",
      "reward: 40.44461106656673 setps: 33 count: 783\n",
      "reward: 32.569414322792724 setps: 30 count: 813\n",
      "reward: 30.57711024210876 setps: 28 count: 841\n",
      "reward: 23.62081041332131 setps: 22 count: 863\n",
      "reward: 38.173326623452894 setps: 31 count: 894\n",
      "reward: 34.00455879120709 setps: 29 count: 923\n",
      "reward: 44.578662170682215 setps: 34 count: 957\n",
      "reward: 41.23004751666012 setps: 31 count: 988\n",
      "avg rewards: 35.80361868162228\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.50946\n",
      "Epoch:20 Batch:11 Loss:0.11538\n",
      "Epoch:40 Batch:11 Loss:0.09693\n",
      "Epoch:60 Batch:11 Loss:0.07945\n",
      "Epoch:80 Batch:11 Loss:0.07370\n",
      "Epoch:100 Batch:11 Loss:0.06934\n",
      "Epoch:120 Batch:11 Loss:0.06880\n",
      "Epoch:140 Batch:11 Loss:0.06252\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.097\n",
      "Epoch:10 Batch:8 Loss:0.095\n",
      "Epoch:20 Batch:8 Loss:0.094\n",
      "Epoch:30 Batch:8 Loss:0.093\n",
      "Epoch:40 Batch:8 Loss:0.095\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 56.058105294402054 setps: 43 count: 43\n",
      "reward: 49.332472818600934 setps: 33 count: 76\n",
      "reward: 70.17273089068765 setps: 41 count: 117\n",
      "reward: 53.011737956722214 setps: 33 count: 150\n",
      "reward: 61.50308310104593 setps: 46 count: 196\n",
      "reward: 54.98277853076287 setps: 37 count: 233\n",
      "reward: 67.51293501381586 setps: 42 count: 275\n",
      "reward: 84.47175359158717 setps: 52 count: 327\n",
      "reward: 49.52321308157554 setps: 43 count: 370\n",
      "reward: 53.78188377585466 setps: 38 count: 408\n",
      "reward: 44.7354755383465 setps: 41 count: 449\n",
      "reward: 48.86247628577404 setps: 33 count: 482\n",
      "reward: 64.68172923855599 setps: 42 count: 524\n",
      "reward: 38.676192201700175 setps: 30 count: 554\n",
      "reward: 46.46667159805656 setps: 33 count: 587\n",
      "reward: 77.09335827426548 setps: 52 count: 639\n",
      "reward: 64.10433672696384 setps: 42 count: 681\n",
      "reward: 37.75051993255621 setps: 29 count: 710\n",
      "reward: 48.881895230400545 setps: 34 count: 744\n",
      "reward: 60.938484699006935 setps: 41 count: 785\n",
      "reward: 28.0422024626314 setps: 27 count: 812\n",
      "reward: 77.35707705615934 setps: 45 count: 857\n",
      "reward: 69.94007188557151 setps: 43 count: 900\n",
      "reward: 69.91693921041441 setps: 42 count: 942\n",
      "reward: 73.10271080552488 setps: 44 count: 986\n",
      "avg rewards: 58.0360334080393\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.75755\n",
      "Epoch:20 Batch:12 Loss:0.09494\n",
      "Epoch:40 Batch:12 Loss:0.07830\n",
      "Epoch:60 Batch:12 Loss:0.06602\n",
      "Epoch:80 Batch:12 Loss:0.06105\n",
      "Epoch:100 Batch:12 Loss:0.05854\n",
      "Epoch:120 Batch:12 Loss:0.06090\n",
      "Epoch:140 Batch:12 Loss:0.05545\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.095\n",
      "Epoch:10 Batch:8 Loss:0.092\n",
      "Epoch:20 Batch:8 Loss:0.091\n",
      "Epoch:30 Batch:8 Loss:0.091\n",
      "Epoch:40 Batch:8 Loss:0.092\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 38.50613229322044 setps: 32 count: 32\n",
      "reward: 40.92727641222738 setps: 35 count: 67\n",
      "reward: 41.377608813418185 setps: 33 count: 100\n",
      "reward: 30.278167265010424 setps: 30 count: 130\n",
      "reward: 44.04260095469653 setps: 37 count: 167\n",
      "reward: 49.51880773115699 setps: 37 count: 204\n",
      "reward: 36.24759376548608 setps: 30 count: 234\n",
      "reward: 34.284556259804226 setps: 29 count: 263\n",
      "reward: 36.80201457831863 setps: 30 count: 293\n",
      "reward: 29.99713087201817 setps: 25 count: 318\n",
      "reward: 36.94179481598985 setps: 30 count: 348\n",
      "reward: 29.14054722563742 setps: 29 count: 377\n",
      "reward: 44.52276327176076 setps: 36 count: 413\n",
      "reward: 40.964355343594804 setps: 39 count: 452\n",
      "reward: 48.17579605576174 setps: 37 count: 489\n",
      "reward: 51.938420215905346 setps: 37 count: 526\n",
      "reward: 43.41038581489556 setps: 32 count: 558\n",
      "reward: 30.035534893278964 setps: 30 count: 588\n",
      "reward: 49.84082920437358 setps: 44 count: 632\n",
      "reward: 50.35341268219636 setps: 37 count: 669\n",
      "reward: 53.64468672965159 setps: 38 count: 707\n",
      "reward: 47.1074788144615 setps: 40 count: 747\n",
      "reward: 52.83806457081809 setps: 37 count: 784\n",
      "reward: 27.68201325218979 setps: 29 count: 813\n",
      "reward: 30.80643839898985 setps: 29 count: 842\n",
      "reward: 45.03716915054682 setps: 35 count: 877\n",
      "reward: 33.34761367576721 setps: 30 count: 907\n",
      "reward: 52.33067138916666 setps: 36 count: 943\n",
      "reward: 38.04014257331437 setps: 32 count: 975\n",
      "avg rewards: 40.97034506978129\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.52053\n",
      "Epoch:20 Batch:13 Loss:0.09942\n",
      "Epoch:40 Batch:13 Loss:0.07679\n",
      "Epoch:60 Batch:13 Loss:0.07369\n",
      "Epoch:80 Batch:13 Loss:0.06320\n",
      "Epoch:100 Batch:13 Loss:0.05961\n",
      "Epoch:120 Batch:13 Loss:0.06055\n",
      "Epoch:140 Batch:13 Loss:0.06121\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.090\n",
      "Epoch:10 Batch:8 Loss:0.087\n",
      "Epoch:20 Batch:8 Loss:0.087\n",
      "Epoch:30 Batch:8 Loss:0.089\n",
      "Epoch:40 Batch:8 Loss:0.088\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 42.25993692817138 setps: 36 count: 36\n",
      "reward: 59.44776887352781 setps: 38 count: 74\n",
      "reward: 35.75559215909889 setps: 32 count: 106\n",
      "reward: 77.51126193341335 setps: 44 count: 150\n",
      "reward: 61.27420654804593 setps: 41 count: 191\n",
      "reward: 55.682614422873314 setps: 45 count: 236\n",
      "reward: 15.80369733380794 setps: 17 count: 253\n",
      "reward: 35.97606549617631 setps: 35 count: 288\n",
      "reward: 51.35611590761693 setps: 44 count: 332\n",
      "reward: 47.46961313309438 setps: 37 count: 369\n",
      "reward: 48.148629220375724 setps: 37 count: 406\n",
      "reward: 78.09595863536234 setps: 44 count: 450\n",
      "reward: 75.50455093752244 setps: 51 count: 501\n",
      "reward: 17.939548009712595 setps: 18 count: 519\n",
      "reward: 64.80348081128032 setps: 50 count: 569\n",
      "reward: 46.550089240353564 setps: 37 count: 606\n",
      "reward: 36.091814241337126 setps: 29 count: 635\n",
      "reward: 45.732005250423384 setps: 39 count: 674\n",
      "reward: 67.65830422977307 setps: 48 count: 722\n",
      "reward: 21.78409881678963 setps: 21 count: 743\n",
      "reward: 52.054937735096594 setps: 39 count: 782\n",
      "reward: 38.17157162665971 setps: 37 count: 819\n",
      "reward: 47.57297561062879 setps: 41 count: 860\n",
      "reward: 23.844746229967857 setps: 21 count: 881\n",
      "reward: 56.67122466962174 setps: 39 count: 920\n",
      "reward: 50.08411625836889 setps: 40 count: 960\n",
      "reward: 45.894189944778915 setps: 38 count: 998\n",
      "avg rewards: 48.11626348903255\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.36039\n",
      "Epoch:20 Batch:14 Loss:0.09533\n",
      "Epoch:40 Batch:14 Loss:0.07599\n",
      "Epoch:60 Batch:14 Loss:0.06890\n",
      "Epoch:80 Batch:14 Loss:0.06620\n",
      "Epoch:100 Batch:14 Loss:0.06275\n",
      "Epoch:120 Batch:14 Loss:0.06203\n",
      "Epoch:140 Batch:14 Loss:0.05565\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.091\n",
      "Epoch:10 Batch:8 Loss:0.089\n",
      "Epoch:20 Batch:8 Loss:0.088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:30 Batch:8 Loss:0.089\n",
      "Epoch:40 Batch:8 Loss:0.088\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 36.04423027804005 setps: 26 count: 26\n",
      "reward: 40.17718759348062 setps: 26 count: 52\n",
      "reward: 40.056401566200655 setps: 27 count: 79\n",
      "reward: 36.168443385821604 setps: 25 count: 104\n",
      "reward: 41.775525397155434 setps: 27 count: 131\n",
      "reward: 42.23912244664243 setps: 27 count: 158\n",
      "reward: 28.54770794949873 setps: 26 count: 184\n",
      "reward: 37.336272363932224 setps: 25 count: 209\n",
      "reward: 44.088028807831876 setps: 28 count: 237\n",
      "reward: 35.20604817940477 setps: 25 count: 262\n",
      "reward: 35.35436104225955 setps: 24 count: 286\n",
      "reward: 43.65671684314731 setps: 27 count: 313\n",
      "reward: 23.074518622344478 setps: 20 count: 333\n",
      "reward: 41.90990983849042 setps: 27 count: 360\n",
      "reward: 24.660453410209445 setps: 21 count: 381\n",
      "reward: 43.635281738196504 setps: 27 count: 408\n",
      "reward: 34.648159228579615 setps: 25 count: 433\n",
      "reward: 44.14072391959199 setps: 28 count: 461\n",
      "reward: 36.771475611870116 setps: 24 count: 485\n",
      "reward: 16.651543233067787 setps: 17 count: 502\n",
      "reward: 35.53305293242447 setps: 24 count: 526\n",
      "reward: 39.895657066698185 setps: 26 count: 552\n",
      "reward: 38.404265321418634 setps: 27 count: 579\n",
      "reward: 43.53604310504597 setps: 27 count: 606\n",
      "reward: 28.810741091106323 setps: 22 count: 628\n",
      "reward: 35.94896956496668 setps: 24 count: 652\n",
      "reward: 41.05870143109788 setps: 26 count: 678\n",
      "reward: 38.86001071473874 setps: 26 count: 704\n",
      "reward: 36.97052599667367 setps: 25 count: 729\n",
      "reward: 34.80018815434887 setps: 25 count: 754\n",
      "reward: 37.6101806558203 setps: 27 count: 781\n",
      "reward: 39.213230899210615 setps: 25 count: 806\n",
      "reward: 37.480200705114115 setps: 25 count: 831\n",
      "reward: 36.19102503942123 setps: 24 count: 855\n",
      "reward: 34.9602533141151 setps: 25 count: 880\n",
      "reward: 39.49546613151615 setps: 26 count: 906\n",
      "reward: 52.292570373279155 setps: 32 count: 938\n",
      "reward: 43.30987686604786 setps: 28 count: 966\n",
      "reward: 35.46536375520955 setps: 24 count: 990\n",
      "avg rewards: 37.332780373692806\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.34849\n",
      "Epoch:20 Batch:15 Loss:0.09425\n",
      "Epoch:40 Batch:15 Loss:0.07340\n",
      "Epoch:60 Batch:15 Loss:0.05824\n",
      "Epoch:80 Batch:15 Loss:0.06184\n",
      "Epoch:100 Batch:15 Loss:0.05973\n",
      "Epoch:120 Batch:15 Loss:0.05447\n",
      "Epoch:140 Batch:15 Loss:0.05480\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.090\n",
      "Epoch:10 Batch:8 Loss:0.089\n",
      "Epoch:20 Batch:8 Loss:0.089\n",
      "Epoch:30 Batch:8 Loss:0.091\n",
      "Epoch:40 Batch:8 Loss:0.089\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 46.40695265497052 setps: 30 count: 30\n",
      "reward: 46.535301511536815 setps: 30 count: 60\n",
      "reward: 54.78022842260397 setps: 32 count: 92\n",
      "reward: 45.761444792032115 setps: 30 count: 122\n",
      "reward: 62.197462309748516 setps: 38 count: 160\n",
      "reward: 46.84188540915347 setps: 29 count: 189\n",
      "reward: 46.93018942067138 setps: 31 count: 220\n",
      "reward: 51.340820385536055 setps: 31 count: 251\n",
      "reward: 48.87150103374297 setps: 32 count: 283\n",
      "reward: 52.38004464952828 setps: 32 count: 315\n",
      "reward: 15.544994574198792 setps: 17 count: 332\n",
      "reward: 21.16625302522589 setps: 20 count: 352\n",
      "reward: 57.8219255486867 setps: 35 count: 387\n",
      "reward: 73.8644908709655 setps: 42 count: 429\n",
      "reward: 47.1435877714539 setps: 30 count: 459\n",
      "reward: 51.00039257017633 setps: 31 count: 490\n",
      "reward: 45.202936254390806 setps: 31 count: 521\n",
      "reward: 49.78684651714721 setps: 31 count: 552\n",
      "reward: 18.219945932153493 setps: 17 count: 569\n",
      "reward: 50.77178535439598 setps: 32 count: 601\n",
      "reward: 21.73899638849252 setps: 19 count: 620\n",
      "reward: 49.5414884828715 setps: 33 count: 653\n",
      "reward: 23.66495742251573 setps: 20 count: 673\n",
      "reward: 56.92808623607271 setps: 35 count: 708\n",
      "reward: 19.998985614636332 setps: 19 count: 727\n",
      "reward: 44.65182785743819 setps: 29 count: 756\n",
      "reward: 44.948184251695054 setps: 30 count: 786\n",
      "reward: 50.336646323600135 setps: 33 count: 819\n",
      "reward: 50.952581348484095 setps: 31 count: 850\n",
      "reward: 43.89962043895793 setps: 29 count: 879\n",
      "reward: 26.158122291478502 setps: 21 count: 900\n",
      "reward: 54.45142546292335 setps: 33 count: 933\n",
      "reward: 52.80475982494682 setps: 35 count: 968\n",
      "reward: 46.18095107377594 setps: 29 count: 997\n",
      "avg rewards: 44.671341824300214\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.37648\n",
      "Epoch:20 Batch:16 Loss:0.08693\n",
      "Epoch:40 Batch:16 Loss:0.07030\n",
      "Epoch:60 Batch:16 Loss:0.06181\n",
      "Epoch:80 Batch:16 Loss:0.06196\n",
      "Epoch:100 Batch:16 Loss:0.05448\n",
      "Epoch:120 Batch:16 Loss:0.05784\n",
      "Epoch:140 Batch:16 Loss:0.05696\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.080\n",
      "Epoch:10 Batch:8 Loss:0.079\n",
      "Epoch:20 Batch:8 Loss:0.077\n",
      "Epoch:30 Batch:8 Loss:0.078\n",
      "Epoch:40 Batch:8 Loss:0.077\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 40.7623242023532 setps: 27 count: 27\n",
      "reward: 13.798714065675453 setps: 17 count: 44\n",
      "reward: 38.065533506973594 setps: 27 count: 71\n",
      "reward: 55.31958011578389 setps: 37 count: 108\n",
      "reward: 44.7978754091615 setps: 36 count: 144\n",
      "reward: 65.71112582862841 setps: 39 count: 183\n",
      "reward: 24.37802879808005 setps: 20 count: 203\n",
      "reward: 49.82256171208429 setps: 38 count: 241\n",
      "reward: 59.04478983159496 setps: 39 count: 280\n",
      "reward: 64.49350210112607 setps: 38 count: 318\n",
      "reward: 53.46819254178553 setps: 35 count: 353\n",
      "reward: 53.030804254305274 setps: 40 count: 393\n",
      "reward: 58.37607548831729 setps: 38 count: 431\n",
      "reward: 61.31645958419831 setps: 39 count: 470\n",
      "reward: 59.26272023454658 setps: 41 count: 511\n",
      "reward: 60.066255701689805 setps: 37 count: 548\n",
      "reward: 50.40891949782089 setps: 38 count: 586\n",
      "reward: 53.58836615237377 setps: 40 count: 626\n",
      "reward: 53.07306136867846 setps: 34 count: 660\n",
      "reward: 58.280544401025686 setps: 35 count: 695\n",
      "reward: 42.57268873789872 setps: 33 count: 728\n",
      "reward: 32.65518857075804 setps: 27 count: 755\n",
      "reward: 64.66093361889945 setps: 42 count: 797\n",
      "reward: 54.375664686773966 setps: 33 count: 830\n",
      "reward: 51.99577707182324 setps: 41 count: 871\n",
      "reward: 44.97260453037161 setps: 30 count: 901\n",
      "reward: 37.65615781966335 setps: 26 count: 927\n",
      "reward: 68.36442501863056 setps: 37 count: 964\n",
      "avg rewards: 50.5113883875365\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.45092\n",
      "Epoch:20 Batch:17 Loss:0.08475\n",
      "Epoch:40 Batch:17 Loss:0.06862\n",
      "Epoch:60 Batch:17 Loss:0.06070\n",
      "Epoch:80 Batch:17 Loss:0.05439\n",
      "Epoch:100 Batch:17 Loss:0.05889\n",
      "Epoch:120 Batch:17 Loss:0.05629\n",
      "Epoch:140 Batch:17 Loss:0.04732\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.081\n",
      "Epoch:10 Batch:8 Loss:0.081\n",
      "Epoch:20 Batch:8 Loss:0.083\n",
      "Epoch:30 Batch:8 Loss:0.081\n",
      "Epoch:40 Batch:8 Loss:0.080\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 61.096834255599234 setps: 43 count: 43\n",
      "reward: 44.6715146080358 setps: 42 count: 85\n",
      "reward: 48.12756103162829 setps: 43 count: 128\n",
      "reward: 57.474134323785265 setps: 41 count: 169\n",
      "reward: 70.69635067663039 setps: 64 count: 233\n",
      "reward: 89.87248954733366 setps: 52 count: 285\n",
      "reward: 47.86110058055636 setps: 38 count: 323\n",
      "reward: 59.753968795249236 setps: 47 count: 370\n",
      "reward: 99.35129255271455 setps: 58 count: 428\n",
      "reward: 67.50782038340694 setps: 44 count: 472\n",
      "reward: 47.855635833441916 setps: 36 count: 508\n",
      "reward: 80.24829424551716 setps: 54 count: 562\n",
      "reward: 66.00067796800721 setps: 45 count: 607\n",
      "reward: 54.80799876575911 setps: 42 count: 649\n",
      "reward: 20.470341708083293 setps: 20 count: 669\n",
      "reward: 53.36371051720925 setps: 38 count: 707\n",
      "reward: 96.23221252845833 setps: 57 count: 764\n",
      "reward: 20.0389641883492 setps: 20 count: 784\n",
      "reward: 47.876072471399674 setps: 37 count: 821\n",
      "reward: 54.22930572930054 setps: 38 count: 859\n",
      "reward: 100.68268331597359 setps: 67 count: 926\n",
      "reward: 65.2368915549756 setps: 52 count: 978\n",
      "avg rewards: 61.520720708246124\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.30099\n",
      "Epoch:20 Batch:18 Loss:0.07900\n",
      "Epoch:40 Batch:18 Loss:0.06744\n",
      "Epoch:60 Batch:18 Loss:0.05887\n",
      "Epoch:80 Batch:18 Loss:0.05908\n",
      "Epoch:100 Batch:18 Loss:0.05451\n",
      "Epoch:120 Batch:18 Loss:0.05507\n",
      "Epoch:140 Batch:18 Loss:0.05303\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.077\n",
      "Epoch:10 Batch:8 Loss:0.078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 Batch:8 Loss:0.076\n",
      "Epoch:30 Batch:8 Loss:0.077\n",
      "Epoch:40 Batch:8 Loss:0.077\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 38.94994538587053 setps: 33 count: 33\n",
      "reward: 42.483784535048464 setps: 34 count: 67\n",
      "reward: 34.147274268911865 setps: 29 count: 96\n",
      "reward: 40.63034617276716 setps: 40 count: 136\n",
      "reward: 60.53061731625056 setps: 43 count: 179\n",
      "reward: 35.99912075674802 setps: 29 count: 208\n",
      "reward: 43.625096549853325 setps: 35 count: 243\n",
      "reward: 36.9765174383967 setps: 36 count: 279\n",
      "reward: 36.795836533819966 setps: 33 count: 312\n",
      "reward: 13.450480507689644 setps: 18 count: 330\n",
      "reward: 21.455471499303535 setps: 21 count: 351\n",
      "reward: 66.88324312719486 setps: 47 count: 398\n",
      "reward: 57.92258805577004 setps: 45 count: 443\n",
      "reward: 53.037956457542876 setps: 42 count: 485\n",
      "reward: 36.18043295823008 setps: 31 count: 516\n",
      "reward: 32.30346769514436 setps: 29 count: 545\n",
      "reward: 41.217652244259064 setps: 32 count: 577\n",
      "reward: 23.021581990507546 setps: 21 count: 598\n",
      "reward: 18.796532691361787 setps: 21 count: 619\n",
      "reward: 52.55780807561386 setps: 38 count: 657\n",
      "reward: 45.18895454614686 setps: 33 count: 690\n",
      "reward: 16.18927824832499 setps: 19 count: 709\n",
      "reward: 65.30940036298271 setps: 46 count: 755\n",
      "reward: 37.05128027025057 setps: 31 count: 786\n",
      "reward: 41.696463334810694 setps: 38 count: 824\n",
      "reward: 42.62280556970072 setps: 32 count: 856\n",
      "reward: 25.709898139725556 setps: 22 count: 878\n",
      "reward: 58.49448990516248 setps: 43 count: 921\n",
      "reward: 52.43571696002182 setps: 43 count: 964\n",
      "reward: 38.95800300266856 setps: 35 count: 999\n",
      "avg rewards: 40.35406815333598\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.28727\n",
      "Epoch:20 Batch:19 Loss:0.07291\n",
      "Epoch:40 Batch:19 Loss:0.07168\n",
      "Epoch:60 Batch:19 Loss:0.06302\n",
      "Epoch:80 Batch:19 Loss:0.05390\n",
      "Epoch:100 Batch:19 Loss:0.05333\n",
      "Epoch:120 Batch:19 Loss:0.05084\n",
      "Epoch:140 Batch:19 Loss:0.05575\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.077\n",
      "Epoch:10 Batch:8 Loss:0.075\n",
      "Epoch:20 Batch:8 Loss:0.074\n",
      "Epoch:30 Batch:8 Loss:0.075\n",
      "Epoch:40 Batch:8 Loss:0.073\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 17.325350075197633 setps: 18 count: 18\n",
      "reward: 34.02860192501539 setps: 25 count: 43\n",
      "reward: 32.37652807133126 setps: 26 count: 69\n",
      "reward: 43.11178578208782 setps: 32 count: 101\n",
      "reward: 44.27416929085884 setps: 31 count: 132\n",
      "reward: 30.074863221426497 setps: 23 count: 155\n",
      "reward: 44.24015488054866 setps: 33 count: 188\n",
      "reward: 33.70472951012344 setps: 27 count: 215\n",
      "reward: 34.57740796074358 setps: 26 count: 241\n",
      "reward: 11.734633942598885 setps: 17 count: 258\n",
      "reward: 49.28409612291143 setps: 35 count: 293\n",
      "reward: 31.57056327628525 setps: 24 count: 317\n",
      "reward: 32.461920000890686 setps: 25 count: 342\n",
      "reward: 35.7898411344897 setps: 26 count: 368\n",
      "reward: 27.661423048624417 setps: 22 count: 390\n",
      "reward: 30.7546896012631 setps: 24 count: 414\n",
      "reward: 33.24510780960263 setps: 24 count: 438\n",
      "reward: 38.252772530833326 setps: 27 count: 465\n",
      "reward: 36.76011861596635 setps: 29 count: 494\n",
      "reward: 18.886616796400627 setps: 17 count: 511\n",
      "reward: 32.70619362352881 setps: 25 count: 536\n",
      "reward: 29.821028652650426 setps: 24 count: 560\n",
      "reward: 21.104211606123133 setps: 19 count: 579\n",
      "reward: 40.8785311109008 setps: 28 count: 607\n",
      "reward: 18.849449226904838 setps: 19 count: 626\n",
      "reward: 41.90493855302339 setps: 31 count: 657\n",
      "reward: 45.190054654613775 setps: 31 count: 688\n",
      "reward: 32.236537652637345 setps: 25 count: 713\n",
      "reward: 16.988094800130057 setps: 18 count: 731\n",
      "reward: 34.99687609213578 setps: 26 count: 757\n",
      "reward: 31.645766535353328 setps: 26 count: 783\n",
      "reward: 33.91710251359327 setps: 27 count: 810\n",
      "reward: 39.06506577983382 setps: 28 count: 838\n",
      "reward: 28.201229775852696 setps: 22 count: 860\n",
      "reward: 22.390054889449672 setps: 20 count: 880\n",
      "reward: 31.798046482604697 setps: 24 count: 904\n",
      "reward: 35.62340687653778 setps: 27 count: 931\n",
      "reward: 45.32540720563702 setps: 33 count: 964\n",
      "reward: 34.321812358917676 setps: 26 count: 990\n",
      "avg rewards: 32.7456200509648\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.27260\n",
      "Epoch:20 Batch:20 Loss:0.07412\n",
      "Epoch:40 Batch:20 Loss:0.06276\n",
      "Epoch:60 Batch:20 Loss:0.05350\n",
      "Epoch:80 Batch:20 Loss:0.05368\n",
      "Epoch:100 Batch:20 Loss:0.05603\n",
      "Epoch:120 Batch:20 Loss:0.05328\n",
      "Epoch:140 Batch:20 Loss:0.04492\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.078\n",
      "Epoch:10 Batch:8 Loss:0.078\n",
      "Epoch:20 Batch:8 Loss:0.076\n",
      "Epoch:30 Batch:8 Loss:0.077\n",
      "Epoch:40 Batch:8 Loss:0.080\n",
      "Done!\n",
      "############# start BipedalWalker-v3 training ###################\n",
      "(49,)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -106.51782285279114 setps: 98 count: 98\n",
      "reward: -97.12831929902724 setps: 82 count: 180\n",
      "reward: -98.1580530881829 setps: 84 count: 264\n",
      "reward: -102.67251352002222 setps: 134 count: 398\n",
      "reward: -124.83895313414128 setps: 128 count: 526\n",
      "reward: -114.26748292708645 setps: 58 count: 584\n",
      "reward: -115.70578029473126 setps: 76 count: 660\n",
      "reward: -98.63186475820342 setps: 162 count: 822\n",
      "reward: -99.85810176943491 setps: 100 count: 922\n",
      "avg rewards: -106.4198768492912\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:1.36627\n",
      "Epoch:20 Batch:1 Loss:0.18873\n",
      "Epoch:40 Batch:1 Loss:0.14237\n",
      "Epoch:60 Batch:1 Loss:0.12431\n",
      "Epoch:80 Batch:1 Loss:0.10509\n",
      "Epoch:100 Batch:1 Loss:0.08855\n",
      "Epoch:120 Batch:1 Loss:0.07987\n",
      "Epoch:140 Batch:1 Loss:0.07256\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.125\n",
      "Epoch:10 Batch:9 Loss:0.124\n",
      "Epoch:20 Batch:9 Loss:0.110\n",
      "Epoch:30 Batch:9 Loss:0.104\n",
      "Epoch:40 Batch:9 Loss:0.103\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -10.186714400191178 setps: 800 count: 800\n",
      "avg rewards: -10.186714400191178\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:1.28855\n",
      "Epoch:20 Batch:2 Loss:0.11931\n",
      "Epoch:40 Batch:2 Loss:0.08106\n",
      "Epoch:60 Batch:2 Loss:0.06668\n",
      "Epoch:80 Batch:2 Loss:0.05320\n",
      "Epoch:100 Batch:2 Loss:0.04744\n",
      "Epoch:120 Batch:2 Loss:0.04344\n",
      "Epoch:140 Batch:2 Loss:0.04121\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.100\n",
      "Epoch:10 Batch:9 Loss:0.089\n",
      "Epoch:20 Batch:9 Loss:0.083\n",
      "Epoch:30 Batch:9 Loss:0.087\n",
      "Epoch:40 Batch:9 Loss:0.086\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -110.37858054800331 setps: 43 count: 43\n",
      "reward: -111.64511818592622 setps: 47 count: 90\n",
      "reward: -111.11186375919668 setps: 48 count: 138\n",
      "reward: -110.3420119048059 setps: 46 count: 184\n",
      "reward: -110.93567331092991 setps: 43 count: 227\n",
      "reward: -110.60757372038688 setps: 59 count: 286\n",
      "reward: -114.01398920544858 setps: 96 count: 382\n",
      "reward: -110.76068930239913 setps: 48 count: 430\n",
      "avg rewards: -111.22443749213708\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:1.13010\n",
      "Epoch:20 Batch:3 Loss:0.08990\n",
      "Epoch:40 Batch:3 Loss:0.05946\n",
      "Epoch:60 Batch:3 Loss:0.04844\n",
      "Epoch:80 Batch:3 Loss:0.03943\n",
      "Epoch:100 Batch:3 Loss:0.03636\n",
      "Epoch:120 Batch:3 Loss:0.03789\n",
      "Epoch:140 Batch:3 Loss:0.03359\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.071\n",
      "Epoch:10 Batch:9 Loss:0.072\n",
      "Epoch:20 Batch:9 Loss:0.072\n",
      "Epoch:30 Batch:9 Loss:0.063\n",
      "Epoch:40 Batch:9 Loss:0.071\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -121.17328809369417 setps: 72 count: 72\n",
      "reward: -107.48934672279718 setps: 42 count: 114\n",
      "reward: -114.86505086853975 setps: 60 count: 174\n",
      "reward: -110.40598531554825 setps: 47 count: 221\n",
      "reward: -112.1869676493754 setps: 57 count: 278\n",
      "reward: -113.89176094986126 setps: 55 count: 333\n",
      "reward: -112.36350096278949 setps: 84 count: 417\n",
      "reward: -108.54440415719772 setps: 49 count: 466\n",
      "reward: -111.54792523744453 setps: 50 count: 516\n",
      "reward: -123.09899514217054 setps: 78 count: 594\n",
      "avg rewards: -113.55672250994182\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.83327\n",
      "Epoch:20 Batch:4 Loss:0.07462\n",
      "Epoch:40 Batch:4 Loss:0.05450\n",
      "Epoch:60 Batch:4 Loss:0.04523\n",
      "Epoch:80 Batch:4 Loss:0.03692\n",
      "Epoch:100 Batch:4 Loss:0.03579\n",
      "Epoch:120 Batch:4 Loss:0.03433\n",
      "Epoch:140 Batch:4 Loss:0.03254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.071\n",
      "Epoch:10 Batch:9 Loss:0.065\n",
      "Epoch:20 Batch:9 Loss:0.067\n",
      "Epoch:30 Batch:9 Loss:0.065\n",
      "Epoch:40 Batch:9 Loss:0.066\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -36.44701844064366 setps: 800 count: 800\n",
      "reward: -119.98286327225591 setps: 69 count: 869\n",
      "reward: -106.89704945200744 setps: 46 count: 915\n",
      "reward: -107.6378281761681 setps: 48 count: 963\n",
      "avg rewards: -92.74118983526878\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.86200\n",
      "Epoch:20 Batch:5 Loss:0.06552\n",
      "Epoch:40 Batch:5 Loss:0.04775\n",
      "Epoch:60 Batch:5 Loss:0.04086\n",
      "Epoch:80 Batch:5 Loss:0.03693\n",
      "Epoch:100 Batch:5 Loss:0.03410\n",
      "Epoch:120 Batch:5 Loss:0.03421\n",
      "Epoch:140 Batch:5 Loss:0.03122\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.068\n",
      "Epoch:10 Batch:9 Loss:0.065\n",
      "Epoch:20 Batch:9 Loss:0.062\n",
      "Epoch:30 Batch:9 Loss:0.062\n",
      "Epoch:40 Batch:9 Loss:0.061\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -40.681655464279416 setps: 800 count: 800\n",
      "reward: -119.99096055842875 setps: 72 count: 872\n",
      "avg rewards: -80.33630801135408\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.66615\n",
      "Epoch:20 Batch:6 Loss:0.06307\n",
      "Epoch:40 Batch:6 Loss:0.04659\n",
      "Epoch:60 Batch:6 Loss:0.04212\n",
      "Epoch:80 Batch:6 Loss:0.03717\n",
      "Epoch:100 Batch:6 Loss:0.03539\n",
      "Epoch:120 Batch:6 Loss:0.03375\n",
      "Epoch:140 Batch:6 Loss:0.03300\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.060\n",
      "Epoch:10 Batch:9 Loss:0.059\n",
      "Epoch:20 Batch:9 Loss:0.062\n",
      "Epoch:30 Batch:9 Loss:0.057\n",
      "Epoch:40 Batch:9 Loss:0.059\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -116.94352321036595 setps: 58 count: 58\n",
      "reward: -111.24080041810808 setps: 61 count: 119\n",
      "reward: -111.08492102998619 setps: 51 count: 170\n",
      "reward: -116.2389350131955 setps: 61 count: 231\n",
      "reward: -111.11630732277159 setps: 51 count: 282\n",
      "reward: -122.46096430632161 setps: 73 count: 355\n",
      "reward: -117.92797713965923 setps: 69 count: 424\n",
      "reward: -120.62673702799653 setps: 72 count: 496\n",
      "reward: -119.50111404450486 setps: 64 count: 560\n",
      "reward: -116.92791991067429 setps: 62 count: 622\n",
      "reward: -117.29236152244856 setps: 63 count: 685\n",
      "reward: -118.17012658951928 setps: 64 count: 749\n",
      "reward: -117.05672951977266 setps: 65 count: 814\n",
      "reward: -114.80367216487043 setps: 63 count: 877\n",
      "reward: -116.74621039329345 setps: 62 count: 939\n",
      "avg rewards: -116.54255330756588\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.63089\n",
      "Epoch:20 Batch:7 Loss:0.05882\n",
      "Epoch:40 Batch:7 Loss:0.04571\n",
      "Epoch:60 Batch:7 Loss:0.03847\n",
      "Epoch:80 Batch:7 Loss:0.03556\n",
      "Epoch:100 Batch:7 Loss:0.03419\n",
      "Epoch:120 Batch:7 Loss:0.03233\n",
      "Epoch:140 Batch:7 Loss:0.03253\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.058\n",
      "Epoch:10 Batch:9 Loss:0.061\n",
      "Epoch:20 Batch:9 Loss:0.060\n",
      "Epoch:30 Batch:9 Loss:0.059\n",
      "Epoch:40 Batch:9 Loss:0.059\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -113.06171623611637 setps: 53 count: 53\n",
      "reward: -116.50486348643402 setps: 61 count: 114\n",
      "reward: -115.25138007402794 setps: 59 count: 173\n",
      "reward: -119.35961591202704 setps: 65 count: 238\n",
      "reward: -117.59158701623107 setps: 66 count: 304\n",
      "reward: -120.09639832590024 setps: 71 count: 375\n",
      "reward: -119.257346524816 setps: 67 count: 442\n",
      "reward: -111.88890392358664 setps: 59 count: 501\n",
      "reward: -118.76009264124991 setps: 65 count: 566\n",
      "reward: -117.31840213433219 setps: 64 count: 630\n",
      "reward: -119.22841730840629 setps: 67 count: 697\n",
      "reward: -114.45490515399413 setps: 60 count: 757\n",
      "reward: -115.51826219704002 setps: 63 count: 820\n",
      "reward: -116.0621613468534 setps: 60 count: 880\n",
      "reward: -117.67501290234054 setps: 64 count: 944\n",
      "avg rewards: -116.8019376788904\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.65938\n",
      "Epoch:20 Batch:8 Loss:0.05774\n",
      "Epoch:40 Batch:8 Loss:0.04522\n",
      "Epoch:60 Batch:8 Loss:0.03870\n",
      "Epoch:80 Batch:8 Loss:0.03667\n",
      "Epoch:100 Batch:8 Loss:0.03393\n",
      "Epoch:120 Batch:8 Loss:0.03168\n",
      "Epoch:140 Batch:8 Loss:0.03069\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.059\n",
      "Epoch:10 Batch:9 Loss:0.059\n",
      "Epoch:20 Batch:9 Loss:0.059\n",
      "Epoch:30 Batch:9 Loss:0.058\n",
      "Epoch:40 Batch:9 Loss:0.056\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -122.464852952376 setps: 76 count: 76\n",
      "reward: -120.1795087416613 setps: 72 count: 148\n",
      "reward: -115.88065605181643 setps: 79 count: 227\n",
      "reward: -121.97363889334537 setps: 73 count: 300\n",
      "reward: -122.02286487787714 setps: 81 count: 381\n",
      "reward: -119.48410698591235 setps: 90 count: 471\n",
      "reward: -121.4241150033443 setps: 73 count: 544\n",
      "avg rewards: -120.48996335804756\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.31285\n",
      "Epoch:20 Batch:9 Loss:0.05305\n",
      "Epoch:40 Batch:9 Loss:0.03991\n",
      "Epoch:60 Batch:9 Loss:0.03897\n",
      "Epoch:80 Batch:9 Loss:0.03322\n",
      "Epoch:100 Batch:9 Loss:0.03171\n",
      "Epoch:120 Batch:9 Loss:0.02949\n",
      "Epoch:140 Batch:9 Loss:0.03130\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.057\n",
      "Epoch:10 Batch:9 Loss:0.057\n",
      "Epoch:20 Batch:9 Loss:0.055\n",
      "Epoch:30 Batch:9 Loss:0.055\n",
      "Epoch:40 Batch:9 Loss:0.052\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -122.55187512875534 setps: 76 count: 76\n",
      "reward: -112.85653267092134 setps: 72 count: 148\n",
      "reward: -115.55185110750111 setps: 64 count: 212\n",
      "reward: -117.54183693916227 setps: 92 count: 304\n",
      "reward: -112.2295964326846 setps: 56 count: 360\n",
      "reward: -112.18105562696357 setps: 62 count: 422\n",
      "reward: -115.18681078242696 setps: 59 count: 481\n",
      "reward: -118.98549759390082 setps: 70 count: 551\n",
      "reward: -114.39043944376024 setps: 69 count: 620\n",
      "reward: -113.97174158122452 setps: 66 count: 686\n",
      "reward: -113.01246556785702 setps: 68 count: 754\n",
      "reward: -117.03752373288447 setps: 164 count: 918\n",
      "reward: -117.05357867215885 setps: 65 count: 983\n",
      "avg rewards: -115.58083117540008\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.34150\n",
      "Epoch:20 Batch:10 Loss:0.04905\n",
      "Epoch:40 Batch:10 Loss:0.03958\n",
      "Epoch:60 Batch:10 Loss:0.03664\n",
      "Epoch:80 Batch:10 Loss:0.03229\n",
      "Epoch:100 Batch:10 Loss:0.03296\n",
      "Epoch:120 Batch:10 Loss:0.03114\n",
      "Epoch:140 Batch:10 Loss:0.02915\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.057\n",
      "Epoch:10 Batch:9 Loss:0.055\n",
      "Epoch:20 Batch:9 Loss:0.055\n",
      "Epoch:30 Batch:9 Loss:0.056\n",
      "Epoch:40 Batch:9 Loss:0.057\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -119.18621562855628 setps: 141 count: 141\n",
      "reward: -117.74272680874853 setps: 67 count: 208\n",
      "reward: -112.3072259463314 setps: 59 count: 267\n",
      "reward: -115.83719990317275 setps: 74 count: 341\n",
      "reward: -121.48751331723606 setps: 99 count: 440\n",
      "avg rewards: -117.312176320809\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.33568\n",
      "Epoch:20 Batch:11 Loss:0.04800\n",
      "Epoch:40 Batch:11 Loss:0.03947\n",
      "Epoch:60 Batch:11 Loss:0.03463\n",
      "Epoch:80 Batch:11 Loss:0.03199\n",
      "Epoch:100 Batch:11 Loss:0.03187\n",
      "Epoch:120 Batch:11 Loss:0.03084\n",
      "Epoch:140 Batch:11 Loss:0.03040\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.056\n",
      "Epoch:10 Batch:9 Loss:0.056\n",
      "Epoch:20 Batch:9 Loss:0.057\n",
      "Epoch:30 Batch:9 Loss:0.057\n",
      "Epoch:40 Batch:9 Loss:0.052\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -121.04834318707387 setps: 65 count: 65\n",
      "reward: -114.22206009535181 setps: 75 count: 140\n",
      "reward: -117.12296284063285 setps: 78 count: 218\n",
      "avg rewards: -117.46445537435284\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.34415\n",
      "Epoch:20 Batch:12 Loss:0.04669\n",
      "Epoch:40 Batch:12 Loss:0.03800\n",
      "Epoch:60 Batch:12 Loss:0.03293\n",
      "Epoch:80 Batch:12 Loss:0.03307\n",
      "Epoch:100 Batch:12 Loss:0.03025\n",
      "Epoch:120 Batch:12 Loss:0.02838\n",
      "Epoch:140 Batch:12 Loss:0.02793\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.053\n",
      "Epoch:10 Batch:9 Loss:0.055\n",
      "Epoch:20 Batch:9 Loss:0.053\n",
      "Epoch:30 Batch:9 Loss:0.055\n",
      "Epoch:40 Batch:9 Loss:0.053\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -9.764031865647688 setps: 800 count: 800\n",
      "reward: -93.1641074767361 setps: 133 count: 933\n",
      "reward: -114.10362920261423 setps: 62 count: 995\n",
      "avg rewards: -72.34392284833267\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.22822\n",
      "Epoch:20 Batch:13 Loss:0.04909\n",
      "Epoch:40 Batch:13 Loss:0.03729\n",
      "Epoch:60 Batch:13 Loss:0.03614\n",
      "Epoch:80 Batch:13 Loss:0.03492\n",
      "Epoch:100 Batch:13 Loss:0.03012\n",
      "Epoch:120 Batch:13 Loss:0.02971\n",
      "Epoch:140 Batch:13 Loss:0.02760\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.056\n",
      "Epoch:10 Batch:9 Loss:0.051\n",
      "Epoch:20 Batch:9 Loss:0.050\n",
      "Epoch:30 Batch:9 Loss:0.053\n",
      "Epoch:40 Batch:9 Loss:0.049\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -24.15470471581259 setps: 800 count: 800\n",
      "reward: -108.61216019980424 setps: 98 count: 898\n",
      "avg rewards: -66.38343245780841\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.21164\n",
      "Epoch:20 Batch:14 Loss:0.04442\n",
      "Epoch:40 Batch:14 Loss:0.03731\n",
      "Epoch:60 Batch:14 Loss:0.03439\n",
      "Epoch:80 Batch:14 Loss:0.02975\n",
      "Epoch:100 Batch:14 Loss:0.03019\n",
      "Epoch:120 Batch:14 Loss:0.02715\n",
      "Epoch:140 Batch:14 Loss:0.02711\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.054\n",
      "Epoch:10 Batch:9 Loss:0.051\n",
      "Epoch:20 Batch:9 Loss:0.051\n",
      "Epoch:30 Batch:9 Loss:0.049\n",
      "Epoch:40 Batch:9 Loss:0.050\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -126.58094080068419 setps: 92 count: 92\n",
      "reward: -115.17864921854002 setps: 60 count: 152\n",
      "reward: -1.5512123340815274 setps: 800 count: 952\n",
      "avg rewards: -81.10360078443524\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.18145\n",
      "Epoch:20 Batch:15 Loss:0.04499\n",
      "Epoch:40 Batch:15 Loss:0.03802\n",
      "Epoch:60 Batch:15 Loss:0.03249\n",
      "Epoch:80 Batch:15 Loss:0.02997\n",
      "Epoch:100 Batch:15 Loss:0.03092\n",
      "Epoch:120 Batch:15 Loss:0.02903\n",
      "Epoch:140 Batch:15 Loss:0.02777\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.047\n",
      "Epoch:10 Batch:9 Loss:0.048\n",
      "Epoch:20 Batch:9 Loss:0.048\n",
      "Epoch:30 Batch:9 Loss:0.053\n",
      "Epoch:40 Batch:9 Loss:0.048\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -9.342820297326268 setps: 800 count: 800\n",
      "reward: -113.90682321391503 setps: 55 count: 855\n",
      "reward: -88.6939692547551 setps: 144 count: 999\n",
      "avg rewards: -70.6478709219988\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.16650\n",
      "Epoch:20 Batch:16 Loss:0.04497\n",
      "Epoch:40 Batch:16 Loss:0.03614\n",
      "Epoch:60 Batch:16 Loss:0.03208\n",
      "Epoch:80 Batch:16 Loss:0.03247\n",
      "Epoch:100 Batch:16 Loss:0.03189\n",
      "Epoch:120 Batch:16 Loss:0.02941\n",
      "Epoch:140 Batch:16 Loss:0.02867\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.047\n",
      "Epoch:10 Batch:9 Loss:0.045\n",
      "Epoch:20 Batch:9 Loss:0.044\n",
      "Epoch:30 Batch:9 Loss:0.047\n",
      "Epoch:40 Batch:9 Loss:0.045\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 117.56191582287657 setps: 800 count: 800\n",
      "avg rewards: 117.56191582287657\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.17306\n",
      "Epoch:20 Batch:17 Loss:0.04751\n",
      "Epoch:40 Batch:17 Loss:0.03529\n",
      "Epoch:60 Batch:17 Loss:0.03485\n",
      "Epoch:80 Batch:17 Loss:0.03439\n",
      "Epoch:100 Batch:17 Loss:0.02841\n",
      "Epoch:120 Batch:17 Loss:0.02872\n",
      "Epoch:140 Batch:17 Loss:0.02764\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.039\n",
      "Epoch:10 Batch:9 Loss:0.041\n",
      "Epoch:20 Batch:9 Loss:0.040\n",
      "Epoch:30 Batch:9 Loss:0.046\n",
      "Epoch:40 Batch:9 Loss:0.039\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -4.40948771168292 setps: 800 count: 800\n",
      "avg rewards: -4.40948771168292\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.16221\n",
      "Epoch:20 Batch:18 Loss:0.04511\n",
      "Epoch:40 Batch:18 Loss:0.03602\n",
      "Epoch:60 Batch:18 Loss:0.03130\n",
      "Epoch:80 Batch:18 Loss:0.03042\n",
      "Epoch:100 Batch:18 Loss:0.03032\n",
      "Epoch:120 Batch:18 Loss:0.02790\n",
      "Epoch:140 Batch:18 Loss:0.02852\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.039\n",
      "Epoch:10 Batch:9 Loss:0.038\n",
      "Epoch:20 Batch:9 Loss:0.037\n",
      "Epoch:30 Batch:9 Loss:0.041\n",
      "Epoch:40 Batch:9 Loss:0.037\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 6.825625170470839 setps: 800 count: 800\n",
      "avg rewards: 6.825625170470839\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.15811\n",
      "Epoch:20 Batch:19 Loss:0.04133\n",
      "Epoch:40 Batch:19 Loss:0.03703\n",
      "Epoch:60 Batch:19 Loss:0.03380\n",
      "Epoch:80 Batch:19 Loss:0.03188\n",
      "Epoch:100 Batch:19 Loss:0.03028\n",
      "Epoch:120 Batch:19 Loss:0.02876\n",
      "Epoch:140 Batch:19 Loss:0.02584\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.039\n",
      "Epoch:10 Batch:9 Loss:0.041\n",
      "Epoch:20 Batch:9 Loss:0.038\n",
      "Epoch:30 Batch:9 Loss:0.036\n",
      "Epoch:40 Batch:9 Loss:0.042\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -114.35458184492589 setps: 113 count: 113\n",
      "reward: -113.10727676125244 setps: 57 count: 170\n",
      "reward: -109.99978231091549 setps: 86 count: 256\n",
      "reward: -57.946729780880744 setps: 347 count: 603\n",
      "avg rewards: -98.85209267449365\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.13793\n",
      "Epoch:20 Batch:20 Loss:0.04001\n",
      "Epoch:40 Batch:20 Loss:0.03353\n",
      "Epoch:60 Batch:20 Loss:0.03225\n",
      "Epoch:80 Batch:20 Loss:0.02995\n",
      "Epoch:100 Batch:20 Loss:0.02975\n",
      "Epoch:120 Batch:20 Loss:0.02862\n",
      "Epoch:140 Batch:20 Loss:0.02860\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.040\n",
      "Epoch:10 Batch:9 Loss:0.037\n",
      "Epoch:20 Batch:9 Loss:0.040\n",
      "Epoch:30 Batch:9 Loss:0.037\n",
      "Epoch:40 Batch:9 Loss:0.038\n",
      "Done!\n",
      "############# start Walker2DBulletEnv-v0 training ###################\n",
      "(49, 1000, 22)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 23.3598238965118 setps: 27 count: 27\n",
      "reward: 13.427080848017068 setps: 10 count: 37\n",
      "reward: 20.447937644658666 setps: 21 count: 58\n",
      "reward: 15.89743476329022 setps: 11 count: 69\n",
      "reward: 12.8848741660433 setps: 8 count: 77\n",
      "reward: 15.188405656484246 setps: 14 count: 91\n",
      "reward: 14.644253449182726 setps: 10 count: 101\n",
      "reward: 12.268839567886607 setps: 7 count: 108\n",
      "reward: 22.879957070975788 setps: 38 count: 146\n",
      "reward: 20.522875732048124 setps: 20 count: 166\n",
      "reward: 17.80420306020387 setps: 18 count: 184\n",
      "reward: 23.57552472402458 setps: 23 count: 207\n",
      "reward: 18.91639764422871 setps: 15 count: 222\n",
      "reward: 19.579976184986297 setps: 19 count: 241\n",
      "reward: 18.567585506930477 setps: 19 count: 260\n",
      "reward: 13.182927738827132 setps: 9 count: 269\n",
      "reward: 15.426981963751313 setps: 12 count: 281\n",
      "reward: 23.036540000706733 setps: 19 count: 300\n",
      "reward: 20.059228059115412 setps: 18 count: 318\n",
      "reward: 17.218703174342224 setps: 13 count: 331\n",
      "reward: 15.450910336208473 setps: 13 count: 344\n",
      "reward: 17.335596105142027 setps: 14 count: 358\n",
      "reward: 15.936367014636929 setps: 16 count: 374\n",
      "reward: 18.878027819246928 setps: 15 count: 389\n",
      "reward: 14.904123069619526 setps: 11 count: 400\n",
      "reward: 16.414455984444068 setps: 11 count: 411\n",
      "reward: 14.114786853561235 setps: 11 count: 422\n",
      "reward: 22.543155703875524 setps: 22 count: 444\n",
      "reward: 19.082036440697266 setps: 18 count: 462\n",
      "reward: 17.201050974809913 setps: 16 count: 478\n",
      "reward: 20.33845903098845 setps: 15 count: 493\n",
      "reward: 12.997955675171397 setps: 9 count: 502\n",
      "reward: 20.00628185101959 setps: 19 count: 521\n",
      "reward: 13.724182081763864 setps: 10 count: 531\n",
      "reward: 18.90627508321922 setps: 15 count: 546\n",
      "reward: 21.32550403205387 setps: 21 count: 567\n",
      "reward: 24.04043122374715 setps: 24 count: 591\n",
      "reward: 17.07126523296756 setps: 23 count: 614\n",
      "reward: 15.972613652025757 setps: 10 count: 624\n",
      "reward: 16.80172079876647 setps: 16 count: 640\n",
      "reward: 15.286474653735057 setps: 10 count: 650\n",
      "reward: 29.780375466399708 setps: 33 count: 683\n",
      "reward: 16.540354495529026 setps: 17 count: 700\n",
      "reward: 22.04590901549091 setps: 22 count: 722\n",
      "reward: 15.46703833794745 setps: 18 count: 740\n",
      "reward: 18.677262847745443 setps: 14 count: 754\n",
      "reward: 19.75043375710666 setps: 19 count: 773\n",
      "reward: 17.56491007366567 setps: 15 count: 788\n",
      "reward: 17.304181547183546 setps: 13 count: 801\n",
      "reward: 24.459950622131874 setps: 25 count: 826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 15.08075326136459 setps: 12 count: 838\n",
      "reward: 14.631682121122138 setps: 15 count: 853\n",
      "reward: 15.342971074323579 setps: 10 count: 863\n",
      "reward: 19.34638860751002 setps: 19 count: 882\n",
      "reward: 18.63123921203369 setps: 17 count: 899\n",
      "reward: 20.79012041430105 setps: 22 count: 921\n",
      "reward: 17.032250068732537 setps: 11 count: 932\n",
      "reward: 19.22291105356126 setps: 17 count: 949\n",
      "reward: 15.178495415404905 setps: 13 count: 962\n",
      "reward: 22.368147764010065 setps: 21 count: 983\n",
      "reward: 17.395201157190606 setps: 14 count: 997\n",
      "avg rewards: 18.095603291519186\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:1.28251\n",
      "Epoch:20 Batch:1 Loss:0.12761\n",
      "Epoch:40 Batch:1 Loss:0.10130\n",
      "Epoch:60 Batch:1 Loss:0.07531\n",
      "Epoch:80 Batch:1 Loss:0.06139\n",
      "Epoch:100 Batch:1 Loss:0.05394\n",
      "Epoch:120 Batch:1 Loss:0.05007\n",
      "Epoch:140 Batch:1 Loss:0.04643\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.115\n",
      "Epoch:10 Batch:10 Loss:0.105\n",
      "Epoch:20 Batch:10 Loss:0.093\n",
      "Epoch:30 Batch:10 Loss:0.094\n",
      "Epoch:40 Batch:10 Loss:0.092\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 15.468264362450286 setps: 64 count: 64\n",
      "reward: 15.989136333277557 setps: 65 count: 129\n",
      "reward: 13.16473419318208 setps: 66 count: 195\n",
      "reward: 20.079278487416737 setps: 74 count: 269\n",
      "reward: 18.684425993720637 setps: 65 count: 334\n",
      "reward: 13.12355923399446 setps: 67 count: 401\n",
      "reward: 18.955692074856778 setps: 73 count: 474\n",
      "reward: 21.730493705690595 setps: 79 count: 553\n",
      "reward: 34.22625819692623 setps: 97 count: 650\n",
      "reward: 19.173492694568996 setps: 65 count: 715\n",
      "reward: 15.591394426376796 setps: 61 count: 776\n",
      "reward: 20.919838114139566 setps: 73 count: 849\n",
      "reward: 8.533034001161287 setps: 56 count: 905\n",
      "reward: 14.300717198464557 setps: 68 count: 973\n",
      "avg rewards: 17.85287992973047\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:1.19176\n",
      "Epoch:20 Batch:2 Loss:0.07534\n",
      "Epoch:40 Batch:2 Loss:0.04991\n",
      "Epoch:60 Batch:2 Loss:0.03738\n",
      "Epoch:80 Batch:2 Loss:0.03298\n",
      "Epoch:100 Batch:2 Loss:0.02902\n",
      "Epoch:120 Batch:2 Loss:0.02696\n",
      "Epoch:140 Batch:2 Loss:0.02515\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.102\n",
      "Epoch:10 Batch:10 Loss:0.094\n",
      "Epoch:20 Batch:10 Loss:0.091\n",
      "Epoch:30 Batch:10 Loss:0.093\n",
      "Epoch:40 Batch:10 Loss:0.092\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 55.380684962132364 setps: 72 count: 72\n",
      "reward: 77.75556647563063 setps: 93 count: 165\n",
      "reward: 554.3040561146784 setps: 800 count: 965\n",
      "avg rewards: 229.14676918414713\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:1.11938\n",
      "Epoch:20 Batch:3 Loss:0.05811\n",
      "Epoch:40 Batch:3 Loss:0.03793\n",
      "Epoch:60 Batch:3 Loss:0.02678\n",
      "Epoch:80 Batch:3 Loss:0.02337\n",
      "Epoch:100 Batch:3 Loss:0.02065\n",
      "Epoch:120 Batch:3 Loss:0.01917\n",
      "Epoch:140 Batch:3 Loss:0.01804\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.083\n",
      "Epoch:10 Batch:10 Loss:0.082\n",
      "Epoch:20 Batch:10 Loss:0.082\n",
      "Epoch:30 Batch:10 Loss:0.080\n",
      "Epoch:40 Batch:10 Loss:0.081\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 392.0618434384508 setps: 800 count: 800\n",
      "reward: 30.30039554912656 setps: 85 count: 885\n",
      "reward: 51.590876914952226 setps: 65 count: 950\n",
      "avg rewards: 157.98437196750987\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.94368\n",
      "Epoch:20 Batch:4 Loss:0.04217\n",
      "Epoch:40 Batch:4 Loss:0.02522\n",
      "Epoch:60 Batch:4 Loss:0.01939\n",
      "Epoch:80 Batch:4 Loss:0.01828\n",
      "Epoch:100 Batch:4 Loss:0.01531\n",
      "Epoch:120 Batch:4 Loss:0.01378\n",
      "Epoch:140 Batch:4 Loss:0.01363\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.088\n",
      "Epoch:10 Batch:10 Loss:0.085\n",
      "Epoch:20 Batch:10 Loss:0.085\n",
      "Epoch:30 Batch:10 Loss:0.083\n",
      "Epoch:40 Batch:10 Loss:0.082\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 73.91991452562506 setps: 97 count: 97\n",
      "reward: 69.25787815398218 setps: 87 count: 184\n",
      "reward: 200.17999293873 setps: 278 count: 462\n",
      "avg rewards: 114.45259520611243\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.69576\n",
      "Epoch:20 Batch:5 Loss:0.03624\n",
      "Epoch:40 Batch:5 Loss:0.02265\n",
      "Epoch:60 Batch:5 Loss:0.01686\n",
      "Epoch:80 Batch:5 Loss:0.01454\n",
      "Epoch:100 Batch:5 Loss:0.01335\n",
      "Epoch:120 Batch:5 Loss:0.01263\n",
      "Epoch:140 Batch:5 Loss:0.01120\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.088\n",
      "Epoch:10 Batch:10 Loss:0.088\n",
      "Epoch:20 Batch:10 Loss:0.087\n",
      "Epoch:30 Batch:10 Loss:0.088\n",
      "Epoch:40 Batch:10 Loss:0.085\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 69.89090167915856 setps: 82 count: 82\n",
      "reward: 13.863504001103864 setps: 52 count: 134\n",
      "reward: 130.44768752113595 setps: 219 count: 353\n",
      "avg rewards: 71.40069773379946\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.27733\n",
      "Epoch:20 Batch:6 Loss:0.02809\n",
      "Epoch:40 Batch:6 Loss:0.01717\n",
      "Epoch:60 Batch:6 Loss:0.01364\n",
      "Epoch:80 Batch:6 Loss:0.01221\n",
      "Epoch:100 Batch:6 Loss:0.01027\n",
      "Epoch:120 Batch:6 Loss:0.01016\n",
      "Epoch:140 Batch:6 Loss:0.00938\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.079\n",
      "Epoch:10 Batch:10 Loss:0.081\n",
      "Epoch:20 Batch:10 Loss:0.081\n",
      "Epoch:30 Batch:10 Loss:0.079\n",
      "Epoch:40 Batch:10 Loss:0.080\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 68.64420497526298 setps: 53 count: 53\n",
      "reward: 74.44280685210397 setps: 60 count: 113\n",
      "reward: 63.63392747041071 setps: 60 count: 173\n",
      "reward: 21.655845254463202 setps: 30 count: 203\n",
      "reward: 26.138950113592728 setps: 29 count: 232\n",
      "reward: 28.433147409290545 setps: 29 count: 261\n",
      "reward: 96.43391299195059 setps: 104 count: 365\n",
      "reward: 31.38436608622723 setps: 62 count: 427\n",
      "reward: 122.63061087263341 setps: 111 count: 538\n",
      "reward: 65.54968584583403 setps: 48 count: 586\n",
      "reward: 62.44020634393818 setps: 62 count: 648\n",
      "reward: 31.099698170922053 setps: 47 count: 695\n",
      "reward: 68.8592317884657 setps: 53 count: 748\n",
      "reward: 34.79672549735841 setps: 36 count: 784\n",
      "reward: 73.48007492114996 setps: 56 count: 840\n",
      "reward: 66.0156517814845 setps: 53 count: 893\n",
      "reward: 33.30007940788199 setps: 71 count: 964\n",
      "avg rewards: 56.99641916370413\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.63257\n",
      "Epoch:20 Batch:7 Loss:0.02870\n",
      "Epoch:40 Batch:7 Loss:0.01812\n",
      "Epoch:60 Batch:7 Loss:0.01373\n",
      "Epoch:80 Batch:7 Loss:0.01110\n",
      "Epoch:100 Batch:7 Loss:0.01180\n",
      "Epoch:120 Batch:7 Loss:0.01058\n",
      "Epoch:140 Batch:7 Loss:0.00951\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.073\n",
      "Epoch:10 Batch:10 Loss:0.071\n",
      "Epoch:20 Batch:10 Loss:0.069\n",
      "Epoch:30 Batch:10 Loss:0.071\n",
      "Epoch:40 Batch:10 Loss:0.068\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 246.4358737424421 setps: 533 count: 533\n",
      "reward: 54.078425553938736 setps: 70 count: 603\n",
      "reward: 188.67795048138495 setps: 312 count: 915\n",
      "reward: 16.98996269871422 setps: 66 count: 981\n",
      "avg rewards: 126.54555311912002\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.40323\n",
      "Epoch:20 Batch:8 Loss:0.02508\n",
      "Epoch:40 Batch:8 Loss:0.01767\n",
      "Epoch:60 Batch:8 Loss:0.01248\n",
      "Epoch:80 Batch:8 Loss:0.01160\n",
      "Epoch:100 Batch:8 Loss:0.01074\n",
      "Epoch:120 Batch:8 Loss:0.00936\n",
      "Epoch:140 Batch:8 Loss:0.00920\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.080\n",
      "Epoch:10 Batch:10 Loss:0.078\n",
      "Epoch:20 Batch:10 Loss:0.079\n",
      "Epoch:30 Batch:10 Loss:0.077\n",
      "Epoch:40 Batch:10 Loss:0.077\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 23.5119983254408 setps: 61 count: 61\n",
      "reward: 20.701842117792697 setps: 61 count: 122\n",
      "reward: 36.16548364497137 setps: 87 count: 209\n",
      "reward: 49.78312399107964 setps: 111 count: 320\n",
      "avg rewards: 32.540612019821126\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.27850\n",
      "Epoch:20 Batch:9 Loss:0.02268\n",
      "Epoch:40 Batch:9 Loss:0.01372\n",
      "Epoch:60 Batch:9 Loss:0.01155\n",
      "Epoch:80 Batch:9 Loss:0.00997\n",
      "Epoch:100 Batch:9 Loss:0.00945\n",
      "Epoch:120 Batch:9 Loss:0.00924\n",
      "Epoch:140 Batch:9 Loss:0.00856\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.077\n",
      "Epoch:10 Batch:10 Loss:0.075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 Batch:10 Loss:0.076\n",
      "Epoch:30 Batch:10 Loss:0.075\n",
      "Epoch:40 Batch:10 Loss:0.075\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 57.457991392933764 setps: 73 count: 73\n",
      "reward: 100.36591604285059 setps: 131 count: 204\n",
      "reward: 36.161728888659844 setps: 69 count: 273\n",
      "reward: 41.27151691388134 setps: 47 count: 320\n",
      "reward: 106.00181187886632 setps: 123 count: 443\n",
      "reward: 35.896222610691616 setps: 74 count: 517\n",
      "reward: 39.84094329529326 setps: 47 count: 564\n",
      "reward: 71.53453461282044 setps: 89 count: 653\n",
      "reward: 46.026991726899 setps: 48 count: 701\n",
      "reward: 38.481296117564504 setps: 71 count: 772\n",
      "reward: 100.25753630020044 setps: 125 count: 897\n",
      "reward: 76.31049384304931 setps: 91 count: 988\n",
      "avg rewards: 62.467248635309204\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.24905\n",
      "Epoch:20 Batch:10 Loss:0.01917\n",
      "Epoch:40 Batch:10 Loss:0.01291\n",
      "Epoch:60 Batch:10 Loss:0.01131\n",
      "Epoch:80 Batch:10 Loss:0.01036\n",
      "Epoch:100 Batch:10 Loss:0.00910\n",
      "Epoch:120 Batch:10 Loss:0.00884\n",
      "Epoch:140 Batch:10 Loss:0.00788\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.073\n",
      "Epoch:10 Batch:10 Loss:0.074\n",
      "Epoch:20 Batch:10 Loss:0.073\n",
      "Epoch:30 Batch:10 Loss:0.073\n",
      "Epoch:40 Batch:10 Loss:0.072\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 23.11775108999136 setps: 18 count: 18\n",
      "reward: 23.060779613252087 setps: 15 count: 33\n",
      "reward: 22.409937151253686 setps: 16 count: 49\n",
      "reward: 22.04882986182347 setps: 16 count: 65\n",
      "reward: 20.463667740621894 setps: 15 count: 80\n",
      "reward: 21.86984428720752 setps: 17 count: 97\n",
      "reward: 22.642572498240042 setps: 16 count: 113\n",
      "reward: 21.091537096287357 setps: 15 count: 128\n",
      "reward: 21.268761742998322 setps: 14 count: 142\n",
      "reward: 22.92099445919593 setps: 17 count: 159\n",
      "reward: 23.48975656122202 setps: 15 count: 174\n",
      "reward: 22.207052192911213 setps: 16 count: 190\n",
      "reward: 21.105315560026796 setps: 14 count: 204\n",
      "reward: 22.164416080081715 setps: 15 count: 219\n",
      "reward: 19.920424518623623 setps: 14 count: 233\n",
      "reward: 22.11237010374316 setps: 14 count: 247\n",
      "reward: 20.380809494828277 setps: 15 count: 262\n",
      "reward: 20.214960318741213 setps: 14 count: 276\n",
      "reward: 22.416427722451044 setps: 16 count: 292\n",
      "reward: 23.49415140711935 setps: 16 count: 308\n",
      "reward: 23.380412629627973 setps: 16 count: 324\n",
      "reward: 18.544494962587486 setps: 14 count: 338\n",
      "reward: 23.390817471804624 setps: 16 count: 354\n",
      "reward: 19.867070940791745 setps: 14 count: 368\n",
      "reward: 22.762501003500073 setps: 15 count: 383\n",
      "reward: 21.759546517678245 setps: 15 count: 398\n",
      "reward: 21.654617908537332 setps: 16 count: 414\n",
      "reward: 23.084377841766397 setps: 15 count: 429\n",
      "reward: 23.104181590394003 setps: 17 count: 446\n",
      "reward: 23.991423978934474 setps: 17 count: 463\n",
      "reward: 23.655544302151245 setps: 16 count: 479\n",
      "reward: 20.508907600039674 setps: 14 count: 493\n",
      "reward: 23.076174046490635 setps: 15 count: 508\n",
      "reward: 20.307272763988294 setps: 16 count: 524\n",
      "reward: 22.788515436164744 setps: 16 count: 540\n",
      "reward: 22.082688026454704 setps: 15 count: 555\n",
      "reward: 20.112786521024827 setps: 15 count: 570\n",
      "reward: 22.432026565747215 setps: 16 count: 586\n",
      "reward: 23.90603692957229 setps: 16 count: 602\n",
      "reward: 21.01102563209279 setps: 15 count: 617\n",
      "reward: 20.28582064511866 setps: 16 count: 633\n",
      "reward: 22.417086699082578 setps: 15 count: 648\n",
      "reward: 23.071630420003203 setps: 16 count: 664\n",
      "reward: 23.222901145464856 setps: 16 count: 680\n",
      "reward: 19.550634705754057 setps: 14 count: 694\n",
      "reward: 23.414454041105635 setps: 18 count: 712\n",
      "reward: 24.65418918929208 setps: 18 count: 730\n",
      "reward: 20.522984841543074 setps: 15 count: 745\n",
      "reward: 21.97482595907204 setps: 14 count: 759\n",
      "reward: 20.97259941281373 setps: 16 count: 775\n",
      "reward: 22.83312239261431 setps: 16 count: 791\n",
      "reward: 19.621462773870736 setps: 15 count: 806\n",
      "reward: 23.93650354412239 setps: 16 count: 822\n",
      "reward: 22.674070130001926 setps: 15 count: 837\n",
      "reward: 20.340289391652917 setps: 15 count: 852\n",
      "reward: 21.687968637421733 setps: 15 count: 867\n",
      "reward: 22.77974150212976 setps: 16 count: 883\n",
      "reward: 20.741914765100226 setps: 17 count: 900\n",
      "reward: 23.56485659175814 setps: 16 count: 916\n",
      "reward: 20.57244310391979 setps: 15 count: 931\n",
      "reward: 21.593526936820126 setps: 16 count: 947\n",
      "reward: 21.486301091747013 setps: 16 count: 963\n",
      "reward: 22.218365124287086 setps: 16 count: 979\n",
      "reward: 21.864952287050258 setps: 14 count: 993\n",
      "avg rewards: 21.9659910547143\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.31734\n",
      "Epoch:20 Batch:11 Loss:0.01959\n",
      "Epoch:40 Batch:11 Loss:0.01268\n",
      "Epoch:60 Batch:11 Loss:0.01215\n",
      "Epoch:80 Batch:11 Loss:0.01042\n",
      "Epoch:100 Batch:11 Loss:0.00944\n",
      "Epoch:120 Batch:11 Loss:0.01013\n",
      "Epoch:140 Batch:11 Loss:0.01005\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.070\n",
      "Epoch:10 Batch:10 Loss:0.071\n",
      "Epoch:20 Batch:10 Loss:0.069\n",
      "Epoch:30 Batch:10 Loss:0.068\n",
      "Epoch:40 Batch:10 Loss:0.069\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 39.51118838794646 setps: 29 count: 29\n",
      "reward: 36.79987905493472 setps: 37 count: 66\n",
      "reward: 44.5144311400436 setps: 32 count: 98\n",
      "reward: 44.584980605077 setps: 34 count: 132\n",
      "reward: 25.899306825529493 setps: 24 count: 156\n",
      "reward: 32.198137655190656 setps: 28 count: 184\n",
      "reward: 26.87211197575961 setps: 26 count: 210\n",
      "reward: 40.36725791206846 setps: 42 count: 252\n",
      "reward: 46.9142924277825 setps: 36 count: 288\n",
      "reward: 31.65240015708987 setps: 28 count: 316\n",
      "reward: 27.245293739726183 setps: 33 count: 349\n",
      "reward: 43.53744217599452 setps: 33 count: 382\n",
      "reward: 31.908208726698643 setps: 29 count: 411\n",
      "reward: 29.120346351343322 setps: 25 count: 436\n",
      "reward: 28.948435435637663 setps: 36 count: 472\n",
      "reward: 29.475194797106084 setps: 34 count: 506\n",
      "reward: 38.57255719454552 setps: 32 count: 538\n",
      "reward: 36.46313135876699 setps: 37 count: 575\n",
      "reward: 30.123092793476825 setps: 23 count: 598\n",
      "reward: 47.030976000387454 setps: 32 count: 630\n",
      "reward: 25.670684788336807 setps: 27 count: 657\n",
      "reward: 42.44719701620487 setps: 35 count: 692\n",
      "reward: 31.66495933194674 setps: 32 count: 724\n",
      "reward: 27.70462050176138 setps: 32 count: 756\n",
      "reward: 29.858202115338642 setps: 29 count: 785\n",
      "reward: 44.51249701811467 setps: 32 count: 817\n",
      "reward: 29.5915118553763 setps: 28 count: 845\n",
      "reward: 33.997537301498234 setps: 25 count: 870\n",
      "reward: 27.74449089966365 setps: 24 count: 894\n",
      "reward: 33.94428131547175 setps: 28 count: 922\n",
      "reward: 29.367700227105527 setps: 24 count: 946\n",
      "reward: 27.85574724879698 setps: 24 count: 970\n",
      "reward: 28.89480351859966 setps: 26 count: 996\n",
      "avg rewards: 34.09069387434305\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.19247\n",
      "Epoch:20 Batch:12 Loss:0.01895\n",
      "Epoch:40 Batch:12 Loss:0.01321\n",
      "Epoch:60 Batch:12 Loss:0.01032\n",
      "Epoch:80 Batch:12 Loss:0.01023\n",
      "Epoch:100 Batch:12 Loss:0.00923\n",
      "Epoch:120 Batch:12 Loss:0.00845\n",
      "Epoch:140 Batch:12 Loss:0.00895\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.069\n",
      "Epoch:10 Batch:10 Loss:0.067\n",
      "Epoch:20 Batch:10 Loss:0.065\n",
      "Epoch:30 Batch:10 Loss:0.066\n",
      "Epoch:40 Batch:10 Loss:0.066\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 27.130936872231537 setps: 52 count: 52\n",
      "reward: 78.05333175420529 setps: 95 count: 147\n",
      "reward: 60.72066270727373 setps: 70 count: 217\n",
      "reward: 48.58579359419672 setps: 112 count: 329\n",
      "reward: 59.946065608729256 setps: 73 count: 402\n",
      "reward: 43.53215029449202 setps: 86 count: 488\n",
      "reward: 30.84957961079781 setps: 66 count: 554\n",
      "reward: 58.63125359502448 setps: 125 count: 679\n",
      "reward: 32.75067685593095 setps: 70 count: 749\n",
      "reward: 61.54555159703015 setps: 119 count: 868\n",
      "reward: 70.7775136343742 setps: 93 count: 961\n",
      "avg rewards: 52.04759237493511\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.13308\n",
      "Epoch:20 Batch:13 Loss:0.01747\n",
      "Epoch:40 Batch:13 Loss:0.01390\n",
      "Epoch:60 Batch:13 Loss:0.01052\n",
      "Epoch:80 Batch:13 Loss:0.00941\n",
      "Epoch:100 Batch:13 Loss:0.00972\n",
      "Epoch:120 Batch:13 Loss:0.00976\n",
      "Epoch:140 Batch:13 Loss:0.00879\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.068\n",
      "Epoch:10 Batch:10 Loss:0.065\n",
      "Epoch:20 Batch:10 Loss:0.066\n",
      "Epoch:30 Batch:10 Loss:0.066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:40 Batch:10 Loss:0.067\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 26.448595700884482 setps: 51 count: 51\n",
      "reward: 60.693763136345666 setps: 120 count: 171\n",
      "reward: 18.248539722221913 setps: 59 count: 230\n",
      "reward: 47.808321948982474 setps: 98 count: 328\n",
      "avg rewards: 38.299805127108634\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.11689\n",
      "Epoch:20 Batch:14 Loss:0.01825\n",
      "Epoch:40 Batch:14 Loss:0.01156\n",
      "Epoch:60 Batch:14 Loss:0.00973\n",
      "Epoch:80 Batch:14 Loss:0.00846\n",
      "Epoch:100 Batch:14 Loss:0.00922\n",
      "Epoch:120 Batch:14 Loss:0.00748\n",
      "Epoch:140 Batch:14 Loss:0.00719\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.060\n",
      "Epoch:10 Batch:10 Loss:0.059\n",
      "Epoch:20 Batch:10 Loss:0.058\n",
      "Epoch:30 Batch:10 Loss:0.061\n",
      "Epoch:40 Batch:10 Loss:0.059\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 93.66704142390078 setps: 87 count: 87\n",
      "reward: 25.962711706088154 setps: 57 count: 144\n",
      "reward: 71.00188464434467 setps: 55 count: 199\n",
      "reward: 31.98746415156346 setps: 62 count: 261\n",
      "reward: 33.93600250201414 setps: 60 count: 321\n",
      "reward: 28.044952601783734 setps: 44 count: 365\n",
      "reward: 31.96511852499099 setps: 71 count: 436\n",
      "reward: 73.09004135401626 setps: 54 count: 490\n",
      "reward: 25.03286069988826 setps: 40 count: 530\n",
      "reward: 78.68530251017512 setps: 62 count: 592\n",
      "reward: 26.02354765198688 setps: 37 count: 629\n",
      "reward: 85.46070727443875 setps: 79 count: 708\n",
      "reward: 28.281765349228237 setps: 42 count: 750\n",
      "reward: 43.5715127849602 setps: 75 count: 825\n",
      "reward: 118.86947086224825 setps: 113 count: 938\n",
      "avg rewards: 53.03869226944186\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.12561\n",
      "Epoch:20 Batch:15 Loss:0.01647\n",
      "Epoch:40 Batch:15 Loss:0.01022\n",
      "Epoch:60 Batch:15 Loss:0.00860\n",
      "Epoch:80 Batch:15 Loss:0.00948\n",
      "Epoch:100 Batch:15 Loss:0.00818\n",
      "Epoch:120 Batch:15 Loss:0.00809\n",
      "Epoch:140 Batch:15 Loss:0.00692\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.058\n",
      "Epoch:10 Batch:10 Loss:0.058\n",
      "Epoch:20 Batch:10 Loss:0.057\n",
      "Epoch:30 Batch:10 Loss:0.058\n",
      "Epoch:40 Batch:10 Loss:0.058\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 25.929440229007746 setps: 21 count: 21\n",
      "reward: 22.408557200891664 setps: 20 count: 41\n",
      "reward: 23.54408563331381 setps: 17 count: 58\n",
      "reward: 24.275073487625924 setps: 20 count: 78\n",
      "reward: 25.157981369207842 setps: 18 count: 96\n",
      "reward: 24.338590818304514 setps: 19 count: 115\n",
      "reward: 21.950897323721435 setps: 17 count: 132\n",
      "reward: 20.87239463116421 setps: 19 count: 151\n",
      "reward: 26.21025280478789 setps: 18 count: 169\n",
      "reward: 22.247872941268724 setps: 15 count: 184\n",
      "reward: 25.055950484071214 setps: 21 count: 205\n",
      "reward: 24.518918645977102 setps: 22 count: 227\n",
      "reward: 23.614677177452542 setps: 17 count: 244\n",
      "reward: 25.192476719284606 setps: 19 count: 263\n",
      "reward: 21.471507991550606 setps: 21 count: 284\n",
      "reward: 26.095548631811106 setps: 21 count: 305\n",
      "reward: 21.279065955054833 setps: 18 count: 323\n",
      "reward: 25.96733282559871 setps: 20 count: 343\n",
      "reward: 22.826751524092106 setps: 18 count: 361\n",
      "reward: 23.87796566564793 setps: 17 count: 378\n",
      "reward: 25.252462028316224 setps: 19 count: 397\n",
      "reward: 23.64086327499681 setps: 18 count: 415\n",
      "reward: 20.822771058502255 setps: 18 count: 433\n",
      "reward: 24.405854846599688 setps: 19 count: 452\n",
      "reward: 23.974452764177112 setps: 19 count: 471\n",
      "reward: 26.06339260603708 setps: 19 count: 490\n",
      "reward: 24.326863248756855 setps: 23 count: 513\n",
      "reward: 22.8201557737033 setps: 18 count: 531\n",
      "reward: 25.397993321847746 setps: 19 count: 550\n",
      "reward: 26.30136626789754 setps: 20 count: 570\n",
      "reward: 26.213625934235456 setps: 19 count: 589\n",
      "reward: 25.880623397098677 setps: 20 count: 609\n",
      "reward: 25.37910943742463 setps: 20 count: 629\n",
      "reward: 24.08153313863731 setps: 18 count: 647\n",
      "reward: 23.240862864418883 setps: 16 count: 663\n",
      "reward: 24.5139294372857 setps: 19 count: 682\n",
      "reward: 25.556183707450693 setps: 20 count: 702\n",
      "reward: 24.45694445345289 setps: 20 count: 722\n",
      "reward: 27.499645396736735 setps: 21 count: 743\n",
      "reward: 26.556136991482347 setps: 20 count: 763\n",
      "reward: 22.394540372556364 setps: 16 count: 779\n",
      "reward: 24.040112960626722 setps: 20 count: 799\n",
      "reward: 25.741600740626744 setps: 21 count: 820\n",
      "reward: 26.53678998578107 setps: 20 count: 840\n",
      "reward: 23.215942854643796 setps: 20 count: 860\n",
      "reward: 26.632629106410604 setps: 20 count: 880\n",
      "reward: 24.298561484312817 setps: 20 count: 900\n",
      "reward: 25.989398108099703 setps: 21 count: 921\n",
      "reward: 24.34422530802258 setps: 19 count: 940\n",
      "reward: 23.042519960226485 setps: 18 count: 958\n",
      "reward: 24.01712147585931 setps: 19 count: 977\n",
      "reward: 24.131409748684383 setps: 17 count: 994\n",
      "avg rewards: 24.377018540668132\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.10158\n",
      "Epoch:20 Batch:16 Loss:0.01585\n",
      "Epoch:40 Batch:16 Loss:0.01153\n",
      "Epoch:60 Batch:16 Loss:0.00917\n",
      "Epoch:80 Batch:16 Loss:0.00849\n",
      "Epoch:100 Batch:16 Loss:0.00768\n",
      "Epoch:120 Batch:16 Loss:0.00831\n",
      "Epoch:140 Batch:16 Loss:0.00771\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.062\n",
      "Epoch:10 Batch:10 Loss:0.058\n",
      "Epoch:20 Batch:10 Loss:0.057\n",
      "Epoch:30 Batch:10 Loss:0.056\n",
      "Epoch:40 Batch:10 Loss:0.057\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 42.64007153757266 setps: 32 count: 32\n",
      "reward: 32.66673497512965 setps: 30 count: 62\n",
      "reward: 50.433609022312154 setps: 39 count: 101\n",
      "reward: 36.389862296612414 setps: 32 count: 133\n",
      "reward: 39.58233600380918 setps: 31 count: 164\n",
      "reward: 35.39510803604354 setps: 33 count: 197\n",
      "reward: 32.10362225041608 setps: 28 count: 225\n",
      "reward: 69.42578736449794 setps: 56 count: 281\n",
      "reward: 33.60442122610693 setps: 30 count: 311\n",
      "reward: 31.55782050374401 setps: 31 count: 342\n",
      "reward: 35.64959615012049 setps: 26 count: 368\n",
      "reward: 36.214184666784426 setps: 31 count: 399\n",
      "reward: 30.454071362724054 setps: 26 count: 425\n",
      "reward: 31.878090161678855 setps: 28 count: 453\n",
      "reward: 49.32784429526508 setps: 43 count: 496\n",
      "reward: 31.790125635560255 setps: 30 count: 526\n",
      "reward: 45.19048755535186 setps: 38 count: 564\n",
      "reward: 34.65118267538345 setps: 28 count: 592\n",
      "reward: 37.36238703838754 setps: 32 count: 624\n",
      "reward: 43.43633034749946 setps: 33 count: 657\n",
      "reward: 37.755537738636484 setps: 29 count: 686\n",
      "reward: 57.27792525205733 setps: 40 count: 726\n",
      "reward: 38.65738185020455 setps: 36 count: 762\n",
      "reward: 88.54778177033879 setps: 63 count: 825\n",
      "reward: 30.653062829143888 setps: 26 count: 851\n",
      "reward: 34.331761753586775 setps: 28 count: 879\n",
      "reward: 31.577166521806795 setps: 27 count: 906\n",
      "reward: 50.04183950476436 setps: 43 count: 949\n",
      "reward: 39.47647138275061 setps: 30 count: 979\n",
      "avg rewards: 40.9680207485617\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.10678\n",
      "Epoch:20 Batch:17 Loss:0.01711\n",
      "Epoch:40 Batch:17 Loss:0.01318\n",
      "Epoch:60 Batch:17 Loss:0.01033\n",
      "Epoch:80 Batch:17 Loss:0.00919\n",
      "Epoch:100 Batch:17 Loss:0.00801\n",
      "Epoch:120 Batch:17 Loss:0.00863\n",
      "Epoch:140 Batch:17 Loss:0.00762\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.056\n",
      "Epoch:10 Batch:10 Loss:0.056\n",
      "Epoch:20 Batch:10 Loss:0.055\n",
      "Epoch:30 Batch:10 Loss:0.054\n",
      "Epoch:40 Batch:10 Loss:0.054\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 38.235903893075005 setps: 62 count: 62\n",
      "reward: 58.18060498377745 setps: 55 count: 117\n",
      "reward: 54.927408634762074 setps: 100 count: 217\n",
      "reward: 107.61995543441881 setps: 106 count: 323\n",
      "reward: 132.9572821583642 setps: 140 count: 463\n",
      "reward: 54.970659291873744 setps: 115 count: 578\n",
      "reward: 86.21777700457315 setps: 82 count: 660\n",
      "reward: 59.662615709616496 setps: 72 count: 732\n",
      "reward: 45.06144068038849 setps: 83 count: 815\n",
      "reward: 37.44792954938604 setps: 52 count: 867\n",
      "reward: 63.566183962801034 setps: 60 count: 927\n",
      "reward: 44.21770294937159 setps: 36 count: 963\n",
      "avg rewards: 65.25545535436734\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.09404\n",
      "Epoch:20 Batch:18 Loss:0.01689\n",
      "Epoch:40 Batch:18 Loss:0.01267\n",
      "Epoch:60 Batch:18 Loss:0.01067\n",
      "Epoch:80 Batch:18 Loss:0.00893\n",
      "Epoch:100 Batch:18 Loss:0.00968\n",
      "Epoch:120 Batch:18 Loss:0.00876\n",
      "Epoch:140 Batch:18 Loss:0.00772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.059\n",
      "Epoch:10 Batch:10 Loss:0.059\n",
      "Epoch:20 Batch:10 Loss:0.057\n",
      "Epoch:30 Batch:10 Loss:0.060\n",
      "Epoch:40 Batch:10 Loss:0.057\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 34.58666786123213 setps: 59 count: 59\n",
      "reward: 25.12640824717236 setps: 60 count: 119\n",
      "reward: 33.46565829517495 setps: 70 count: 189\n",
      "reward: 74.42547690569047 setps: 72 count: 261\n",
      "reward: 37.51297584572458 setps: 42 count: 303\n",
      "reward: 72.51366000601996 setps: 119 count: 422\n",
      "reward: 32.36623209195676 setps: 64 count: 486\n",
      "reward: 100.0135520249387 setps: 90 count: 576\n",
      "reward: 53.76369376011572 setps: 47 count: 623\n",
      "reward: 55.78033983823844 setps: 44 count: 667\n",
      "reward: 102.28463827637755 setps: 97 count: 764\n",
      "reward: 32.991164303680144 setps: 73 count: 837\n",
      "reward: 24.867426436497766 setps: 62 count: 899\n",
      "reward: 37.24852677726157 setps: 76 count: 975\n",
      "avg rewards: 51.21045861929151\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.08660\n",
      "Epoch:20 Batch:19 Loss:0.01486\n",
      "Epoch:40 Batch:19 Loss:0.01288\n",
      "Epoch:60 Batch:19 Loss:0.01143\n",
      "Epoch:80 Batch:19 Loss:0.00999\n",
      "Epoch:100 Batch:19 Loss:0.00921\n",
      "Epoch:120 Batch:19 Loss:0.00749\n",
      "Epoch:140 Batch:19 Loss:0.00793\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.059\n",
      "Epoch:10 Batch:10 Loss:0.056\n",
      "Epoch:20 Batch:10 Loss:0.056\n",
      "Epoch:30 Batch:10 Loss:0.055\n",
      "Epoch:40 Batch:10 Loss:0.055\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 329.3295966076653 setps: 800 count: 800\n",
      "avg rewards: 329.3295966076653\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.08552\n",
      "Epoch:20 Batch:20 Loss:0.01471\n",
      "Epoch:40 Batch:20 Loss:0.01187\n",
      "Epoch:60 Batch:20 Loss:0.00924\n",
      "Epoch:80 Batch:20 Loss:0.00988\n",
      "Epoch:100 Batch:20 Loss:0.00816\n",
      "Epoch:120 Batch:20 Loss:0.00782\n",
      "Epoch:140 Batch:20 Loss:0.00760\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.052\n",
      "Epoch:10 Batch:10 Loss:0.051\n",
      "Epoch:20 Batch:10 Loss:0.051\n",
      "Epoch:30 Batch:10 Loss:0.051\n",
      "Epoch:40 Batch:10 Loss:0.050\n",
      "Done!\n",
      "############# start HopperBulletEnv-v0 training ###################\n",
      "(49,)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 23.882970571222536 setps: 16 count: 16\n",
      "reward: 25.77333283731713 setps: 20 count: 36\n",
      "reward: 19.492831864685282 setps: 7 count: 43\n",
      "reward: 18.192747276357842 setps: 13 count: 56\n",
      "reward: 21.15314723471965 setps: 13 count: 69\n",
      "reward: 19.518454981704416 setps: 7 count: 76\n",
      "reward: 19.379697091480192 setps: 12 count: 88\n",
      "reward: 22.289910380149376 setps: 15 count: 103\n",
      "reward: 17.559274790919154 setps: 12 count: 115\n",
      "reward: 15.310067922304734 setps: 6 count: 121\n",
      "reward: 22.853220536868317 setps: 13 count: 134\n",
      "reward: 16.960894888463375 setps: 6 count: 140\n",
      "reward: 21.985216878214853 setps: 10 count: 150\n",
      "reward: 17.521467491572547 setps: 14 count: 164\n",
      "reward: 20.93758308546676 setps: 14 count: 178\n",
      "reward: 19.983398391485387 setps: 12 count: 190\n",
      "reward: 24.77373765563534 setps: 12 count: 202\n",
      "reward: 17.361751148557232 setps: 6 count: 208\n",
      "reward: 18.251198862388264 setps: 7 count: 215\n",
      "reward: 17.153876800343276 setps: 11 count: 226\n",
      "reward: 20.050667008553866 setps: 17 count: 243\n",
      "reward: 16.894707907368137 setps: 28 count: 271\n",
      "reward: 23.927094945563294 setps: 20 count: 291\n",
      "reward: 22.17488849833171 setps: 19 count: 310\n",
      "reward: 23.476012997746878 setps: 16 count: 326\n",
      "reward: 20.016304672979462 setps: 6 count: 332\n",
      "reward: 21.02450613266701 setps: 8 count: 340\n",
      "reward: 23.183337195735657 setps: 17 count: 357\n",
      "reward: 15.96957965316542 setps: 10 count: 367\n",
      "reward: 26.05255952417938 setps: 21 count: 388\n",
      "reward: 25.67388937028591 setps: 17 count: 405\n",
      "reward: 22.823568344558588 setps: 18 count: 423\n",
      "reward: 20.473823551423262 setps: 16 count: 439\n",
      "reward: 21.21325226861955 setps: 15 count: 454\n",
      "reward: 25.912644732126502 setps: 17 count: 471\n",
      "reward: 20.70656607906276 setps: 9 count: 480\n",
      "reward: 20.141161935057603 setps: 10 count: 490\n",
      "reward: 18.182721619021322 setps: 12 count: 502\n",
      "reward: 24.605956768852774 setps: 22 count: 524\n",
      "reward: 19.106595502038545 setps: 8 count: 532\n",
      "reward: 18.74057168627187 setps: 10 count: 542\n",
      "reward: 19.184327946242412 setps: 9 count: 551\n",
      "reward: 17.925327985519836 setps: 8 count: 559\n",
      "reward: 23.510943467456674 setps: 16 count: 575\n",
      "reward: 19.868844129686476 setps: 18 count: 593\n",
      "reward: 21.382361138664418 setps: 9 count: 602\n",
      "reward: 18.283211395413673 setps: 10 count: 612\n",
      "reward: 22.052670708434015 setps: 13 count: 625\n",
      "reward: 20.38328785512131 setps: 8 count: 633\n",
      "reward: 17.687608318503774 setps: 13 count: 646\n",
      "reward: 18.69347983791522 setps: 9 count: 655\n",
      "reward: 16.30461863457167 setps: 20 count: 675\n",
      "reward: 18.11971466857067 setps: 8 count: 683\n",
      "reward: 23.164248626033075 setps: 12 count: 695\n",
      "reward: 16.5037636300287 setps: 9 count: 704\n",
      "reward: 28.06955174484901 setps: 18 count: 722\n",
      "reward: 22.20975439484464 setps: 10 count: 732\n",
      "reward: 18.364814836285948 setps: 13 count: 745\n",
      "reward: 20.037957856152204 setps: 9 count: 754\n",
      "reward: 15.416177902868366 setps: 10 count: 764\n",
      "reward: 17.56774065005011 setps: 8 count: 772\n",
      "reward: 28.234505356529553 setps: 19 count: 791\n",
      "reward: 19.503324217768387 setps: 12 count: 803\n",
      "reward: 21.13053579438565 setps: 7 count: 810\n",
      "reward: 22.01644679704623 setps: 13 count: 823\n",
      "reward: 16.852793517665123 setps: 6 count: 829\n",
      "reward: 22.979768488180706 setps: 9 count: 838\n",
      "reward: 23.113229132644484 setps: 16 count: 854\n",
      "reward: 26.738378327057585 setps: 31 count: 885\n",
      "reward: 20.912390428026264 setps: 15 count: 900\n",
      "reward: 22.58735235220374 setps: 15 count: 915\n",
      "reward: 22.480922398895196 setps: 18 count: 933\n",
      "reward: 19.13930281615176 setps: 11 count: 944\n",
      "reward: 20.446870451090216 setps: 8 count: 952\n",
      "reward: 19.797664687117503 setps: 9 count: 961\n",
      "reward: 21.05995721584768 setps: 12 count: 973\n",
      "reward: 20.26123328831018 setps: 12 count: 985\n",
      "reward: 19.535015288081198 setps: 10 count: 995\n",
      "avg rewards: 20.669298555995884\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:1.41390\n",
      "Epoch:20 Batch:1 Loss:0.15052\n",
      "Epoch:40 Batch:1 Loss:0.09637\n",
      "Epoch:60 Batch:1 Loss:0.07147\n",
      "Epoch:80 Batch:1 Loss:0.06305\n",
      "Epoch:100 Batch:1 Loss:0.05667\n",
      "Epoch:120 Batch:1 Loss:0.05158\n",
      "Epoch:140 Batch:1 Loss:0.04996\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.131\n",
      "Epoch:10 Batch:10 Loss:0.105\n",
      "Epoch:20 Batch:10 Loss:0.111\n",
      "Epoch:30 Batch:10 Loss:0.109\n",
      "Epoch:40 Batch:10 Loss:0.105\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -3.8119716021858054 setps: 29 count: 29\n",
      "reward: 7.231003818605675 setps: 30 count: 59\n",
      "reward: 5.314750496625855 setps: 31 count: 90\n",
      "reward: 0.5240342962221343 setps: 30 count: 120\n",
      "reward: 1.4459851306586642 setps: 28 count: 148\n",
      "reward: -4.172536011268678 setps: 28 count: 176\n",
      "reward: 1.5608910433453265 setps: 29 count: 205\n",
      "reward: -0.9734751846757732 setps: 28 count: 233\n",
      "reward: -4.302588689616824 setps: 29 count: 262\n",
      "reward: 0.6249199028243297 setps: 28 count: 290\n",
      "reward: -1.0588549920750676 setps: 29 count: 319\n",
      "reward: 0.998227407371449 setps: 29 count: 348\n",
      "reward: -0.27335003189946394 setps: 28 count: 376\n",
      "reward: -3.8703614720201607 setps: 28 count: 404\n",
      "reward: 1.1010898999720655 setps: 29 count: 433\n",
      "reward: 6.923147005327337 setps: 31 count: 464\n",
      "reward: 5.043220828387711 setps: 32 count: 496\n",
      "reward: 0.21580228069069474 setps: 29 count: 525\n",
      "reward: -1.4531064480426847 setps: 29 count: 554\n",
      "reward: -2.535842604648497 setps: 29 count: 583\n",
      "reward: -1.4766710543059174 setps: 29 count: 612\n",
      "reward: 2.094220377815872 setps: 29 count: 641\n",
      "reward: 1.3156793470156112 setps: 28 count: 669\n",
      "reward: 2.6214481376766328 setps: 30 count: 699\n",
      "reward: -0.23861912995635137 setps: 28 count: 727\n",
      "reward: -6.756894529669079 setps: 28 count: 755\n",
      "reward: -3.5240903188794634 setps: 27 count: 782\n",
      "reward: -1.0864895724149402 setps: 30 count: 812\n",
      "reward: 1.2165491715728418 setps: 27 count: 839\n",
      "reward: 0.6590076443411799 setps: 29 count: 868\n",
      "reward: 1.4059600755463166 setps: 30 count: 898\n",
      "reward: -2.3080440084289884 setps: 27 count: 925\n",
      "reward: 0.0672826834845166 setps: 29 count: 954\n",
      "reward: -0.10796106679990891 setps: 29 count: 983\n",
      "avg rewards: 0.0709518479587238\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:2 Loss:1.26297\n",
      "Epoch:20 Batch:2 Loss:0.09235\n",
      "Epoch:40 Batch:2 Loss:0.05775\n",
      "Epoch:60 Batch:2 Loss:0.04345\n",
      "Epoch:80 Batch:2 Loss:0.03700\n",
      "Epoch:100 Batch:2 Loss:0.03327\n",
      "Epoch:120 Batch:2 Loss:0.03327\n",
      "Epoch:140 Batch:2 Loss:0.03016\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.092\n",
      "Epoch:10 Batch:10 Loss:0.085\n",
      "Epoch:20 Batch:10 Loss:0.099\n",
      "Epoch:30 Batch:10 Loss:0.086\n",
      "Epoch:40 Batch:10 Loss:0.086\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 17.45484940786846 setps: 33 count: 33\n",
      "reward: 27.71320727897837 setps: 39 count: 72\n",
      "reward: 29.241820317847306 setps: 37 count: 109\n",
      "reward: 11.338796209171413 setps: 32 count: 141\n",
      "reward: 20.311644499418612 setps: 34 count: 175\n",
      "reward: 18.051317115883286 setps: 32 count: 207\n",
      "reward: 18.50246346358617 setps: 33 count: 240\n",
      "reward: 15.039421604653763 setps: 32 count: 272\n",
      "reward: 21.15309795200738 setps: 37 count: 309\n",
      "reward: 28.960939847770717 setps: 37 count: 346\n",
      "reward: 23.500971449572535 setps: 34 count: 380\n",
      "reward: 25.58696986593277 setps: 37 count: 417\n",
      "reward: 17.92764444382337 setps: 33 count: 450\n",
      "reward: 14.651146518367748 setps: 34 count: 484\n",
      "reward: 18.999922477232758 setps: 35 count: 519\n",
      "reward: 24.345553288298834 setps: 34 count: 553\n",
      "reward: 29.32503506233043 setps: 38 count: 591\n",
      "reward: 27.990642343327636 setps: 36 count: 627\n",
      "reward: 22.477392894329387 setps: 34 count: 661\n",
      "reward: 16.75266200244659 setps: 33 count: 694\n",
      "reward: 21.274423039767132 setps: 34 count: 728\n",
      "reward: 15.270333752617443 setps: 32 count: 760\n",
      "reward: 25.943952474504478 setps: 35 count: 795\n",
      "reward: 22.0998840445769 setps: 34 count: 829\n",
      "reward: 20.759094819298483 setps: 32 count: 861\n",
      "reward: 13.304603032063461 setps: 31 count: 892\n",
      "reward: 25.54643141316192 setps: 36 count: 928\n",
      "reward: 22.87191274784127 setps: 38 count: 966\n",
      "reward: 21.748662215660445 setps: 34 count: 1000\n",
      "avg rewards: 21.315337778701355\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:1.00152\n",
      "Epoch:20 Batch:3 Loss:0.06473\n",
      "Epoch:40 Batch:3 Loss:0.04002\n",
      "Epoch:60 Batch:3 Loss:0.03139\n",
      "Epoch:80 Batch:3 Loss:0.02902\n",
      "Epoch:100 Batch:3 Loss:0.02523\n",
      "Epoch:120 Batch:3 Loss:0.02510\n",
      "Epoch:140 Batch:3 Loss:0.02268\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.089\n",
      "Epoch:10 Batch:10 Loss:0.086\n",
      "Epoch:20 Batch:10 Loss:0.083\n",
      "Epoch:30 Batch:10 Loss:0.079\n",
      "Epoch:40 Batch:10 Loss:0.079\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 25.4478652518068 setps: 15 count: 15\n",
      "reward: 24.76655111397267 setps: 14 count: 29\n",
      "reward: 24.91270341975323 setps: 14 count: 43\n",
      "reward: 24.509567357545894 setps: 14 count: 57\n",
      "reward: 25.548758067903687 setps: 15 count: 72\n",
      "reward: 27.014047313955956 setps: 15 count: 87\n",
      "reward: 25.951268500386504 setps: 14 count: 101\n",
      "reward: 29.311708262050526 setps: 16 count: 117\n",
      "reward: 25.370032518768856 setps: 15 count: 132\n",
      "reward: 25.71298312675499 setps: 15 count: 147\n",
      "reward: 24.128796220358346 setps: 14 count: 161\n",
      "reward: 23.847499292423894 setps: 14 count: 175\n",
      "reward: 23.90366422013467 setps: 14 count: 189\n",
      "reward: 27.134768641933626 setps: 17 count: 206\n",
      "reward: 26.762329931411657 setps: 15 count: 221\n",
      "reward: 27.9211040932787 setps: 16 count: 237\n",
      "reward: 25.995388138193814 setps: 15 count: 252\n",
      "reward: 26.77563630476507 setps: 15 count: 267\n",
      "reward: 26.46070714896632 setps: 15 count: 282\n",
      "reward: 23.901015809213277 setps: 14 count: 296\n",
      "reward: 26.632562891210547 setps: 15 count: 311\n",
      "reward: 26.961174601985846 setps: 16 count: 327\n",
      "reward: 26.104817701173314 setps: 15 count: 342\n",
      "reward: 20.175086786855537 setps: 14 count: 356\n",
      "reward: 25.214630967499392 setps: 14 count: 370\n",
      "reward: 25.62175148349488 setps: 14 count: 384\n",
      "reward: 26.947598742262926 setps: 15 count: 399\n",
      "reward: 26.081558659368604 setps: 15 count: 414\n",
      "reward: 23.928130233607956 setps: 14 count: 428\n",
      "reward: 24.983601354538404 setps: 15 count: 443\n",
      "reward: 25.469260296250283 setps: 15 count: 458\n",
      "reward: 26.870443956898818 setps: 15 count: 473\n",
      "reward: 23.67053695527429 setps: 15 count: 488\n",
      "reward: 26.2008860326474 setps: 15 count: 503\n",
      "reward: 26.568454675161043 setps: 15 count: 518\n",
      "reward: 25.80865333321271 setps: 15 count: 533\n",
      "reward: 27.110379083720908 setps: 16 count: 549\n",
      "reward: 25.091898843024683 setps: 14 count: 563\n",
      "reward: 24.280139929825964 setps: 14 count: 577\n",
      "reward: 25.86053224909847 setps: 15 count: 592\n",
      "reward: 25.70672325062333 setps: 15 count: 607\n",
      "reward: 26.744866025054943 setps: 15 count: 622\n",
      "reward: 25.929758696259523 setps: 15 count: 637\n",
      "reward: 26.002660552378803 setps: 15 count: 652\n",
      "reward: 26.4248093845541 setps: 15 count: 667\n",
      "reward: 21.98749550174398 setps: 13 count: 680\n",
      "reward: 26.46722975752345 setps: 15 count: 695\n",
      "reward: 24.718245474241844 setps: 14 count: 709\n",
      "reward: 27.334007758960066 setps: 16 count: 725\n",
      "reward: 23.034891146370498 setps: 14 count: 739\n",
      "reward: 25.82258251078601 setps: 15 count: 754\n",
      "reward: 27.448905029716844 setps: 16 count: 770\n",
      "reward: 24.956930377597747 setps: 14 count: 784\n",
      "reward: 27.84531948142685 setps: 16 count: 800\n",
      "reward: 25.903794361861948 setps: 15 count: 815\n",
      "reward: 24.054978223974466 setps: 14 count: 829\n",
      "reward: 25.949501715287624 setps: 15 count: 844\n",
      "reward: 25.30797147347184 setps: 14 count: 858\n",
      "reward: 24.844957098187294 setps: 14 count: 872\n",
      "reward: 25.696187754592387 setps: 15 count: 887\n",
      "reward: 17.651532979469632 setps: 11 count: 898\n",
      "reward: 21.120700937158833 setps: 15 count: 913\n",
      "reward: 27.297906418294588 setps: 16 count: 929\n",
      "reward: 22.36500098267279 setps: 14 count: 943\n",
      "reward: 25.775098987981618 setps: 15 count: 958\n",
      "reward: 24.906767782451066 setps: 16 count: 974\n",
      "reward: 25.39816018188285 setps: 15 count: 989\n",
      "avg rewards: 25.397842945600228\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.87259\n",
      "Epoch:20 Batch:4 Loss:0.05374\n",
      "Epoch:40 Batch:4 Loss:0.03496\n",
      "Epoch:60 Batch:4 Loss:0.02919\n",
      "Epoch:80 Batch:4 Loss:0.02802\n",
      "Epoch:100 Batch:4 Loss:0.02615\n",
      "Epoch:120 Batch:4 Loss:0.02444\n",
      "Epoch:140 Batch:4 Loss:0.02212\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.077\n",
      "Epoch:10 Batch:10 Loss:0.076\n",
      "Epoch:20 Batch:10 Loss:0.077\n",
      "Epoch:30 Batch:10 Loss:0.080\n",
      "Epoch:40 Batch:10 Loss:0.073\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 3.447402866111042 setps: 30 count: 30\n",
      "reward: 9.799923821078844 setps: 32 count: 62\n",
      "reward: 5.2591897850429845 setps: 28 count: 90\n",
      "reward: 3.0131057912469252 setps: 29 count: 119\n",
      "reward: 3.5426974545776826 setps: 28 count: 147\n",
      "reward: 17.110554588858214 setps: 35 count: 182\n",
      "reward: 6.825563452174536 setps: 31 count: 213\n",
      "reward: 12.727248996503386 setps: 33 count: 246\n",
      "reward: 10.404476322376283 setps: 33 count: 279\n",
      "reward: 4.11577824530832 setps: 32 count: 311\n",
      "reward: -2.4720423144375685 setps: 30 count: 341\n",
      "reward: 1.8802862444903437 setps: 29 count: 370\n",
      "reward: 5.827602514396675 setps: 30 count: 400\n",
      "reward: 9.417955384436935 setps: 30 count: 430\n",
      "reward: 7.525843187464488 setps: 30 count: 460\n",
      "reward: 6.823175375199934 setps: 33 count: 493\n",
      "reward: 8.603586748354425 setps: 34 count: 527\n",
      "reward: -0.2755301385986977 setps: 31 count: 558\n",
      "reward: 5.382453673492999 setps: 30 count: 588\n",
      "reward: 2.7124849632775283 setps: 31 count: 619\n",
      "reward: 7.3235265379684265 setps: 33 count: 652\n",
      "reward: 4.86227850849682 setps: 31 count: 683\n",
      "reward: 1.2834083375477339 setps: 30 count: 713\n",
      "reward: 4.801121543176121 setps: 31 count: 744\n",
      "reward: 7.833407287531006 setps: 31 count: 775\n",
      "reward: 8.668920246191373 setps: 31 count: 806\n",
      "reward: 1.7901683522795786 setps: 31 count: 837\n",
      "reward: 11.491446814221852 setps: 32 count: 869\n",
      "reward: 7.694557491077648 setps: 33 count: 902\n",
      "reward: 2.294918280144338 setps: 29 count: 931\n",
      "reward: 8.085621315687604 setps: 31 count: 962\n",
      "reward: 8.39201864554925 setps: 31 count: 993\n",
      "avg rewards: 6.131035947538345\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.57227\n",
      "Epoch:20 Batch:5 Loss:0.04483\n",
      "Epoch:40 Batch:5 Loss:0.02989\n",
      "Epoch:60 Batch:5 Loss:0.02687\n",
      "Epoch:80 Batch:5 Loss:0.02369\n",
      "Epoch:100 Batch:5 Loss:0.02174\n",
      "Epoch:120 Batch:5 Loss:0.02270\n",
      "Epoch:140 Batch:5 Loss:0.02169\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:10 Loss:0.083\n",
      "Epoch:10 Batch:10 Loss:0.078\n",
      "Epoch:20 Batch:10 Loss:0.075\n",
      "Epoch:30 Batch:10 Loss:0.073\n",
      "Epoch:40 Batch:10 Loss:0.071\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 25.783909751453024 setps: 15 count: 15\n",
      "reward: 26.55128642138152 setps: 15 count: 30\n",
      "reward: 26.540138476948776 setps: 15 count: 45\n",
      "reward: 25.949328432294717 setps: 15 count: 60\n",
      "reward: 27.000570168998095 setps: 16 count: 76\n",
      "reward: 23.104119928686128 setps: 14 count: 90\n",
      "reward: 26.206261790687854 setps: 15 count: 105\n",
      "reward: 27.370304326964835 setps: 16 count: 121\n",
      "reward: 26.523393325322832 setps: 15 count: 136\n",
      "reward: 24.639197795416113 setps: 15 count: 151\n",
      "reward: 26.108344894186303 setps: 15 count: 166\n",
      "reward: 26.721057586403905 setps: 16 count: 182\n",
      "reward: 25.850028115433815 setps: 15 count: 197\n",
      "reward: 24.666995073381987 setps: 14 count: 211\n",
      "reward: 24.4477931411413 setps: 14 count: 225\n",
      "reward: 24.686987835114994 setps: 14 count: 239\n",
      "reward: 26.226766563665294 setps: 15 count: 254\n",
      "reward: 25.43910036318557 setps: 15 count: 269\n",
      "reward: 24.728675412875596 setps: 14 count: 283\n",
      "reward: 25.32634053736838 setps: 14 count: 297\n",
      "reward: 25.844155841055905 setps: 15 count: 312\n",
      "reward: 22.44969108859222 setps: 13 count: 325\n",
      "reward: 25.972227467785704 setps: 15 count: 340\n",
      "reward: 23.98947573576006 setps: 15 count: 355\n",
      "reward: 25.88669240206654 setps: 15 count: 370\n",
      "reward: 27.18887807639985 setps: 15 count: 385\n",
      "reward: 24.86686884203227 setps: 15 count: 400\n",
      "reward: 27.758345609800013 setps: 16 count: 416\n",
      "reward: 24.563499953679273 setps: 14 count: 430\n",
      "reward: 26.702360440029587 setps: 16 count: 446\n",
      "reward: 30.538862653053364 setps: 17 count: 463\n",
      "reward: 26.461560392021777 setps: 15 count: 478\n",
      "reward: 24.46815028952114 setps: 14 count: 492\n",
      "reward: 22.548247750819424 setps: 14 count: 506\n",
      "reward: 26.918981820743646 setps: 16 count: 522\n",
      "reward: 25.522903443624084 setps: 15 count: 537\n",
      "reward: 26.074609989310552 setps: 15 count: 552\n",
      "reward: 24.252979096899804 setps: 14 count: 566\n",
      "reward: 27.967641586475654 setps: 16 count: 582\n",
      "reward: 27.530718367644294 setps: 16 count: 598\n",
      "reward: 24.00075545045838 setps: 15 count: 613\n",
      "reward: 22.28945626004279 setps: 14 count: 627\n",
      "reward: 26.34625866172719 setps: 15 count: 642\n",
      "reward: 23.015219975964282 setps: 14 count: 656\n",
      "reward: 21.60767872845463 setps: 14 count: 670\n",
      "reward: 22.159325910879122 setps: 14 count: 684\n",
      "reward: 27.333163329484528 setps: 16 count: 700\n",
      "reward: 26.077540518985188 setps: 15 count: 715\n",
      "reward: 26.073118345970578 setps: 15 count: 730\n",
      "reward: 25.809842783270874 setps: 15 count: 745\n",
      "reward: 25.90162973739061 setps: 15 count: 760\n",
      "reward: 28.520926481872444 setps: 17 count: 777\n",
      "reward: 25.1943537391824 setps: 15 count: 792\n",
      "reward: 24.757948447158558 setps: 14 count: 806\n",
      "reward: 10.584297161670108 setps: 18 count: 824\n",
      "reward: 26.164959757165345 setps: 15 count: 839\n",
      "reward: 28.156689606739384 setps: 16 count: 855\n",
      "reward: 22.84446102823858 setps: 14 count: 869\n",
      "reward: 25.796472701375023 setps: 15 count: 884\n",
      "reward: 25.890374055705607 setps: 14 count: 898\n",
      "reward: 25.822527282671945 setps: 15 count: 913\n",
      "reward: 24.07204239646962 setps: 14 count: 927\n",
      "reward: 25.62059510547988 setps: 14 count: 941\n",
      "reward: 27.287825295572112 setps: 15 count: 956\n",
      "reward: 25.506147177625092 setps: 15 count: 971\n",
      "reward: 25.337559017213064 setps: 14 count: 985\n",
      "reward: 24.55506254376814 setps: 14 count: 999\n",
      "avg rewards: 25.34483107893674\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.44645\n",
      "Epoch:20 Batch:6 Loss:0.03786\n",
      "Epoch:40 Batch:6 Loss:0.02879\n",
      "Epoch:60 Batch:6 Loss:0.02447\n",
      "Epoch:80 Batch:6 Loss:0.02212\n",
      "Epoch:100 Batch:6 Loss:0.02113\n",
      "Epoch:120 Batch:6 Loss:0.02112\n",
      "Epoch:140 Batch:6 Loss:0.01973\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.079\n",
      "Epoch:10 Batch:10 Loss:0.070\n",
      "Epoch:20 Batch:10 Loss:0.064\n",
      "Epoch:30 Batch:10 Loss:0.075\n",
      "Epoch:40 Batch:10 Loss:0.073\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 27.562904809230524 setps: 12 count: 12\n",
      "reward: 31.008280903854754 setps: 18 count: 30\n",
      "reward: 36.526467462936125 setps: 20 count: 50\n",
      "reward: 30.060557898331894 setps: 15 count: 65\n",
      "reward: 26.48264898090856 setps: 14 count: 79\n",
      "reward: 31.262341224816918 setps: 17 count: 96\n",
      "reward: 31.574383125978052 setps: 18 count: 114\n",
      "reward: 29.91212618670397 setps: 14 count: 128\n",
      "reward: 28.88944566505961 setps: 17 count: 145\n",
      "reward: 32.334026763538716 setps: 17 count: 162\n",
      "reward: 22.716016088164178 setps: 14 count: 176\n",
      "reward: 33.31614644562506 setps: 20 count: 196\n",
      "reward: 25.11881230757572 setps: 17 count: 213\n",
      "reward: 31.091213688002604 setps: 18 count: 231\n",
      "reward: 34.280932090549314 setps: 26 count: 257\n",
      "reward: 28.431639328466556 setps: 16 count: 273\n",
      "reward: 21.871357169224932 setps: 16 count: 289\n",
      "reward: 28.437147602393818 setps: 15 count: 304\n",
      "reward: 27.10605579947879 setps: 21 count: 325\n",
      "reward: 31.83467690032121 setps: 18 count: 343\n",
      "reward: 27.693663429823932 setps: 17 count: 360\n",
      "reward: 31.458750846363547 setps: 16 count: 376\n",
      "reward: 24.514813499354933 setps: 13 count: 389\n",
      "reward: 23.84055525054864 setps: 13 count: 402\n",
      "reward: 32.22939517852647 setps: 20 count: 422\n",
      "reward: 31.451307535629894 setps: 16 count: 438\n",
      "reward: 27.551422541703502 setps: 13 count: 451\n",
      "reward: 31.209217341973268 setps: 16 count: 467\n",
      "reward: 27.62996659136552 setps: 15 count: 482\n",
      "reward: 32.17630704296898 setps: 16 count: 498\n",
      "reward: 30.68862801866926 setps: 17 count: 515\n",
      "reward: 33.67901700336443 setps: 19 count: 534\n",
      "reward: 27.720506627261056 setps: 12 count: 546\n",
      "reward: 28.030479620686677 setps: 16 count: 562\n",
      "reward: 29.16946105728566 setps: 15 count: 577\n",
      "reward: 30.775414733063368 setps: 16 count: 593\n",
      "reward: 29.784208397100155 setps: 18 count: 611\n",
      "reward: 28.741318602545654 setps: 12 count: 623\n",
      "reward: 28.753254587828998 setps: 13 count: 636\n",
      "reward: 30.919229880106283 setps: 19 count: 655\n",
      "reward: 22.77209809729102 setps: 18 count: 673\n",
      "reward: 28.208235117758157 setps: 17 count: 690\n",
      "reward: 26.70680600892083 setps: 13 count: 703\n",
      "reward: 25.503992653076427 setps: 13 count: 716\n",
      "reward: 27.632538352016127 setps: 13 count: 729\n",
      "reward: 26.567121949899594 setps: 15 count: 744\n",
      "reward: 29.499374955600068 setps: 15 count: 759\n",
      "reward: 19.63170917428943 setps: 22 count: 781\n",
      "reward: 29.894801649342114 setps: 17 count: 798\n",
      "reward: 29.291396314381565 setps: 13 count: 811\n",
      "reward: 28.234834164239874 setps: 14 count: 825\n",
      "reward: 26.673458251498236 setps: 14 count: 839\n",
      "reward: 16.76932181515731 setps: 14 count: 853\n",
      "reward: 30.364002507919217 setps: 14 count: 867\n",
      "reward: 26.09940858136251 setps: 14 count: 881\n",
      "reward: 29.896160693823184 setps: 12 count: 893\n",
      "reward: 31.792883364022426 setps: 17 count: 910\n",
      "reward: 27.930115229455982 setps: 16 count: 926\n",
      "reward: 20.91016094719816 setps: 15 count: 941\n",
      "reward: 25.694432384456743 setps: 13 count: 954\n",
      "reward: 26.396697594434954 setps: 19 count: 973\n",
      "reward: 29.78841631472751 setps: 17 count: 990\n",
      "avg rewards: 28.453097812067792\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.69968\n",
      "Epoch:20 Batch:7 Loss:0.04030\n",
      "Epoch:40 Batch:7 Loss:0.02991\n",
      "Epoch:60 Batch:7 Loss:0.02474\n",
      "Epoch:80 Batch:7 Loss:0.02229\n",
      "Epoch:100 Batch:7 Loss:0.02257\n",
      "Epoch:120 Batch:7 Loss:0.02010\n",
      "Epoch:140 Batch:7 Loss:0.01797\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.069\n",
      "Epoch:10 Batch:10 Loss:0.072\n",
      "Epoch:20 Batch:10 Loss:0.073\n",
      "Epoch:30 Batch:10 Loss:0.078\n",
      "Epoch:40 Batch:10 Loss:0.068\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 22.856719382559824 setps: 18 count: 18\n",
      "reward: 28.299932832161716 setps: 19 count: 37\n",
      "reward: 28.291276009505964 setps: 17 count: 54\n",
      "reward: 27.764919771395213 setps: 17 count: 71\n",
      "reward: 28.553213702754878 setps: 33 count: 104\n",
      "reward: 32.72360487989208 setps: 21 count: 125\n",
      "reward: 28.48837227999029 setps: 19 count: 144\n",
      "reward: 42.45213160661514 setps: 30 count: 174\n",
      "reward: 33.65093849084659 setps: 21 count: 195\n",
      "reward: 33.79724511395033 setps: 19 count: 214\n",
      "reward: 22.980697662422607 setps: 17 count: 231\n",
      "reward: 20.56607612551743 setps: 15 count: 246\n",
      "reward: 37.73991264726064 setps: 27 count: 273\n",
      "reward: 30.25672662184079 setps: 19 count: 292\n",
      "reward: 32.88567101388035 setps: 20 count: 312\n",
      "reward: 30.726769061178484 setps: 20 count: 332\n",
      "reward: 32.11057984398503 setps: 19 count: 351\n",
      "reward: 31.12074168974796 setps: 20 count: 371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 35.3468762845936 setps: 23 count: 394\n",
      "reward: 21.291385964537042 setps: 18 count: 412\n",
      "reward: 26.23380339874566 setps: 15 count: 427\n",
      "reward: 25.070037334375957 setps: 15 count: 442\n",
      "reward: 25.9262360999157 setps: 38 count: 480\n",
      "reward: 19.826261522737333 setps: 20 count: 500\n",
      "reward: 37.564126167492944 setps: 23 count: 523\n",
      "reward: -1.042498417195742 setps: 59 count: 582\n",
      "reward: 29.664946662017606 setps: 17 count: 599\n",
      "reward: 21.466141500376402 setps: 18 count: 617\n",
      "reward: 29.039419820574402 setps: 18 count: 635\n",
      "reward: 28.09153032631875 setps: 17 count: 652\n",
      "reward: 29.30318076300318 setps: 19 count: 671\n",
      "reward: 38.383140077676224 setps: 28 count: 699\n",
      "reward: 29.19444619818823 setps: 19 count: 718\n",
      "reward: 36.29882886063715 setps: 25 count: 743\n",
      "reward: 29.19988690715836 setps: 19 count: 762\n",
      "reward: 35.08743095549143 setps: 24 count: 786\n",
      "reward: 27.88939659598109 setps: 15 count: 801\n",
      "reward: 32.99237239518697 setps: 33 count: 834\n",
      "reward: 33.784533968076 setps: 20 count: 854\n",
      "reward: 30.707478051036016 setps: 19 count: 873\n",
      "reward: 17.829152317007537 setps: 15 count: 888\n",
      "reward: 32.035176185495224 setps: 22 count: 910\n",
      "reward: 25.715441960313182 setps: 16 count: 926\n",
      "reward: 39.27643985771719 setps: 28 count: 954\n",
      "reward: 40.36851596010238 setps: 27 count: 981\n",
      "reward: 30.19631727441738 setps: 17 count: 998\n",
      "avg rewards: 29.391424646249614\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.41341\n",
      "Epoch:20 Batch:8 Loss:0.04070\n",
      "Epoch:40 Batch:8 Loss:0.02979\n",
      "Epoch:60 Batch:8 Loss:0.02656\n",
      "Epoch:80 Batch:8 Loss:0.02140\n",
      "Epoch:100 Batch:8 Loss:0.02243\n",
      "Epoch:120 Batch:8 Loss:0.02190\n",
      "Epoch:140 Batch:8 Loss:0.01879\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.060\n",
      "Epoch:10 Batch:10 Loss:0.061\n",
      "Epoch:20 Batch:10 Loss:0.054\n",
      "Epoch:30 Batch:10 Loss:0.060\n",
      "Epoch:40 Batch:10 Loss:0.057\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 23.600893883331448 setps: 34 count: 34\n",
      "reward: 27.821066738344957 setps: 41 count: 75\n",
      "reward: 20.27021668393426 setps: 34 count: 109\n",
      "reward: 22.4531898201778 setps: 35 count: 144\n",
      "reward: 19.811777566585803 setps: 31 count: 175\n",
      "reward: 24.388858070629063 setps: 39 count: 214\n",
      "reward: 18.353521341511804 setps: 30 count: 244\n",
      "reward: 19.228321449471693 setps: 37 count: 281\n",
      "reward: 31.29090975439758 setps: 48 count: 329\n",
      "reward: 25.81964589295384 setps: 42 count: 371\n",
      "reward: 20.37699018746061 setps: 33 count: 404\n",
      "reward: 18.642117964274078 setps: 32 count: 436\n",
      "reward: 32.18213046660241 setps: 51 count: 487\n",
      "reward: 23.50048642629116 setps: 38 count: 525\n",
      "reward: 65.44090352562199 setps: 48 count: 573\n",
      "reward: 41.37646067330788 setps: 63 count: 636\n",
      "reward: 19.422518579917956 setps: 31 count: 667\n",
      "reward: 41.94800676202288 setps: 64 count: 731\n",
      "reward: 27.864281397932807 setps: 44 count: 775\n",
      "reward: 32.85580336091953 setps: 49 count: 824\n",
      "reward: 20.18859186919726 setps: 35 count: 859\n",
      "reward: 30.30773885925446 setps: 43 count: 902\n",
      "reward: 19.932017712641397 setps: 31 count: 933\n",
      "reward: 25.913486614915005 setps: 47 count: 980\n",
      "avg rewards: 27.20791398340407\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.32381\n",
      "Epoch:20 Batch:9 Loss:0.03702\n",
      "Epoch:40 Batch:9 Loss:0.02914\n",
      "Epoch:60 Batch:9 Loss:0.02386\n",
      "Epoch:80 Batch:9 Loss:0.02262\n",
      "Epoch:100 Batch:9 Loss:0.02051\n",
      "Epoch:120 Batch:9 Loss:0.01842\n",
      "Epoch:140 Batch:9 Loss:0.01669\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.060\n",
      "Epoch:10 Batch:10 Loss:0.066\n",
      "Epoch:20 Batch:10 Loss:0.061\n",
      "Epoch:30 Batch:10 Loss:0.061\n",
      "Epoch:40 Batch:10 Loss:0.063\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 21.042399554279108 setps: 32 count: 32\n",
      "reward: 23.03101999900828 setps: 24 count: 56\n",
      "reward: 18.30490024344326 setps: 30 count: 86\n",
      "reward: 16.919534469743667 setps: 38 count: 124\n",
      "reward: 22.789718740792882 setps: 40 count: 164\n",
      "reward: 42.24017230017635 setps: 29 count: 193\n",
      "reward: 51.540651724458435 setps: 37 count: 230\n",
      "reward: 18.810126924562795 setps: 28 count: 258\n",
      "reward: 21.794982027620424 setps: 33 count: 291\n",
      "reward: 22.8702887868305 setps: 39 count: 330\n",
      "reward: 18.970014027928116 setps: 30 count: 360\n",
      "reward: 20.89546101611777 setps: 32 count: 392\n",
      "reward: 25.183281712679314 setps: 35 count: 427\n",
      "reward: 20.343753436970296 setps: 32 count: 459\n",
      "reward: 21.901510986837103 setps: 35 count: 494\n",
      "reward: 18.31059248247184 setps: 27 count: 521\n",
      "reward: 18.13533393066318 setps: 30 count: 551\n",
      "reward: 29.279919487157894 setps: 34 count: 585\n",
      "reward: 17.26179825451545 setps: 32 count: 617\n",
      "reward: 25.664184653028496 setps: 41 count: 658\n",
      "reward: 28.327139053089187 setps: 44 count: 702\n",
      "reward: 22.182115168857855 setps: 37 count: 739\n",
      "reward: 29.245007048231496 setps: 45 count: 784\n",
      "reward: 81.75355107508662 setps: 68 count: 852\n",
      "reward: 31.524959801754445 setps: 48 count: 900\n",
      "reward: 22.961963554361137 setps: 33 count: 933\n",
      "reward: 26.89383275336877 setps: 45 count: 978\n",
      "avg rewards: 26.599193082001285\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.29412\n",
      "Epoch:20 Batch:10 Loss:0.03663\n",
      "Epoch:40 Batch:10 Loss:0.02867\n",
      "Epoch:60 Batch:10 Loss:0.02224\n",
      "Epoch:80 Batch:10 Loss:0.02165\n",
      "Epoch:100 Batch:10 Loss:0.02205\n",
      "Epoch:120 Batch:10 Loss:0.01957\n",
      "Epoch:140 Batch:10 Loss:0.01714\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.066\n",
      "Epoch:10 Batch:10 Loss:0.058\n",
      "Epoch:20 Batch:10 Loss:0.064\n",
      "Epoch:30 Batch:10 Loss:0.054\n",
      "Epoch:40 Batch:10 Loss:0.061\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 23.97100630348577 setps: 46 count: 46\n",
      "reward: 24.003687221383736 setps: 47 count: 93\n",
      "reward: 20.233370813734652 setps: 35 count: 128\n",
      "reward: 24.046414801149517 setps: 40 count: 168\n",
      "reward: 29.258948193956165 setps: 48 count: 216\n",
      "reward: 40.2611978123634 setps: 70 count: 286\n",
      "reward: 23.519153747586824 setps: 41 count: 327\n",
      "reward: 20.776146039438025 setps: 37 count: 364\n",
      "reward: 29.669805050620933 setps: 49 count: 413\n",
      "reward: 20.236277646600506 setps: 35 count: 448\n",
      "reward: 45.4768942149676 setps: 33 count: 481\n",
      "reward: 20.18628435898863 setps: 47 count: 528\n",
      "reward: 40.389380698541935 setps: 27 count: 555\n",
      "reward: 30.765424648771297 setps: 49 count: 604\n",
      "reward: 17.892163965690994 setps: 37 count: 641\n",
      "reward: 24.367924090550503 setps: 43 count: 684\n",
      "reward: 48.41305654632889 setps: 36 count: 720\n",
      "reward: 26.469374350094586 setps: 27 count: 747\n",
      "reward: 21.566163985629096 setps: 40 count: 787\n",
      "reward: 18.784161913648134 setps: 40 count: 827\n",
      "reward: 31.86641662228649 setps: 52 count: 879\n",
      "reward: 27.780088328068093 setps: 50 count: 929\n",
      "reward: 29.57891720096522 setps: 53 count: 982\n",
      "avg rewards: 27.804880806732655\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.29931\n",
      "Epoch:20 Batch:11 Loss:0.03259\n",
      "Epoch:40 Batch:11 Loss:0.02551\n",
      "Epoch:60 Batch:11 Loss:0.02181\n",
      "Epoch:80 Batch:11 Loss:0.01962\n",
      "Epoch:100 Batch:11 Loss:0.01960\n",
      "Epoch:120 Batch:11 Loss:0.01626\n",
      "Epoch:140 Batch:11 Loss:0.01580\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.064\n",
      "Epoch:10 Batch:10 Loss:0.060\n",
      "Epoch:20 Batch:10 Loss:0.060\n",
      "Epoch:30 Batch:10 Loss:0.057\n",
      "Epoch:40 Batch:10 Loss:0.058\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 27.11155924526538 setps: 48 count: 48\n",
      "reward: 17.491073711375062 setps: 33 count: 81\n",
      "reward: 17.550005420470548 setps: 29 count: 110\n",
      "reward: 19.3788995277675 setps: 24 count: 134\n",
      "reward: 47.67496758637716 setps: 40 count: 174\n",
      "reward: 16.37016100767359 setps: 34 count: 208\n",
      "reward: 21.2208778863147 setps: 34 count: 242\n",
      "reward: 16.752248622634212 setps: 32 count: 274\n",
      "reward: 16.173835710666026 setps: 25 count: 299\n",
      "reward: 20.687122674509016 setps: 36 count: 335\n",
      "reward: 21.60379126326152 setps: 23 count: 358\n",
      "reward: 25.933984794597198 setps: 28 count: 386\n",
      "reward: 19.603540305544445 setps: 25 count: 411\n",
      "reward: 23.504423688721722 setps: 25 count: 436\n",
      "reward: 17.925507165462477 setps: 16 count: 452\n",
      "reward: 42.13364920822351 setps: 31 count: 483\n",
      "reward: 17.617335567202826 setps: 27 count: 510\n",
      "reward: 20.267251246311933 setps: 22 count: 532\n",
      "reward: 42.44186095490295 setps: 34 count: 566\n",
      "reward: 18.822948475441077 setps: 39 count: 605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 16.96114766761166 setps: 30 count: 635\n",
      "reward: 43.464042830980894 setps: 39 count: 674\n",
      "reward: 23.84410120192479 setps: 25 count: 699\n",
      "reward: 28.182099245388226 setps: 27 count: 726\n",
      "reward: 16.258680658349476 setps: 29 count: 755\n",
      "reward: 37.970633201784224 setps: 27 count: 782\n",
      "reward: 38.78010665362527 setps: 28 count: 810\n",
      "reward: 19.31056554653333 setps: 37 count: 847\n",
      "reward: 21.937672113714505 setps: 36 count: 883\n",
      "reward: 43.97366007240781 setps: 34 count: 917\n",
      "reward: 21.58727989795734 setps: 27 count: 944\n",
      "reward: 21.86505446032388 setps: 27 count: 971\n",
      "avg rewards: 25.137502737916382\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.31825\n",
      "Epoch:20 Batch:12 Loss:0.03261\n",
      "Epoch:40 Batch:12 Loss:0.02538\n",
      "Epoch:60 Batch:12 Loss:0.02060\n",
      "Epoch:80 Batch:12 Loss:0.01857\n",
      "Epoch:100 Batch:12 Loss:0.01659\n",
      "Epoch:120 Batch:12 Loss:0.01663\n",
      "Epoch:140 Batch:12 Loss:0.01558\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.053\n",
      "Epoch:10 Batch:10 Loss:0.057\n",
      "Epoch:20 Batch:10 Loss:0.060\n",
      "Epoch:30 Batch:10 Loss:0.056\n",
      "Epoch:40 Batch:10 Loss:0.059\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 23.632085739716423 setps: 38 count: 38\n",
      "reward: 10.685545049859503 setps: 37 count: 75\n",
      "reward: 19.920954722313038 setps: 37 count: 112\n",
      "reward: 65.14906323081232 setps: 59 count: 171\n",
      "reward: 29.643106133434046 setps: 42 count: 213\n",
      "reward: 25.70820512533828 setps: 37 count: 250\n",
      "reward: 30.850893681144225 setps: 63 count: 313\n",
      "reward: 60.686368423650855 setps: 57 count: 370\n",
      "reward: 29.435863973740194 setps: 43 count: 413\n",
      "reward: 24.791578406526245 setps: 38 count: 451\n",
      "reward: 15.207550747753698 setps: 40 count: 491\n",
      "reward: 13.409917128276719 setps: 39 count: 530\n",
      "reward: 13.933541535903352 setps: 37 count: 567\n",
      "reward: 22.73590927539335 setps: 37 count: 604\n",
      "reward: 15.121050515570095 setps: 40 count: 644\n",
      "reward: 69.14815057535309 setps: 85 count: 729\n",
      "reward: 22.855630952039796 setps: 49 count: 778\n",
      "reward: 62.031171979087226 setps: 74 count: 852\n",
      "reward: 14.628245695268559 setps: 39 count: 891\n",
      "reward: 17.947468269719685 setps: 36 count: 927\n",
      "reward: 11.275319560916975 setps: 40 count: 967\n",
      "avg rewards: 28.51417241532465\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.19838\n",
      "Epoch:20 Batch:13 Loss:0.02811\n",
      "Epoch:40 Batch:13 Loss:0.02280\n",
      "Epoch:60 Batch:13 Loss:0.01999\n",
      "Epoch:80 Batch:13 Loss:0.01709\n",
      "Epoch:100 Batch:13 Loss:0.01674\n",
      "Epoch:120 Batch:13 Loss:0.01593\n",
      "Epoch:140 Batch:13 Loss:0.01742\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.057\n",
      "Epoch:10 Batch:10 Loss:0.052\n",
      "Epoch:20 Batch:10 Loss:0.052\n",
      "Epoch:30 Batch:10 Loss:0.053\n",
      "Epoch:40 Batch:10 Loss:0.053\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 17.595988128895982 setps: 49 count: 49\n",
      "reward: 19.002767549217968 setps: 49 count: 98\n",
      "reward: 28.653043703487487 setps: 40 count: 138\n",
      "reward: 28.299507575773177 setps: 42 count: 180\n",
      "reward: 18.296364196906506 setps: 49 count: 229\n",
      "reward: 10.61283337378991 setps: 36 count: 265\n",
      "reward: 19.072479043788913 setps: 37 count: 302\n",
      "reward: 21.7261507143252 setps: 38 count: 340\n",
      "reward: 28.307184943767794 setps: 41 count: 381\n",
      "reward: 17.27226268076629 setps: 36 count: 417\n",
      "reward: 18.25923801682219 setps: 38 count: 455\n",
      "reward: 24.43019347796217 setps: 39 count: 494\n",
      "reward: 23.76455427088512 setps: 56 count: 550\n",
      "reward: 21.40324366070999 setps: 36 count: 586\n",
      "reward: 14.742881013962323 setps: 44 count: 630\n",
      "reward: 26.644671345471593 setps: 61 count: 691\n",
      "reward: 17.824118712790373 setps: 47 count: 738\n",
      "reward: 19.841153422945354 setps: 52 count: 790\n",
      "reward: 29.34053431195061 setps: 41 count: 831\n",
      "reward: 13.323728724794636 setps: 41 count: 872\n",
      "reward: 19.61680274954124 setps: 35 count: 907\n",
      "reward: 12.273778321708955 setps: 35 count: 942\n",
      "reward: 19.398565224889897 setps: 36 count: 978\n",
      "avg rewards: 20.421828050658856\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.17554\n",
      "Epoch:20 Batch:14 Loss:0.03118\n",
      "Epoch:40 Batch:14 Loss:0.02199\n",
      "Epoch:60 Batch:14 Loss:0.01793\n",
      "Epoch:80 Batch:14 Loss:0.01599\n",
      "Epoch:100 Batch:14 Loss:0.01595\n",
      "Epoch:120 Batch:14 Loss:0.01609\n",
      "Epoch:140 Batch:14 Loss:0.01434\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.044\n",
      "Epoch:10 Batch:10 Loss:0.042\n",
      "Epoch:20 Batch:10 Loss:0.045\n",
      "Epoch:30 Batch:10 Loss:0.042\n",
      "Epoch:40 Batch:10 Loss:0.040\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 67.67965520120781 setps: 56 count: 56\n",
      "reward: 20.804477052613223 setps: 35 count: 91\n",
      "reward: 19.7149265291504 setps: 37 count: 128\n",
      "reward: 15.942042095660877 setps: 45 count: 173\n",
      "reward: 21.30365438896697 setps: 35 count: 208\n",
      "reward: 22.475627235500724 setps: 36 count: 244\n",
      "reward: 15.236254669551275 setps: 44 count: 288\n",
      "reward: 21.7012484785766 setps: 38 count: 326\n",
      "reward: 12.405903117138951 setps: 36 count: 362\n",
      "reward: 23.374068562527828 setps: 51 count: 413\n",
      "reward: 48.198247037750825 setps: 38 count: 451\n",
      "reward: 22.973301917225758 setps: 49 count: 500\n",
      "reward: 30.425949152380063 setps: 62 count: 562\n",
      "reward: 32.12281898168876 setps: 44 count: 606\n",
      "reward: 15.668263430670777 setps: 41 count: 647\n",
      "reward: 20.82011926156701 setps: 38 count: 685\n",
      "reward: 30.51467554711271 setps: 59 count: 744\n",
      "reward: 22.71926940573322 setps: 38 count: 782\n",
      "reward: 26.26896616583108 setps: 46 count: 828\n",
      "reward: 27.064111937997225 setps: 40 count: 868\n",
      "reward: 21.879414065982562 setps: 45 count: 913\n",
      "reward: 24.822837969276588 setps: 42 count: 955\n",
      "reward: 14.40229244454095 setps: 38 count: 993\n",
      "avg rewards: 25.15296194124575\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.14347\n",
      "Epoch:20 Batch:15 Loss:0.02654\n",
      "Epoch:40 Batch:15 Loss:0.02021\n",
      "Epoch:60 Batch:15 Loss:0.01794\n",
      "Epoch:80 Batch:15 Loss:0.01729\n",
      "Epoch:100 Batch:15 Loss:0.01412\n",
      "Epoch:120 Batch:15 Loss:0.01244\n",
      "Epoch:140 Batch:15 Loss:0.01381\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.046\n",
      "Epoch:10 Batch:10 Loss:0.048\n",
      "Epoch:20 Batch:10 Loss:0.048\n",
      "Epoch:30 Batch:10 Loss:0.042\n",
      "Epoch:40 Batch:10 Loss:0.047\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 13.815032931776658 setps: 34 count: 34\n",
      "reward: 22.345419757215133 setps: 40 count: 74\n",
      "reward: 10.806175187142797 setps: 31 count: 105\n",
      "reward: 8.5553841898276 setps: 36 count: 141\n",
      "reward: 14.591145538155976 setps: 39 count: 180\n",
      "reward: 20.876944085980355 setps: 37 count: 217\n",
      "reward: 22.46632602545869 setps: 50 count: 267\n",
      "reward: 15.412071459210718 setps: 45 count: 312\n",
      "reward: 13.142463353132191 setps: 42 count: 354\n",
      "reward: 31.405478480577585 setps: 45 count: 399\n",
      "reward: 11.081923697142331 setps: 34 count: 433\n",
      "reward: 7.835621061114942 setps: 35 count: 468\n",
      "reward: 20.40194163696724 setps: 50 count: 518\n",
      "reward: 13.35416931972286 setps: 37 count: 555\n",
      "reward: 20.5742676198206 setps: 39 count: 594\n",
      "reward: 12.482867915176033 setps: 40 count: 634\n",
      "reward: 8.55245522756159 setps: 33 count: 667\n",
      "reward: 8.989682981981607 setps: 35 count: 702\n",
      "reward: 25.297103039496868 setps: 41 count: 743\n",
      "reward: 17.601427997727296 setps: 39 count: 782\n",
      "reward: 24.756038973717665 setps: 41 count: 823\n",
      "reward: 16.263541940168945 setps: 37 count: 860\n",
      "reward: 9.952966903171909 setps: 35 count: 895\n",
      "reward: 17.923163615635715 setps: 38 count: 933\n",
      "reward: 18.030628855481336 setps: 38 count: 971\n",
      "avg rewards: 16.260569671734583\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.14471\n",
      "Epoch:20 Batch:16 Loss:0.02249\n",
      "Epoch:40 Batch:16 Loss:0.01821\n",
      "Epoch:60 Batch:16 Loss:0.01637\n",
      "Epoch:80 Batch:16 Loss:0.01516\n",
      "Epoch:100 Batch:16 Loss:0.01454\n",
      "Epoch:120 Batch:16 Loss:0.01443\n",
      "Epoch:140 Batch:16 Loss:0.01281\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.044\n",
      "Epoch:10 Batch:10 Loss:0.043\n",
      "Epoch:20 Batch:10 Loss:0.040\n",
      "Epoch:30 Batch:10 Loss:0.044\n",
      "Epoch:40 Batch:10 Loss:0.042\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 36.92539364509721 setps: 24 count: 24\n",
      "reward: 33.691969618581055 setps: 20 count: 44\n",
      "reward: 35.353576663677806 setps: 22 count: 66\n",
      "reward: 49.41029400912958 setps: 46 count: 112\n",
      "reward: 39.723686289084434 setps: 27 count: 139\n",
      "reward: 38.35091861945838 setps: 25 count: 164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 37.43723571384908 setps: 24 count: 188\n",
      "reward: 19.342141939868455 setps: 41 count: 229\n",
      "reward: 44.646108252118566 setps: 31 count: 260\n",
      "reward: 34.742281169243505 setps: 20 count: 280\n",
      "reward: 33.83054384779825 setps: 21 count: 301\n",
      "reward: 35.77023090961884 setps: 21 count: 322\n",
      "reward: 34.221936131890104 setps: 20 count: 342\n",
      "reward: 18.419159395956378 setps: 40 count: 382\n",
      "reward: 34.53033038447902 setps: 20 count: 402\n",
      "reward: 39.736591620044784 setps: 26 count: 428\n",
      "reward: 42.40065703474246 setps: 30 count: 458\n",
      "reward: 35.81292549147766 setps: 22 count: 480\n",
      "reward: 42.664204306094334 setps: 29 count: 509\n",
      "reward: 36.57096551486465 setps: 23 count: 532\n",
      "reward: 23.183153661449612 setps: 46 count: 578\n",
      "reward: 36.75842748229916 setps: 22 count: 600\n",
      "reward: 38.693153583163806 setps: 25 count: 625\n",
      "reward: 30.988258176845555 setps: 20 count: 645\n",
      "reward: 39.77120847214974 setps: 26 count: 671\n",
      "reward: 34.72886522375484 setps: 39 count: 710\n",
      "reward: 19.607059246221613 setps: 45 count: 755\n",
      "reward: 32.215842331819296 setps: 45 count: 800\n",
      "reward: 27.548685285240026 setps: 37 count: 837\n",
      "reward: 38.85682021108368 setps: 25 count: 862\n",
      "reward: 34.72638185858085 setps: 21 count: 883\n",
      "reward: 42.92525828889773 setps: 32 count: 915\n",
      "reward: 37.51162974681066 setps: 23 count: 938\n",
      "reward: 36.55971389180341 setps: 22 count: 960\n",
      "reward: 38.05408871964028 setps: 23 count: 983\n",
      "avg rewards: 35.305991335338135\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.13538\n",
      "Epoch:20 Batch:17 Loss:0.02485\n",
      "Epoch:40 Batch:17 Loss:0.01999\n",
      "Epoch:60 Batch:17 Loss:0.01738\n",
      "Epoch:80 Batch:17 Loss:0.01533\n",
      "Epoch:100 Batch:17 Loss:0.01663\n",
      "Epoch:120 Batch:17 Loss:0.01310\n",
      "Epoch:140 Batch:17 Loss:0.01141\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.046\n",
      "Epoch:10 Batch:10 Loss:0.046\n",
      "Epoch:20 Batch:10 Loss:0.053\n",
      "Epoch:30 Batch:10 Loss:0.048\n",
      "Epoch:40 Batch:10 Loss:0.048\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 17.752725180158453 setps: 36 count: 36\n",
      "reward: 40.68560434390384 setps: 45 count: 81\n",
      "reward: 16.756977113967885 setps: 45 count: 126\n",
      "reward: 18.94907578351413 setps: 37 count: 163\n",
      "reward: 16.72313011680672 setps: 40 count: 203\n",
      "reward: 16.477389861075785 setps: 46 count: 249\n",
      "reward: 22.713258017000044 setps: 37 count: 286\n",
      "reward: 36.22700442359928 setps: 45 count: 331\n",
      "reward: 17.8927687195508 setps: 45 count: 376\n",
      "reward: 17.327104504474843 setps: 42 count: 418\n",
      "reward: 18.636560519562046 setps: 39 count: 457\n",
      "reward: 30.413936088155605 setps: 53 count: 510\n",
      "reward: 48.13336673397152 setps: 85 count: 595\n",
      "reward: 23.06846493419289 setps: 44 count: 639\n",
      "reward: 20.596955385453477 setps: 51 count: 690\n",
      "reward: 16.248012617509815 setps: 43 count: 733\n",
      "reward: 23.598350266351186 setps: 58 count: 791\n",
      "reward: 20.15040969003311 setps: 50 count: 841\n",
      "reward: 14.947661752330895 setps: 39 count: 880\n",
      "reward: 19.09525326048896 setps: 47 count: 927\n",
      "reward: 20.41250913038966 setps: 50 count: 977\n",
      "avg rewards: 22.705072306785283\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.13187\n",
      "Epoch:20 Batch:18 Loss:0.02380\n",
      "Epoch:40 Batch:18 Loss:0.01976\n",
      "Epoch:60 Batch:18 Loss:0.01779\n",
      "Epoch:80 Batch:18 Loss:0.01430\n",
      "Epoch:100 Batch:18 Loss:0.01407\n",
      "Epoch:120 Batch:18 Loss:0.01370\n",
      "Epoch:140 Batch:18 Loss:0.01112\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.046\n",
      "Epoch:10 Batch:10 Loss:0.041\n",
      "Epoch:20 Batch:10 Loss:0.039\n",
      "Epoch:30 Batch:10 Loss:0.036\n",
      "Epoch:40 Batch:10 Loss:0.046\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 52.032560633822875 setps: 37 count: 37\n",
      "reward: 80.80622872819804 setps: 70 count: 107\n",
      "reward: 24.339469271943397 setps: 51 count: 158\n",
      "reward: 17.797757197101603 setps: 38 count: 196\n",
      "reward: 91.47503455648112 setps: 84 count: 280\n",
      "reward: 12.124913393474715 setps: 35 count: 315\n",
      "reward: 27.785530162704532 setps: 48 count: 363\n",
      "reward: 27.654947999982678 setps: 58 count: 421\n",
      "reward: 75.85240805248034 setps: 70 count: 491\n",
      "reward: 23.811193007977263 setps: 55 count: 546\n",
      "reward: 35.06741970802978 setps: 59 count: 605\n",
      "reward: 21.164240256433544 setps: 42 count: 647\n",
      "reward: 54.312482116460174 setps: 39 count: 686\n",
      "reward: 77.98197710062995 setps: 62 count: 748\n",
      "reward: 37.812404961638094 setps: 70 count: 818\n",
      "reward: 22.315387070053838 setps: 41 count: 859\n",
      "reward: 19.211145165766357 setps: 38 count: 897\n",
      "reward: 30.44211204025196 setps: 57 count: 954\n",
      "reward: 18.438471337643573 setps: 39 count: 993\n",
      "avg rewards: 39.4960885663723\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.13252\n",
      "Epoch:20 Batch:19 Loss:0.02288\n",
      "Epoch:40 Batch:19 Loss:0.01613\n",
      "Epoch:60 Batch:19 Loss:0.01504\n",
      "Epoch:80 Batch:19 Loss:0.01304\n",
      "Epoch:100 Batch:19 Loss:0.01256\n",
      "Epoch:120 Batch:19 Loss:0.01380\n",
      "Epoch:140 Batch:19 Loss:0.01135\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.039\n",
      "Epoch:10 Batch:10 Loss:0.033\n",
      "Epoch:20 Batch:10 Loss:0.037\n",
      "Epoch:30 Batch:10 Loss:0.037\n",
      "Epoch:40 Batch:10 Loss:0.039\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 17.448132826125946 setps: 43 count: 43\n",
      "reward: 52.52273603354551 setps: 37 count: 80\n",
      "reward: 22.18308373817417 setps: 50 count: 130\n",
      "reward: 89.18157030091533 setps: 85 count: 215\n",
      "reward: 40.13981348251255 setps: 75 count: 290\n",
      "reward: 20.33800836808222 setps: 47 count: 337\n",
      "reward: 16.651783870616054 setps: 43 count: 380\n",
      "reward: 36.31339186900948 setps: 69 count: 449\n",
      "reward: 75.71401210419135 setps: 69 count: 518\n",
      "reward: 18.27272069138999 setps: 44 count: 562\n",
      "reward: 83.21510310749143 setps: 77 count: 639\n",
      "reward: 65.15611365861695 setps: 57 count: 696\n",
      "reward: 71.29083004079003 setps: 64 count: 760\n",
      "reward: 70.61383643128248 setps: 59 count: 819\n",
      "reward: 24.46638456079526 setps: 52 count: 871\n",
      "reward: 15.206753914424908 setps: 40 count: 911\n",
      "reward: 20.474306440139472 setps: 47 count: 958\n",
      "avg rewards: 43.48168126106489\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.10258\n",
      "Epoch:20 Batch:20 Loss:0.02388\n",
      "Epoch:40 Batch:20 Loss:0.01784\n",
      "Epoch:60 Batch:20 Loss:0.01427\n",
      "Epoch:80 Batch:20 Loss:0.01576\n",
      "Epoch:100 Batch:20 Loss:0.01324\n",
      "Epoch:120 Batch:20 Loss:0.01162\n",
      "Epoch:140 Batch:20 Loss:0.01157\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.045\n",
      "Epoch:10 Batch:10 Loss:0.041\n",
      "Epoch:20 Batch:10 Loss:0.046\n",
      "Epoch:30 Batch:10 Loss:0.050\n",
      "Epoch:40 Batch:10 Loss:0.046\n",
      "Done!\n",
      "############# start HalfCheetahBulletEnv-v0 training ###################\n",
      "(50, 1000, 26)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -930.2400049518964 setps: 800 count: 800\n",
      "avg rewards: -930.2400049518964\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:1.32790\n",
      "Epoch:20 Batch:1 Loss:0.22525\n",
      "Epoch:40 Batch:1 Loss:0.17515\n",
      "Epoch:60 Batch:1 Loss:0.13694\n",
      "Epoch:80 Batch:1 Loss:0.11601\n",
      "Epoch:100 Batch:1 Loss:0.10305\n",
      "Epoch:120 Batch:1 Loss:0.09515\n",
      "Epoch:140 Batch:1 Loss:0.08902\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.197\n",
      "Epoch:10 Batch:10 Loss:0.208\n",
      "Epoch:20 Batch:10 Loss:0.206\n",
      "Epoch:30 Batch:10 Loss:0.205\n",
      "Epoch:40 Batch:10 Loss:0.200\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1254.707526885782 setps: 800 count: 800\n",
      "avg rewards: -1254.707526885782\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:1.41641\n",
      "Epoch:20 Batch:2 Loss:0.13168\n",
      "Epoch:40 Batch:2 Loss:0.10048\n",
      "Epoch:60 Batch:2 Loss:0.07620\n",
      "Epoch:80 Batch:2 Loss:0.06495\n",
      "Epoch:100 Batch:2 Loss:0.05679\n",
      "Epoch:120 Batch:2 Loss:0.05584\n",
      "Epoch:140 Batch:2 Loss:0.05407\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.138\n",
      "Epoch:10 Batch:10 Loss:0.137\n",
      "Epoch:20 Batch:10 Loss:0.136\n",
      "Epoch:30 Batch:10 Loss:0.136\n",
      "Epoch:40 Batch:10 Loss:0.136\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1353.4635400171962 setps: 800 count: 800\n",
      "avg rewards: -1353.4635400171962\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:1.22337\n",
      "Epoch:20 Batch:3 Loss:0.10256\n",
      "Epoch:40 Batch:3 Loss:0.06581\n",
      "Epoch:60 Batch:3 Loss:0.05483\n",
      "Epoch:80 Batch:3 Loss:0.04517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:100 Batch:3 Loss:0.04397\n",
      "Epoch:120 Batch:3 Loss:0.03827\n",
      "Epoch:140 Batch:3 Loss:0.03401\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.115\n",
      "Epoch:10 Batch:10 Loss:0.116\n",
      "Epoch:20 Batch:10 Loss:0.109\n",
      "Epoch:30 Batch:10 Loss:0.112\n",
      "Epoch:40 Batch:10 Loss:0.110\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1449.0120741328171 setps: 800 count: 800\n",
      "avg rewards: -1449.0120741328171\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:1.03384\n",
      "Epoch:20 Batch:4 Loss:0.08310\n",
      "Epoch:40 Batch:4 Loss:0.05406\n",
      "Epoch:60 Batch:4 Loss:0.04641\n",
      "Epoch:80 Batch:4 Loss:0.04368\n",
      "Epoch:100 Batch:4 Loss:0.03844\n",
      "Epoch:120 Batch:4 Loss:0.03462\n",
      "Epoch:140 Batch:4 Loss:0.03620\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.091\n",
      "Epoch:10 Batch:10 Loss:0.093\n",
      "Epoch:20 Batch:10 Loss:0.089\n",
      "Epoch:30 Batch:10 Loss:0.087\n",
      "Epoch:40 Batch:10 Loss:0.088\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1165.0749802578962 setps: 800 count: 800\n",
      "avg rewards: -1165.0749802578962\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.89998\n",
      "Epoch:20 Batch:5 Loss:0.06974\n",
      "Epoch:40 Batch:5 Loss:0.04512\n",
      "Epoch:60 Batch:5 Loss:0.03700\n",
      "Epoch:80 Batch:5 Loss:0.03335\n",
      "Epoch:100 Batch:5 Loss:0.03187\n",
      "Epoch:120 Batch:5 Loss:0.03174\n",
      "Epoch:140 Batch:5 Loss:0.02705\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.080\n",
      "Epoch:10 Batch:10 Loss:0.077\n",
      "Epoch:20 Batch:10 Loss:0.077\n",
      "Epoch:30 Batch:10 Loss:0.078\n",
      "Epoch:40 Batch:10 Loss:0.079\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1397.3286823413657 setps: 800 count: 800\n",
      "avg rewards: -1397.3286823413657\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.41651\n",
      "Epoch:20 Batch:6 Loss:0.05994\n",
      "Epoch:40 Batch:6 Loss:0.04055\n",
      "Epoch:60 Batch:6 Loss:0.03566\n",
      "Epoch:80 Batch:6 Loss:0.03228\n",
      "Epoch:100 Batch:6 Loss:0.02829\n",
      "Epoch:120 Batch:6 Loss:0.02518\n",
      "Epoch:140 Batch:6 Loss:0.02528\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.085\n",
      "Epoch:10 Batch:10 Loss:0.082\n",
      "Epoch:20 Batch:10 Loss:0.084\n",
      "Epoch:30 Batch:10 Loss:0.081\n",
      "Epoch:40 Batch:10 Loss:0.084\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1199.891032751274 setps: 800 count: 800\n",
      "avg rewards: -1199.891032751274\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.68850\n",
      "Epoch:20 Batch:7 Loss:0.05243\n",
      "Epoch:40 Batch:7 Loss:0.03598\n",
      "Epoch:60 Batch:7 Loss:0.03099\n",
      "Epoch:80 Batch:7 Loss:0.02551\n",
      "Epoch:100 Batch:7 Loss:0.02352\n",
      "Epoch:120 Batch:7 Loss:0.02409\n",
      "Epoch:140 Batch:7 Loss:0.02183\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.081\n",
      "Epoch:10 Batch:10 Loss:0.079\n",
      "Epoch:20 Batch:10 Loss:0.078\n",
      "Epoch:30 Batch:10 Loss:0.076\n",
      "Epoch:40 Batch:10 Loss:0.075\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1098.702402434407 setps: 800 count: 800\n",
      "avg rewards: -1098.702402434407\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.41780\n",
      "Epoch:20 Batch:8 Loss:0.04477\n",
      "Epoch:40 Batch:8 Loss:0.03316\n",
      "Epoch:60 Batch:8 Loss:0.02950\n",
      "Epoch:80 Batch:8 Loss:0.02549\n",
      "Epoch:100 Batch:8 Loss:0.02054\n",
      "Epoch:120 Batch:8 Loss:0.02345\n",
      "Epoch:140 Batch:8 Loss:0.02224\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.075\n",
      "Epoch:10 Batch:10 Loss:0.074\n",
      "Epoch:20 Batch:10 Loss:0.074\n",
      "Epoch:30 Batch:10 Loss:0.070\n",
      "Epoch:40 Batch:10 Loss:0.078\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1171.4803918215152 setps: 800 count: 800\n",
      "avg rewards: -1171.4803918215152\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.31640\n",
      "Epoch:20 Batch:9 Loss:0.04350\n",
      "Epoch:40 Batch:9 Loss:0.03501\n",
      "Epoch:60 Batch:9 Loss:0.02864\n",
      "Epoch:80 Batch:9 Loss:0.02261\n",
      "Epoch:100 Batch:9 Loss:0.02161\n",
      "Epoch:120 Batch:9 Loss:0.02006\n",
      "Epoch:140 Batch:9 Loss:0.02080\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.075\n",
      "Epoch:10 Batch:10 Loss:0.070\n",
      "Epoch:20 Batch:10 Loss:0.072\n",
      "Epoch:30 Batch:10 Loss:0.070\n",
      "Epoch:40 Batch:10 Loss:0.069\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -612.4831198314073 setps: 800 count: 800\n",
      "avg rewards: -612.4831198314073\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.47267\n",
      "Epoch:20 Batch:10 Loss:0.05400\n",
      "Epoch:40 Batch:10 Loss:0.03495\n",
      "Epoch:60 Batch:10 Loss:0.02850\n",
      "Epoch:80 Batch:10 Loss:0.03001\n",
      "Epoch:100 Batch:10 Loss:0.02682\n",
      "Epoch:120 Batch:10 Loss:0.02320\n",
      "Epoch:140 Batch:10 Loss:0.02331\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.056\n",
      "Epoch:10 Batch:10 Loss:0.054\n",
      "Epoch:20 Batch:10 Loss:0.055\n",
      "Epoch:30 Batch:10 Loss:0.054\n",
      "Epoch:40 Batch:10 Loss:0.052\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1029.7054017718522 setps: 800 count: 800\n",
      "avg rewards: -1029.7054017718522\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.40596\n",
      "Epoch:20 Batch:11 Loss:0.04374\n",
      "Epoch:40 Batch:11 Loss:0.03457\n",
      "Epoch:60 Batch:11 Loss:0.02753\n",
      "Epoch:80 Batch:11 Loss:0.02766\n",
      "Epoch:100 Batch:11 Loss:0.02445\n",
      "Epoch:120 Batch:11 Loss:0.02410\n",
      "Epoch:140 Batch:11 Loss:0.02361\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.053\n",
      "Epoch:10 Batch:10 Loss:0.051\n",
      "Epoch:20 Batch:10 Loss:0.052\n",
      "Epoch:30 Batch:10 Loss:0.052\n",
      "Epoch:40 Batch:10 Loss:0.049\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -375.74193064045534 setps: 800 count: 800\n",
      "avg rewards: -375.74193064045534\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.26076\n",
      "Epoch:20 Batch:12 Loss:0.04518\n",
      "Epoch:40 Batch:12 Loss:0.03599\n",
      "Epoch:60 Batch:12 Loss:0.02985\n",
      "Epoch:80 Batch:12 Loss:0.02616\n",
      "Epoch:100 Batch:12 Loss:0.02686\n",
      "Epoch:120 Batch:12 Loss:0.02851\n",
      "Epoch:140 Batch:12 Loss:0.02681\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.048\n",
      "Epoch:10 Batch:10 Loss:0.044\n",
      "Epoch:20 Batch:10 Loss:0.043\n",
      "Epoch:30 Batch:10 Loss:0.044\n",
      "Epoch:40 Batch:10 Loss:0.043\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -890.6725883074288 setps: 800 count: 800\n",
      "avg rewards: -890.6725883074288\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.24928\n",
      "Epoch:20 Batch:13 Loss:0.04498\n",
      "Epoch:40 Batch:13 Loss:0.03548\n",
      "Epoch:60 Batch:13 Loss:0.02566\n",
      "Epoch:80 Batch:13 Loss:0.02729\n",
      "Epoch:100 Batch:13 Loss:0.02452\n",
      "Epoch:120 Batch:13 Loss:0.02436\n",
      "Epoch:140 Batch:13 Loss:0.02422\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.046\n",
      "Epoch:10 Batch:10 Loss:0.045\n",
      "Epoch:20 Batch:10 Loss:0.046\n",
      "Epoch:30 Batch:10 Loss:0.045\n",
      "Epoch:40 Batch:10 Loss:0.045\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -650.6781164741565 setps: 800 count: 800\n",
      "avg rewards: -650.6781164741565\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.21182\n",
      "Epoch:20 Batch:14 Loss:0.03919\n",
      "Epoch:40 Batch:14 Loss:0.03327\n",
      "Epoch:60 Batch:14 Loss:0.02504\n",
      "Epoch:80 Batch:14 Loss:0.02525\n",
      "Epoch:100 Batch:14 Loss:0.02080\n",
      "Epoch:120 Batch:14 Loss:0.02261\n",
      "Epoch:140 Batch:14 Loss:0.02268\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.044\n",
      "Epoch:10 Batch:10 Loss:0.040\n",
      "Epoch:20 Batch:10 Loss:0.042\n",
      "Epoch:30 Batch:10 Loss:0.041\n",
      "Epoch:40 Batch:10 Loss:0.042\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -986.9348189410553 setps: 800 count: 800\n",
      "avg rewards: -986.9348189410553\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.20897\n",
      "Epoch:20 Batch:15 Loss:0.03919\n",
      "Epoch:40 Batch:15 Loss:0.03282\n",
      "Epoch:60 Batch:15 Loss:0.02589\n",
      "Epoch:80 Batch:15 Loss:0.02799\n",
      "Epoch:100 Batch:15 Loss:0.02482\n",
      "Epoch:120 Batch:15 Loss:0.02067\n",
      "Epoch:140 Batch:15 Loss:0.02100\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.040\n",
      "Epoch:10 Batch:10 Loss:0.037\n",
      "Epoch:20 Batch:10 Loss:0.039\n",
      "Epoch:30 Batch:10 Loss:0.037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:40 Batch:10 Loss:0.039\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 508.22587273029376 setps: 800 count: 800\n",
      "avg rewards: 508.22587273029376\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.21140\n",
      "Epoch:20 Batch:16 Loss:0.03856\n",
      "Epoch:40 Batch:16 Loss:0.03362\n",
      "Epoch:60 Batch:16 Loss:0.02689\n",
      "Epoch:80 Batch:16 Loss:0.02534\n",
      "Epoch:100 Batch:16 Loss:0.02131\n",
      "Epoch:120 Batch:16 Loss:0.02015\n",
      "Epoch:140 Batch:16 Loss:0.01912\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.036\n",
      "Epoch:10 Batch:10 Loss:0.034\n",
      "Epoch:20 Batch:10 Loss:0.035\n",
      "Epoch:30 Batch:10 Loss:0.034\n",
      "Epoch:40 Batch:10 Loss:0.035\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 280.0789399539733 setps: 800 count: 800\n",
      "avg rewards: 280.0789399539733\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.19894\n",
      "Epoch:20 Batch:17 Loss:0.04339\n",
      "Epoch:40 Batch:17 Loss:0.03107\n",
      "Epoch:60 Batch:17 Loss:0.03008\n",
      "Epoch:80 Batch:17 Loss:0.02770\n",
      "Epoch:100 Batch:17 Loss:0.02585\n",
      "Epoch:120 Batch:17 Loss:0.02269\n",
      "Epoch:140 Batch:17 Loss:0.02274\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.032\n",
      "Epoch:10 Batch:10 Loss:0.031\n",
      "Epoch:20 Batch:10 Loss:0.029\n",
      "Epoch:30 Batch:10 Loss:0.030\n",
      "Epoch:40 Batch:10 Loss:0.031\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 365.3893046242984 setps: 800 count: 800\n",
      "avg rewards: 365.3893046242984\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.17660\n",
      "Epoch:20 Batch:18 Loss:0.04033\n",
      "Epoch:40 Batch:18 Loss:0.03100\n",
      "Epoch:60 Batch:18 Loss:0.02897\n",
      "Epoch:80 Batch:18 Loss:0.02894\n",
      "Epoch:100 Batch:18 Loss:0.02562\n",
      "Epoch:120 Batch:18 Loss:0.02650\n",
      "Epoch:140 Batch:18 Loss:0.02498\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.031\n",
      "Epoch:10 Batch:10 Loss:0.030\n",
      "Epoch:20 Batch:10 Loss:0.030\n",
      "Epoch:30 Batch:10 Loss:0.028\n",
      "Epoch:40 Batch:10 Loss:0.028\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 419.0154901684262 setps: 800 count: 800\n",
      "avg rewards: 419.0154901684262\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.19003\n",
      "Epoch:20 Batch:19 Loss:0.04158\n",
      "Epoch:40 Batch:19 Loss:0.03163\n",
      "Epoch:60 Batch:19 Loss:0.02739\n",
      "Epoch:80 Batch:19 Loss:0.02440\n",
      "Epoch:100 Batch:19 Loss:0.02657\n",
      "Epoch:120 Batch:19 Loss:0.02386\n",
      "Epoch:140 Batch:19 Loss:0.02204\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.027\n",
      "Epoch:10 Batch:10 Loss:0.028\n",
      "Epoch:20 Batch:10 Loss:0.027\n",
      "Epoch:30 Batch:10 Loss:0.029\n",
      "Epoch:40 Batch:10 Loss:0.027\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1064.139456446626 setps: 800 count: 800\n",
      "avg rewards: -1064.139456446626\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.18711\n",
      "Epoch:20 Batch:20 Loss:0.04027\n",
      "Epoch:40 Batch:20 Loss:0.03301\n",
      "Epoch:60 Batch:20 Loss:0.03053\n",
      "Epoch:80 Batch:20 Loss:0.02604\n",
      "Epoch:100 Batch:20 Loss:0.02252\n",
      "Epoch:120 Batch:20 Loss:0.02445\n",
      "Epoch:140 Batch:20 Loss:0.02654\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.030\n",
      "Epoch:10 Batch:10 Loss:0.032\n",
      "Epoch:20 Batch:10 Loss:0.029\n",
      "Epoch:30 Batch:10 Loss:0.030\n",
      "Epoch:40 Batch:10 Loss:0.028\n",
      "Done!\n",
      "############# start AntBulletEnv-v0 training ###################\n",
      "(50, 1000, 28)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 141.98239282669812 setps: 302 count: 302\n",
      "avg rewards: 141.98239282669812\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:1.43120\n",
      "Epoch:20 Batch:1 Loss:0.09363\n",
      "Epoch:40 Batch:1 Loss:0.06562\n",
      "Epoch:60 Batch:1 Loss:0.04271\n",
      "Epoch:80 Batch:1 Loss:0.03523\n",
      "Epoch:100 Batch:1 Loss:0.03101\n",
      "Epoch:120 Batch:1 Loss:0.02702\n",
      "Epoch:140 Batch:1 Loss:0.02396\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.425\n",
      "Epoch:10 Batch:10 Loss:0.426\n",
      "Epoch:20 Batch:10 Loss:0.388\n",
      "Epoch:30 Batch:10 Loss:0.382\n",
      "Epoch:40 Batch:10 Loss:0.382\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 8.150699835298292 setps: 24 count: 24\n",
      "reward: 69.26558395409376 setps: 214 count: 238\n",
      "avg rewards: 38.70814189469603\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:1.31100\n",
      "Epoch:20 Batch:2 Loss:0.05462\n",
      "Epoch:40 Batch:2 Loss:0.04054\n",
      "Epoch:60 Batch:2 Loss:0.03054\n",
      "Epoch:80 Batch:2 Loss:0.02567\n",
      "Epoch:100 Batch:2 Loss:0.02243\n",
      "Epoch:120 Batch:2 Loss:0.01913\n",
      "Epoch:140 Batch:2 Loss:0.01749\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.381\n",
      "Epoch:10 Batch:10 Loss:0.366\n",
      "Epoch:20 Batch:10 Loss:0.363\n",
      "Epoch:30 Batch:10 Loss:0.362\n",
      "Epoch:40 Batch:10 Loss:0.358\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 458.0368502928846 setps: 800 count: 800\n",
      "avg rewards: 458.0368502928846\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:1.16150\n",
      "Epoch:20 Batch:3 Loss:0.04792\n",
      "Epoch:40 Batch:3 Loss:0.02984\n",
      "Epoch:60 Batch:3 Loss:0.02230\n",
      "Epoch:80 Batch:3 Loss:0.01776\n",
      "Epoch:100 Batch:3 Loss:0.01598\n",
      "Epoch:120 Batch:3 Loss:0.01303\n",
      "Epoch:140 Batch:3 Loss:0.01178\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.349\n",
      "Epoch:10 Batch:10 Loss:0.347\n",
      "Epoch:20 Batch:10 Loss:0.342\n",
      "Epoch:30 Batch:10 Loss:0.343\n",
      "Epoch:40 Batch:10 Loss:0.341\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 430.1282769005541 setps: 800 count: 800\n",
      "avg rewards: 430.1282769005541\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:1.00662\n",
      "Epoch:20 Batch:4 Loss:0.04311\n",
      "Epoch:40 Batch:4 Loss:0.02587\n",
      "Epoch:60 Batch:4 Loss:0.01863\n",
      "Epoch:80 Batch:4 Loss:0.01506\n",
      "Epoch:100 Batch:4 Loss:0.01294\n",
      "Epoch:120 Batch:4 Loss:0.01158\n",
      "Epoch:140 Batch:4 Loss:0.01044\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.351\n",
      "Epoch:10 Batch:10 Loss:0.345\n",
      "Epoch:20 Batch:10 Loss:0.338\n",
      "Epoch:30 Batch:10 Loss:0.337\n",
      "Epoch:40 Batch:10 Loss:0.334\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 321.0584979161908 setps: 800 count: 800\n",
      "avg rewards: 321.0584979161908\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.70698\n",
      "Epoch:20 Batch:5 Loss:0.03894\n",
      "Epoch:40 Batch:5 Loss:0.02472\n",
      "Epoch:60 Batch:5 Loss:0.01869\n",
      "Epoch:80 Batch:5 Loss:0.01430\n",
      "Epoch:100 Batch:5 Loss:0.01275\n",
      "Epoch:120 Batch:5 Loss:0.01143\n",
      "Epoch:140 Batch:5 Loss:0.01069\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.344\n",
      "Epoch:10 Batch:10 Loss:0.340\n",
      "Epoch:20 Batch:10 Loss:0.331\n",
      "Epoch:30 Batch:10 Loss:0.332\n",
      "Epoch:40 Batch:10 Loss:0.333\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 8.452546791851635 setps: 21 count: 21\n",
      "reward: 8.228160938261134 setps: 21 count: 42\n",
      "reward: 8.564777130701984 setps: 23 count: 65\n",
      "reward: 128.25350515092003 setps: 263 count: 328\n",
      "reward: 150.23850595249672 setps: 261 count: 589\n",
      "reward: 36.088224939831704 setps: 78 count: 667\n",
      "reward: 8.453847256748121 setps: 21 count: 688\n",
      "avg rewards: 49.75422402297305\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.50539\n",
      "Epoch:20 Batch:6 Loss:0.03477\n",
      "Epoch:40 Batch:6 Loss:0.01956\n",
      "Epoch:60 Batch:6 Loss:0.01555\n",
      "Epoch:80 Batch:6 Loss:0.01385\n",
      "Epoch:100 Batch:6 Loss:0.01198\n",
      "Epoch:120 Batch:6 Loss:0.01195\n",
      "Epoch:140 Batch:6 Loss:0.01048\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.337\n",
      "Epoch:10 Batch:10 Loss:0.322\n",
      "Epoch:20 Batch:10 Loss:0.320\n",
      "Epoch:30 Batch:10 Loss:0.321\n",
      "Epoch:40 Batch:10 Loss:0.322\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 21.285563356356583 setps: 30 count: 30\n",
      "reward: 27.909131018030163 setps: 40 count: 70\n",
      "reward: 21.87881303974573 setps: 33 count: 103\n",
      "reward: 27.198945267085218 setps: 43 count: 146\n",
      "reward: 18.90312914534152 setps: 29 count: 175\n",
      "reward: 66.39977048627625 setps: 103 count: 278\n",
      "reward: 16.02885938541294 setps: 26 count: 304\n",
      "reward: 32.84284929915884 setps: 46 count: 350\n",
      "reward: 17.689743734493092 setps: 28 count: 378\n",
      "reward: 24.92136731369828 setps: 35 count: 413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 44.447714006106246 setps: 59 count: 472\n",
      "reward: 19.327118490454453 setps: 30 count: 502\n",
      "reward: 11.52187422842253 setps: 22 count: 524\n",
      "reward: 13.608187355287372 setps: 25 count: 549\n",
      "reward: 29.915290483378342 setps: 44 count: 593\n",
      "reward: 21.317333722018525 setps: 33 count: 626\n",
      "reward: 40.37096327590844 setps: 50 count: 676\n",
      "reward: 17.24093544108764 setps: 28 count: 704\n",
      "reward: 25.101111019155365 setps: 39 count: 743\n",
      "reward: 20.58906144291832 setps: 32 count: 775\n",
      "reward: 26.626928231318015 setps: 37 count: 812\n",
      "reward: 15.39330766860512 setps: 26 count: 838\n",
      "reward: 19.85420233317709 setps: 31 count: 869\n",
      "reward: 26.304727283450486 setps: 37 count: 906\n",
      "reward: 25.626413016299193 setps: 37 count: 943\n",
      "reward: 15.295847402415529 setps: 25 count: 968\n",
      "avg rewards: 24.90766105560005\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.82247\n",
      "Epoch:20 Batch:7 Loss:0.02752\n",
      "Epoch:40 Batch:7 Loss:0.01795\n",
      "Epoch:60 Batch:7 Loss:0.01493\n",
      "Epoch:80 Batch:7 Loss:0.01395\n",
      "Epoch:100 Batch:7 Loss:0.01176\n",
      "Epoch:120 Batch:7 Loss:0.01168\n",
      "Epoch:140 Batch:7 Loss:0.01083\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.307\n",
      "Epoch:10 Batch:10 Loss:0.306\n",
      "Epoch:20 Batch:10 Loss:0.306\n",
      "Epoch:30 Batch:10 Loss:0.304\n",
      "Epoch:40 Batch:10 Loss:0.301\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 17.400013661467533 setps: 38 count: 38\n",
      "reward: 228.1670319966158 setps: 408 count: 446\n",
      "reward: 107.70641905031518 setps: 184 count: 630\n",
      "reward: 31.307134170820063 setps: 75 count: 705\n",
      "avg rewards: 96.14514971980465\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.74219\n",
      "Epoch:20 Batch:8 Loss:0.02646\n",
      "Epoch:40 Batch:8 Loss:0.01811\n",
      "Epoch:60 Batch:8 Loss:0.01465\n",
      "Epoch:80 Batch:8 Loss:0.01329\n",
      "Epoch:100 Batch:8 Loss:0.01241\n",
      "Epoch:120 Batch:8 Loss:0.01167\n",
      "Epoch:140 Batch:8 Loss:0.01169\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.324\n",
      "Epoch:10 Batch:10 Loss:0.320\n",
      "Epoch:20 Batch:10 Loss:0.322\n",
      "Epoch:30 Batch:10 Loss:0.316\n",
      "Epoch:40 Batch:10 Loss:0.316\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 371.610763546308 setps: 800 count: 800\n",
      "avg rewards: 371.610763546308\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.31649\n",
      "Epoch:20 Batch:9 Loss:0.02460\n",
      "Epoch:40 Batch:9 Loss:0.01665\n",
      "Epoch:60 Batch:9 Loss:0.01349\n",
      "Epoch:80 Batch:9 Loss:0.01297\n",
      "Epoch:100 Batch:9 Loss:0.01210\n",
      "Epoch:120 Batch:9 Loss:0.01153\n",
      "Epoch:140 Batch:9 Loss:0.01130\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.306\n",
      "Epoch:10 Batch:10 Loss:0.297\n",
      "Epoch:20 Batch:10 Loss:0.294\n",
      "Epoch:30 Batch:10 Loss:0.294\n",
      "Epoch:40 Batch:10 Loss:0.295\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 293.31206676177396 setps: 800 count: 800\n",
      "avg rewards: 293.31206676177396\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.39208\n",
      "Epoch:20 Batch:10 Loss:0.02227\n",
      "Epoch:40 Batch:10 Loss:0.01750\n",
      "Epoch:60 Batch:10 Loss:0.01279\n",
      "Epoch:80 Batch:10 Loss:0.01320\n",
      "Epoch:100 Batch:10 Loss:0.01159\n",
      "Epoch:120 Batch:10 Loss:0.01053\n",
      "Epoch:140 Batch:10 Loss:0.01067\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.307\n",
      "Epoch:10 Batch:10 Loss:0.298\n",
      "Epoch:20 Batch:10 Loss:0.294\n",
      "Epoch:30 Batch:10 Loss:0.298\n",
      "Epoch:40 Batch:10 Loss:0.299\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 264.50550877103683 setps: 770 count: 770\n",
      "avg rewards: 264.50550877103683\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.34766\n",
      "Epoch:20 Batch:11 Loss:0.02086\n",
      "Epoch:40 Batch:11 Loss:0.01494\n",
      "Epoch:60 Batch:11 Loss:0.01281\n",
      "Epoch:80 Batch:11 Loss:0.01199\n",
      "Epoch:100 Batch:11 Loss:0.01197\n",
      "Epoch:120 Batch:11 Loss:0.01046\n",
      "Epoch:140 Batch:11 Loss:0.00967\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.321\n",
      "Epoch:10 Batch:10 Loss:0.310\n",
      "Epoch:20 Batch:10 Loss:0.306\n",
      "Epoch:30 Batch:10 Loss:0.305\n",
      "Epoch:40 Batch:10 Loss:0.307\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 30.76565861020208 setps: 39 count: 39\n",
      "reward: 41.40821194622113 setps: 51 count: 90\n",
      "reward: 36.949977005382244 setps: 45 count: 135\n",
      "reward: 36.263499866287745 setps: 47 count: 182\n",
      "reward: 38.364557711503586 setps: 49 count: 231\n",
      "reward: 27.68397811111063 setps: 38 count: 269\n",
      "reward: 30.76649375356646 setps: 42 count: 311\n",
      "reward: 36.69533223312318 setps: 53 count: 364\n",
      "avg rewards: 34.86221365467463\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.32281\n",
      "Epoch:20 Batch:12 Loss:0.01947\n",
      "Epoch:40 Batch:12 Loss:0.01563\n",
      "Epoch:60 Batch:12 Loss:0.01337\n",
      "Epoch:80 Batch:12 Loss:0.01274\n",
      "Epoch:100 Batch:12 Loss:0.01166\n",
      "Epoch:120 Batch:12 Loss:0.01126\n",
      "Epoch:140 Batch:12 Loss:0.00996\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.311\n",
      "Epoch:10 Batch:10 Loss:0.308\n",
      "Epoch:20 Batch:10 Loss:0.303\n",
      "Epoch:30 Batch:10 Loss:0.298\n",
      "Epoch:40 Batch:10 Loss:0.297\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 441.96356902309503 setps: 800 count: 800\n",
      "avg rewards: 441.96356902309503\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.29864\n",
      "Epoch:20 Batch:13 Loss:0.02006\n",
      "Epoch:40 Batch:13 Loss:0.01436\n",
      "Epoch:60 Batch:13 Loss:0.01334\n",
      "Epoch:80 Batch:13 Loss:0.01124\n",
      "Epoch:100 Batch:13 Loss:0.01118\n",
      "Epoch:120 Batch:13 Loss:0.00992\n",
      "Epoch:140 Batch:13 Loss:0.01041\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.293\n",
      "Epoch:10 Batch:10 Loss:0.289\n",
      "Epoch:20 Batch:10 Loss:0.286\n",
      "Epoch:30 Batch:10 Loss:0.286\n",
      "Epoch:40 Batch:10 Loss:0.286\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 186.3805978869324 setps: 800 count: 800\n",
      "avg rewards: 186.3805978869324\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.25593\n",
      "Epoch:20 Batch:14 Loss:0.02110\n",
      "Epoch:40 Batch:14 Loss:0.01470\n",
      "Epoch:60 Batch:14 Loss:0.01345\n",
      "Epoch:80 Batch:14 Loss:0.01188\n",
      "Epoch:100 Batch:14 Loss:0.01092\n",
      "Epoch:120 Batch:14 Loss:0.01092\n",
      "Epoch:140 Batch:14 Loss:0.01045\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.257\n",
      "Epoch:10 Batch:10 Loss:0.251\n",
      "Epoch:20 Batch:10 Loss:0.246\n",
      "Epoch:30 Batch:10 Loss:0.248\n",
      "Epoch:40 Batch:10 Loss:0.245\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 171.7624095824577 setps: 800 count: 800\n",
      "avg rewards: 171.7624095824577\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.27196\n",
      "Epoch:20 Batch:15 Loss:0.01968\n",
      "Epoch:40 Batch:15 Loss:0.01668\n",
      "Epoch:60 Batch:15 Loss:0.01302\n",
      "Epoch:80 Batch:15 Loss:0.01178\n",
      "Epoch:100 Batch:15 Loss:0.01114\n",
      "Epoch:120 Batch:15 Loss:0.01019\n",
      "Epoch:140 Batch:15 Loss:0.01048\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.145\n",
      "Epoch:10 Batch:10 Loss:0.140\n",
      "Epoch:20 Batch:10 Loss:0.141\n",
      "Epoch:30 Batch:10 Loss:0.135\n",
      "Epoch:40 Batch:10 Loss:0.138\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -260.0051247161826 setps: 800 count: 800\n",
      "avg rewards: -260.0051247161826\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.31953\n",
      "Epoch:20 Batch:16 Loss:0.02187\n",
      "Epoch:40 Batch:16 Loss:0.01506\n",
      "Epoch:60 Batch:16 Loss:0.01324\n",
      "Epoch:80 Batch:16 Loss:0.01239\n",
      "Epoch:100 Batch:16 Loss:0.01195\n",
      "Epoch:120 Batch:16 Loss:0.01007\n",
      "Epoch:140 Batch:16 Loss:0.01028\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.126\n",
      "Epoch:10 Batch:10 Loss:0.124\n",
      "Epoch:20 Batch:10 Loss:0.126\n",
      "Epoch:30 Batch:10 Loss:0.124\n",
      "Epoch:40 Batch:10 Loss:0.125\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -362.367355488564 setps: 800 count: 800\n",
      "avg rewards: -362.367355488564\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.23224\n",
      "Epoch:20 Batch:17 Loss:0.01970\n",
      "Epoch:40 Batch:17 Loss:0.01466\n",
      "Epoch:60 Batch:17 Loss:0.01359\n",
      "Epoch:80 Batch:17 Loss:0.01111\n",
      "Epoch:100 Batch:17 Loss:0.01005\n",
      "Epoch:120 Batch:17 Loss:0.01034\n",
      "Epoch:140 Batch:17 Loss:0.00940\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:10 Loss:0.123\n",
      "Epoch:10 Batch:10 Loss:0.113\n",
      "Epoch:20 Batch:10 Loss:0.111\n",
      "Epoch:30 Batch:10 Loss:0.112\n",
      "Epoch:40 Batch:10 Loss:0.111\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1291.3393175996066 setps: 800 count: 800\n",
      "avg rewards: -1291.3393175996066\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.18677\n",
      "Epoch:20 Batch:18 Loss:0.02321\n",
      "Epoch:40 Batch:18 Loss:0.01630\n",
      "Epoch:60 Batch:18 Loss:0.01521\n",
      "Epoch:80 Batch:18 Loss:0.01322\n",
      "Epoch:100 Batch:18 Loss:0.01175\n",
      "Epoch:120 Batch:18 Loss:0.01004\n",
      "Epoch:140 Batch:18 Loss:0.00959\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.116\n",
      "Epoch:10 Batch:10 Loss:0.105\n",
      "Epoch:20 Batch:10 Loss:0.103\n",
      "Epoch:30 Batch:10 Loss:0.100\n",
      "Epoch:40 Batch:10 Loss:0.098\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -230.77416552437222 setps: 800 count: 800\n",
      "avg rewards: -230.77416552437222\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.17884\n",
      "Epoch:20 Batch:19 Loss:0.01999\n",
      "Epoch:40 Batch:19 Loss:0.01521\n",
      "Epoch:60 Batch:19 Loss:0.01318\n",
      "Epoch:80 Batch:19 Loss:0.01186\n",
      "Epoch:100 Batch:19 Loss:0.01180\n",
      "Epoch:120 Batch:19 Loss:0.00960\n",
      "Epoch:140 Batch:19 Loss:0.00912\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.091\n",
      "Epoch:10 Batch:10 Loss:0.088\n",
      "Epoch:20 Batch:10 Loss:0.085\n",
      "Epoch:30 Batch:10 Loss:0.085\n",
      "Epoch:40 Batch:10 Loss:0.086\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -805.8553482183115 setps: 800 count: 800\n",
      "avg rewards: -805.8553482183115\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.16959\n",
      "Epoch:20 Batch:20 Loss:0.01834\n",
      "Epoch:40 Batch:20 Loss:0.01529\n",
      "Epoch:60 Batch:20 Loss:0.01295\n",
      "Epoch:80 Batch:20 Loss:0.01221\n",
      "Epoch:100 Batch:20 Loss:0.01009\n",
      "Epoch:120 Batch:20 Loss:0.01065\n",
      "Epoch:140 Batch:20 Loss:0.00971\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.092\n",
      "Epoch:10 Batch:10 Loss:0.091\n",
      "Epoch:20 Batch:10 Loss:0.091\n",
      "Epoch:30 Batch:10 Loss:0.089\n",
      "Epoch:40 Batch:10 Loss:0.091\n",
      "Done!\n",
      "############# start HumanoidBulletEnv-v0 training ###################\n",
      "(49,)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -23.930504222029416 setps: 21 count: 21\n",
      "reward: -44.832111978673495 setps: 19 count: 40\n",
      "reward: -33.58876023627963 setps: 19 count: 59\n",
      "reward: -25.92361880076642 setps: 19 count: 78\n",
      "reward: -26.585914267043705 setps: 20 count: 98\n",
      "reward: -28.786084618244786 setps: 19 count: 117\n",
      "reward: -34.1914647980986 setps: 17 count: 134\n",
      "reward: -30.358921716589247 setps: 20 count: 154\n",
      "reward: -18.08054182232445 setps: 19 count: 173\n",
      "reward: -57.65301880087355 setps: 19 count: 192\n",
      "reward: -27.16646399433666 setps: 20 count: 212\n",
      "reward: -28.022576801269317 setps: 19 count: 231\n",
      "reward: -36.95166069329862 setps: 19 count: 250\n",
      "reward: -27.409624021807392 setps: 22 count: 272\n",
      "reward: -23.648219335862084 setps: 17 count: 289\n",
      "reward: -22.391593500814636 setps: 18 count: 307\n",
      "reward: -32.326349868203394 setps: 19 count: 326\n",
      "reward: -31.04949865229865 setps: 19 count: 345\n",
      "reward: -30.520214039421994 setps: 19 count: 364\n",
      "reward: -30.10761921199737 setps: 18 count: 382\n",
      "reward: -17.6856600570376 setps: 17 count: 399\n",
      "reward: -38.02910445531306 setps: 18 count: 417\n",
      "reward: -41.96597049820265 setps: 19 count: 436\n",
      "reward: -22.744429027616572 setps: 20 count: 456\n",
      "reward: -27.466838021275187 setps: 21 count: 477\n",
      "reward: -25.418721662537425 setps: 22 count: 499\n",
      "reward: -13.383837923497778 setps: 20 count: 519\n",
      "reward: -29.334350272355362 setps: 19 count: 538\n",
      "reward: -32.68848228619609 setps: 19 count: 557\n",
      "reward: -29.951686735973638 setps: 21 count: 578\n",
      "reward: -19.160243210608318 setps: 18 count: 596\n",
      "reward: -16.999182235320042 setps: 18 count: 614\n",
      "reward: -26.010695673814915 setps: 19 count: 633\n",
      "reward: -22.91498172552674 setps: 18 count: 651\n",
      "reward: -34.79813765357831 setps: 19 count: 670\n",
      "reward: -44.06062952949869 setps: 17 count: 687\n",
      "reward: -30.75969831473048 setps: 19 count: 706\n",
      "reward: -19.832492106322025 setps: 19 count: 725\n",
      "reward: -31.070794982687215 setps: 19 count: 744\n",
      "reward: -47.582544886093814 setps: 18 count: 762\n",
      "reward: -66.7593067625392 setps: 33 count: 795\n",
      "reward: -41.14785905380995 setps: 22 count: 817\n",
      "reward: -40.77110538025736 setps: 18 count: 835\n",
      "reward: -31.554773128077795 setps: 19 count: 854\n",
      "reward: -27.048210133392423 setps: 19 count: 873\n",
      "reward: -42.03738607068517 setps: 18 count: 891\n",
      "reward: -17.79626596370508 setps: 21 count: 912\n",
      "reward: -28.548658798259563 setps: 19 count: 931\n",
      "reward: -30.842491852634698 setps: 18 count: 949\n",
      "reward: -23.387407220486782 setps: 19 count: 968\n",
      "reward: -30.513364205423564 setps: 19 count: 987\n",
      "avg rewards: -30.701766102111588\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:1.64698\n",
      "Epoch:20 Batch:1 Loss:0.54791\n",
      "Epoch:40 Batch:1 Loss:0.45409\n",
      "Epoch:60 Batch:1 Loss:0.38045\n",
      "Epoch:80 Batch:1 Loss:0.32984\n",
      "Epoch:100 Batch:1 Loss:0.28337\n",
      "Epoch:120 Batch:1 Loss:0.24888\n",
      "Epoch:140 Batch:1 Loss:0.23041\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.243\n",
      "Epoch:10 Batch:8 Loss:0.228\n",
      "Epoch:20 Batch:8 Loss:0.223\n",
      "Epoch:30 Batch:8 Loss:0.219\n",
      "Epoch:40 Batch:8 Loss:0.216\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 14.696634256051041 setps: 24 count: 24\n",
      "reward: 13.727309434275956 setps: 22 count: 46\n",
      "reward: 21.786962872438014 setps: 24 count: 70\n",
      "reward: 12.423504254159344 setps: 23 count: 93\n",
      "reward: 13.30418935319176 setps: 23 count: 116\n",
      "reward: 15.511981337564066 setps: 25 count: 141\n",
      "reward: 16.045581283842324 setps: 24 count: 165\n",
      "reward: 25.148959554212347 setps: 25 count: 190\n",
      "reward: 18.830655482075233 setps: 24 count: 214\n",
      "reward: 8.192130582277604 setps: 22 count: 236\n",
      "reward: 13.710216857554043 setps: 25 count: 261\n",
      "reward: 26.429677479885868 setps: 25 count: 286\n",
      "reward: 16.708712799429485 setps: 23 count: 309\n",
      "reward: 11.639956069247273 setps: 24 count: 333\n",
      "reward: 18.162594777341287 setps: 23 count: 356\n",
      "reward: 10.595016791090892 setps: 22 count: 378\n",
      "reward: 13.037319754669442 setps: 22 count: 400\n",
      "reward: 7.307761522529474 setps: 16 count: 416\n",
      "reward: 11.86342595408496 setps: 22 count: 438\n",
      "reward: 17.711694702738892 setps: 22 count: 460\n",
      "reward: 15.797665886192405 setps: 24 count: 484\n",
      "reward: 17.39433738838415 setps: 26 count: 510\n",
      "reward: 21.907321191563092 setps: 25 count: 535\n",
      "reward: 12.457313652380252 setps: 21 count: 556\n",
      "reward: 9.352482225881246 setps: 18 count: 574\n",
      "reward: 12.867223738553005 setps: 22 count: 596\n",
      "reward: 8.150010097520134 setps: 20 count: 616\n",
      "reward: 16.897333505544523 setps: 23 count: 639\n",
      "reward: 11.877197243858241 setps: 20 count: 659\n",
      "reward: 16.520423023140754 setps: 24 count: 683\n",
      "reward: 19.531188699291672 setps: 24 count: 707\n",
      "reward: 9.724652294718542 setps: 23 count: 730\n",
      "reward: 14.648444978559562 setps: 22 count: 752\n",
      "reward: 17.255188262053707 setps: 22 count: 774\n",
      "reward: 14.548381699260792 setps: 22 count: 796\n",
      "reward: -2.338324791716877 setps: 14 count: 810\n",
      "reward: 2.1504071847346484 setps: 14 count: 824\n",
      "reward: 9.339233284676444 setps: 18 count: 842\n",
      "reward: 19.71456935262686 setps: 25 count: 867\n",
      "reward: 20.508711025855156 setps: 23 count: 890\n",
      "reward: 22.761426336204746 setps: 27 count: 917\n",
      "reward: 16.785009334592907 setps: 24 count: 941\n",
      "reward: 13.245443211303789 setps: 24 count: 965\n",
      "reward: 12.142916637859887 setps: 26 count: 991\n",
      "avg rewards: 14.547110013220431\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:1.49426\n",
      "Epoch:20 Batch:2 Loss:0.37031\n",
      "Epoch:40 Batch:2 Loss:0.26737\n",
      "Epoch:60 Batch:2 Loss:0.23098\n",
      "Epoch:80 Batch:2 Loss:0.19448\n",
      "Epoch:100 Batch:2 Loss:0.17000\n",
      "Epoch:120 Batch:2 Loss:0.15702\n",
      "Epoch:140 Batch:2 Loss:0.14050\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.223\n",
      "Epoch:10 Batch:8 Loss:0.205\n",
      "Epoch:20 Batch:8 Loss:0.198\n",
      "Epoch:30 Batch:8 Loss:0.197\n",
      "Epoch:40 Batch:8 Loss:0.193\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 55.99234999828623 setps: 50 count: 50\n",
      "reward: 19.217134692976835 setps: 27 count: 77\n",
      "reward: 45.78445458286005 setps: 40 count: 117\n",
      "reward: 34.81692472313153 setps: 45 count: 162\n",
      "reward: 42.359986983310954 setps: 42 count: 204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 48.469255104713376 setps: 40 count: 244\n",
      "reward: 55.77719914746413 setps: 45 count: 289\n",
      "reward: 26.3748645547952 setps: 42 count: 331\n",
      "reward: 11.628819039340305 setps: 19 count: 350\n",
      "reward: 5.426477454886481 setps: 17 count: 367\n",
      "reward: 68.2795807303919 setps: 57 count: 424\n",
      "reward: 55.28016901438677 setps: 54 count: 478\n",
      "reward: 47.14519255143823 setps: 40 count: 518\n",
      "reward: 65.07370776103345 setps: 47 count: 565\n",
      "reward: 61.20208845200394 setps: 49 count: 614\n",
      "reward: 52.8361121491922 setps: 44 count: 658\n",
      "reward: 55.38802169049304 setps: 46 count: 704\n",
      "reward: 81.3277013460145 setps: 64 count: 768\n",
      "reward: 61.36255152252997 setps: 50 count: 818\n",
      "reward: 43.0865718768924 setps: 86 count: 904\n",
      "reward: 9.72129132808768 setps: 17 count: 921\n",
      "reward: 46.69422603702406 setps: 45 count: 966\n",
      "avg rewards: 45.14748548823878\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:1.40104\n",
      "Epoch:20 Batch:3 Loss:0.25626\n",
      "Epoch:40 Batch:3 Loss:0.19401\n",
      "Epoch:60 Batch:3 Loss:0.16222\n",
      "Epoch:80 Batch:3 Loss:0.13426\n",
      "Epoch:100 Batch:3 Loss:0.12618\n",
      "Epoch:120 Batch:3 Loss:0.11896\n",
      "Epoch:140 Batch:3 Loss:0.11362\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.180\n",
      "Epoch:10 Batch:8 Loss:0.170\n",
      "Epoch:20 Batch:8 Loss:0.168\n",
      "Epoch:30 Batch:8 Loss:0.167\n",
      "Epoch:40 Batch:8 Loss:0.163\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 14.432624388027763 setps: 20 count: 20\n",
      "reward: 14.860711914893182 setps: 20 count: 40\n",
      "reward: 19.090884416796325 setps: 21 count: 61\n",
      "reward: 19.011154867430744 setps: 21 count: 82\n",
      "reward: 16.184941500384596 setps: 20 count: 102\n",
      "reward: 6.1730444417946275 setps: 17 count: 119\n",
      "reward: 13.904851479563513 setps: 19 count: 138\n",
      "reward: 16.728013449734135 setps: 27 count: 165\n",
      "reward: 16.82887942302623 setps: 20 count: 185\n",
      "reward: 19.80420623702375 setps: 23 count: 208\n",
      "reward: 16.282233138330042 setps: 20 count: 228\n",
      "reward: 8.999688949115807 setps: 18 count: 246\n",
      "reward: 21.08286854101316 setps: 22 count: 268\n",
      "reward: 19.40908812588168 setps: 21 count: 289\n",
      "reward: 15.932532110804459 setps: 20 count: 309\n",
      "reward: 20.10862417931639 setps: 22 count: 331\n",
      "reward: 16.571954358223586 setps: 20 count: 351\n",
      "reward: 9.339637096943623 setps: 18 count: 369\n",
      "reward: 14.828415843218677 setps: 20 count: 389\n",
      "reward: 12.864003771226272 setps: 18 count: 407\n",
      "reward: 14.79062754706101 setps: 20 count: 427\n",
      "reward: 15.89335559707397 setps: 20 count: 447\n",
      "reward: 17.044601621663602 setps: 20 count: 467\n",
      "reward: 10.250033073883968 setps: 18 count: 485\n",
      "reward: 18.20884516144433 setps: 20 count: 505\n",
      "reward: 15.218682598753361 setps: 19 count: 524\n",
      "reward: 13.472011727726205 setps: 19 count: 543\n",
      "reward: 18.72567051389051 setps: 22 count: 565\n",
      "reward: 17.67266776197939 setps: 26 count: 591\n",
      "reward: 22.61366910418147 setps: 23 count: 614\n",
      "reward: 10.675047371443359 setps: 17 count: 631\n",
      "reward: 21.642801426802183 setps: 26 count: 657\n",
      "reward: 17.99114398746024 setps: 20 count: 677\n",
      "reward: 16.517943689895034 setps: 20 count: 697\n",
      "reward: 15.477483285847114 setps: 20 count: 717\n",
      "reward: 12.457814882554523 setps: 19 count: 736\n",
      "reward: 4.400001849685213 setps: 18 count: 754\n",
      "reward: 15.32818087774067 setps: 19 count: 773\n",
      "reward: 16.137303355624315 setps: 20 count: 793\n",
      "reward: 15.211741234709914 setps: 20 count: 813\n",
      "reward: 20.949491380518882 setps: 26 count: 839\n",
      "reward: 11.406194802194657 setps: 18 count: 857\n",
      "reward: 4.1184887784533215 setps: 17 count: 874\n",
      "reward: 19.039299769375067 setps: 22 count: 896\n",
      "reward: 10.546906319589471 setps: 18 count: 914\n",
      "reward: 20.848303361021674 setps: 23 count: 937\n",
      "reward: 20.168955821215057 setps: 22 count: 959\n",
      "reward: 16.512182855198624 setps: 20 count: 979\n",
      "reward: 16.873334643758426 setps: 20 count: 999\n",
      "avg rewards: 15.56390087007131\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:1.28001\n",
      "Epoch:20 Batch:4 Loss:0.21978\n",
      "Epoch:40 Batch:4 Loss:0.16639\n",
      "Epoch:60 Batch:4 Loss:0.13281\n",
      "Epoch:80 Batch:4 Loss:0.11947\n",
      "Epoch:100 Batch:4 Loss:0.11413\n",
      "Epoch:120 Batch:4 Loss:0.11403\n",
      "Epoch:140 Batch:4 Loss:0.09800\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.178\n",
      "Epoch:10 Batch:8 Loss:0.171\n",
      "Epoch:20 Batch:8 Loss:0.169\n",
      "Epoch:30 Batch:8 Loss:0.167\n",
      "Epoch:40 Batch:8 Loss:0.169\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 25.91164404763695 setps: 23 count: 23\n",
      "reward: 25.464976297410615 setps: 26 count: 49\n",
      "reward: 25.246461161714976 setps: 23 count: 72\n",
      "reward: 27.38483193639259 setps: 31 count: 103\n",
      "reward: 26.471527957057692 setps: 27 count: 130\n",
      "reward: 22.178768354649947 setps: 25 count: 155\n",
      "reward: 23.457507174080817 setps: 26 count: 181\n",
      "reward: 20.91915722540288 setps: 22 count: 203\n",
      "reward: 14.256931796050049 setps: 17 count: 220\n",
      "reward: 26.621726364224735 setps: 25 count: 245\n",
      "reward: 23.912533131778765 setps: 27 count: 272\n",
      "reward: 23.26126768525573 setps: 25 count: 297\n",
      "reward: 32.04566657063551 setps: 30 count: 327\n",
      "reward: 23.487916428192694 setps: 26 count: 353\n",
      "reward: 14.272927084584076 setps: 19 count: 372\n",
      "reward: 24.642028964639756 setps: 26 count: 398\n",
      "reward: 25.96977320134173 setps: 28 count: 426\n",
      "reward: 26.93123524233524 setps: 27 count: 453\n",
      "reward: 25.272056030218668 setps: 27 count: 480\n",
      "reward: 24.704682124727693 setps: 25 count: 505\n",
      "reward: 26.42479804505419 setps: 27 count: 532\n",
      "reward: 25.779688330330835 setps: 26 count: 558\n",
      "reward: 26.175760409059876 setps: 26 count: 584\n",
      "reward: 28.96958259156527 setps: 27 count: 611\n",
      "reward: 25.949405658975586 setps: 26 count: 637\n",
      "reward: 20.13493065946532 setps: 20 count: 657\n",
      "reward: 22.328025570604943 setps: 26 count: 683\n",
      "reward: 24.85659567799448 setps: 27 count: 710\n",
      "reward: 25.79166868348839 setps: 27 count: 737\n",
      "reward: 24.490275221114285 setps: 26 count: 763\n",
      "reward: 23.125921237540023 setps: 25 count: 788\n",
      "reward: 25.766536824303323 setps: 28 count: 816\n",
      "reward: 26.445154216629454 setps: 27 count: 843\n",
      "reward: 24.881456448906107 setps: 22 count: 865\n",
      "reward: 18.706284722838607 setps: 19 count: 884\n",
      "reward: 19.392204187171597 setps: 21 count: 905\n",
      "reward: 25.378809263733277 setps: 26 count: 931\n",
      "reward: 28.446276688507346 setps: 27 count: 958\n",
      "reward: 22.27151287913876 setps: 22 count: 980\n",
      "avg rewards: 24.300730925506485\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:1.13734\n",
      "Epoch:20 Batch:5 Loss:0.17250\n",
      "Epoch:40 Batch:5 Loss:0.12409\n",
      "Epoch:60 Batch:5 Loss:0.11043\n",
      "Epoch:80 Batch:5 Loss:0.09610\n",
      "Epoch:100 Batch:5 Loss:0.09170\n",
      "Epoch:120 Batch:5 Loss:0.09228\n",
      "Epoch:140 Batch:5 Loss:0.08450\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.156\n",
      "Epoch:10 Batch:8 Loss:0.155\n",
      "Epoch:20 Batch:8 Loss:0.152\n",
      "Epoch:30 Batch:8 Loss:0.150\n",
      "Epoch:40 Batch:8 Loss:0.151\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 56.51148413228218 setps: 47 count: 47\n",
      "reward: 50.79018349257238 setps: 38 count: 85\n",
      "reward: 17.297782462945907 setps: 20 count: 105\n",
      "reward: 65.75599267235084 setps: 48 count: 153\n",
      "reward: 18.361694496481505 setps: 20 count: 173\n",
      "reward: 61.915291310286555 setps: 51 count: 224\n",
      "reward: 63.54148225201791 setps: 46 count: 270\n",
      "reward: 50.20093673799129 setps: 43 count: 313\n",
      "reward: 82.95801207421174 setps: 62 count: 375\n",
      "reward: 37.55095833162195 setps: 36 count: 411\n",
      "reward: 80.68270333390828 setps: 59 count: 470\n",
      "reward: 62.31422397370626 setps: 50 count: 520\n",
      "reward: 54.38467326054815 setps: 41 count: 561\n",
      "reward: 11.063094965489288 setps: 18 count: 579\n",
      "reward: 18.054999268488604 setps: 21 count: 600\n",
      "reward: 67.68084572268855 setps: 89 count: 689\n",
      "reward: 38.57373407070555 setps: 39 count: 728\n",
      "reward: 18.67174622934835 setps: 20 count: 748\n",
      "reward: 52.00435970164255 setps: 38 count: 786\n",
      "reward: 50.92110310602439 setps: 67 count: 853\n",
      "reward: 59.718083631097414 setps: 55 count: 908\n",
      "reward: 63.74601671377894 setps: 51 count: 959\n",
      "reward: 12.403065466810947 setps: 17 count: 976\n",
      "reward: 13.537938328256132 setps: 17 count: 993\n",
      "avg rewards: 46.19335023896898\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:1.06260\n",
      "Epoch:20 Batch:6 Loss:0.15471\n",
      "Epoch:40 Batch:6 Loss:0.11433\n",
      "Epoch:60 Batch:6 Loss:0.10430\n",
      "Epoch:80 Batch:6 Loss:0.09081\n",
      "Epoch:100 Batch:6 Loss:0.08948\n",
      "Epoch:120 Batch:6 Loss:0.07947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:140 Batch:6 Loss:0.07858\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.108\n",
      "Epoch:10 Batch:8 Loss:0.104\n",
      "Epoch:20 Batch:8 Loss:0.110\n",
      "Epoch:30 Batch:8 Loss:0.107\n",
      "Epoch:40 Batch:8 Loss:0.108\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 59.154131272583626 setps: 43 count: 43\n",
      "reward: 35.94917407883912 setps: 58 count: 101\n",
      "reward: 53.030173748880046 setps: 57 count: 158\n",
      "reward: 90.58228495606892 setps: 88 count: 246\n",
      "reward: 32.92895755471108 setps: 34 count: 280\n",
      "reward: 81.14471393080602 setps: 62 count: 342\n",
      "reward: 18.149205389647975 setps: 20 count: 362\n",
      "reward: 79.00022144580143 setps: 57 count: 419\n",
      "reward: 18.13840992522018 setps: 19 count: 438\n",
      "reward: 8.219554726620844 setps: 17 count: 455\n",
      "reward: 58.407180488329324 setps: 55 count: 510\n",
      "reward: 42.89072834961699 setps: 36 count: 546\n",
      "reward: 60.31454158210691 setps: 47 count: 593\n",
      "reward: 73.2372278967596 setps: 55 count: 648\n",
      "reward: 71.53300261065161 setps: 56 count: 704\n",
      "reward: 57.21185721541114 setps: 57 count: 761\n",
      "reward: 47.64433664573298 setps: 67 count: 828\n",
      "reward: 17.967672254313943 setps: 20 count: 848\n",
      "reward: 97.01291237989938 setps: 87 count: 935\n",
      "avg rewards: 52.76401507642112\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:1.24016\n",
      "Epoch:20 Batch:7 Loss:0.13414\n",
      "Epoch:40 Batch:7 Loss:0.10673\n",
      "Epoch:60 Batch:7 Loss:0.09752\n",
      "Epoch:80 Batch:7 Loss:0.08521\n",
      "Epoch:100 Batch:7 Loss:0.08060\n",
      "Epoch:120 Batch:7 Loss:0.08296\n",
      "Epoch:140 Batch:7 Loss:0.07461\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.088\n",
      "Epoch:10 Batch:8 Loss:0.085\n",
      "Epoch:20 Batch:8 Loss:0.088\n",
      "Epoch:30 Batch:8 Loss:0.091\n",
      "Epoch:40 Batch:8 Loss:0.086\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 65.24446138904747 setps: 46 count: 46\n",
      "reward: 43.987962657460585 setps: 31 count: 77\n",
      "reward: 97.12237303284785 setps: 86 count: 163\n",
      "reward: 46.23699103650869 setps: 55 count: 218\n",
      "reward: 59.44396751768071 setps: 39 count: 257\n",
      "reward: 45.662382649233045 setps: 34 count: 291\n",
      "reward: 28.651916729139337 setps: 25 count: 316\n",
      "reward: 11.720302909234306 setps: 17 count: 333\n",
      "reward: 98.84311685646973 setps: 105 count: 438\n",
      "reward: 55.14632247507398 setps: 36 count: 474\n",
      "reward: 50.06966464986763 setps: 46 count: 520\n",
      "reward: 18.25577053024899 setps: 19 count: 539\n",
      "reward: 68.10486287660896 setps: 87 count: 626\n",
      "reward: 55.87621756142036 setps: 51 count: 677\n",
      "reward: 39.125613252988835 setps: 32 count: 709\n",
      "reward: 44.47218080787423 setps: 43 count: 752\n",
      "reward: 68.1285007040555 setps: 41 count: 793\n",
      "reward: 22.554158270322663 setps: 21 count: 814\n",
      "reward: 93.78496283652784 setps: 60 count: 874\n",
      "reward: 52.62216280166758 setps: 40 count: 914\n",
      "reward: 15.23574442665849 setps: 18 count: 932\n",
      "reward: 70.89732638398127 setps: 47 count: 979\n",
      "avg rewards: 52.32668010704173\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.97061\n",
      "Epoch:20 Batch:8 Loss:0.14870\n",
      "Epoch:40 Batch:8 Loss:0.10721\n",
      "Epoch:60 Batch:8 Loss:0.08851\n",
      "Epoch:80 Batch:8 Loss:0.07824\n",
      "Epoch:100 Batch:8 Loss:0.07606\n",
      "Epoch:120 Batch:8 Loss:0.07319\n",
      "Epoch:140 Batch:8 Loss:0.07247\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.079\n",
      "Epoch:10 Batch:8 Loss:0.079\n",
      "Epoch:20 Batch:8 Loss:0.080\n",
      "Epoch:30 Batch:8 Loss:0.079\n",
      "Epoch:40 Batch:8 Loss:0.079\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 45.83736637208349 setps: 38 count: 38\n",
      "reward: 47.41139632229606 setps: 34 count: 72\n",
      "reward: 42.89457355098275 setps: 30 count: 102\n",
      "reward: 37.97411808974138 setps: 30 count: 132\n",
      "reward: 56.566981879361265 setps: 37 count: 169\n",
      "reward: 44.933582468444364 setps: 35 count: 204\n",
      "reward: 31.855220892844958 setps: 26 count: 230\n",
      "reward: 10.723758225048368 setps: 19 count: 249\n",
      "reward: 36.88386731492064 setps: 32 count: 281\n",
      "reward: 39.109751502827564 setps: 32 count: 313\n",
      "reward: 33.069736713232125 setps: 28 count: 341\n",
      "reward: 47.074331528120105 setps: 37 count: 378\n",
      "reward: 16.329640677889984 setps: 18 count: 396\n",
      "reward: 42.523942713250285 setps: 38 count: 434\n",
      "reward: 44.12059985814121 setps: 34 count: 468\n",
      "reward: 41.762249538312595 setps: 30 count: 498\n",
      "reward: 38.66336279657843 setps: 33 count: 531\n",
      "reward: 35.58168919545861 setps: 30 count: 561\n",
      "reward: 36.393284927358046 setps: 30 count: 591\n",
      "reward: 20.750740268625666 setps: 20 count: 611\n",
      "reward: 37.08388589956739 setps: 30 count: 641\n",
      "reward: 39.096702213480604 setps: 32 count: 673\n",
      "reward: 45.72398329161368 setps: 35 count: 708\n",
      "reward: 31.6335638280274 setps: 31 count: 739\n",
      "reward: 39.41921032391255 setps: 30 count: 769\n",
      "reward: 35.424047759531824 setps: 28 count: 797\n",
      "reward: 44.316252522874855 setps: 34 count: 831\n",
      "reward: 39.1622649945377 setps: 30 count: 861\n",
      "reward: 40.58150346695183 setps: 30 count: 891\n",
      "reward: 31.04626943004696 setps: 26 count: 917\n",
      "reward: 60.64948025874182 setps: 49 count: 966\n",
      "reward: 11.339625212138344 setps: 17 count: 983\n",
      "reward: 12.542922964543688 setps: 17 count: 1000\n",
      "avg rewards: 36.9236335454996\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.94619\n",
      "Epoch:20 Batch:9 Loss:0.13051\n",
      "Epoch:40 Batch:9 Loss:0.09840\n",
      "Epoch:60 Batch:9 Loss:0.08633\n",
      "Epoch:80 Batch:9 Loss:0.07787\n",
      "Epoch:100 Batch:9 Loss:0.07122\n",
      "Epoch:120 Batch:9 Loss:0.07129\n",
      "Epoch:140 Batch:9 Loss:0.06429\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.079\n",
      "Epoch:10 Batch:8 Loss:0.077\n",
      "Epoch:20 Batch:8 Loss:0.078\n",
      "Epoch:30 Batch:8 Loss:0.077\n",
      "Epoch:40 Batch:8 Loss:0.078\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 83.85696730250636 setps: 61 count: 61\n",
      "reward: 73.73727961735568 setps: 51 count: 112\n",
      "reward: 75.82456500584085 setps: 45 count: 157\n",
      "reward: 27.483160230267092 setps: 24 count: 181\n",
      "reward: 20.65491197072988 setps: 19 count: 200\n",
      "reward: 37.24525052194077 setps: 27 count: 227\n",
      "reward: 59.0070438047551 setps: 40 count: 267\n",
      "reward: 126.92176415296123 setps: 82 count: 349\n",
      "reward: 65.26744395458809 setps: 47 count: 396\n",
      "reward: 47.51391185562824 setps: 36 count: 432\n",
      "reward: 31.320390828049856 setps: 24 count: 456\n",
      "reward: 21.750189067941392 setps: 20 count: 476\n",
      "reward: 52.305413966346535 setps: 37 count: 513\n",
      "reward: 69.42996983751507 setps: 52 count: 565\n",
      "reward: 35.61803385576059 setps: 29 count: 594\n",
      "reward: 67.8108219296715 setps: 41 count: 635\n",
      "reward: 64.10368910782417 setps: 51 count: 686\n",
      "reward: 89.82077378685499 setps: 54 count: 740\n",
      "reward: 75.36127181998889 setps: 42 count: 782\n",
      "reward: 66.63438570017024 setps: 41 count: 823\n",
      "reward: 83.30252003773495 setps: 52 count: 875\n",
      "reward: 65.25004021587813 setps: 44 count: 919\n",
      "reward: 56.46789913190589 setps: 44 count: 963\n",
      "avg rewards: 60.72555207400937\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.60578\n",
      "Epoch:20 Batch:10 Loss:0.11075\n",
      "Epoch:40 Batch:10 Loss:0.09194\n",
      "Epoch:60 Batch:10 Loss:0.07425\n",
      "Epoch:80 Batch:10 Loss:0.06511\n",
      "Epoch:100 Batch:10 Loss:0.06873\n",
      "Epoch:120 Batch:10 Loss:0.05810\n",
      "Epoch:140 Batch:10 Loss:0.06414\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.074\n",
      "Epoch:10 Batch:8 Loss:0.072\n",
      "Epoch:20 Batch:8 Loss:0.072\n",
      "Epoch:30 Batch:8 Loss:0.073\n",
      "Epoch:40 Batch:8 Loss:0.074\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 49.860143705338125 setps: 41 count: 41\n",
      "reward: 67.29673539434008 setps: 44 count: 85\n",
      "reward: 51.98069817121287 setps: 37 count: 122\n",
      "reward: 61.8699001190107 setps: 49 count: 171\n",
      "reward: 19.072870597444123 setps: 20 count: 191\n",
      "reward: 75.1706285103486 setps: 45 count: 236\n",
      "reward: 37.25445138165669 setps: 26 count: 262\n",
      "reward: 59.77537586648833 setps: 43 count: 305\n",
      "reward: 77.76514315475212 setps: 46 count: 351\n",
      "reward: 70.70495224048064 setps: 42 count: 393\n",
      "reward: 55.13462584497317 setps: 38 count: 431\n",
      "reward: 61.652543692500345 setps: 44 count: 475\n",
      "reward: 66.39715196457983 setps: 49 count: 524\n",
      "reward: 73.90477175486593 setps: 48 count: 572\n",
      "reward: 50.42581912534808 setps: 35 count: 607\n",
      "reward: 49.51550146793888 setps: 33 count: 640\n",
      "reward: 52.40817907382298 setps: 38 count: 678\n",
      "reward: 64.52990416828396 setps: 44 count: 722\n",
      "reward: 88.00415753310808 setps: 52 count: 774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 88.92098005355075 setps: 57 count: 831\n",
      "reward: 69.23514683254616 setps: 49 count: 880\n",
      "reward: 61.47185317670227 setps: 44 count: 924\n",
      "reward: 66.85495719602768 setps: 48 count: 972\n",
      "avg rewards: 61.70463004457916\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.44176\n",
      "Epoch:20 Batch:11 Loss:0.10951\n",
      "Epoch:40 Batch:11 Loss:0.08273\n",
      "Epoch:60 Batch:11 Loss:0.07386\n",
      "Epoch:80 Batch:11 Loss:0.06914\n",
      "Epoch:100 Batch:11 Loss:0.06546\n",
      "Epoch:120 Batch:11 Loss:0.06565\n",
      "Epoch:140 Batch:11 Loss:0.06824\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.075\n",
      "Epoch:10 Batch:8 Loss:0.073\n",
      "Epoch:20 Batch:8 Loss:0.074\n",
      "Epoch:30 Batch:8 Loss:0.074\n",
      "Epoch:40 Batch:8 Loss:0.075\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 40.69318415964809 setps: 30 count: 30\n",
      "reward: 41.8936330359793 setps: 30 count: 60\n",
      "reward: 41.8142375012787 setps: 31 count: 91\n",
      "reward: 40.561295463542045 setps: 30 count: 121\n",
      "reward: 40.87967678793793 setps: 30 count: 151\n",
      "reward: 50.15633698494493 setps: 36 count: 187\n",
      "reward: 31.539628980385903 setps: 28 count: 215\n",
      "reward: 46.529020215413766 setps: 35 count: 250\n",
      "reward: 35.28165258913359 setps: 28 count: 278\n",
      "reward: 41.13181566041021 setps: 29 count: 307\n",
      "reward: 49.88224060027424 setps: 38 count: 345\n",
      "reward: 35.25537804834311 setps: 29 count: 374\n",
      "reward: 55.66255576647527 setps: 42 count: 416\n",
      "reward: 10.2560777098799 setps: 17 count: 433\n",
      "reward: 38.09577283321414 setps: 29 count: 462\n",
      "reward: 38.84936158349446 setps: 31 count: 493\n",
      "reward: 51.30128035512316 setps: 39 count: 532\n",
      "reward: 39.14085072149027 setps: 29 count: 561\n",
      "reward: 37.176092969530146 setps: 30 count: 591\n",
      "reward: 25.841577940445863 setps: 23 count: 614\n",
      "reward: 36.15190298822854 setps: 29 count: 643\n",
      "reward: 38.832004699559185 setps: 32 count: 675\n",
      "reward: 37.33044395306642 setps: 28 count: 703\n",
      "reward: 49.08693600026162 setps: 41 count: 744\n",
      "reward: 50.03605488465983 setps: 37 count: 781\n",
      "reward: 37.49208626184844 setps: 29 count: 810\n",
      "reward: 44.76331056983036 setps: 38 count: 848\n",
      "reward: 34.900677985281796 setps: 28 count: 876\n",
      "reward: 39.124090173382136 setps: 30 count: 906\n",
      "reward: 40.33032285616354 setps: 30 count: 936\n",
      "reward: 34.46369561859902 setps: 31 count: 967\n",
      "avg rewards: 39.82107083541375\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.72430\n",
      "Epoch:20 Batch:12 Loss:0.10341\n",
      "Epoch:40 Batch:12 Loss:0.07902\n",
      "Epoch:60 Batch:12 Loss:0.06505\n",
      "Epoch:80 Batch:12 Loss:0.06552\n",
      "Epoch:100 Batch:12 Loss:0.06186\n",
      "Epoch:120 Batch:12 Loss:0.06124\n",
      "Epoch:140 Batch:12 Loss:0.05833\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.072\n",
      "Epoch:10 Batch:8 Loss:0.071\n",
      "Epoch:20 Batch:8 Loss:0.073\n",
      "Epoch:30 Batch:8 Loss:0.071\n",
      "Epoch:40 Batch:8 Loss:0.073\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 50.75251309709855 setps: 34 count: 34\n",
      "reward: 44.09891577054078 setps: 29 count: 63\n",
      "reward: 42.63085487018835 setps: 31 count: 94\n",
      "reward: 43.469433997395384 setps: 29 count: 123\n",
      "reward: 47.778082261706004 setps: 32 count: 155\n",
      "reward: 18.296232826414048 setps: 18 count: 173\n",
      "reward: 40.95111852012342 setps: 30 count: 203\n",
      "reward: 47.06577644040952 setps: 32 count: 235\n",
      "reward: 56.68749248145906 setps: 38 count: 273\n",
      "reward: 41.37765951574984 setps: 32 count: 305\n",
      "reward: 50.754064149399355 setps: 33 count: 338\n",
      "reward: 49.00786938385572 setps: 32 count: 370\n",
      "reward: 34.73987509786821 setps: 28 count: 398\n",
      "reward: 38.98774152525729 setps: 29 count: 427\n",
      "reward: 50.6776012367205 setps: 37 count: 464\n",
      "reward: 47.20053694626986 setps: 32 count: 496\n",
      "reward: 45.23339199244655 setps: 32 count: 528\n",
      "reward: 37.883077624831635 setps: 30 count: 558\n",
      "reward: 48.41503031464817 setps: 31 count: 589\n",
      "reward: 53.13182001022214 setps: 36 count: 625\n",
      "reward: 17.87503160112537 setps: 19 count: 644\n",
      "reward: 41.94425868926628 setps: 32 count: 676\n",
      "reward: 41.35615366542332 setps: 31 count: 707\n",
      "reward: 42.87113657082664 setps: 31 count: 738\n",
      "reward: 50.12147204474022 setps: 35 count: 773\n",
      "reward: 19.147228471968265 setps: 18 count: 791\n",
      "reward: 45.18383714348747 setps: 30 count: 821\n",
      "reward: 61.32075891858403 setps: 43 count: 864\n",
      "reward: 53.275753747753335 setps: 36 count: 900\n",
      "reward: 42.91936483951723 setps: 30 count: 930\n",
      "reward: 43.16757378953625 setps: 29 count: 959\n",
      "reward: 20.616470692695295 setps: 20 count: 979\n",
      "avg rewards: 42.77931650742275\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.55614\n",
      "Epoch:20 Batch:13 Loss:0.10371\n",
      "Epoch:40 Batch:13 Loss:0.08335\n",
      "Epoch:60 Batch:13 Loss:0.07458\n",
      "Epoch:80 Batch:13 Loss:0.06247\n",
      "Epoch:100 Batch:13 Loss:0.05960\n",
      "Epoch:120 Batch:13 Loss:0.06038\n",
      "Epoch:140 Batch:13 Loss:0.05417\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.070\n",
      "Epoch:10 Batch:8 Loss:0.069\n",
      "Epoch:20 Batch:8 Loss:0.069\n",
      "Epoch:30 Batch:8 Loss:0.070\n",
      "Epoch:40 Batch:8 Loss:0.068\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 32.81305547415832 setps: 26 count: 26\n",
      "reward: 52.70299961750424 setps: 41 count: 67\n",
      "reward: 55.767634722671936 setps: 42 count: 109\n",
      "reward: 80.03314929350309 setps: 52 count: 161\n",
      "reward: 53.573582095468005 setps: 43 count: 204\n",
      "reward: 19.337497180217177 setps: 20 count: 224\n",
      "reward: 73.94827131458443 setps: 58 count: 282\n",
      "reward: 58.37310486102942 setps: 48 count: 330\n",
      "reward: 82.5113710767153 setps: 56 count: 386\n",
      "reward: 67.24723687669902 setps: 51 count: 437\n",
      "reward: 48.379722404427596 setps: 43 count: 480\n",
      "reward: 84.81302296748035 setps: 57 count: 537\n",
      "reward: 73.3062489694421 setps: 48 count: 585\n",
      "reward: 65.55674223152306 setps: 47 count: 632\n",
      "reward: 79.66414341174678 setps: 50 count: 682\n",
      "reward: 74.10050555825147 setps: 48 count: 730\n",
      "reward: 61.37871081987251 setps: 42 count: 772\n",
      "reward: 69.77086740410887 setps: 46 count: 818\n",
      "reward: 60.696904249247744 setps: 49 count: 867\n",
      "reward: 66.40903058036926 setps: 49 count: 916\n",
      "reward: 79.81904292103575 setps: 56 count: 972\n",
      "avg rewards: 63.81918304905032\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.37766\n",
      "Epoch:20 Batch:14 Loss:0.09231\n",
      "Epoch:40 Batch:14 Loss:0.06962\n",
      "Epoch:60 Batch:14 Loss:0.06928\n",
      "Epoch:80 Batch:14 Loss:0.05802\n",
      "Epoch:100 Batch:14 Loss:0.05542\n",
      "Epoch:120 Batch:14 Loss:0.05810\n",
      "Epoch:140 Batch:14 Loss:0.05189\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.064\n",
      "Epoch:10 Batch:8 Loss:0.064\n",
      "Epoch:20 Batch:8 Loss:0.062\n",
      "Epoch:30 Batch:8 Loss:0.062\n",
      "Epoch:40 Batch:8 Loss:0.059\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 88.07920558320478 setps: 88 count: 88\n",
      "reward: 73.33506624417058 setps: 56 count: 144\n",
      "reward: 67.68557984253857 setps: 58 count: 202\n",
      "reward: 104.71552514788928 setps: 71 count: 273\n",
      "reward: 39.30921258707677 setps: 61 count: 334\n",
      "reward: 71.84098689168749 setps: 48 count: 382\n",
      "reward: 45.20103027821024 setps: 32 count: 414\n",
      "reward: 87.18441997478078 setps: 73 count: 487\n",
      "reward: 62.35807583613495 setps: 58 count: 545\n",
      "reward: 61.62405285537679 setps: 54 count: 599\n",
      "reward: 20.950438264381958 setps: 20 count: 619\n",
      "reward: 21.99120743914391 setps: 21 count: 640\n",
      "reward: 58.46332351275925 setps: 84 count: 724\n",
      "reward: 19.47753532325732 setps: 20 count: 744\n",
      "reward: 63.317985819552284 setps: 42 count: 786\n",
      "reward: 86.7011542452674 setps: 55 count: 841\n",
      "reward: 98.35310036307494 setps: 71 count: 912\n",
      "reward: 24.933363327471305 setps: 22 count: 934\n",
      "reward: 87.94245126539899 setps: 65 count: 999\n",
      "avg rewards: 62.28756393691462\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.36144\n",
      "Epoch:20 Batch:15 Loss:0.08804\n",
      "Epoch:40 Batch:15 Loss:0.07128\n",
      "Epoch:60 Batch:15 Loss:0.06471\n",
      "Epoch:80 Batch:15 Loss:0.06660\n",
      "Epoch:100 Batch:15 Loss:0.05574\n",
      "Epoch:120 Batch:15 Loss:0.04966\n",
      "Epoch:140 Batch:15 Loss:0.04961\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.057\n",
      "Epoch:10 Batch:8 Loss:0.058\n",
      "Epoch:20 Batch:8 Loss:0.056\n",
      "Epoch:30 Batch:8 Loss:0.057\n",
      "Epoch:40 Batch:8 Loss:0.060\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 20.212231093745512 setps: 23 count: 23\n",
      "reward: 7.2435162449255595 setps: 17 count: 40\n",
      "reward: 56.294528221923954 setps: 68 count: 108\n",
      "reward: 82.88256926109753 setps: 58 count: 166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 85.0837819129141 setps: 64 count: 230\n",
      "reward: 68.40213978440296 setps: 62 count: 292\n",
      "reward: 61.043971319832785 setps: 50 count: 342\n",
      "reward: 42.336497529818736 setps: 51 count: 393\n",
      "reward: 26.913206477671334 setps: 24 count: 417\n",
      "reward: 13.582716678713041 setps: 17 count: 434\n",
      "reward: 9.824687212983555 setps: 17 count: 451\n",
      "reward: 62.42889748099405 setps: 48 count: 499\n",
      "reward: 89.51407798132861 setps: 70 count: 569\n",
      "reward: 94.26213735306374 setps: 66 count: 635\n",
      "reward: 53.682795521925414 setps: 60 count: 695\n",
      "reward: 16.775333587579368 setps: 20 count: 715\n",
      "reward: 61.56702355313318 setps: 75 count: 790\n",
      "reward: 51.206698794430125 setps: 54 count: 844\n",
      "reward: 74.52256423186338 setps: 61 count: 905\n",
      "reward: 41.16680256828696 setps: 51 count: 956\n",
      "avg rewards: 50.9473088405317\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.38817\n",
      "Epoch:20 Batch:16 Loss:0.07840\n",
      "Epoch:40 Batch:16 Loss:0.07196\n",
      "Epoch:60 Batch:16 Loss:0.05835\n",
      "Epoch:80 Batch:16 Loss:0.05760\n",
      "Epoch:100 Batch:16 Loss:0.05721\n",
      "Epoch:120 Batch:16 Loss:0.05288\n",
      "Epoch:140 Batch:16 Loss:0.04328\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.056\n",
      "Epoch:10 Batch:8 Loss:0.054\n",
      "Epoch:20 Batch:8 Loss:0.054\n",
      "Epoch:30 Batch:8 Loss:0.054\n",
      "Epoch:40 Batch:8 Loss:0.053\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 85.77387515566515 setps: 57 count: 57\n",
      "reward: 78.64344651808497 setps: 54 count: 111\n",
      "reward: 73.47346483071888 setps: 53 count: 164\n",
      "reward: 56.41924141367778 setps: 70 count: 234\n",
      "reward: 74.63394432886709 setps: 53 count: 287\n",
      "reward: 48.32203372736402 setps: 66 count: 353\n",
      "reward: 59.52800127824594 setps: 62 count: 415\n",
      "reward: 52.92787773153397 setps: 59 count: 474\n",
      "reward: 79.6715353230029 setps: 70 count: 544\n",
      "reward: 76.16746306575368 setps: 56 count: 600\n",
      "reward: 19.81012559342635 setps: 21 count: 621\n",
      "reward: 57.6909688721804 setps: 71 count: 692\n",
      "reward: 48.388953839709565 setps: 81 count: 773\n",
      "reward: 83.4203263312069 setps: 57 count: 830\n",
      "reward: 78.30465983327159 setps: 66 count: 896\n",
      "reward: 40.47155535597121 setps: 42 count: 938\n",
      "avg rewards: 63.352967074917515\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.34858\n",
      "Epoch:20 Batch:17 Loss:0.07918\n",
      "Epoch:40 Batch:17 Loss:0.07255\n",
      "Epoch:60 Batch:17 Loss:0.05456\n",
      "Epoch:80 Batch:17 Loss:0.05318\n",
      "Epoch:100 Batch:17 Loss:0.05429\n",
      "Epoch:120 Batch:17 Loss:0.05122\n",
      "Epoch:140 Batch:17 Loss:0.04821\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.057\n",
      "Epoch:10 Batch:8 Loss:0.057\n",
      "Epoch:20 Batch:8 Loss:0.055\n",
      "Epoch:30 Batch:8 Loss:0.055\n",
      "Epoch:40 Batch:8 Loss:0.055\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 93.97410890744212 setps: 87 count: 87\n",
      "reward: 56.987482094959695 setps: 46 count: 133\n",
      "reward: 52.48660509386972 setps: 45 count: 178\n",
      "reward: 78.96781404048235 setps: 69 count: 247\n",
      "reward: 41.08110020327295 setps: 37 count: 284\n",
      "reward: 77.54242824553802 setps: 54 count: 338\n",
      "reward: 25.268919806215848 setps: 26 count: 364\n",
      "reward: 35.99070270771482 setps: 37 count: 401\n",
      "reward: 49.142832908575656 setps: 52 count: 453\n",
      "reward: 58.1676065465508 setps: 43 count: 496\n",
      "reward: 52.532421288584004 setps: 41 count: 537\n",
      "reward: 41.48009784809692 setps: 46 count: 583\n",
      "reward: 83.88284029903151 setps: 59 count: 642\n",
      "reward: 14.908278936996066 setps: 18 count: 660\n",
      "reward: 71.83284268085849 setps: 83 count: 743\n",
      "reward: 58.049285854309 setps: 40 count: 783\n",
      "reward: 44.73275630837889 setps: 48 count: 831\n",
      "reward: 18.627153748816635 setps: 20 count: 851\n",
      "reward: 61.856970433925746 setps: 52 count: 903\n",
      "reward: 78.61963091823856 setps: 56 count: 959\n",
      "avg rewards: 54.806593943592894\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.27624\n",
      "Epoch:20 Batch:18 Loss:0.07691\n",
      "Epoch:40 Batch:18 Loss:0.06395\n",
      "Epoch:60 Batch:18 Loss:0.05372\n",
      "Epoch:80 Batch:18 Loss:0.05531\n",
      "Epoch:100 Batch:18 Loss:0.05388\n",
      "Epoch:120 Batch:18 Loss:0.05071\n",
      "Epoch:140 Batch:18 Loss:0.04926\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.059\n",
      "Epoch:10 Batch:8 Loss:0.060\n",
      "Epoch:20 Batch:8 Loss:0.059\n",
      "Epoch:30 Batch:8 Loss:0.059\n",
      "Epoch:40 Batch:8 Loss:0.060\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 88.20351196580155 setps: 65 count: 65\n",
      "reward: 22.57871326802851 setps: 22 count: 87\n",
      "reward: 64.52172969632375 setps: 57 count: 144\n",
      "reward: 78.18150719816767 setps: 59 count: 203\n",
      "reward: 52.27181895213727 setps: 46 count: 249\n",
      "reward: 66.32205278411855 setps: 51 count: 300\n",
      "reward: 25.98392419316806 setps: 21 count: 321\n",
      "reward: 76.39075312401113 setps: 60 count: 381\n",
      "reward: 59.52777093684562 setps: 47 count: 428\n",
      "reward: 67.45736054878068 setps: 51 count: 479\n",
      "reward: 102.03492402662378 setps: 67 count: 546\n",
      "reward: 55.90328129284608 setps: 44 count: 590\n",
      "reward: 66.41837011097523 setps: 47 count: 637\n",
      "reward: 19.143431570845138 setps: 19 count: 656\n",
      "reward: 61.48869385086728 setps: 46 count: 702\n",
      "reward: 47.502207754629495 setps: 44 count: 746\n",
      "reward: 41.94639397784051 setps: 38 count: 784\n",
      "reward: 70.65708690080646 setps: 66 count: 850\n",
      "reward: 54.841869562219664 setps: 52 count: 902\n",
      "reward: 74.91139518531128 setps: 68 count: 970\n",
      "reward: 30.862703258493273 setps: 28 count: 998\n",
      "avg rewards: 58.43569048375433\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.26936\n",
      "Epoch:20 Batch:19 Loss:0.07534\n",
      "Epoch:40 Batch:19 Loss:0.06358\n",
      "Epoch:60 Batch:19 Loss:0.05521\n",
      "Epoch:80 Batch:19 Loss:0.04965\n",
      "Epoch:100 Batch:19 Loss:0.04751\n",
      "Epoch:120 Batch:19 Loss:0.04636\n",
      "Epoch:140 Batch:19 Loss:0.04545\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.057\n",
      "Epoch:10 Batch:8 Loss:0.055\n",
      "Epoch:20 Batch:8 Loss:0.054\n",
      "Epoch:30 Batch:8 Loss:0.056\n",
      "Epoch:40 Batch:8 Loss:0.054\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 39.515148881515785 setps: 41 count: 41\n",
      "reward: 20.19470925811474 setps: 20 count: 61\n",
      "reward: 81.5958442434814 setps: 52 count: 113\n",
      "reward: 98.15429411377556 setps: 65 count: 178\n",
      "reward: 64.20719580134755 setps: 44 count: 222\n",
      "reward: 74.03183741671673 setps: 49 count: 271\n",
      "reward: 88.33167935774689 setps: 58 count: 329\n",
      "reward: 66.2380243816413 setps: 51 count: 380\n",
      "reward: 36.6815239806674 setps: 44 count: 424\n",
      "reward: 68.38283200751903 setps: 52 count: 476\n",
      "reward: 46.310256453027236 setps: 44 count: 520\n",
      "reward: 90.01625358896416 setps: 60 count: 580\n",
      "reward: 69.09746932958222 setps: 57 count: 637\n",
      "reward: 51.9632749773984 setps: 51 count: 688\n",
      "reward: 89.57415675495872 setps: 65 count: 753\n",
      "reward: 42.071812397164464 setps: 41 count: 794\n",
      "reward: 75.55873219638161 setps: 50 count: 844\n",
      "reward: 41.63723063883663 setps: 38 count: 882\n",
      "reward: 65.99717265707149 setps: 48 count: 930\n",
      "reward: 21.468026828003346 setps: 21 count: 951\n",
      "reward: 55.84026767456816 setps: 49 count: 1000\n",
      "avg rewards: 61.27941633040395\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.26934\n",
      "Epoch:20 Batch:20 Loss:0.07112\n",
      "Epoch:40 Batch:20 Loss:0.06086\n",
      "Epoch:60 Batch:20 Loss:0.05571\n",
      "Epoch:80 Batch:20 Loss:0.04791\n",
      "Epoch:100 Batch:20 Loss:0.05400\n",
      "Epoch:120 Batch:20 Loss:0.05094\n",
      "Epoch:140 Batch:20 Loss:0.04396\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.056\n",
      "Epoch:10 Batch:8 Loss:0.056\n",
      "Epoch:20 Batch:8 Loss:0.059\n",
      "Epoch:30 Batch:8 Loss:0.056\n",
      "Epoch:40 Batch:8 Loss:0.054\n",
      "Done!\n",
      "############# start BipedalWalker-v3 training ###################\n",
      "(49,)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -99.0992617930056 setps: 76 count: 76\n",
      "reward: -103.89475929486287 setps: 92 count: 168\n",
      "reward: -99.33922472825057 setps: 72 count: 240\n",
      "avg rewards: -100.77774860537302\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:1.35113\n",
      "Epoch:20 Batch:1 Loss:0.14747\n",
      "Epoch:40 Batch:1 Loss:0.12726\n",
      "Epoch:60 Batch:1 Loss:0.10868\n",
      "Epoch:80 Batch:1 Loss:0.09520\n",
      "Epoch:100 Batch:1 Loss:0.08474\n",
      "Epoch:120 Batch:1 Loss:0.07706\n",
      "Epoch:140 Batch:1 Loss:0.07115\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.159\n",
      "Epoch:10 Batch:9 Loss:0.155\n",
      "Epoch:20 Batch:9 Loss:0.135\n",
      "Epoch:30 Batch:9 Loss:0.137\n",
      "Epoch:40 Batch:9 Loss:0.130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -120.75483087736492 setps: 70 count: 70\n",
      "reward: -114.34141056087178 setps: 69 count: 139\n",
      "reward: -117.44932009782906 setps: 64 count: 203\n",
      "reward: -115.80246188528774 setps: 67 count: 270\n",
      "reward: -112.88401185808455 setps: 69 count: 339\n",
      "reward: -120.15541444734049 setps: 67 count: 406\n",
      "reward: -115.93497408397185 setps: 66 count: 472\n",
      "reward: -118.45460094231119 setps: 69 count: 541\n",
      "reward: -120.08334531299957 setps: 68 count: 609\n",
      "reward: -121.29289151818553 setps: 69 count: 678\n",
      "reward: -122.13527774760126 setps: 72 count: 750\n",
      "reward: -118.94386216447006 setps: 73 count: 823\n",
      "reward: -118.6751183979977 setps: 71 count: 894\n",
      "reward: -124.66200676964534 setps: 74 count: 968\n",
      "avg rewards: -118.68353761885439\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:1.31913\n",
      "Epoch:20 Batch:2 Loss:0.11782\n",
      "Epoch:40 Batch:2 Loss:0.07692\n",
      "Epoch:60 Batch:2 Loss:0.06518\n",
      "Epoch:80 Batch:2 Loss:0.05648\n",
      "Epoch:100 Batch:2 Loss:0.05068\n",
      "Epoch:120 Batch:2 Loss:0.04723\n",
      "Epoch:140 Batch:2 Loss:0.04367\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.095\n",
      "Epoch:10 Batch:9 Loss:0.093\n",
      "Epoch:20 Batch:9 Loss:0.089\n",
      "Epoch:30 Batch:9 Loss:0.096\n",
      "Epoch:40 Batch:9 Loss:0.090\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -94.29909815589463 setps: 166 count: 166\n",
      "reward: -128.6167337765582 setps: 170 count: 336\n",
      "avg rewards: -111.45791596622641\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:1.19834\n",
      "Epoch:20 Batch:3 Loss:0.09279\n",
      "Epoch:40 Batch:3 Loss:0.06346\n",
      "Epoch:60 Batch:3 Loss:0.05071\n",
      "Epoch:80 Batch:3 Loss:0.04717\n",
      "Epoch:100 Batch:3 Loss:0.04089\n",
      "Epoch:120 Batch:3 Loss:0.04047\n",
      "Epoch:140 Batch:3 Loss:0.03693\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.079\n",
      "Epoch:10 Batch:9 Loss:0.076\n",
      "Epoch:20 Batch:9 Loss:0.076\n",
      "Epoch:30 Batch:9 Loss:0.080\n",
      "Epoch:40 Batch:9 Loss:0.081\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -30.766854288189354 setps: 800 count: 800\n",
      "reward: -95.80688380964472 setps: 147 count: 947\n",
      "avg rewards: -63.28686904891703\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:1.01874\n",
      "Epoch:20 Batch:4 Loss:0.07140\n",
      "Epoch:40 Batch:4 Loss:0.05200\n",
      "Epoch:60 Batch:4 Loss:0.04271\n",
      "Epoch:80 Batch:4 Loss:0.03865\n",
      "Epoch:100 Batch:4 Loss:0.03651\n",
      "Epoch:120 Batch:4 Loss:0.03493\n",
      "Epoch:140 Batch:4 Loss:0.03265\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.086\n",
      "Epoch:10 Batch:9 Loss:0.082\n",
      "Epoch:20 Batch:9 Loss:0.080\n",
      "Epoch:30 Batch:9 Loss:0.078\n",
      "Epoch:40 Batch:9 Loss:0.079\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -18.838589123489648 setps: 800 count: 800\n",
      "avg rewards: -18.838589123489648\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.81729\n",
      "Epoch:20 Batch:5 Loss:0.06493\n",
      "Epoch:40 Batch:5 Loss:0.04422\n",
      "Epoch:60 Batch:5 Loss:0.03801\n",
      "Epoch:80 Batch:5 Loss:0.03499\n",
      "Epoch:100 Batch:5 Loss:0.03146\n",
      "Epoch:120 Batch:5 Loss:0.03129\n",
      "Epoch:140 Batch:5 Loss:0.02832\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.075\n",
      "Epoch:10 Batch:9 Loss:0.082\n",
      "Epoch:20 Batch:9 Loss:0.076\n",
      "Epoch:30 Batch:9 Loss:0.076\n",
      "Epoch:40 Batch:9 Loss:0.075\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -25.571614828703975 setps: 800 count: 800\n",
      "avg rewards: -25.571614828703975\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.56977\n",
      "Epoch:20 Batch:6 Loss:0.05451\n",
      "Epoch:40 Batch:6 Loss:0.03986\n",
      "Epoch:60 Batch:6 Loss:0.03377\n",
      "Epoch:80 Batch:6 Loss:0.03071\n",
      "Epoch:100 Batch:6 Loss:0.02891\n",
      "Epoch:120 Batch:6 Loss:0.02601\n",
      "Epoch:140 Batch:6 Loss:0.02668\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.074\n",
      "Epoch:10 Batch:9 Loss:0.073\n",
      "Epoch:20 Batch:9 Loss:0.075\n",
      "Epoch:30 Batch:9 Loss:0.075\n",
      "Epoch:40 Batch:9 Loss:0.073\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -116.3708416249423 setps: 138 count: 138\n",
      "reward: -69.98610616330056 setps: 320 count: 458\n",
      "reward: -93.9843109596384 setps: 120 count: 578\n",
      "reward: -89.41323426956932 setps: 162 count: 740\n",
      "reward: -135.64998046779507 setps: 166 count: 906\n",
      "avg rewards: -101.08089469704915\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.83449\n",
      "Epoch:20 Batch:7 Loss:0.05122\n",
      "Epoch:40 Batch:7 Loss:0.03700\n",
      "Epoch:60 Batch:7 Loss:0.03423\n",
      "Epoch:80 Batch:7 Loss:0.02956\n",
      "Epoch:100 Batch:7 Loss:0.03133\n",
      "Epoch:120 Batch:7 Loss:0.02705\n",
      "Epoch:140 Batch:7 Loss:0.02694\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.069\n",
      "Epoch:10 Batch:9 Loss:0.074\n",
      "Epoch:20 Batch:9 Loss:0.076\n",
      "Epoch:30 Batch:9 Loss:0.072\n",
      "Epoch:40 Batch:9 Loss:0.070\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -88.64492271491761 setps: 165 count: 165\n",
      "reward: -102.84339157977328 setps: 66 count: 231\n",
      "reward: -103.8937429779116 setps: 83 count: 314\n",
      "reward: -109.62278183630418 setps: 71 count: 385\n",
      "avg rewards: -101.25120977722666\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.59776\n",
      "Epoch:20 Batch:8 Loss:0.05557\n",
      "Epoch:40 Batch:8 Loss:0.04153\n",
      "Epoch:60 Batch:8 Loss:0.03470\n",
      "Epoch:80 Batch:8 Loss:0.03276\n",
      "Epoch:100 Batch:8 Loss:0.02943\n",
      "Epoch:120 Batch:8 Loss:0.02927\n",
      "Epoch:140 Batch:8 Loss:0.02843\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.066\n",
      "Epoch:10 Batch:9 Loss:0.066\n",
      "Epoch:20 Batch:9 Loss:0.067\n",
      "Epoch:30 Batch:9 Loss:0.064\n",
      "Epoch:40 Batch:9 Loss:0.064\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -97.24531563333298 setps: 133 count: 133\n",
      "reward: -102.84122555395899 setps: 84 count: 217\n",
      "reward: -67.93156836802328 setps: 593 count: 810\n",
      "avg rewards: -89.33936985177176\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.37669\n",
      "Epoch:20 Batch:9 Loss:0.05115\n",
      "Epoch:40 Batch:9 Loss:0.04059\n",
      "Epoch:60 Batch:9 Loss:0.03625\n",
      "Epoch:80 Batch:9 Loss:0.03208\n",
      "Epoch:100 Batch:9 Loss:0.03011\n",
      "Epoch:120 Batch:9 Loss:0.02986\n",
      "Epoch:140 Batch:9 Loss:0.02961\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.054\n",
      "Epoch:10 Batch:9 Loss:0.058\n",
      "Epoch:20 Batch:9 Loss:0.060\n",
      "Epoch:30 Batch:9 Loss:0.058\n",
      "Epoch:40 Batch:9 Loss:0.058\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -107.87410204640962 setps: 85 count: 85\n",
      "reward: -105.60948874930293 setps: 53 count: 138\n",
      "reward: 14.19594903327028 setps: 800 count: 938\n",
      "avg rewards: -66.42921392081409\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.29281\n",
      "Epoch:20 Batch:10 Loss:0.04722\n",
      "Epoch:40 Batch:10 Loss:0.03863\n",
      "Epoch:60 Batch:10 Loss:0.03363\n",
      "Epoch:80 Batch:10 Loss:0.03265\n",
      "Epoch:100 Batch:10 Loss:0.03118\n",
      "Epoch:120 Batch:10 Loss:0.03015\n",
      "Epoch:140 Batch:10 Loss:0.02833\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.058\n",
      "Epoch:10 Batch:9 Loss:0.057\n",
      "Epoch:20 Batch:9 Loss:0.057\n",
      "Epoch:30 Batch:9 Loss:0.056\n",
      "Epoch:40 Batch:9 Loss:0.059\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -105.89437990598007 setps: 156 count: 156\n",
      "reward: -119.41981271756565 setps: 86 count: 242\n",
      "reward: -121.08609198235162 setps: 78 count: 320\n",
      "reward: -116.02266419394687 setps: 97 count: 417\n",
      "reward: -113.36690232097233 setps: 95 count: 512\n",
      "reward: -101.587334514305 setps: 127 count: 639\n",
      "reward: -97.85356016012032 setps: 91 count: 730\n",
      "avg rewards: -110.74724939932027\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.33879\n",
      "Epoch:20 Batch:11 Loss:0.04818\n",
      "Epoch:40 Batch:11 Loss:0.03928\n",
      "Epoch:60 Batch:11 Loss:0.03429\n",
      "Epoch:80 Batch:11 Loss:0.03236\n",
      "Epoch:100 Batch:11 Loss:0.02994\n",
      "Epoch:120 Batch:11 Loss:0.02922\n",
      "Epoch:140 Batch:11 Loss:0.02788\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.055\n",
      "Epoch:10 Batch:9 Loss:0.049\n",
      "Epoch:20 Batch:9 Loss:0.050\n",
      "Epoch:30 Batch:9 Loss:0.051\n",
      "Epoch:40 Batch:9 Loss:0.055\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -8.207463086945646 setps: 800 count: 800\n",
      "reward: -114.81138842584565 setps: 76 count: 876\n",
      "avg rewards: -61.50942575639565\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:12 Loss:0.34475\n",
      "Epoch:20 Batch:12 Loss:0.04655\n",
      "Epoch:40 Batch:12 Loss:0.03484\n",
      "Epoch:60 Batch:12 Loss:0.03541\n",
      "Epoch:80 Batch:12 Loss:0.03133\n",
      "Epoch:100 Batch:12 Loss:0.03296\n",
      "Epoch:120 Batch:12 Loss:0.02949\n",
      "Epoch:140 Batch:12 Loss:0.02925\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.049\n",
      "Epoch:10 Batch:9 Loss:0.051\n",
      "Epoch:20 Batch:9 Loss:0.050\n",
      "Epoch:30 Batch:9 Loss:0.047\n",
      "Epoch:40 Batch:9 Loss:0.050\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -5.399219402426241 setps: 800 count: 800\n",
      "avg rewards: -5.399219402426241\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.28386\n",
      "Epoch:20 Batch:13 Loss:0.04613\n",
      "Epoch:40 Batch:13 Loss:0.03785\n",
      "Epoch:60 Batch:13 Loss:0.03511\n",
      "Epoch:80 Batch:13 Loss:0.03192\n",
      "Epoch:100 Batch:13 Loss:0.03214\n",
      "Epoch:120 Batch:13 Loss:0.02933\n",
      "Epoch:140 Batch:13 Loss:0.02818\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.054\n",
      "Epoch:10 Batch:9 Loss:0.055\n",
      "Epoch:20 Batch:9 Loss:0.048\n",
      "Epoch:30 Batch:9 Loss:0.049\n",
      "Epoch:40 Batch:9 Loss:0.049\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 3.0478698577011456 setps: 800 count: 800\n",
      "avg rewards: 3.0478698577011456\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.23135\n",
      "Epoch:20 Batch:14 Loss:0.04493\n",
      "Epoch:40 Batch:14 Loss:0.03870\n",
      "Epoch:60 Batch:14 Loss:0.03254\n",
      "Epoch:80 Batch:14 Loss:0.03421\n",
      "Epoch:100 Batch:14 Loss:0.02935\n",
      "Epoch:120 Batch:14 Loss:0.03044\n",
      "Epoch:140 Batch:14 Loss:0.02890\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.050\n",
      "Epoch:10 Batch:9 Loss:0.049\n",
      "Epoch:20 Batch:9 Loss:0.049\n",
      "Epoch:30 Batch:9 Loss:0.047\n",
      "Epoch:40 Batch:9 Loss:0.047\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -122.82732447175495 setps: 77 count: 77\n",
      "reward: -110.49141534237998 setps: 64 count: 141\n",
      "reward: 20.458516735364974 setps: 800 count: 941\n",
      "avg rewards: -70.9534076929233\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.28046\n",
      "Epoch:20 Batch:15 Loss:0.04614\n",
      "Epoch:40 Batch:15 Loss:0.03615\n",
      "Epoch:60 Batch:15 Loss:0.03431\n",
      "Epoch:80 Batch:15 Loss:0.03284\n",
      "Epoch:100 Batch:15 Loss:0.02995\n",
      "Epoch:120 Batch:15 Loss:0.02783\n",
      "Epoch:140 Batch:15 Loss:0.02838\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.049\n",
      "Epoch:10 Batch:9 Loss:0.050\n",
      "Epoch:20 Batch:9 Loss:0.052\n",
      "Epoch:30 Batch:9 Loss:0.051\n",
      "Epoch:40 Batch:9 Loss:0.049\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 22.90711442569084 setps: 800 count: 800\n",
      "avg rewards: 22.90711442569084\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.18904\n",
      "Epoch:20 Batch:16 Loss:0.04264\n",
      "Epoch:40 Batch:16 Loss:0.03639\n",
      "Epoch:60 Batch:16 Loss:0.03459\n",
      "Epoch:80 Batch:16 Loss:0.03443\n",
      "Epoch:100 Batch:16 Loss:0.02894\n",
      "Epoch:120 Batch:16 Loss:0.02948\n",
      "Epoch:140 Batch:16 Loss:0.02863\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.049\n",
      "Epoch:10 Batch:9 Loss:0.047\n",
      "Epoch:20 Batch:9 Loss:0.046\n",
      "Epoch:30 Batch:9 Loss:0.045\n",
      "Epoch:40 Batch:9 Loss:0.044\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 8.731049498012295 setps: 800 count: 800\n",
      "avg rewards: 8.731049498012295\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.19040\n",
      "Epoch:20 Batch:17 Loss:0.04249\n",
      "Epoch:40 Batch:17 Loss:0.03694\n",
      "Epoch:60 Batch:17 Loss:0.03280\n",
      "Epoch:80 Batch:17 Loss:0.03127\n",
      "Epoch:100 Batch:17 Loss:0.03070\n",
      "Epoch:120 Batch:17 Loss:0.02887\n",
      "Epoch:140 Batch:17 Loss:0.02606\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.047\n",
      "Epoch:10 Batch:9 Loss:0.046\n",
      "Epoch:20 Batch:9 Loss:0.047\n",
      "Epoch:30 Batch:9 Loss:0.047\n",
      "Epoch:40 Batch:9 Loss:0.047\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -112.83702179495805 setps: 63 count: 63\n",
      "reward: -110.63712400956514 setps: 60 count: 123\n",
      "reward: 20.17792029087245 setps: 800 count: 923\n",
      "avg rewards: -67.76540850455024\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.17325\n",
      "Epoch:20 Batch:18 Loss:0.04202\n",
      "Epoch:40 Batch:18 Loss:0.03588\n",
      "Epoch:60 Batch:18 Loss:0.03473\n",
      "Epoch:80 Batch:18 Loss:0.03073\n",
      "Epoch:100 Batch:18 Loss:0.02921\n",
      "Epoch:120 Batch:18 Loss:0.02916\n",
      "Epoch:140 Batch:18 Loss:0.02784\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.044\n",
      "Epoch:10 Batch:9 Loss:0.049\n",
      "Epoch:20 Batch:9 Loss:0.046\n",
      "Epoch:30 Batch:9 Loss:0.049\n",
      "Epoch:40 Batch:9 Loss:0.046\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 18.380461831542988 setps: 800 count: 800\n",
      "avg rewards: 18.380461831542988\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.17884\n",
      "Epoch:20 Batch:19 Loss:0.04235\n",
      "Epoch:40 Batch:19 Loss:0.03614\n",
      "Epoch:60 Batch:19 Loss:0.03351\n",
      "Epoch:80 Batch:19 Loss:0.03411\n",
      "Epoch:100 Batch:19 Loss:0.03035\n",
      "Epoch:120 Batch:19 Loss:0.02800\n",
      "Epoch:140 Batch:19 Loss:0.02749\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.045\n",
      "Epoch:10 Batch:9 Loss:0.045\n",
      "Epoch:20 Batch:9 Loss:0.047\n",
      "Epoch:30 Batch:9 Loss:0.048\n",
      "Epoch:40 Batch:9 Loss:0.047\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 82.16070939693046 setps: 800 count: 800\n",
      "avg rewards: 82.16070939693046\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.17153\n",
      "Epoch:20 Batch:20 Loss:0.04117\n",
      "Epoch:40 Batch:20 Loss:0.03559\n",
      "Epoch:60 Batch:20 Loss:0.03326\n",
      "Epoch:80 Batch:20 Loss:0.02883\n",
      "Epoch:100 Batch:20 Loss:0.02861\n",
      "Epoch:120 Batch:20 Loss:0.02641\n",
      "Epoch:140 Batch:20 Loss:0.02805\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.042\n",
      "Epoch:10 Batch:9 Loss:0.046\n",
      "Epoch:20 Batch:9 Loss:0.041\n",
      "Epoch:30 Batch:9 Loss:0.043\n",
      "Epoch:40 Batch:9 Loss:0.044\n",
      "Done!\n",
      "############# start Walker2DBulletEnv-v0 training ###################\n",
      "(49, 1000, 22)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 13.414238475778255 setps: 9 count: 9\n",
      "reward: 14.186659335390143 setps: 12 count: 21\n",
      "reward: 27.318206631367502 setps: 26 count: 47\n",
      "reward: 16.85843329593772 setps: 15 count: 62\n",
      "reward: 21.397395350715666 setps: 22 count: 84\n",
      "reward: 14.089880464441375 setps: 10 count: 94\n",
      "reward: 15.02987096743018 setps: 10 count: 104\n",
      "reward: 12.356591047155963 setps: 8 count: 112\n",
      "reward: 15.39555321254302 setps: 12 count: 124\n",
      "reward: 21.90494244393631 setps: 32 count: 156\n",
      "reward: 17.712883703976697 setps: 12 count: 168\n",
      "reward: 15.081593605913802 setps: 12 count: 180\n",
      "reward: 18.173342215211598 setps: 15 count: 195\n",
      "reward: 14.495556036867493 setps: 11 count: 206\n",
      "reward: 20.50079650690896 setps: 18 count: 224\n",
      "reward: 14.092909942976258 setps: 11 count: 235\n",
      "reward: 22.970959771334307 setps: 22 count: 257\n",
      "reward: 19.444699858610694 setps: 17 count: 274\n",
      "reward: 14.374875546406837 setps: 11 count: 285\n",
      "reward: 15.00305928625894 setps: 11 count: 296\n",
      "reward: 15.243818437888697 setps: 11 count: 307\n",
      "reward: 17.307870128586362 setps: 16 count: 323\n",
      "reward: 21.64621891505667 setps: 24 count: 347\n",
      "reward: 18.4285451403106 setps: 23 count: 370\n",
      "reward: 14.281873992230974 setps: 9 count: 379\n",
      "reward: 15.119889230746777 setps: 12 count: 391\n",
      "reward: 18.797276167415834 setps: 17 count: 408\n",
      "reward: 15.763713427065523 setps: 12 count: 420\n",
      "reward: 27.516996524079882 setps: 27 count: 447\n",
      "reward: 14.47504786134523 setps: 14 count: 461\n",
      "reward: 13.766097358506522 setps: 9 count: 470\n",
      "reward: 15.688697406946448 setps: 13 count: 483\n",
      "reward: 16.920485684316375 setps: 12 count: 495\n",
      "reward: 17.479992435606253 setps: 17 count: 512\n",
      "reward: 14.840384492183512 setps: 10 count: 522\n",
      "reward: 14.28074710001092 setps: 12 count: 534\n",
      "reward: 19.182011935595074 setps: 18 count: 552\n",
      "reward: 20.766663108166536 setps: 20 count: 572\n",
      "reward: 18.396893175621518 setps: 14 count: 586\n",
      "reward: 13.09486350460356 setps: 12 count: 598\n",
      "reward: 12.057490507187321 setps: 8 count: 606\n",
      "reward: 13.962488749361365 setps: 11 count: 617\n",
      "reward: 15.674586982601614 setps: 15 count: 632\n",
      "reward: 16.85779056666797 setps: 14 count: 646\n",
      "reward: 21.517029446948435 setps: 18 count: 664\n",
      "reward: 17.094856722462282 setps: 13 count: 677\n",
      "reward: 14.998163725581254 setps: 11 count: 688\n",
      "reward: 16.90535577871633 setps: 14 count: 702\n",
      "reward: 14.277336815766466 setps: 10 count: 712\n",
      "reward: 20.004583934898253 setps: 19 count: 731\n",
      "reward: 17.435491645979347 setps: 16 count: 747\n",
      "reward: 15.532703819955351 setps: 16 count: 763\n",
      "reward: 16.185374740588305 setps: 12 count: 775\n",
      "reward: 15.464810367433529 setps: 11 count: 786\n",
      "reward: 18.658744219029902 setps: 17 count: 803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 30.083171446801856 setps: 38 count: 841\n",
      "reward: 15.39770746484719 setps: 14 count: 855\n",
      "reward: 13.569465087012212 setps: 9 count: 864\n",
      "reward: 18.393536165813565 setps: 16 count: 880\n",
      "reward: 13.687444823609257 setps: 11 count: 891\n",
      "reward: 12.510306997306179 setps: 10 count: 901\n",
      "reward: 14.378060417677624 setps: 15 count: 916\n",
      "reward: 34.87067628929944 setps: 34 count: 950\n",
      "reward: 13.436906401602025 setps: 10 count: 960\n",
      "reward: 18.5650234917819 setps: 15 count: 975\n",
      "reward: 16.098111184232405 setps: 12 count: 987\n",
      "reward: 13.769625021524552 setps: 9 count: 996\n",
      "avg rewards: 17.13712502298703\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:1.32142\n",
      "Epoch:20 Batch:1 Loss:0.12190\n",
      "Epoch:40 Batch:1 Loss:0.09290\n",
      "Epoch:60 Batch:1 Loss:0.06712\n",
      "Epoch:80 Batch:1 Loss:0.05751\n",
      "Epoch:100 Batch:1 Loss:0.05190\n",
      "Epoch:120 Batch:1 Loss:0.04679\n",
      "Epoch:140 Batch:1 Loss:0.04383\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.101\n",
      "Epoch:10 Batch:10 Loss:0.101\n",
      "Epoch:20 Batch:10 Loss:0.098\n",
      "Epoch:30 Batch:10 Loss:0.093\n",
      "Epoch:40 Batch:10 Loss:0.092\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 59.17760955864797 setps: 102 count: 102\n",
      "reward: 9.57227124296769 setps: 77 count: 179\n",
      "reward: 322.3800141980142 setps: 800 count: 979\n",
      "avg rewards: 130.3766316665433\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:1.27152\n",
      "Epoch:20 Batch:2 Loss:0.07351\n",
      "Epoch:40 Batch:2 Loss:0.05149\n",
      "Epoch:60 Batch:2 Loss:0.03529\n",
      "Epoch:80 Batch:2 Loss:0.03041\n",
      "Epoch:100 Batch:2 Loss:0.02855\n",
      "Epoch:120 Batch:2 Loss:0.02711\n",
      "Epoch:140 Batch:2 Loss:0.02457\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.108\n",
      "Epoch:10 Batch:10 Loss:0.101\n",
      "Epoch:20 Batch:10 Loss:0.100\n",
      "Epoch:30 Batch:10 Loss:0.098\n",
      "Epoch:40 Batch:10 Loss:0.099\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 73.02196438329844 setps: 134 count: 134\n",
      "reward: 101.05012419552806 setps: 108 count: 242\n",
      "reward: 35.279152358187986 setps: 76 count: 318\n",
      "reward: 29.20335479448404 setps: 75 count: 393\n",
      "reward: 80.19017769630734 setps: 101 count: 494\n",
      "reward: 71.11877130456412 setps: 109 count: 603\n",
      "reward: 87.33786961849106 setps: 86 count: 689\n",
      "reward: 87.03743191670654 setps: 89 count: 778\n",
      "reward: 23.901245418217158 setps: 64 count: 842\n",
      "reward: 83.54236667784136 setps: 85 count: 927\n",
      "avg rewards: 67.16824583636262\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:1.13324\n",
      "Epoch:20 Batch:3 Loss:0.05720\n",
      "Epoch:40 Batch:3 Loss:0.03276\n",
      "Epoch:60 Batch:3 Loss:0.02643\n",
      "Epoch:80 Batch:3 Loss:0.02354\n",
      "Epoch:100 Batch:3 Loss:0.02028\n",
      "Epoch:120 Batch:3 Loss:0.01876\n",
      "Epoch:140 Batch:3 Loss:0.01755\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.082\n",
      "Epoch:10 Batch:10 Loss:0.082\n",
      "Epoch:20 Batch:10 Loss:0.082\n",
      "Epoch:30 Batch:10 Loss:0.082\n",
      "Epoch:40 Batch:10 Loss:0.081\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 47.59169120859151 setps: 105 count: 105\n",
      "reward: 57.21232849056105 setps: 89 count: 194\n",
      "reward: 495.1397822832025 setps: 719 count: 913\n",
      "avg rewards: 199.98126732745172\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.83110\n",
      "Epoch:20 Batch:4 Loss:0.04072\n",
      "Epoch:40 Batch:4 Loss:0.02505\n",
      "Epoch:60 Batch:4 Loss:0.02013\n",
      "Epoch:80 Batch:4 Loss:0.01815\n",
      "Epoch:100 Batch:4 Loss:0.01536\n",
      "Epoch:120 Batch:4 Loss:0.01518\n",
      "Epoch:140 Batch:4 Loss:0.01429\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.075\n",
      "Epoch:10 Batch:10 Loss:0.078\n",
      "Epoch:20 Batch:10 Loss:0.076\n",
      "Epoch:30 Batch:10 Loss:0.076\n",
      "Epoch:40 Batch:10 Loss:0.076\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 18.071337106112328 setps: 61 count: 61\n",
      "reward: 51.17198365302902 setps: 62 count: 123\n",
      "reward: 50.19668716248707 setps: 111 count: 234\n",
      "reward: 89.07063696498953 setps: 112 count: 346\n",
      "avg rewards: 52.12766122165449\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.74441\n",
      "Epoch:20 Batch:5 Loss:0.03235\n",
      "Epoch:40 Batch:5 Loss:0.02167\n",
      "Epoch:60 Batch:5 Loss:0.01684\n",
      "Epoch:80 Batch:5 Loss:0.01397\n",
      "Epoch:100 Batch:5 Loss:0.01322\n",
      "Epoch:120 Batch:5 Loss:0.01195\n",
      "Epoch:140 Batch:5 Loss:0.01050\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.076\n",
      "Epoch:10 Batch:10 Loss:0.074\n",
      "Epoch:20 Batch:10 Loss:0.074\n",
      "Epoch:30 Batch:10 Loss:0.074\n",
      "Epoch:40 Batch:10 Loss:0.073\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 23.958400416075893 setps: 67 count: 67\n",
      "reward: 17.220308203478627 setps: 59 count: 126\n",
      "reward: 29.136508815262644 setps: 76 count: 202\n",
      "reward: 25.51787621857948 setps: 69 count: 271\n",
      "reward: 20.716344384671544 setps: 65 count: 336\n",
      "reward: 13.903896934918883 setps: 55 count: 391\n",
      "reward: 15.489489289831539 setps: 59 count: 450\n",
      "reward: 30.379371329663254 setps: 79 count: 529\n",
      "reward: 22.570699994887406 setps: 62 count: 591\n",
      "reward: 15.273894808268226 setps: 53 count: 644\n",
      "reward: 15.52944754891941 setps: 57 count: 701\n",
      "reward: 26.200063697424774 setps: 71 count: 772\n",
      "reward: 25.557367230857196 setps: 74 count: 846\n",
      "reward: 31.23991626329661 setps: 82 count: 928\n",
      "reward: 18.192457925170295 setps: 66 count: 994\n",
      "avg rewards: 22.05906953742039\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.47973\n",
      "Epoch:20 Batch:6 Loss:0.02814\n",
      "Epoch:40 Batch:6 Loss:0.01928\n",
      "Epoch:60 Batch:6 Loss:0.01662\n",
      "Epoch:80 Batch:6 Loss:0.01327\n",
      "Epoch:100 Batch:6 Loss:0.01091\n",
      "Epoch:120 Batch:6 Loss:0.01230\n",
      "Epoch:140 Batch:6 Loss:0.01072\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.075\n",
      "Epoch:10 Batch:10 Loss:0.075\n",
      "Epoch:20 Batch:10 Loss:0.075\n",
      "Epoch:30 Batch:10 Loss:0.076\n",
      "Epoch:40 Batch:10 Loss:0.075\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 23.410114939045165 setps: 66 count: 66\n",
      "reward: 15.278669676941352 setps: 50 count: 116\n",
      "reward: 489.0468394463622 setps: 800 count: 916\n",
      "avg rewards: 175.91187468744957\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.79666\n",
      "Epoch:20 Batch:7 Loss:0.02643\n",
      "Epoch:40 Batch:7 Loss:0.01593\n",
      "Epoch:60 Batch:7 Loss:0.01364\n",
      "Epoch:80 Batch:7 Loss:0.01222\n",
      "Epoch:100 Batch:7 Loss:0.01157\n",
      "Epoch:120 Batch:7 Loss:0.01224\n",
      "Epoch:140 Batch:7 Loss:0.00953\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.078\n",
      "Epoch:10 Batch:10 Loss:0.078\n",
      "Epoch:20 Batch:10 Loss:0.078\n",
      "Epoch:30 Batch:10 Loss:0.076\n",
      "Epoch:40 Batch:10 Loss:0.077\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 502.37312045834375 setps: 800 count: 800\n",
      "reward: 110.73869379702221 setps: 163 count: 963\n",
      "avg rewards: 306.555907127683\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.43201\n",
      "Epoch:20 Batch:8 Loss:0.02289\n",
      "Epoch:40 Batch:8 Loss:0.01388\n",
      "Epoch:60 Batch:8 Loss:0.01254\n",
      "Epoch:80 Batch:8 Loss:0.01087\n",
      "Epoch:100 Batch:8 Loss:0.00912\n",
      "Epoch:120 Batch:8 Loss:0.00915\n",
      "Epoch:140 Batch:8 Loss:0.00769\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.075\n",
      "Epoch:10 Batch:10 Loss:0.075\n",
      "Epoch:20 Batch:10 Loss:0.074\n",
      "Epoch:30 Batch:10 Loss:0.074\n",
      "Epoch:40 Batch:10 Loss:0.074\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 40.54000120124837 setps: 76 count: 76\n",
      "reward: 85.49048663950381 setps: 82 count: 158\n",
      "reward: 53.71751761656923 setps: 93 count: 251\n",
      "reward: 79.18741668905423 setps: 71 count: 322\n",
      "reward: 44.74003135212554 setps: 86 count: 408\n",
      "reward: 28.945711308525635 setps: 49 count: 457\n",
      "reward: 82.40173227626511 setps: 77 count: 534\n",
      "reward: 70.68618021501024 setps: 51 count: 585\n",
      "reward: 67.14185225645343 setps: 72 count: 657\n",
      "reward: 25.132122007852022 setps: 58 count: 715\n",
      "reward: 49.5498635449476 setps: 87 count: 802\n",
      "reward: 92.2906733454045 setps: 83 count: 885\n",
      "reward: 93.0369827155577 setps: 74 count: 959\n",
      "avg rewards: 62.52773624373209\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.25751\n",
      "Epoch:20 Batch:9 Loss:0.02325\n",
      "Epoch:40 Batch:9 Loss:0.01499\n",
      "Epoch:60 Batch:9 Loss:0.01194\n",
      "Epoch:80 Batch:9 Loss:0.01035\n",
      "Epoch:100 Batch:9 Loss:0.00903\n",
      "Epoch:120 Batch:9 Loss:0.00866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:140 Batch:9 Loss:0.00861\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.076\n",
      "Epoch:10 Batch:10 Loss:0.073\n",
      "Epoch:20 Batch:10 Loss:0.070\n",
      "Epoch:30 Batch:10 Loss:0.070\n",
      "Epoch:40 Batch:10 Loss:0.069\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 462.72195531138374 setps: 800 count: 800\n",
      "avg rewards: 462.72195531138374\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.19751\n",
      "Epoch:20 Batch:10 Loss:0.01963\n",
      "Epoch:40 Batch:10 Loss:0.01436\n",
      "Epoch:60 Batch:10 Loss:0.01011\n",
      "Epoch:80 Batch:10 Loss:0.01007\n",
      "Epoch:100 Batch:10 Loss:0.00862\n",
      "Epoch:120 Batch:10 Loss:0.00794\n",
      "Epoch:140 Batch:10 Loss:0.00709\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.076\n",
      "Epoch:10 Batch:10 Loss:0.073\n",
      "Epoch:20 Batch:10 Loss:0.072\n",
      "Epoch:30 Batch:10 Loss:0.072\n",
      "Epoch:40 Batch:10 Loss:0.074\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 87.92313293031472 setps: 178 count: 178\n",
      "reward: 22.233260099709163 setps: 59 count: 237\n",
      "reward: 101.72123442389127 setps: 189 count: 426\n",
      "avg rewards: 70.62587581797172\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.28457\n",
      "Epoch:20 Batch:11 Loss:0.01717\n",
      "Epoch:40 Batch:11 Loss:0.01244\n",
      "Epoch:60 Batch:11 Loss:0.00961\n",
      "Epoch:80 Batch:11 Loss:0.00829\n",
      "Epoch:100 Batch:11 Loss:0.00849\n",
      "Epoch:120 Batch:11 Loss:0.00763\n",
      "Epoch:140 Batch:11 Loss:0.00765\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.071\n",
      "Epoch:10 Batch:10 Loss:0.069\n",
      "Epoch:20 Batch:10 Loss:0.070\n",
      "Epoch:30 Batch:10 Loss:0.070\n",
      "Epoch:40 Batch:10 Loss:0.071\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 112.09389835774198 setps: 113 count: 113\n",
      "reward: 38.548403049162864 setps: 66 count: 179\n",
      "reward: 91.31947306508371 setps: 84 count: 263\n",
      "reward: 36.17525046486407 setps: 61 count: 324\n",
      "reward: 76.30160426535878 setps: 59 count: 383\n",
      "reward: 109.84074448217577 setps: 168 count: 551\n",
      "reward: 39.99582680327004 setps: 64 count: 615\n",
      "reward: 40.50070226891257 setps: 64 count: 679\n",
      "reward: 93.12200268615997 setps: 86 count: 765\n",
      "reward: 76.03397755764162 setps: 62 count: 827\n",
      "reward: 49.630291093302404 setps: 80 count: 907\n",
      "reward: 36.706076514560834 setps: 63 count: 970\n",
      "avg rewards: 66.68902088401956\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.19921\n",
      "Epoch:20 Batch:12 Loss:0.01837\n",
      "Epoch:40 Batch:12 Loss:0.01105\n",
      "Epoch:60 Batch:12 Loss:0.00895\n",
      "Epoch:80 Batch:12 Loss:0.00926\n",
      "Epoch:100 Batch:12 Loss:0.00754\n",
      "Epoch:120 Batch:12 Loss:0.00696\n",
      "Epoch:140 Batch:12 Loss:0.00688\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.049\n",
      "Epoch:10 Batch:10 Loss:0.049\n",
      "Epoch:20 Batch:10 Loss:0.048\n",
      "Epoch:30 Batch:10 Loss:0.049\n",
      "Epoch:40 Batch:10 Loss:0.049\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 28.594940365049116 setps: 71 count: 71\n",
      "reward: 65.42888638537404 setps: 115 count: 186\n",
      "reward: 84.58831390919438 setps: 141 count: 327\n",
      "reward: 34.30953569748962 setps: 84 count: 411\n",
      "reward: 75.35128246268576 setps: 140 count: 551\n",
      "avg rewards: 57.65459176395858\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.14161\n",
      "Epoch:20 Batch:13 Loss:0.01890\n",
      "Epoch:40 Batch:13 Loss:0.01205\n",
      "Epoch:60 Batch:13 Loss:0.00969\n",
      "Epoch:80 Batch:13 Loss:0.00828\n",
      "Epoch:100 Batch:13 Loss:0.00780\n",
      "Epoch:120 Batch:13 Loss:0.00815\n",
      "Epoch:140 Batch:13 Loss:0.00810\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.052\n",
      "Epoch:10 Batch:10 Loss:0.052\n",
      "Epoch:20 Batch:10 Loss:0.052\n",
      "Epoch:30 Batch:10 Loss:0.052\n",
      "Epoch:40 Batch:10 Loss:0.052\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 338.4783985891649 setps: 487 count: 487\n",
      "avg rewards: 338.4783985891649\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.09310\n",
      "Epoch:20 Batch:14 Loss:0.01462\n",
      "Epoch:40 Batch:14 Loss:0.01085\n",
      "Epoch:60 Batch:14 Loss:0.00951\n",
      "Epoch:80 Batch:14 Loss:0.00762\n",
      "Epoch:100 Batch:14 Loss:0.00741\n",
      "Epoch:120 Batch:14 Loss:0.00681\n",
      "Epoch:140 Batch:14 Loss:0.00654\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.054\n",
      "Epoch:10 Batch:10 Loss:0.053\n",
      "Epoch:20 Batch:10 Loss:0.052\n",
      "Epoch:30 Batch:10 Loss:0.053\n",
      "Epoch:40 Batch:10 Loss:0.054\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 66.18068879902133 setps: 127 count: 127\n",
      "reward: 65.07068301863183 setps: 71 count: 198\n",
      "reward: 26.514311146984976 setps: 73 count: 271\n",
      "avg rewards: 52.5885609882127\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.16743\n",
      "Epoch:20 Batch:15 Loss:0.01537\n",
      "Epoch:40 Batch:15 Loss:0.00991\n",
      "Epoch:60 Batch:15 Loss:0.00879\n",
      "Epoch:80 Batch:15 Loss:0.00801\n",
      "Epoch:100 Batch:15 Loss:0.00739\n",
      "Epoch:120 Batch:15 Loss:0.00593\n",
      "Epoch:140 Batch:15 Loss:0.00646\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.055\n",
      "Epoch:10 Batch:10 Loss:0.054\n",
      "Epoch:20 Batch:10 Loss:0.054\n",
      "Epoch:30 Batch:10 Loss:0.053\n",
      "Epoch:40 Batch:10 Loss:0.053\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 560.5249286467026 setps: 800 count: 800\n",
      "reward: 75.63525981758717 setps: 130 count: 930\n",
      "avg rewards: 318.0800942321449\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.07321\n",
      "Epoch:20 Batch:16 Loss:0.01228\n",
      "Epoch:40 Batch:16 Loss:0.00989\n",
      "Epoch:60 Batch:16 Loss:0.00780\n",
      "Epoch:80 Batch:16 Loss:0.00714\n",
      "Epoch:100 Batch:16 Loss:0.00680\n",
      "Epoch:120 Batch:16 Loss:0.00681\n",
      "Epoch:140 Batch:16 Loss:0.00560\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.053\n",
      "Epoch:10 Batch:10 Loss:0.053\n",
      "Epoch:20 Batch:10 Loss:0.052\n",
      "Epoch:30 Batch:10 Loss:0.052\n",
      "Epoch:40 Batch:10 Loss:0.053\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 45.735074510860436 setps: 81 count: 81\n",
      "reward: 104.38532545574564 setps: 110 count: 191\n",
      "reward: 72.07935808874464 setps: 69 count: 260\n",
      "reward: 91.85869886332802 setps: 93 count: 353\n",
      "reward: 35.49309556988883 setps: 58 count: 411\n",
      "reward: 97.64579370855063 setps: 99 count: 510\n",
      "reward: 32.43334583596589 setps: 68 count: 578\n",
      "reward: 67.87345571120677 setps: 66 count: 644\n",
      "reward: 39.29001806122542 setps: 71 count: 715\n",
      "reward: 40.05422078802949 setps: 69 count: 784\n",
      "reward: 34.63064011210954 setps: 62 count: 846\n",
      "reward: 70.961633147049 setps: 122 count: 968\n",
      "avg rewards: 61.03672165439203\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.06651\n",
      "Epoch:20 Batch:17 Loss:0.01288\n",
      "Epoch:40 Batch:17 Loss:0.00895\n",
      "Epoch:60 Batch:17 Loss:0.00809\n",
      "Epoch:80 Batch:17 Loss:0.00732\n",
      "Epoch:100 Batch:17 Loss:0.00694\n",
      "Epoch:120 Batch:17 Loss:0.00580\n",
      "Epoch:140 Batch:17 Loss:0.00594\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.051\n",
      "Epoch:10 Batch:10 Loss:0.050\n",
      "Epoch:20 Batch:10 Loss:0.050\n",
      "Epoch:30 Batch:10 Loss:0.049\n",
      "Epoch:40 Batch:10 Loss:0.050\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 72.15221991341973 setps: 66 count: 66\n",
      "reward: 80.31065746142272 setps: 123 count: 189\n",
      "reward: 71.32035875141592 setps: 64 count: 253\n",
      "reward: 79.72379738863384 setps: 77 count: 330\n",
      "reward: 95.18463753604443 setps: 94 count: 424\n",
      "reward: 32.054242406001144 setps: 63 count: 487\n",
      "reward: 75.84240533751581 setps: 70 count: 557\n",
      "reward: 80.83520427203186 setps: 76 count: 633\n",
      "reward: 31.655712060125364 setps: 58 count: 691\n",
      "reward: 73.25908352568949 setps: 61 count: 752\n",
      "reward: 55.892086992325495 setps: 96 count: 848\n",
      "reward: 71.45882003106671 setps: 67 count: 915\n",
      "reward: 71.85503000155734 setps: 64 count: 979\n",
      "avg rewards: 68.58032735978844\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.06918\n",
      "Epoch:20 Batch:18 Loss:0.01344\n",
      "Epoch:40 Batch:18 Loss:0.00940\n",
      "Epoch:60 Batch:18 Loss:0.00711\n",
      "Epoch:80 Batch:18 Loss:0.00666\n",
      "Epoch:100 Batch:18 Loss:0.00663\n",
      "Epoch:120 Batch:18 Loss:0.00657\n",
      "Epoch:140 Batch:18 Loss:0.00637\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.050\n",
      "Epoch:10 Batch:10 Loss:0.049\n",
      "Epoch:20 Batch:10 Loss:0.048\n",
      "Epoch:30 Batch:10 Loss:0.049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:40 Batch:10 Loss:0.048\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 54.78618648075208 setps: 38 count: 38\n",
      "reward: 57.856040076271164 setps: 45 count: 83\n",
      "reward: 40.890231130564636 setps: 40 count: 123\n",
      "reward: 35.53147961218928 setps: 27 count: 150\n",
      "reward: 47.72674782999966 setps: 39 count: 189\n",
      "reward: 53.81743331873004 setps: 43 count: 232\n",
      "reward: 47.689892064199384 setps: 41 count: 273\n",
      "reward: 35.505379427851594 setps: 33 count: 306\n",
      "reward: 44.02044988432754 setps: 34 count: 340\n",
      "reward: 54.56162435526204 setps: 50 count: 390\n",
      "reward: 41.849988806425245 setps: 33 count: 423\n",
      "reward: 41.76652932784781 setps: 38 count: 461\n",
      "reward: 53.40755653045053 setps: 48 count: 509\n",
      "reward: 45.11848143875977 setps: 34 count: 543\n",
      "reward: 43.12820487857971 setps: 33 count: 576\n",
      "reward: 39.79410715506092 setps: 34 count: 610\n",
      "reward: 44.497182556102054 setps: 51 count: 661\n",
      "reward: 44.36932478145173 setps: 45 count: 706\n",
      "reward: 40.90978123928071 setps: 33 count: 739\n",
      "reward: 45.57391567581509 setps: 39 count: 778\n",
      "reward: 43.43715692173573 setps: 45 count: 823\n",
      "reward: 55.9616370718606 setps: 43 count: 866\n",
      "reward: 45.22603758765036 setps: 42 count: 908\n",
      "reward: 41.46422605973931 setps: 47 count: 955\n",
      "avg rewards: 45.78706642545446\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.05733\n",
      "Epoch:20 Batch:19 Loss:0.01307\n",
      "Epoch:40 Batch:19 Loss:0.00933\n",
      "Epoch:60 Batch:19 Loss:0.00822\n",
      "Epoch:80 Batch:19 Loss:0.00789\n",
      "Epoch:100 Batch:19 Loss:0.00713\n",
      "Epoch:120 Batch:19 Loss:0.00622\n",
      "Epoch:140 Batch:19 Loss:0.00596\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.051\n",
      "Epoch:10 Batch:10 Loss:0.050\n",
      "Epoch:20 Batch:10 Loss:0.050\n",
      "Epoch:30 Batch:10 Loss:0.051\n",
      "Epoch:40 Batch:10 Loss:0.050\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 115.23668430144315 setps: 115 count: 115\n",
      "reward: 80.40723159808113 setps: 64 count: 179\n",
      "reward: 36.83793639873473 setps: 60 count: 239\n",
      "reward: 43.526805219739614 setps: 66 count: 305\n",
      "reward: 40.59442373249621 setps: 64 count: 369\n",
      "reward: 44.325938824001064 setps: 73 count: 442\n",
      "reward: 68.52399687956205 setps: 56 count: 498\n",
      "reward: 101.44183943129467 setps: 92 count: 590\n",
      "reward: 76.28106579402666 setps: 65 count: 655\n",
      "reward: 39.69594779904583 setps: 65 count: 720\n",
      "reward: 100.54569846284721 setps: 85 count: 805\n",
      "reward: 43.01603861445183 setps: 68 count: 873\n",
      "reward: 68.89753771320973 setps: 58 count: 931\n",
      "reward: 31.522929385770116 setps: 52 count: 983\n",
      "avg rewards: 63.63243386819314\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.07177\n",
      "Epoch:20 Batch:20 Loss:0.01183\n",
      "Epoch:40 Batch:20 Loss:0.00967\n",
      "Epoch:60 Batch:20 Loss:0.00769\n",
      "Epoch:80 Batch:20 Loss:0.00731\n",
      "Epoch:100 Batch:20 Loss:0.00547\n",
      "Epoch:120 Batch:20 Loss:0.00599\n",
      "Epoch:140 Batch:20 Loss:0.00588\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.047\n",
      "Epoch:10 Batch:10 Loss:0.046\n",
      "Epoch:20 Batch:10 Loss:0.045\n",
      "Epoch:30 Batch:10 Loss:0.045\n",
      "Epoch:40 Batch:10 Loss:0.045\n",
      "Done!\n",
      "############# start HopperBulletEnv-v0 training ###################\n",
      "(49,)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 19.940754284516153 setps: 11 count: 11\n",
      "reward: 15.18361010433291 setps: 9 count: 20\n",
      "reward: 20.427877554857695 setps: 13 count: 33\n",
      "reward: 21.88116727068264 setps: 11 count: 44\n",
      "reward: 22.24640270066593 setps: 10 count: 54\n",
      "reward: 24.771825490304035 setps: 17 count: 71\n",
      "reward: 19.916474294567887 setps: 9 count: 80\n",
      "reward: 17.87030452345207 setps: 12 count: 92\n",
      "reward: 14.777987863341693 setps: 12 count: 104\n",
      "reward: 23.02766747134592 setps: 15 count: 119\n",
      "reward: 18.266081375647627 setps: 18 count: 137\n",
      "reward: 24.400351782921643 setps: 12 count: 149\n",
      "reward: 27.543598983506676 setps: 34 count: 183\n",
      "reward: 21.137342184047156 setps: 9 count: 192\n",
      "reward: 23.6751615703819 setps: 11 count: 203\n",
      "reward: 15.675702211250607 setps: 11 count: 214\n",
      "reward: 20.3985522982839 setps: 19 count: 233\n",
      "reward: 19.29716568623553 setps: 9 count: 242\n",
      "reward: 21.29027102702239 setps: 9 count: 251\n",
      "reward: 22.55049308847811 setps: 19 count: 270\n",
      "reward: 24.892823693787793 setps: 24 count: 294\n",
      "reward: 20.740839364171553 setps: 10 count: 304\n",
      "reward: 20.685774354219028 setps: 8 count: 312\n",
      "reward: 13.528804626743659 setps: 13 count: 325\n",
      "reward: 20.673734619058088 setps: 9 count: 334\n",
      "reward: 20.597663600507072 setps: 13 count: 347\n",
      "reward: 18.421369598315504 setps: 8 count: 355\n",
      "reward: 20.34181689176621 setps: 23 count: 378\n",
      "reward: 17.067862376091945 setps: 7 count: 385\n",
      "reward: 17.332808136034874 setps: 7 count: 392\n",
      "reward: 24.818784885395143 setps: 16 count: 408\n",
      "reward: 17.70183660267794 setps: 11 count: 419\n",
      "reward: 16.518992313581112 setps: 18 count: 437\n",
      "reward: 21.415015180989577 setps: 10 count: 447\n",
      "reward: 19.99178239277972 setps: 14 count: 461\n",
      "reward: 24.13352903753257 setps: 36 count: 497\n",
      "reward: 16.29371288717084 setps: 11 count: 508\n",
      "reward: 20.296380166715245 setps: 8 count: 516\n",
      "reward: 22.727720259167835 setps: 11 count: 527\n",
      "reward: 24.936544267747372 setps: 14 count: 541\n",
      "reward: 20.14505569097528 setps: 9 count: 550\n",
      "reward: 18.178493603259266 setps: 12 count: 562\n",
      "reward: 21.499036535067717 setps: 11 count: 573\n",
      "reward: 17.506575718449312 setps: 18 count: 591\n",
      "reward: 16.781879988196305 setps: 5 count: 596\n",
      "reward: 23.855096632603093 setps: 18 count: 614\n",
      "reward: 18.34478324249503 setps: 13 count: 627\n",
      "reward: 20.569078911659016 setps: 10 count: 637\n",
      "reward: 21.610378441622018 setps: 9 count: 646\n",
      "reward: 26.756845662239353 setps: 23 count: 669\n",
      "reward: 19.578665946373075 setps: 8 count: 677\n",
      "reward: 19.765895238333908 setps: 13 count: 690\n",
      "reward: 22.462712116261535 setps: 16 count: 706\n",
      "reward: 19.1706065392209 setps: 14 count: 720\n",
      "reward: 25.21183347274491 setps: 14 count: 734\n",
      "reward: 18.747514857458007 setps: 15 count: 749\n",
      "reward: 20.415830333587653 setps: 8 count: 757\n",
      "reward: 20.178940688955485 setps: 13 count: 770\n",
      "reward: 21.004408671590497 setps: 15 count: 785\n",
      "reward: 21.317462889206944 setps: 11 count: 796\n",
      "reward: 18.98967876029492 setps: 23 count: 819\n",
      "reward: 22.10876309793384 setps: 10 count: 829\n",
      "reward: 17.835255807251087 setps: 8 count: 837\n",
      "reward: 20.434889454625957 setps: 8 count: 845\n",
      "reward: 20.096150887824475 setps: 14 count: 859\n",
      "reward: 28.422177244794145 setps: 23 count: 882\n",
      "reward: 21.92841461336066 setps: 11 count: 893\n",
      "reward: 19.863832522036677 setps: 16 count: 909\n",
      "reward: 20.244243758390073 setps: 9 count: 918\n",
      "reward: 20.740309191960836 setps: 8 count: 926\n",
      "reward: 21.235046094466817 setps: 10 count: 936\n",
      "reward: 20.61790115200274 setps: 15 count: 951\n",
      "reward: 21.763729312180658 setps: 10 count: 961\n",
      "reward: 25.240172596130286 setps: 14 count: 975\n",
      "reward: 22.081607468234143 setps: 12 count: 987\n",
      "avg rewards: 20.694664722161097\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:1.44883\n",
      "Epoch:20 Batch:1 Loss:0.15779\n",
      "Epoch:40 Batch:1 Loss:0.10894\n",
      "Epoch:60 Batch:1 Loss:0.07597\n",
      "Epoch:80 Batch:1 Loss:0.06351\n",
      "Epoch:100 Batch:1 Loss:0.05934\n",
      "Epoch:120 Batch:1 Loss:0.05696\n",
      "Epoch:140 Batch:1 Loss:0.05189\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.114\n",
      "Epoch:10 Batch:10 Loss:0.109\n",
      "Epoch:20 Batch:10 Loss:0.102\n",
      "Epoch:30 Batch:10 Loss:0.106\n",
      "Epoch:40 Batch:10 Loss:0.115\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 16.448426009462857 setps: 32 count: 32\n",
      "reward: 22.621345538263267 setps: 34 count: 66\n",
      "reward: 23.753733076754724 setps: 37 count: 103\n",
      "reward: 32.11825411129393 setps: 42 count: 145\n",
      "reward: 24.854867109819317 setps: 37 count: 182\n",
      "reward: 25.069764378549007 setps: 37 count: 219\n",
      "reward: 15.647420101043826 setps: 32 count: 251\n",
      "reward: 26.005944019704472 setps: 39 count: 290\n",
      "reward: 25.203605186902863 setps: 36 count: 326\n",
      "reward: 23.808286572217185 setps: 37 count: 363\n",
      "reward: 18.653346943293585 setps: 34 count: 397\n",
      "reward: 20.18614555426175 setps: 33 count: 430\n",
      "reward: 19.354857087701383 setps: 32 count: 462\n",
      "reward: 24.84922201712907 setps: 36 count: 498\n",
      "reward: 16.407859123236268 setps: 31 count: 529\n",
      "reward: 23.024301779166855 setps: 35 count: 564\n",
      "reward: 13.066512100855471 setps: 31 count: 595\n",
      "reward: 22.467446254719107 setps: 34 count: 629\n",
      "reward: 26.953398642830145 setps: 37 count: 666\n",
      "reward: 18.74100523347588 setps: 34 count: 700\n",
      "reward: 20.967258767489692 setps: 34 count: 734\n",
      "reward: 7.808770422359519 setps: 31 count: 765\n",
      "reward: 16.249425203323334 setps: 32 count: 797\n",
      "reward: 22.086484173277857 setps: 34 count: 831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 26.2681250719048 setps: 39 count: 870\n",
      "reward: 31.486631624029535 setps: 41 count: 911\n",
      "reward: 22.18574255969961 setps: 36 count: 947\n",
      "reward: 19.208260903094928 setps: 33 count: 980\n",
      "avg rewards: 21.62487284163787\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:1.12823\n",
      "Epoch:20 Batch:2 Loss:0.10004\n",
      "Epoch:40 Batch:2 Loss:0.06014\n",
      "Epoch:60 Batch:2 Loss:0.04437\n",
      "Epoch:80 Batch:2 Loss:0.03899\n",
      "Epoch:100 Batch:2 Loss:0.03475\n",
      "Epoch:120 Batch:2 Loss:0.03314\n",
      "Epoch:140 Batch:2 Loss:0.03074\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.087\n",
      "Epoch:10 Batch:10 Loss:0.079\n",
      "Epoch:20 Batch:10 Loss:0.082\n",
      "Epoch:30 Batch:10 Loss:0.081\n",
      "Epoch:40 Batch:10 Loss:0.077\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 51.61708714796405 setps: 38 count: 38\n",
      "reward: 48.64578471744025 setps: 34 count: 72\n",
      "reward: 40.41579676186957 setps: 27 count: 99\n",
      "reward: 67.54437920903872 setps: 59 count: 158\n",
      "reward: 40.54276213004633 setps: 28 count: 186\n",
      "reward: 66.77161624576256 setps: 78 count: 264\n",
      "reward: 53.63157931411553 setps: 38 count: 302\n",
      "reward: 52.594875180967215 setps: 38 count: 340\n",
      "reward: 43.783571034317724 setps: 30 count: 370\n",
      "reward: 48.13776056468778 setps: 35 count: 405\n",
      "reward: 45.132886880307346 setps: 32 count: 437\n",
      "reward: 49.505832736555014 setps: 35 count: 472\n",
      "reward: 43.12830693813447 setps: 31 count: 503\n",
      "reward: 33.82890552139723 setps: 73 count: 576\n",
      "reward: 53.41230319232418 setps: 40 count: 616\n",
      "reward: 44.875664668345415 setps: 53 count: 669\n",
      "reward: 51.14899353186484 setps: 41 count: 710\n",
      "reward: 52.322540141016354 setps: 39 count: 749\n",
      "reward: 61.77237529988925 setps: 63 count: 812\n",
      "reward: 52.34782216743188 setps: 41 count: 853\n",
      "reward: 49.96493211274064 setps: 37 count: 890\n",
      "reward: 51.36758976526471 setps: 37 count: 927\n",
      "reward: 45.419532750756474 setps: 32 count: 959\n",
      "reward: 51.97502045186848 setps: 36 count: 995\n",
      "avg rewards: 49.99532993600442\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.98150\n",
      "Epoch:20 Batch:3 Loss:0.06427\n",
      "Epoch:40 Batch:3 Loss:0.04246\n",
      "Epoch:60 Batch:3 Loss:0.03303\n",
      "Epoch:80 Batch:3 Loss:0.03097\n",
      "Epoch:100 Batch:3 Loss:0.02811\n",
      "Epoch:120 Batch:3 Loss:0.02566\n",
      "Epoch:140 Batch:3 Loss:0.02518\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.078\n",
      "Epoch:10 Batch:10 Loss:0.069\n",
      "Epoch:20 Batch:10 Loss:0.070\n",
      "Epoch:30 Batch:10 Loss:0.077\n",
      "Epoch:40 Batch:10 Loss:0.071\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 43.14209348177245 setps: 46 count: 46\n",
      "reward: 22.389094631740594 setps: 36 count: 82\n",
      "reward: 30.777011301426686 setps: 37 count: 119\n",
      "reward: 34.35914706045441 setps: 45 count: 164\n",
      "reward: 27.180123568698754 setps: 46 count: 210\n",
      "reward: 35.84645139283384 setps: 42 count: 252\n",
      "reward: 38.33860794524225 setps: 43 count: 295\n",
      "reward: 31.61181738781889 setps: 37 count: 332\n",
      "reward: 45.15659365841129 setps: 47 count: 379\n",
      "reward: 35.97374890273785 setps: 41 count: 420\n",
      "reward: 35.15227252576442 setps: 40 count: 460\n",
      "reward: 34.81700018409029 setps: 41 count: 501\n",
      "reward: 38.21029612151761 setps: 43 count: 544\n",
      "reward: 30.241434761468557 setps: 36 count: 580\n",
      "reward: 43.3918403907708 setps: 55 count: 635\n",
      "reward: 49.414223902662336 setps: 52 count: 687\n",
      "reward: 32.24304736399354 setps: 38 count: 725\n",
      "reward: 35.91168106318073 setps: 40 count: 765\n",
      "reward: 45.18936456594821 setps: 56 count: 821\n",
      "reward: 39.41119777144632 setps: 43 count: 864\n",
      "reward: 41.65810250471114 setps: 48 count: 912\n",
      "reward: 35.6255513632219 setps: 42 count: 954\n",
      "reward: 31.260704369581077 setps: 41 count: 995\n",
      "avg rewards: 36.404408966064956\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.75052\n",
      "Epoch:20 Batch:4 Loss:0.05496\n",
      "Epoch:40 Batch:4 Loss:0.03216\n",
      "Epoch:60 Batch:4 Loss:0.02885\n",
      "Epoch:80 Batch:4 Loss:0.02534\n",
      "Epoch:100 Batch:4 Loss:0.02317\n",
      "Epoch:120 Batch:4 Loss:0.02050\n",
      "Epoch:140 Batch:4 Loss:0.02176\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.058\n",
      "Epoch:10 Batch:10 Loss:0.070\n",
      "Epoch:20 Batch:10 Loss:0.065\n",
      "Epoch:30 Batch:10 Loss:0.063\n",
      "Epoch:40 Batch:10 Loss:0.060\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 45.26460298869061 setps: 30 count: 30\n",
      "reward: 43.52117888946668 setps: 29 count: 59\n",
      "reward: 44.558020961900176 setps: 36 count: 95\n",
      "reward: 44.911173883201265 setps: 30 count: 125\n",
      "reward: 42.769331786788825 setps: 29 count: 154\n",
      "reward: 43.61515970087202 setps: 29 count: 183\n",
      "reward: 39.9614131283277 setps: 27 count: 210\n",
      "reward: 44.61502404827917 setps: 29 count: 239\n",
      "reward: 42.20526708252728 setps: 31 count: 270\n",
      "reward: 39.83738878335134 setps: 26 count: 296\n",
      "reward: 41.83053699184675 setps: 28 count: 324\n",
      "reward: 43.9522300760218 setps: 29 count: 353\n",
      "reward: 42.153098475233115 setps: 28 count: 381\n",
      "reward: 42.05577531827004 setps: 28 count: 409\n",
      "reward: 44.04825192579301 setps: 28 count: 437\n",
      "reward: 42.84725328781932 setps: 37 count: 474\n",
      "reward: 44.774235395595326 setps: 29 count: 503\n",
      "reward: 43.244336348873915 setps: 29 count: 532\n",
      "reward: 45.99616895800137 setps: 31 count: 563\n",
      "reward: 40.72614562327653 setps: 27 count: 590\n",
      "reward: 39.66016343845986 setps: 26 count: 616\n",
      "reward: 37.00725715998852 setps: 25 count: 641\n",
      "reward: 47.54273257810274 setps: 35 count: 676\n",
      "reward: 45.41777615309693 setps: 30 count: 706\n",
      "reward: 44.651775548532896 setps: 32 count: 738\n",
      "reward: 41.33801936428791 setps: 27 count: 765\n",
      "reward: 49.249412227670845 setps: 35 count: 800\n",
      "reward: 44.24094176858925 setps: 32 count: 832\n",
      "reward: 40.36625920643127 setps: 27 count: 859\n",
      "reward: 48.252652566891626 setps: 32 count: 891\n",
      "reward: 44.827782676463556 setps: 31 count: 922\n",
      "reward: 41.06375900603598 setps: 35 count: 957\n",
      "reward: 44.390646030080084 setps: 30 count: 987\n",
      "avg rewards: 43.36047792056872\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.61746\n",
      "Epoch:20 Batch:5 Loss:0.04139\n",
      "Epoch:40 Batch:5 Loss:0.02859\n",
      "Epoch:60 Batch:5 Loss:0.02617\n",
      "Epoch:80 Batch:5 Loss:0.02187\n",
      "Epoch:100 Batch:5 Loss:0.02093\n",
      "Epoch:120 Batch:5 Loss:0.02126\n",
      "Epoch:140 Batch:5 Loss:0.02019\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.074\n",
      "Epoch:10 Batch:10 Loss:0.062\n",
      "Epoch:20 Batch:10 Loss:0.065\n",
      "Epoch:30 Batch:10 Loss:0.066\n",
      "Epoch:40 Batch:10 Loss:0.065\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 21.663964053480594 setps: 33 count: 33\n",
      "reward: 22.481962030912108 setps: 34 count: 67\n",
      "reward: 31.447035791570666 setps: 38 count: 105\n",
      "reward: 31.21860454204725 setps: 37 count: 142\n",
      "reward: 29.59078089951945 setps: 36 count: 178\n",
      "reward: 36.32663213972119 setps: 42 count: 220\n",
      "reward: 33.88779139732068 setps: 41 count: 261\n",
      "reward: 30.32395134536709 setps: 39 count: 300\n",
      "reward: 26.6729683071353 setps: 37 count: 337\n",
      "reward: 24.71473592481052 setps: 36 count: 373\n",
      "reward: 17.25347591959726 setps: 32 count: 405\n",
      "reward: 24.20956384843594 setps: 34 count: 439\n",
      "reward: 27.65179143235582 setps: 36 count: 475\n",
      "reward: 34.73113900017779 setps: 41 count: 516\n",
      "reward: 32.76897457781306 setps: 40 count: 556\n",
      "reward: 29.748628897345046 setps: 39 count: 595\n",
      "reward: 25.962495991685138 setps: 37 count: 632\n",
      "reward: 28.398234284983385 setps: 38 count: 670\n",
      "reward: 17.268392497658166 setps: 32 count: 702\n",
      "reward: 18.028076451580997 setps: 33 count: 735\n",
      "reward: 23.837547671048373 setps: 33 count: 768\n",
      "reward: 33.138677708509206 setps: 41 count: 809\n",
      "reward: 4.412688569250168 setps: 29 count: 838\n",
      "reward: 21.165353809657972 setps: 33 count: 871\n",
      "reward: 30.12302191106719 setps: 39 count: 910\n",
      "reward: 30.063193399862207 setps: 37 count: 947\n",
      "reward: 22.66979423299927 setps: 33 count: 980\n",
      "avg rewards: 26.287388023552293\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.46218\n",
      "Epoch:20 Batch:6 Loss:0.03726\n",
      "Epoch:40 Batch:6 Loss:0.02681\n",
      "Epoch:60 Batch:6 Loss:0.02196\n",
      "Epoch:80 Batch:6 Loss:0.02060\n",
      "Epoch:100 Batch:6 Loss:0.01761\n",
      "Epoch:120 Batch:6 Loss:0.01861\n",
      "Epoch:140 Batch:6 Loss:0.01797\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.068\n",
      "Epoch:10 Batch:10 Loss:0.065\n",
      "Epoch:20 Batch:10 Loss:0.060\n",
      "Epoch:30 Batch:10 Loss:0.064\n",
      "Epoch:40 Batch:10 Loss:0.063\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 34.64505672785307 setps: 41 count: 41\n",
      "reward: 39.31650837708438 setps: 42 count: 83\n",
      "reward: 44.837686573156695 setps: 48 count: 131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 38.77655815846956 setps: 45 count: 176\n",
      "reward: 36.0244909203786 setps: 45 count: 221\n",
      "reward: 27.973283593886297 setps: 35 count: 256\n",
      "reward: 32.989154236993635 setps: 39 count: 295\n",
      "reward: 32.37591528341581 setps: 37 count: 332\n",
      "reward: 40.75175583506934 setps: 43 count: 375\n",
      "reward: 33.90499462870794 setps: 37 count: 412\n",
      "reward: 42.28203317723383 setps: 44 count: 456\n",
      "reward: 40.43864392593096 setps: 42 count: 498\n",
      "reward: 35.602563713450216 setps: 41 count: 539\n",
      "reward: 27.220185902023516 setps: 34 count: 573\n",
      "reward: 21.175766878922875 setps: 38 count: 611\n",
      "reward: 34.20442852440319 setps: 39 count: 650\n",
      "reward: 30.174170562185456 setps: 37 count: 687\n",
      "reward: 36.52562778033607 setps: 39 count: 726\n",
      "reward: 34.18455152289825 setps: 39 count: 765\n",
      "reward: 37.90338078991337 setps: 40 count: 805\n",
      "reward: 34.21678438865783 setps: 38 count: 843\n",
      "reward: 42.09340566506288 setps: 45 count: 888\n",
      "reward: 38.62715761503496 setps: 44 count: 932\n",
      "reward: 32.397279951148086 setps: 39 count: 971\n",
      "avg rewards: 35.3600576971757\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.68396\n",
      "Epoch:20 Batch:7 Loss:0.03411\n",
      "Epoch:40 Batch:7 Loss:0.02372\n",
      "Epoch:60 Batch:7 Loss:0.02002\n",
      "Epoch:80 Batch:7 Loss:0.02033\n",
      "Epoch:100 Batch:7 Loss:0.01749\n",
      "Epoch:120 Batch:7 Loss:0.01706\n",
      "Epoch:140 Batch:7 Loss:0.01536\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.062\n",
      "Epoch:10 Batch:10 Loss:0.062\n",
      "Epoch:20 Batch:10 Loss:0.061\n",
      "Epoch:30 Batch:10 Loss:0.064\n",
      "Epoch:40 Batch:10 Loss:0.058\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 33.65299255480204 setps: 17 count: 17\n",
      "reward: 38.95534372460533 setps: 28 count: 45\n",
      "reward: 40.046051367369344 setps: 28 count: 73\n",
      "reward: 25.236539084017565 setps: 13 count: 86\n",
      "reward: 37.99925652604288 setps: 25 count: 111\n",
      "reward: 43.34002291903016 setps: 29 count: 140\n",
      "reward: 38.37985977570352 setps: 26 count: 166\n",
      "reward: 38.63636681239878 setps: 25 count: 191\n",
      "reward: 45.26076746884646 setps: 31 count: 222\n",
      "reward: 39.42319938604195 setps: 28 count: 250\n",
      "reward: 34.734297895245255 setps: 18 count: 268\n",
      "reward: 39.8342490329771 setps: 28 count: 296\n",
      "reward: 42.38112424485298 setps: 29 count: 325\n",
      "reward: 35.696727048595484 setps: 26 count: 351\n",
      "reward: 36.81320367969746 setps: 26 count: 377\n",
      "reward: 39.88243363217916 setps: 30 count: 407\n",
      "reward: 40.859265864526975 setps: 28 count: 435\n",
      "reward: 41.73904160388775 setps: 35 count: 470\n",
      "reward: 43.31207157461176 setps: 31 count: 501\n",
      "reward: 40.56614195277216 setps: 28 count: 529\n",
      "reward: 41.95904273978376 setps: 28 count: 557\n",
      "reward: 41.29815128407645 setps: 31 count: 588\n",
      "reward: 37.7629673851523 setps: 27 count: 615\n",
      "reward: 39.95963816628937 setps: 25 count: 640\n",
      "reward: 44.123707839228146 setps: 30 count: 670\n",
      "reward: 40.438776343608325 setps: 27 count: 697\n",
      "reward: 42.520809478232685 setps: 29 count: 726\n",
      "reward: 36.85761742271423 setps: 33 count: 759\n",
      "reward: 41.75455047819124 setps: 28 count: 787\n",
      "reward: 38.515469876148565 setps: 25 count: 812\n",
      "reward: 40.146212513418874 setps: 27 count: 839\n",
      "reward: 34.51596991148108 setps: 27 count: 866\n",
      "reward: 42.871566460464955 setps: 29 count: 895\n",
      "reward: 41.03188899139931 setps: 27 count: 922\n",
      "reward: 39.92007072257839 setps: 26 count: 948\n",
      "reward: 37.60315021202404 setps: 26 count: 974\n",
      "avg rewards: 39.389681832583214\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.52192\n",
      "Epoch:20 Batch:8 Loss:0.02885\n",
      "Epoch:40 Batch:8 Loss:0.02230\n",
      "Epoch:60 Batch:8 Loss:0.02067\n",
      "Epoch:80 Batch:8 Loss:0.01767\n",
      "Epoch:100 Batch:8 Loss:0.01757\n",
      "Epoch:120 Batch:8 Loss:0.01584\n",
      "Epoch:140 Batch:8 Loss:0.01484\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.057\n",
      "Epoch:10 Batch:10 Loss:0.059\n",
      "Epoch:20 Batch:10 Loss:0.066\n",
      "Epoch:30 Batch:10 Loss:0.058\n",
      "Epoch:40 Batch:10 Loss:0.052\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 30.415794398778235 setps: 15 count: 15\n",
      "reward: 20.90672571524919 setps: 11 count: 26\n",
      "reward: 60.24628966833406 setps: 45 count: 71\n",
      "reward: 32.370970106987805 setps: 17 count: 88\n",
      "reward: 30.978500057144263 setps: 17 count: 105\n",
      "reward: 54.81096902648132 setps: 40 count: 145\n",
      "reward: 31.775795228879726 setps: 17 count: 162\n",
      "reward: 81.6178369562156 setps: 64 count: 226\n",
      "reward: 71.01614937378037 setps: 56 count: 282\n",
      "reward: 72.69195265089222 setps: 61 count: 343\n",
      "reward: 36.81774255944474 setps: 21 count: 364\n",
      "reward: 21.729900767946674 setps: 13 count: 377\n",
      "reward: 35.591204071616815 setps: 20 count: 397\n",
      "reward: 69.8699480949741 setps: 54 count: 451\n",
      "reward: 29.88681162262801 setps: 15 count: 466\n",
      "reward: 67.23451767147708 setps: 59 count: 525\n",
      "reward: 28.118425284158725 setps: 15 count: 540\n",
      "reward: 47.64923675132885 setps: 37 count: 577\n",
      "reward: 35.43003328322811 setps: 21 count: 598\n",
      "reward: 77.98346542513171 setps: 62 count: 660\n",
      "reward: 26.173726469039682 setps: 13 count: 673\n",
      "reward: 28.346700943572795 setps: 14 count: 687\n",
      "reward: 33.338008468701446 setps: 18 count: 705\n",
      "reward: 33.35614638512052 setps: 17 count: 722\n",
      "reward: 30.943075941711136 setps: 17 count: 739\n",
      "reward: 69.30910576055904 setps: 53 count: 792\n",
      "reward: 32.567467698908885 setps: 17 count: 809\n",
      "reward: 30.99488742284157 setps: 15 count: 824\n",
      "reward: 50.34889700292842 setps: 37 count: 861\n",
      "reward: 69.50417346751782 setps: 52 count: 913\n",
      "reward: 31.45447725365812 setps: 17 count: 930\n",
      "reward: 34.13536073504656 setps: 20 count: 950\n",
      "reward: 28.73778619305376 setps: 15 count: 965\n",
      "reward: 26.50732563491183 setps: 14 count: 979\n",
      "reward: 28.279497781091777 setps: 14 count: 993\n",
      "avg rewards: 42.60396873923831\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.23808\n",
      "Epoch:20 Batch:9 Loss:0.02843\n",
      "Epoch:40 Batch:9 Loss:0.02333\n",
      "Epoch:60 Batch:9 Loss:0.01756\n",
      "Epoch:80 Batch:9 Loss:0.01720\n",
      "Epoch:100 Batch:9 Loss:0.01639\n",
      "Epoch:120 Batch:9 Loss:0.01566\n",
      "Epoch:140 Batch:9 Loss:0.01542\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.057\n",
      "Epoch:10 Batch:10 Loss:0.057\n",
      "Epoch:20 Batch:10 Loss:0.054\n",
      "Epoch:30 Batch:10 Loss:0.056\n",
      "Epoch:40 Batch:10 Loss:0.056\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 43.79047170725391 setps: 47 count: 47\n",
      "reward: 36.536718306347026 setps: 44 count: 91\n",
      "reward: 95.9022947479403 setps: 108 count: 199\n",
      "reward: 55.529365733012675 setps: 58 count: 257\n",
      "reward: 74.82127392646944 setps: 76 count: 333\n",
      "reward: 61.56037912298341 setps: 56 count: 389\n",
      "reward: 45.97472606433587 setps: 48 count: 437\n",
      "reward: 59.66470777030043 setps: 56 count: 493\n",
      "reward: 45.517413464581466 setps: 58 count: 551\n",
      "reward: 57.24741007579432 setps: 55 count: 606\n",
      "reward: 60.87525187385761 setps: 60 count: 666\n",
      "reward: 56.530188467704285 setps: 53 count: 719\n",
      "reward: 49.877930720624875 setps: 49 count: 768\n",
      "reward: 62.48505349021433 setps: 55 count: 823\n",
      "reward: 50.824932054914825 setps: 48 count: 871\n",
      "reward: 47.5337006640868 setps: 51 count: 922\n",
      "reward: 45.626848177979994 setps: 50 count: 972\n",
      "avg rewards: 55.89992155108245\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.22125\n",
      "Epoch:20 Batch:10 Loss:0.02600\n",
      "Epoch:40 Batch:10 Loss:0.01848\n",
      "Epoch:60 Batch:10 Loss:0.01764\n",
      "Epoch:80 Batch:10 Loss:0.01621\n",
      "Epoch:100 Batch:10 Loss:0.01493\n",
      "Epoch:120 Batch:10 Loss:0.01425\n",
      "Epoch:140 Batch:10 Loss:0.01394\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.059\n",
      "Epoch:10 Batch:10 Loss:0.058\n",
      "Epoch:20 Batch:10 Loss:0.053\n",
      "Epoch:30 Batch:10 Loss:0.052\n",
      "Epoch:40 Batch:10 Loss:0.059\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 41.41906432770483 setps: 31 count: 31\n",
      "reward: 53.576076526576095 setps: 53 count: 84\n",
      "reward: 33.98213657939486 setps: 19 count: 103\n",
      "reward: 33.88849013114232 setps: 22 count: 125\n",
      "reward: 76.06487556196953 setps: 74 count: 199\n",
      "reward: 28.71643579429947 setps: 15 count: 214\n",
      "reward: 93.76832524204363 setps: 87 count: 301\n",
      "reward: 50.2016041155439 setps: 40 count: 341\n",
      "reward: 35.76976691162126 setps: 20 count: 361\n",
      "reward: 27.660359181062088 setps: 22 count: 383\n",
      "reward: 89.23111998556413 setps: 97 count: 480\n",
      "reward: 29.40679322620126 setps: 17 count: 497\n",
      "reward: 34.08435897923482 setps: 18 count: 515\n",
      "reward: 33.69387036108092 setps: 19 count: 534\n",
      "reward: 75.99219030528911 setps: 65 count: 599\n",
      "reward: 24.79384123356284 setps: 57 count: 656\n",
      "reward: 36.463687655529064 setps: 36 count: 692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 32.47393495866272 setps: 18 count: 710\n",
      "reward: 48.93559696917364 setps: 37 count: 747\n",
      "reward: 36.07554865695856 setps: 20 count: 767\n",
      "reward: 68.18602215160064 setps: 70 count: 837\n",
      "reward: 28.20203168524604 setps: 16 count: 853\n",
      "reward: 27.422061372497414 setps: 16 count: 869\n",
      "reward: 37.730302837581256 setps: 31 count: 900\n",
      "reward: 31.062969529502155 setps: 17 count: 917\n",
      "reward: 28.209268844949836 setps: 16 count: 933\n",
      "reward: 32.921589292210406 setps: 20 count: 953\n",
      "reward: 34.49255856368836 setps: 18 count: 971\n",
      "reward: 41.33924100210424 setps: 29 count: 1000\n",
      "avg rewards: 42.95738351662053\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.29154\n",
      "Epoch:20 Batch:11 Loss:0.02598\n",
      "Epoch:40 Batch:11 Loss:0.02044\n",
      "Epoch:60 Batch:11 Loss:0.01912\n",
      "Epoch:80 Batch:11 Loss:0.01485\n",
      "Epoch:100 Batch:11 Loss:0.01698\n",
      "Epoch:120 Batch:11 Loss:0.01493\n",
      "Epoch:140 Batch:11 Loss:0.01440\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.046\n",
      "Epoch:10 Batch:10 Loss:0.044\n",
      "Epoch:20 Batch:10 Loss:0.042\n",
      "Epoch:30 Batch:10 Loss:0.045\n",
      "Epoch:40 Batch:10 Loss:0.045\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 65.70107353137135 setps: 67 count: 67\n",
      "reward: 76.08359390192635 setps: 68 count: 135\n",
      "reward: 62.261291319939474 setps: 60 count: 195\n",
      "reward: 104.32809554840202 setps: 118 count: 313\n",
      "reward: 84.0546135833938 setps: 93 count: 406\n",
      "reward: 67.18788586800476 setps: 78 count: 484\n",
      "reward: 88.04896526862139 setps: 93 count: 577\n",
      "reward: 81.63575765632409 setps: 75 count: 652\n",
      "reward: 85.1214778473499 setps: 78 count: 730\n",
      "reward: 62.12247372505519 setps: 64 count: 794\n",
      "reward: 66.483132549435 setps: 68 count: 862\n",
      "reward: 67.04152199251403 setps: 63 count: 925\n",
      "reward: 45.63477236406616 setps: 47 count: 972\n",
      "avg rewards: 73.51574270433873\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.21171\n",
      "Epoch:20 Batch:12 Loss:0.02690\n",
      "Epoch:40 Batch:12 Loss:0.02081\n",
      "Epoch:60 Batch:12 Loss:0.02100\n",
      "Epoch:80 Batch:12 Loss:0.01791\n",
      "Epoch:100 Batch:12 Loss:0.01536\n",
      "Epoch:120 Batch:12 Loss:0.01531\n",
      "Epoch:140 Batch:12 Loss:0.01405\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.050\n",
      "Epoch:10 Batch:10 Loss:0.046\n",
      "Epoch:20 Batch:10 Loss:0.043\n",
      "Epoch:30 Batch:10 Loss:0.044\n",
      "Epoch:40 Batch:10 Loss:0.043\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 86.68221891997386 setps: 81 count: 81\n",
      "reward: 72.60931865067832 setps: 59 count: 140\n",
      "reward: 89.94223399048936 setps: 82 count: 222\n",
      "reward: 76.55102273399389 setps: 79 count: 301\n",
      "reward: 77.27888212850377 setps: 82 count: 383\n",
      "reward: 122.9483831260804 setps: 149 count: 532\n",
      "reward: 38.46029381036934 setps: 37 count: 569\n",
      "reward: 72.36385268879675 setps: 64 count: 633\n",
      "reward: 41.58099826562685 setps: 36 count: 669\n",
      "reward: 106.31037066961872 setps: 99 count: 768\n",
      "reward: 54.126147324564215 setps: 78 count: 846\n",
      "reward: 61.46414292472472 setps: 73 count: 919\n",
      "avg rewards: 75.02648876945166\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.14924\n",
      "Epoch:20 Batch:13 Loss:0.02508\n",
      "Epoch:40 Batch:13 Loss:0.02019\n",
      "Epoch:60 Batch:13 Loss:0.01565\n",
      "Epoch:80 Batch:13 Loss:0.01447\n",
      "Epoch:100 Batch:13 Loss:0.01349\n",
      "Epoch:120 Batch:13 Loss:0.01522\n",
      "Epoch:140 Batch:13 Loss:0.01585\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.037\n",
      "Epoch:10 Batch:10 Loss:0.033\n",
      "Epoch:20 Batch:10 Loss:0.036\n",
      "Epoch:30 Batch:10 Loss:0.037\n",
      "Epoch:40 Batch:10 Loss:0.033\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 34.7948839645178 setps: 68 count: 68\n",
      "reward: 67.08772826587958 setps: 65 count: 133\n",
      "reward: 65.08114253391687 setps: 104 count: 237\n",
      "reward: 34.64565404045424 setps: 65 count: 302\n",
      "reward: 39.882456242057394 setps: 73 count: 375\n",
      "reward: 71.9037459416417 setps: 72 count: 447\n",
      "reward: 22.212353077757868 setps: 50 count: 497\n",
      "reward: 36.7257640830707 setps: 69 count: 566\n",
      "reward: 65.45553784938967 setps: 64 count: 630\n",
      "reward: 37.65286567085567 setps: 69 count: 699\n",
      "reward: 35.4244221295725 setps: 67 count: 766\n",
      "reward: 42.65228224930907 setps: 78 count: 844\n",
      "reward: 35.560269299536586 setps: 69 count: 913\n",
      "reward: 41.6843383628773 setps: 75 count: 988\n",
      "avg rewards: 45.054531693631205\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.12754\n",
      "Epoch:20 Batch:14 Loss:0.02197\n",
      "Epoch:40 Batch:14 Loss:0.02060\n",
      "Epoch:60 Batch:14 Loss:0.01622\n",
      "Epoch:80 Batch:14 Loss:0.01498\n",
      "Epoch:100 Batch:14 Loss:0.01550\n",
      "Epoch:120 Batch:14 Loss:0.01217\n",
      "Epoch:140 Batch:14 Loss:0.01477\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.036\n",
      "Epoch:10 Batch:10 Loss:0.034\n",
      "Epoch:20 Batch:10 Loss:0.036\n",
      "Epoch:30 Batch:10 Loss:0.035\n",
      "Epoch:40 Batch:10 Loss:0.036\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 19.08996918435733 setps: 45 count: 45\n",
      "reward: 31.032959568566007 setps: 59 count: 104\n",
      "reward: 31.9613367743921 setps: 63 count: 167\n",
      "reward: 35.726328276890854 setps: 69 count: 236\n",
      "reward: 36.392324838147005 setps: 67 count: 303\n",
      "reward: 39.17069000877672 setps: 71 count: 374\n",
      "reward: 26.197913145693015 setps: 54 count: 428\n",
      "reward: 30.120330799401557 setps: 58 count: 486\n",
      "reward: 19.000817059323886 setps: 46 count: 532\n",
      "reward: 22.09457186528307 setps: 48 count: 580\n",
      "reward: 54.90511340500961 setps: 66 count: 646\n",
      "reward: 19.994217869799463 setps: 40 count: 686\n",
      "reward: 27.332421886232616 setps: 54 count: 740\n",
      "reward: 22.79710573683114 setps: 49 count: 789\n",
      "reward: 25.102367329185647 setps: 54 count: 843\n",
      "reward: 21.755482468284022 setps: 51 count: 894\n",
      "reward: 22.917338028342066 setps: 51 count: 945\n",
      "reward: 21.116251701569126 setps: 48 count: 993\n",
      "avg rewards: 28.15041888589362\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.11579\n",
      "Epoch:20 Batch:15 Loss:0.02301\n",
      "Epoch:40 Batch:15 Loss:0.01889\n",
      "Epoch:60 Batch:15 Loss:0.01754\n",
      "Epoch:80 Batch:15 Loss:0.01556\n",
      "Epoch:100 Batch:15 Loss:0.01485\n",
      "Epoch:120 Batch:15 Loss:0.01310\n",
      "Epoch:140 Batch:15 Loss:0.01327\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.041\n",
      "Epoch:10 Batch:10 Loss:0.032\n",
      "Epoch:20 Batch:10 Loss:0.036\n",
      "Epoch:30 Batch:10 Loss:0.035\n",
      "Epoch:40 Batch:10 Loss:0.033\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 51.46294442093348 setps: 54 count: 54\n",
      "reward: 35.42909398191404 setps: 37 count: 91\n",
      "reward: 29.540119751667945 setps: 59 count: 150\n",
      "reward: 57.27504774748522 setps: 55 count: 205\n",
      "reward: 63.206314038972764 setps: 63 count: 268\n",
      "reward: 66.91766966671422 setps: 72 count: 340\n",
      "reward: 36.3486479158906 setps: 38 count: 378\n",
      "reward: 22.798641882030644 setps: 50 count: 428\n",
      "reward: 54.00687005662039 setps: 54 count: 482\n",
      "reward: 32.92821010768384 setps: 59 count: 541\n",
      "reward: 40.7049720703275 setps: 44 count: 585\n",
      "reward: 49.9081087201339 setps: 49 count: 634\n",
      "reward: 30.097991985864056 setps: 60 count: 694\n",
      "reward: 39.35200614164496 setps: 41 count: 735\n",
      "reward: 55.6257437817956 setps: 54 count: 789\n",
      "reward: 16.913518508947163 setps: 39 count: 828\n",
      "reward: 46.07491548004472 setps: 48 count: 876\n",
      "reward: 45.128713035030536 setps: 47 count: 923\n",
      "reward: 62.55411567718986 setps: 65 count: 988\n",
      "avg rewards: 44.01440236688902\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.10579\n",
      "Epoch:20 Batch:16 Loss:0.02430\n",
      "Epoch:40 Batch:16 Loss:0.01701\n",
      "Epoch:60 Batch:16 Loss:0.01813\n",
      "Epoch:80 Batch:16 Loss:0.01386\n",
      "Epoch:100 Batch:16 Loss:0.01286\n",
      "Epoch:120 Batch:16 Loss:0.01325\n",
      "Epoch:140 Batch:16 Loss:0.01184\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.035\n",
      "Epoch:10 Batch:10 Loss:0.034\n",
      "Epoch:20 Batch:10 Loss:0.036\n",
      "Epoch:30 Batch:10 Loss:0.034\n",
      "Epoch:40 Batch:10 Loss:0.032\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 45.38230318880232 setps: 46 count: 46\n",
      "reward: 61.82202032700588 setps: 53 count: 99\n",
      "reward: 61.08579698215617 setps: 53 count: 152\n",
      "reward: 50.90986244874367 setps: 48 count: 200\n",
      "reward: 49.20548243562807 setps: 47 count: 247\n",
      "reward: 62.92471622467567 setps: 54 count: 301\n",
      "reward: 61.07260787790147 setps: 55 count: 356\n",
      "reward: 63.436923544354784 setps: 58 count: 414\n",
      "reward: 55.324342898942994 setps: 51 count: 465\n",
      "reward: 53.955607350198264 setps: 50 count: 515\n",
      "reward: 62.018837608821926 setps: 54 count: 569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 59.347116911901665 setps: 53 count: 622\n",
      "reward: 46.66477722624549 setps: 44 count: 666\n",
      "reward: 51.105710278048356 setps: 45 count: 711\n",
      "reward: 63.78897016530098 setps: 59 count: 770\n",
      "reward: 46.21155172936996 setps: 54 count: 824\n",
      "reward: 49.39210766634059 setps: 46 count: 870\n",
      "reward: 51.67331114165574 setps: 53 count: 923\n",
      "reward: 45.13221380825706 setps: 75 count: 998\n",
      "avg rewards: 54.760750516544796\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.11077\n",
      "Epoch:20 Batch:17 Loss:0.01810\n",
      "Epoch:40 Batch:17 Loss:0.01519\n",
      "Epoch:60 Batch:17 Loss:0.01528\n",
      "Epoch:80 Batch:17 Loss:0.01282\n",
      "Epoch:100 Batch:17 Loss:0.01186\n",
      "Epoch:120 Batch:17 Loss:0.01053\n",
      "Epoch:140 Batch:17 Loss:0.01077\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.033\n",
      "Epoch:10 Batch:10 Loss:0.028\n",
      "Epoch:20 Batch:10 Loss:0.031\n",
      "Epoch:30 Batch:10 Loss:0.033\n",
      "Epoch:40 Batch:10 Loss:0.034\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 48.82561975612625 setps: 45 count: 45\n",
      "reward: 37.594177256518726 setps: 68 count: 113\n",
      "reward: 53.06648800388357 setps: 51 count: 164\n",
      "reward: 76.12991355944251 setps: 72 count: 236\n",
      "reward: 55.64441465543494 setps: 52 count: 288\n",
      "reward: 55.83341279101442 setps: 50 count: 338\n",
      "reward: 56.983581381221306 setps: 55 count: 393\n",
      "reward: 58.023742403370846 setps: 92 count: 485\n",
      "reward: 83.46916636810494 setps: 80 count: 565\n",
      "reward: 59.21430926231258 setps: 54 count: 619\n",
      "reward: 60.805554523656596 setps: 57 count: 676\n",
      "reward: 35.16932568852209 setps: 64 count: 740\n",
      "reward: 70.85766822684525 setps: 68 count: 808\n",
      "reward: 61.908571922294506 setps: 60 count: 868\n",
      "reward: 48.31486946079388 setps: 46 count: 914\n",
      "reward: 74.66246554804674 setps: 72 count: 986\n",
      "avg rewards: 58.53145505047432\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.10438\n",
      "Epoch:20 Batch:18 Loss:0.02087\n",
      "Epoch:40 Batch:18 Loss:0.01880\n",
      "Epoch:60 Batch:18 Loss:0.01510\n",
      "Epoch:80 Batch:18 Loss:0.01576\n",
      "Epoch:100 Batch:18 Loss:0.01374\n",
      "Epoch:120 Batch:18 Loss:0.01075\n",
      "Epoch:140 Batch:18 Loss:0.01124\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.034\n",
      "Epoch:10 Batch:10 Loss:0.032\n",
      "Epoch:20 Batch:10 Loss:0.032\n",
      "Epoch:30 Batch:10 Loss:0.035\n",
      "Epoch:40 Batch:10 Loss:0.036\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 53.20010849717508 setps: 51 count: 51\n",
      "reward: 43.85023707000511 setps: 45 count: 96\n",
      "reward: 18.084008548915143 setps: 44 count: 140\n",
      "reward: 96.91320562912158 setps: 130 count: 270\n",
      "reward: 50.63565496055816 setps: 72 count: 342\n",
      "reward: 32.16786393999063 setps: 54 count: 396\n",
      "reward: 31.398757856211155 setps: 56 count: 452\n",
      "reward: 27.924359244971118 setps: 50 count: 502\n",
      "reward: 57.11667969751288 setps: 79 count: 581\n",
      "reward: 49.994697023137995 setps: 48 count: 629\n",
      "reward: 55.62512344539574 setps: 55 count: 684\n",
      "reward: 59.17950255174509 setps: 53 count: 737\n",
      "reward: 47.632385880443316 setps: 44 count: 781\n",
      "reward: 46.82212246163107 setps: 47 count: 828\n",
      "reward: 48.40092618041089 setps: 49 count: 877\n",
      "reward: 33.63113129993727 setps: 58 count: 935\n",
      "reward: 42.772037956320865 setps: 56 count: 991\n",
      "avg rewards: 46.785223661381366\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.08798\n",
      "Epoch:20 Batch:19 Loss:0.01860\n",
      "Epoch:40 Batch:19 Loss:0.01351\n",
      "Epoch:60 Batch:19 Loss:0.01434\n",
      "Epoch:80 Batch:19 Loss:0.01163\n",
      "Epoch:100 Batch:19 Loss:0.01195\n",
      "Epoch:120 Batch:19 Loss:0.01123\n",
      "Epoch:140 Batch:19 Loss:0.01077\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.032\n",
      "Epoch:10 Batch:10 Loss:0.032\n",
      "Epoch:20 Batch:10 Loss:0.033\n",
      "Epoch:30 Batch:10 Loss:0.031\n",
      "Epoch:40 Batch:10 Loss:0.033\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 36.62795564463012 setps: 28 count: 28\n",
      "reward: 36.73607429245022 setps: 31 count: 59\n",
      "reward: 35.42727769707126 setps: 29 count: 88\n",
      "reward: 38.26790231428458 setps: 32 count: 120\n",
      "reward: 37.27690325911098 setps: 29 count: 149\n",
      "reward: 38.45043752441851 setps: 28 count: 177\n",
      "reward: 38.623160005538374 setps: 33 count: 210\n",
      "reward: 40.35879072935495 setps: 30 count: 240\n",
      "reward: 40.14914168550021 setps: 31 count: 271\n",
      "reward: 37.57045447839627 setps: 27 count: 298\n",
      "reward: 37.07556233207869 setps: 31 count: 329\n",
      "reward: 38.58511229198339 setps: 29 count: 358\n",
      "reward: 37.14639404561167 setps: 30 count: 388\n",
      "reward: 39.219765700618154 setps: 30 count: 418\n",
      "reward: 38.705701084979225 setps: 29 count: 447\n",
      "reward: 36.51757246378547 setps: 29 count: 476\n",
      "reward: 34.70892134879833 setps: 27 count: 503\n",
      "reward: 36.60706566590051 setps: 28 count: 531\n",
      "reward: 31.83478944245289 setps: 31 count: 562\n",
      "reward: 36.45142466198594 setps: 29 count: 591\n",
      "reward: 36.73855148062066 setps: 30 count: 621\n",
      "reward: 38.60258229820029 setps: 28 count: 649\n",
      "reward: 38.83485943844281 setps: 30 count: 679\n",
      "reward: 39.14485757625662 setps: 30 count: 709\n",
      "reward: 34.22332695379155 setps: 29 count: 738\n",
      "reward: 27.905471638967 setps: 37 count: 775\n",
      "reward: 38.53986350588093 setps: 29 count: 804\n",
      "reward: 36.57678788471385 setps: 31 count: 835\n",
      "reward: 39.57047450449172 setps: 29 count: 864\n",
      "reward: 38.0091053705517 setps: 29 count: 893\n",
      "reward: 38.05902753131232 setps: 32 count: 925\n",
      "reward: 19.126679816984684 setps: 39 count: 964\n",
      "reward: 33.34017718297836 setps: 29 count: 993\n",
      "avg rewards: 36.51552035915582\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.09068\n",
      "Epoch:20 Batch:20 Loss:0.01990\n",
      "Epoch:40 Batch:20 Loss:0.01638\n",
      "Epoch:60 Batch:20 Loss:0.01369\n",
      "Epoch:80 Batch:20 Loss:0.01142\n",
      "Epoch:100 Batch:20 Loss:0.01268\n",
      "Epoch:120 Batch:20 Loss:0.01071\n",
      "Epoch:140 Batch:20 Loss:0.00981\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.033\n",
      "Epoch:10 Batch:10 Loss:0.032\n",
      "Epoch:20 Batch:10 Loss:0.031\n",
      "Epoch:30 Batch:10 Loss:0.030\n",
      "Epoch:40 Batch:10 Loss:0.032\n",
      "Done!\n",
      "############# start HalfCheetahBulletEnv-v0 training ###################\n",
      "(50, 1000, 26)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1037.5825255152004 setps: 800 count: 800\n",
      "avg rewards: -1037.5825255152004\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:1.30537\n",
      "Epoch:20 Batch:1 Loss:0.22322\n",
      "Epoch:40 Batch:1 Loss:0.18206\n",
      "Epoch:60 Batch:1 Loss:0.14599\n",
      "Epoch:80 Batch:1 Loss:0.12663\n",
      "Epoch:100 Batch:1 Loss:0.11277\n",
      "Epoch:120 Batch:1 Loss:0.10369\n",
      "Epoch:140 Batch:1 Loss:0.09711\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.205\n",
      "Epoch:10 Batch:10 Loss:0.194\n",
      "Epoch:20 Batch:10 Loss:0.191\n",
      "Epoch:30 Batch:10 Loss:0.192\n",
      "Epoch:40 Batch:10 Loss:0.182\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1035.2258515975993 setps: 800 count: 800\n",
      "avg rewards: -1035.2258515975993\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:1.35459\n",
      "Epoch:20 Batch:2 Loss:0.14510\n",
      "Epoch:40 Batch:2 Loss:0.10647\n",
      "Epoch:60 Batch:2 Loss:0.08380\n",
      "Epoch:80 Batch:2 Loss:0.07451\n",
      "Epoch:100 Batch:2 Loss:0.06765\n",
      "Epoch:120 Batch:2 Loss:0.05918\n",
      "Epoch:140 Batch:2 Loss:0.05634\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.141\n",
      "Epoch:10 Batch:10 Loss:0.137\n",
      "Epoch:20 Batch:10 Loss:0.138\n",
      "Epoch:30 Batch:10 Loss:0.135\n",
      "Epoch:40 Batch:10 Loss:0.134\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1069.3548221227798 setps: 800 count: 800\n",
      "avg rewards: -1069.3548221227798\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:1.18120\n",
      "Epoch:20 Batch:3 Loss:0.09977\n",
      "Epoch:40 Batch:3 Loss:0.07357\n",
      "Epoch:60 Batch:3 Loss:0.05636\n",
      "Epoch:80 Batch:3 Loss:0.05256\n",
      "Epoch:100 Batch:3 Loss:0.04814\n",
      "Epoch:120 Batch:3 Loss:0.04243\n",
      "Epoch:140 Batch:3 Loss:0.04062\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.134\n",
      "Epoch:10 Batch:10 Loss:0.135\n",
      "Epoch:20 Batch:10 Loss:0.135\n",
      "Epoch:30 Batch:10 Loss:0.129\n",
      "Epoch:40 Batch:10 Loss:0.128\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1102.105958476708 setps: 800 count: 800\n",
      "avg rewards: -1102.105958476708\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:1.13697\n",
      "Epoch:20 Batch:4 Loss:0.08397\n",
      "Epoch:40 Batch:4 Loss:0.06038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:60 Batch:4 Loss:0.05413\n",
      "Epoch:80 Batch:4 Loss:0.04678\n",
      "Epoch:100 Batch:4 Loss:0.04154\n",
      "Epoch:120 Batch:4 Loss:0.03878\n",
      "Epoch:140 Batch:4 Loss:0.03875\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.117\n",
      "Epoch:10 Batch:10 Loss:0.116\n",
      "Epoch:20 Batch:10 Loss:0.114\n",
      "Epoch:30 Batch:10 Loss:0.109\n",
      "Epoch:40 Batch:10 Loss:0.112\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1134.035872511601 setps: 800 count: 800\n",
      "avg rewards: -1134.035872511601\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.83177\n",
      "Epoch:20 Batch:5 Loss:0.06586\n",
      "Epoch:40 Batch:5 Loss:0.05232\n",
      "Epoch:60 Batch:5 Loss:0.04263\n",
      "Epoch:80 Batch:5 Loss:0.03891\n",
      "Epoch:100 Batch:5 Loss:0.03553\n",
      "Epoch:120 Batch:5 Loss:0.03682\n",
      "Epoch:140 Batch:5 Loss:0.02971\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.114\n",
      "Epoch:10 Batch:10 Loss:0.110\n",
      "Epoch:20 Batch:10 Loss:0.108\n",
      "Epoch:30 Batch:10 Loss:0.108\n",
      "Epoch:40 Batch:10 Loss:0.106\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1103.926130788104 setps: 800 count: 800\n",
      "avg rewards: -1103.926130788104\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.68997\n",
      "Epoch:20 Batch:6 Loss:0.05464\n",
      "Epoch:40 Batch:6 Loss:0.04269\n",
      "Epoch:60 Batch:6 Loss:0.03365\n",
      "Epoch:80 Batch:6 Loss:0.03244\n",
      "Epoch:100 Batch:6 Loss:0.03021\n",
      "Epoch:120 Batch:6 Loss:0.02819\n",
      "Epoch:140 Batch:6 Loss:0.02486\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.106\n",
      "Epoch:10 Batch:10 Loss:0.107\n",
      "Epoch:20 Batch:10 Loss:0.107\n",
      "Epoch:30 Batch:10 Loss:0.106\n",
      "Epoch:40 Batch:10 Loss:0.102\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1179.9925403711109 setps: 800 count: 800\n",
      "avg rewards: -1179.9925403711109\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.91290\n",
      "Epoch:20 Batch:7 Loss:0.04898\n",
      "Epoch:40 Batch:7 Loss:0.03757\n",
      "Epoch:60 Batch:7 Loss:0.03239\n",
      "Epoch:80 Batch:7 Loss:0.03003\n",
      "Epoch:100 Batch:7 Loss:0.02602\n",
      "Epoch:120 Batch:7 Loss:0.02634\n",
      "Epoch:140 Batch:7 Loss:0.02622\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.124\n",
      "Epoch:10 Batch:10 Loss:0.125\n",
      "Epoch:20 Batch:10 Loss:0.123\n",
      "Epoch:30 Batch:10 Loss:0.121\n",
      "Epoch:40 Batch:10 Loss:0.119\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1107.713223629533 setps: 800 count: 800\n",
      "avg rewards: -1107.713223629533\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.58646\n",
      "Epoch:20 Batch:8 Loss:0.04667\n",
      "Epoch:40 Batch:8 Loss:0.03335\n",
      "Epoch:60 Batch:8 Loss:0.03040\n",
      "Epoch:80 Batch:8 Loss:0.02730\n",
      "Epoch:100 Batch:8 Loss:0.02463\n",
      "Epoch:120 Batch:8 Loss:0.02437\n",
      "Epoch:140 Batch:8 Loss:0.02290\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.115\n",
      "Epoch:10 Batch:10 Loss:0.113\n",
      "Epoch:20 Batch:10 Loss:0.113\n",
      "Epoch:30 Batch:10 Loss:0.108\n",
      "Epoch:40 Batch:10 Loss:0.112\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1122.3013101747358 setps: 800 count: 800\n",
      "avg rewards: -1122.3013101747358\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.42385\n",
      "Epoch:20 Batch:9 Loss:0.04271\n",
      "Epoch:40 Batch:9 Loss:0.03088\n",
      "Epoch:60 Batch:9 Loss:0.02531\n",
      "Epoch:80 Batch:9 Loss:0.02379\n",
      "Epoch:100 Batch:9 Loss:0.02005\n",
      "Epoch:120 Batch:9 Loss:0.02221\n",
      "Epoch:140 Batch:9 Loss:0.02055\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.107\n",
      "Epoch:10 Batch:10 Loss:0.105\n",
      "Epoch:20 Batch:10 Loss:0.105\n",
      "Epoch:30 Batch:10 Loss:0.106\n",
      "Epoch:40 Batch:10 Loss:0.106\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1077.1042420595195 setps: 800 count: 800\n",
      "avg rewards: -1077.1042420595195\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.37215\n",
      "Epoch:20 Batch:10 Loss:0.03598\n",
      "Epoch:40 Batch:10 Loss:0.02860\n",
      "Epoch:60 Batch:10 Loss:0.02798\n",
      "Epoch:80 Batch:10 Loss:0.02595\n",
      "Epoch:100 Batch:10 Loss:0.02412\n",
      "Epoch:120 Batch:10 Loss:0.02099\n",
      "Epoch:140 Batch:10 Loss:0.01889\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.099\n",
      "Epoch:10 Batch:10 Loss:0.098\n",
      "Epoch:20 Batch:10 Loss:0.096\n",
      "Epoch:30 Batch:10 Loss:0.096\n",
      "Epoch:40 Batch:10 Loss:0.095\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1025.1940917463712 setps: 800 count: 800\n",
      "avg rewards: -1025.1940917463712\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.37956\n",
      "Epoch:20 Batch:11 Loss:0.04126\n",
      "Epoch:40 Batch:11 Loss:0.02761\n",
      "Epoch:60 Batch:11 Loss:0.02525\n",
      "Epoch:80 Batch:11 Loss:0.02164\n",
      "Epoch:100 Batch:11 Loss:0.02217\n",
      "Epoch:120 Batch:11 Loss:0.01932\n",
      "Epoch:140 Batch:11 Loss:0.01868\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.097\n",
      "Epoch:10 Batch:10 Loss:0.097\n",
      "Epoch:20 Batch:10 Loss:0.094\n",
      "Epoch:30 Batch:10 Loss:0.094\n",
      "Epoch:40 Batch:10 Loss:0.093\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1070.0654422337327 setps: 800 count: 800\n",
      "avg rewards: -1070.0654422337327\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.31424\n",
      "Epoch:20 Batch:12 Loss:0.03557\n",
      "Epoch:40 Batch:12 Loss:0.02698\n",
      "Epoch:60 Batch:12 Loss:0.02381\n",
      "Epoch:80 Batch:12 Loss:0.01977\n",
      "Epoch:100 Batch:12 Loss:0.02034\n",
      "Epoch:120 Batch:12 Loss:0.02149\n",
      "Epoch:140 Batch:12 Loss:0.01817\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.104\n",
      "Epoch:10 Batch:10 Loss:0.100\n",
      "Epoch:20 Batch:10 Loss:0.097\n",
      "Epoch:30 Batch:10 Loss:0.099\n",
      "Epoch:40 Batch:10 Loss:0.095\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -352.9196285971679 setps: 800 count: 800\n",
      "avg rewards: -352.9196285971679\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.23374\n",
      "Epoch:20 Batch:13 Loss:0.03509\n",
      "Epoch:40 Batch:13 Loss:0.02579\n",
      "Epoch:60 Batch:13 Loss:0.02488\n",
      "Epoch:80 Batch:13 Loss:0.01921\n",
      "Epoch:100 Batch:13 Loss:0.02177\n",
      "Epoch:120 Batch:13 Loss:0.01836\n",
      "Epoch:140 Batch:13 Loss:0.01895\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.091\n",
      "Epoch:10 Batch:10 Loss:0.085\n",
      "Epoch:20 Batch:10 Loss:0.086\n",
      "Epoch:30 Batch:10 Loss:0.086\n",
      "Epoch:40 Batch:10 Loss:0.086\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1033.648072619706 setps: 800 count: 800\n",
      "avg rewards: -1033.648072619706\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.21092\n",
      "Epoch:20 Batch:14 Loss:0.02916\n",
      "Epoch:40 Batch:14 Loss:0.02528\n",
      "Epoch:60 Batch:14 Loss:0.02178\n",
      "Epoch:80 Batch:14 Loss:0.02120\n",
      "Epoch:100 Batch:14 Loss:0.01804\n",
      "Epoch:120 Batch:14 Loss:0.01937\n",
      "Epoch:140 Batch:14 Loss:0.01628\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.086\n",
      "Epoch:10 Batch:10 Loss:0.083\n",
      "Epoch:20 Batch:10 Loss:0.082\n",
      "Epoch:30 Batch:10 Loss:0.083\n",
      "Epoch:40 Batch:10 Loss:0.081\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1033.7832124644406 setps: 800 count: 800\n",
      "avg rewards: -1033.7832124644406\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.18079\n",
      "Epoch:20 Batch:15 Loss:0.03327\n",
      "Epoch:40 Batch:15 Loss:0.02373\n",
      "Epoch:60 Batch:15 Loss:0.02348\n",
      "Epoch:80 Batch:15 Loss:0.01770\n",
      "Epoch:100 Batch:15 Loss:0.01790\n",
      "Epoch:120 Batch:15 Loss:0.01829\n",
      "Epoch:140 Batch:15 Loss:0.01857\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.087\n",
      "Epoch:10 Batch:10 Loss:0.085\n",
      "Epoch:20 Batch:10 Loss:0.085\n",
      "Epoch:30 Batch:10 Loss:0.084\n",
      "Epoch:40 Batch:10 Loss:0.082\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1148.8266836084629 setps: 800 count: 800\n",
      "avg rewards: -1148.8266836084629\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.18709\n",
      "Epoch:20 Batch:16 Loss:0.02867\n",
      "Epoch:40 Batch:16 Loss:0.02423\n",
      "Epoch:60 Batch:16 Loss:0.02096\n",
      "Epoch:80 Batch:16 Loss:0.01783\n",
      "Epoch:100 Batch:16 Loss:0.01757\n",
      "Epoch:120 Batch:16 Loss:0.01624\n",
      "Epoch:140 Batch:16 Loss:0.01969\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.095\n",
      "Epoch:10 Batch:10 Loss:0.088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 Batch:10 Loss:0.084\n",
      "Epoch:30 Batch:10 Loss:0.085\n",
      "Epoch:40 Batch:10 Loss:0.087\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1029.7235768237895 setps: 800 count: 800\n",
      "avg rewards: -1029.7235768237895\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.19274\n",
      "Epoch:20 Batch:17 Loss:0.02792\n",
      "Epoch:40 Batch:17 Loss:0.02131\n",
      "Epoch:60 Batch:17 Loss:0.01995\n",
      "Epoch:80 Batch:17 Loss:0.01784\n",
      "Epoch:100 Batch:17 Loss:0.01769\n",
      "Epoch:120 Batch:17 Loss:0.01575\n",
      "Epoch:140 Batch:17 Loss:0.01604\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.086\n",
      "Epoch:10 Batch:10 Loss:0.082\n",
      "Epoch:20 Batch:10 Loss:0.084\n",
      "Epoch:30 Batch:10 Loss:0.083\n",
      "Epoch:40 Batch:10 Loss:0.083\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1127.7853127616688 setps: 800 count: 800\n",
      "avg rewards: -1127.7853127616688\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.18801\n",
      "Epoch:20 Batch:18 Loss:0.02658\n",
      "Epoch:40 Batch:18 Loss:0.02352\n",
      "Epoch:60 Batch:18 Loss:0.02089\n",
      "Epoch:80 Batch:18 Loss:0.01739\n",
      "Epoch:100 Batch:18 Loss:0.01644\n",
      "Epoch:120 Batch:18 Loss:0.01517\n",
      "Epoch:140 Batch:18 Loss:0.01415\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.087\n",
      "Epoch:10 Batch:10 Loss:0.085\n",
      "Epoch:20 Batch:10 Loss:0.085\n",
      "Epoch:30 Batch:10 Loss:0.083\n",
      "Epoch:40 Batch:10 Loss:0.084\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1174.9367537472385 setps: 800 count: 800\n",
      "avg rewards: -1174.9367537472385\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.15191\n",
      "Epoch:20 Batch:19 Loss:0.02668\n",
      "Epoch:40 Batch:19 Loss:0.02245\n",
      "Epoch:60 Batch:19 Loss:0.01814\n",
      "Epoch:80 Batch:19 Loss:0.01711\n",
      "Epoch:100 Batch:19 Loss:0.01678\n",
      "Epoch:120 Batch:19 Loss:0.01529\n",
      "Epoch:140 Batch:19 Loss:0.01407\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.092\n",
      "Epoch:10 Batch:10 Loss:0.085\n",
      "Epoch:20 Batch:10 Loss:0.084\n",
      "Epoch:30 Batch:10 Loss:0.084\n",
      "Epoch:40 Batch:10 Loss:0.085\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1053.9870359958688 setps: 800 count: 800\n",
      "avg rewards: -1053.9870359958688\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.15798\n",
      "Epoch:20 Batch:20 Loss:0.02241\n",
      "Epoch:40 Batch:20 Loss:0.01961\n",
      "Epoch:60 Batch:20 Loss:0.01872\n",
      "Epoch:80 Batch:20 Loss:0.01686\n",
      "Epoch:100 Batch:20 Loss:0.01491\n",
      "Epoch:120 Batch:20 Loss:0.01440\n",
      "Epoch:140 Batch:20 Loss:0.01510\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.091\n",
      "Epoch:10 Batch:10 Loss:0.083\n",
      "Epoch:20 Batch:10 Loss:0.082\n",
      "Epoch:30 Batch:10 Loss:0.081\n",
      "Epoch:40 Batch:10 Loss:0.081\n",
      "Done!\n",
      "############# start AntBulletEnv-v0 training ###################\n",
      "(50, 1000, 28)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 387.879813451547 setps: 800 count: 800\n",
      "avg rewards: 387.879813451547\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:1.45745\n",
      "Epoch:20 Batch:1 Loss:0.07682\n",
      "Epoch:40 Batch:1 Loss:0.05249\n",
      "Epoch:60 Batch:1 Loss:0.03649\n",
      "Epoch:80 Batch:1 Loss:0.03102\n",
      "Epoch:100 Batch:1 Loss:0.02810\n",
      "Epoch:120 Batch:1 Loss:0.02528\n",
      "Epoch:140 Batch:1 Loss:0.02180\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.340\n",
      "Epoch:10 Batch:10 Loss:0.338\n",
      "Epoch:20 Batch:10 Loss:0.334\n",
      "Epoch:30 Batch:10 Loss:0.320\n",
      "Epoch:40 Batch:10 Loss:0.321\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 197.58159600721683 setps: 800 count: 800\n",
      "avg rewards: 197.58159600721683\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:1.40681\n",
      "Epoch:20 Batch:2 Loss:0.06163\n",
      "Epoch:40 Batch:2 Loss:0.04104\n",
      "Epoch:60 Batch:2 Loss:0.03116\n",
      "Epoch:80 Batch:2 Loss:0.02457\n",
      "Epoch:100 Batch:2 Loss:0.02153\n",
      "Epoch:120 Batch:2 Loss:0.01799\n",
      "Epoch:140 Batch:2 Loss:0.01744\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.335\n",
      "Epoch:10 Batch:10 Loss:0.325\n",
      "Epoch:20 Batch:10 Loss:0.323\n",
      "Epoch:30 Batch:10 Loss:0.319\n",
      "Epoch:40 Batch:10 Loss:0.320\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 24.478979521733713 setps: 82 count: 82\n",
      "reward: 30.9563087924209 setps: 97 count: 179\n",
      "reward: 175.52386137544403 setps: 800 count: 979\n",
      "avg rewards: 76.9863832298662\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:1.32095\n",
      "Epoch:20 Batch:3 Loss:0.04656\n",
      "Epoch:40 Batch:3 Loss:0.03165\n",
      "Epoch:60 Batch:3 Loss:0.02352\n",
      "Epoch:80 Batch:3 Loss:0.02062\n",
      "Epoch:100 Batch:3 Loss:0.01750\n",
      "Epoch:120 Batch:3 Loss:0.01614\n",
      "Epoch:140 Batch:3 Loss:0.01545\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.299\n",
      "Epoch:10 Batch:10 Loss:0.292\n",
      "Epoch:20 Batch:10 Loss:0.292\n",
      "Epoch:30 Batch:10 Loss:0.289\n",
      "Epoch:40 Batch:10 Loss:0.290\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 437.84949517947257 setps: 800 count: 800\n",
      "avg rewards: 437.84949517947257\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:1.18711\n",
      "Epoch:20 Batch:4 Loss:0.04130\n",
      "Epoch:40 Batch:4 Loss:0.02798\n",
      "Epoch:60 Batch:4 Loss:0.02040\n",
      "Epoch:80 Batch:4 Loss:0.01718\n",
      "Epoch:100 Batch:4 Loss:0.01580\n",
      "Epoch:120 Batch:4 Loss:0.01432\n",
      "Epoch:140 Batch:4 Loss:0.01346\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.294\n",
      "Epoch:10 Batch:10 Loss:0.289\n",
      "Epoch:20 Batch:10 Loss:0.292\n",
      "Epoch:30 Batch:10 Loss:0.291\n",
      "Epoch:40 Batch:10 Loss:0.290\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 366.6809863950296 setps: 800 count: 800\n",
      "avg rewards: 366.6809863950296\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.79018\n",
      "Epoch:20 Batch:5 Loss:0.03556\n",
      "Epoch:40 Batch:5 Loss:0.02126\n",
      "Epoch:60 Batch:5 Loss:0.01748\n",
      "Epoch:80 Batch:5 Loss:0.01432\n",
      "Epoch:100 Batch:5 Loss:0.01280\n",
      "Epoch:120 Batch:5 Loss:0.01223\n",
      "Epoch:140 Batch:5 Loss:0.01172\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.283\n",
      "Epoch:10 Batch:10 Loss:0.279\n",
      "Epoch:20 Batch:10 Loss:0.280\n",
      "Epoch:30 Batch:10 Loss:0.276\n",
      "Epoch:40 Batch:10 Loss:0.277\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 8.107643892633494 setps: 22 count: 22\n",
      "reward: 8.415512453313568 setps: 22 count: 44\n",
      "reward: 7.557502142013982 setps: 21 count: 65\n",
      "reward: 7.506923807771819 setps: 21 count: 86\n",
      "reward: 7.159325872447517 setps: 21 count: 107\n",
      "reward: 8.78088083274197 setps: 22 count: 129\n",
      "reward: 7.305542129759851 setps: 21 count: 150\n",
      "reward: 7.475394407016573 setps: 21 count: 171\n",
      "reward: 7.084482851813666 setps: 21 count: 192\n",
      "reward: 7.300738704734246 setps: 21 count: 213\n",
      "reward: 6.536946547433037 setps: 21 count: 234\n",
      "reward: 7.278688096215774 setps: 21 count: 255\n",
      "reward: 7.79080646726943 setps: 21 count: 276\n",
      "reward: 8.81441872669093 setps: 22 count: 298\n",
      "reward: 8.232185015380674 setps: 22 count: 320\n",
      "reward: 7.0908105036141915 setps: 21 count: 341\n",
      "reward: 7.407426164645585 setps: 21 count: 362\n",
      "reward: 7.213000457771703 setps: 21 count: 383\n",
      "reward: 7.74045253604272 setps: 21 count: 404\n",
      "reward: 8.71904026797856 setps: 22 count: 426\n",
      "reward: 6.98201547696517 setps: 21 count: 447\n",
      "reward: 7.204449631638998 setps: 21 count: 468\n",
      "reward: 6.782133712488576 setps: 21 count: 489\n",
      "reward: 7.366690373125312 setps: 21 count: 510\n",
      "reward: 7.510835624989704 setps: 21 count: 531\n",
      "reward: 7.6564221546446785 setps: 21 count: 552\n",
      "reward: 8.474133662864912 setps: 21 count: 573\n",
      "reward: 6.78583209880162 setps: 20 count: 593\n",
      "reward: 8.525308511234469 setps: 22 count: 615\n",
      "reward: 7.752033609530189 setps: 21 count: 636\n",
      "reward: 7.653766698161778 setps: 21 count: 657\n",
      "reward: 7.624751812995237 setps: 21 count: 678\n",
      "reward: 6.789391465717927 setps: 21 count: 699\n",
      "reward: 8.14492869338137 setps: 21 count: 720\n",
      "reward: 8.07783287021157 setps: 21 count: 741\n",
      "reward: 7.701091105361411 setps: 22 count: 763\n",
      "reward: 8.106171663379062 setps: 22 count: 785\n",
      "reward: 7.845112137043906 setps: 21 count: 806\n",
      "reward: 6.808015651773895 setps: 21 count: 827\n",
      "reward: 8.304724506998902 setps: 21 count: 848\n",
      "reward: 8.259242354441085 setps: 21 count: 869\n",
      "reward: 8.373983817991393 setps: 21 count: 890\n",
      "reward: 7.777091707540967 setps: 21 count: 911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 8.211072334385245 setps: 21 count: 932\n",
      "reward: 8.638417591914186 setps: 22 count: 954\n",
      "reward: 8.597369928783154 setps: 21 count: 975\n",
      "reward: 7.716217340572618 setps: 21 count: 996\n",
      "avg rewards: 7.727377838600566\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.72813\n",
      "Epoch:20 Batch:6 Loss:0.03140\n",
      "Epoch:40 Batch:6 Loss:0.02274\n",
      "Epoch:60 Batch:6 Loss:0.01828\n",
      "Epoch:80 Batch:6 Loss:0.01602\n",
      "Epoch:100 Batch:6 Loss:0.01411\n",
      "Epoch:120 Batch:6 Loss:0.01350\n",
      "Epoch:140 Batch:6 Loss:0.01306\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.284\n",
      "Epoch:10 Batch:10 Loss:0.282\n",
      "Epoch:20 Batch:10 Loss:0.281\n",
      "Epoch:30 Batch:10 Loss:0.282\n",
      "Epoch:40 Batch:10 Loss:0.280\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 7.6784665027240395 setps: 20 count: 20\n",
      "reward: 7.600027616525769 setps: 20 count: 40\n",
      "reward: 9.404707878311456 setps: 21 count: 61\n",
      "reward: 41.43070459704176 setps: 73 count: 134\n",
      "reward: 195.6912323273048 setps: 326 count: 460\n",
      "reward: 9.507301992963765 setps: 22 count: 482\n",
      "reward: 21.22287877903145 setps: 37 count: 519\n",
      "reward: 12.885082837242228 setps: 24 count: 543\n",
      "reward: 7.970907820960565 setps: 20 count: 563\n",
      "reward: 8.641632758185732 setps: 21 count: 584\n",
      "reward: 38.94339231549383 setps: 69 count: 653\n",
      "reward: 20.093850057230156 setps: 38 count: 691\n",
      "reward: 8.605628619583149 setps: 21 count: 712\n",
      "reward: 8.819601359065565 setps: 21 count: 733\n",
      "reward: 31.63140501035378 setps: 53 count: 786\n",
      "reward: 13.622865736232782 setps: 25 count: 811\n",
      "avg rewards: 27.734355388015675\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.89549\n",
      "Epoch:20 Batch:7 Loss:0.03032\n",
      "Epoch:40 Batch:7 Loss:0.01993\n",
      "Epoch:60 Batch:7 Loss:0.01641\n",
      "Epoch:80 Batch:7 Loss:0.01586\n",
      "Epoch:100 Batch:7 Loss:0.01374\n",
      "Epoch:120 Batch:7 Loss:0.01214\n",
      "Epoch:140 Batch:7 Loss:0.01157\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.297\n",
      "Epoch:10 Batch:10 Loss:0.288\n",
      "Epoch:20 Batch:10 Loss:0.288\n",
      "Epoch:30 Batch:10 Loss:0.284\n",
      "Epoch:40 Batch:10 Loss:0.282\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 32.650841180711 setps: 86 count: 86\n",
      "reward: 16.04832246694568 setps: 27 count: 113\n",
      "reward: 18.71610410520079 setps: 29 count: 142\n",
      "reward: 317.508380497966 setps: 800 count: 942\n",
      "avg rewards: 96.23091206270587\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.60521\n",
      "Epoch:20 Batch:8 Loss:0.02768\n",
      "Epoch:40 Batch:8 Loss:0.01934\n",
      "Epoch:60 Batch:8 Loss:0.01673\n",
      "Epoch:80 Batch:8 Loss:0.01481\n",
      "Epoch:100 Batch:8 Loss:0.01362\n",
      "Epoch:120 Batch:8 Loss:0.01290\n",
      "Epoch:140 Batch:8 Loss:0.01173\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.287\n",
      "Epoch:10 Batch:10 Loss:0.277\n",
      "Epoch:20 Batch:10 Loss:0.273\n",
      "Epoch:30 Batch:10 Loss:0.277\n",
      "Epoch:40 Batch:10 Loss:0.271\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 8.350186199070594 setps: 21 count: 21\n",
      "reward: 8.569323601861834 setps: 21 count: 42\n",
      "reward: 7.764012041107344 setps: 20 count: 62\n",
      "reward: 7.436575238364456 setps: 20 count: 82\n",
      "reward: 35.85328736933443 setps: 79 count: 161\n",
      "reward: 60.28703584031464 setps: 134 count: 295\n",
      "reward: 16.963121518518893 setps: 32 count: 327\n",
      "reward: 9.196262603103245 setps: 22 count: 349\n",
      "reward: 11.313876851643727 setps: 24 count: 373\n",
      "reward: 40.566972139492286 setps: 73 count: 446\n",
      "reward: 8.74554573329806 setps: 21 count: 467\n",
      "reward: 8.172284719157323 setps: 21 count: 488\n",
      "reward: 7.863746675789299 setps: 20 count: 508\n",
      "reward: 8.770480157263226 setps: 21 count: 529\n",
      "reward: 7.713649127059036 setps: 20 count: 549\n",
      "reward: 30.66588377333683 setps: 70 count: 619\n",
      "reward: 9.068811760506648 setps: 21 count: 640\n",
      "reward: 18.401144944719267 setps: 32 count: 672\n",
      "reward: 8.451893915353864 setps: 21 count: 693\n",
      "reward: 120.12194032244732 setps: 215 count: 908\n",
      "reward: 15.408311664707435 setps: 30 count: 938\n",
      "reward: 8.150441318561207 setps: 20 count: 958\n",
      "avg rewards: 20.810672159773222\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.51691\n",
      "Epoch:20 Batch:9 Loss:0.02619\n",
      "Epoch:40 Batch:9 Loss:0.01858\n",
      "Epoch:60 Batch:9 Loss:0.01531\n",
      "Epoch:80 Batch:9 Loss:0.01407\n",
      "Epoch:100 Batch:9 Loss:0.01299\n",
      "Epoch:120 Batch:9 Loss:0.01119\n",
      "Epoch:140 Batch:9 Loss:0.01152\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.308\n",
      "Epoch:10 Batch:10 Loss:0.275\n",
      "Epoch:20 Batch:10 Loss:0.266\n",
      "Epoch:30 Batch:10 Loss:0.268\n",
      "Epoch:40 Batch:10 Loss:0.268\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 72.82262084651153 setps: 800 count: 800\n",
      "reward: 51.875683994726565 setps: 71 count: 871\n",
      "reward: 53.494826000774616 setps: 95 count: 966\n",
      "avg rewards: 59.39771028067091\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.36623\n",
      "Epoch:20 Batch:10 Loss:0.02664\n",
      "Epoch:40 Batch:10 Loss:0.02014\n",
      "Epoch:60 Batch:10 Loss:0.01829\n",
      "Epoch:80 Batch:10 Loss:0.01538\n",
      "Epoch:100 Batch:10 Loss:0.01406\n",
      "Epoch:120 Batch:10 Loss:0.01284\n",
      "Epoch:140 Batch:10 Loss:0.01199\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.281\n",
      "Epoch:10 Batch:10 Loss:0.279\n",
      "Epoch:20 Batch:10 Loss:0.278\n",
      "Epoch:30 Batch:10 Loss:0.275\n",
      "Epoch:40 Batch:10 Loss:0.280\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 67.51106669938454 setps: 117 count: 117\n",
      "reward: 190.02288026370385 setps: 800 count: 917\n",
      "avg rewards: 128.7669734815442\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.44345\n",
      "Epoch:20 Batch:11 Loss:0.02688\n",
      "Epoch:40 Batch:11 Loss:0.02000\n",
      "Epoch:60 Batch:11 Loss:0.01683\n",
      "Epoch:80 Batch:11 Loss:0.01472\n",
      "Epoch:100 Batch:11 Loss:0.01286\n",
      "Epoch:120 Batch:11 Loss:0.01193\n",
      "Epoch:140 Batch:11 Loss:0.01222\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.288\n",
      "Epoch:10 Batch:10 Loss:0.282\n",
      "Epoch:20 Batch:10 Loss:0.278\n",
      "Epoch:30 Batch:10 Loss:0.274\n",
      "Epoch:40 Batch:10 Loss:0.278\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 338.425900252188 setps: 800 count: 800\n",
      "avg rewards: 338.425900252188\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.34946\n",
      "Epoch:20 Batch:12 Loss:0.02703\n",
      "Epoch:40 Batch:12 Loss:0.01970\n",
      "Epoch:60 Batch:12 Loss:0.01433\n",
      "Epoch:80 Batch:12 Loss:0.01480\n",
      "Epoch:100 Batch:12 Loss:0.01469\n",
      "Epoch:120 Batch:12 Loss:0.01202\n",
      "Epoch:140 Batch:12 Loss:0.01084\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.273\n",
      "Epoch:10 Batch:10 Loss:0.267\n",
      "Epoch:20 Batch:10 Loss:0.267\n",
      "Epoch:30 Batch:10 Loss:0.264\n",
      "Epoch:40 Batch:10 Loss:0.267\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 384.7757194602175 setps: 800 count: 800\n",
      "avg rewards: 384.7757194602175\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.32025\n",
      "Epoch:20 Batch:13 Loss:0.02237\n",
      "Epoch:40 Batch:13 Loss:0.01883\n",
      "Epoch:60 Batch:13 Loss:0.01489\n",
      "Epoch:80 Batch:13 Loss:0.01436\n",
      "Epoch:100 Batch:13 Loss:0.01204\n",
      "Epoch:120 Batch:13 Loss:0.01137\n",
      "Epoch:140 Batch:13 Loss:0.01100\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.297\n",
      "Epoch:10 Batch:10 Loss:0.284\n",
      "Epoch:20 Batch:10 Loss:0.283\n",
      "Epoch:30 Batch:10 Loss:0.283\n",
      "Epoch:40 Batch:10 Loss:0.281\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 211.44868783205445 setps: 800 count: 800\n",
      "avg rewards: 211.44868783205445\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.27878\n",
      "Epoch:20 Batch:14 Loss:0.02395\n",
      "Epoch:40 Batch:14 Loss:0.01918\n",
      "Epoch:60 Batch:14 Loss:0.01522\n",
      "Epoch:80 Batch:14 Loss:0.01365\n",
      "Epoch:100 Batch:14 Loss:0.01405\n",
      "Epoch:120 Batch:14 Loss:0.01149\n",
      "Epoch:140 Batch:14 Loss:0.01410\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.230\n",
      "Epoch:10 Batch:10 Loss:0.220\n",
      "Epoch:20 Batch:10 Loss:0.223\n",
      "Epoch:30 Batch:10 Loss:0.219\n",
      "Epoch:40 Batch:10 Loss:0.217\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 64.16873363472257 setps: 800 count: 800\n",
      "avg rewards: 64.16873363472257\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.24425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 Batch:15 Loss:0.02286\n",
      "Epoch:40 Batch:15 Loss:0.01741\n",
      "Epoch:60 Batch:15 Loss:0.01462\n",
      "Epoch:80 Batch:15 Loss:0.01279\n",
      "Epoch:100 Batch:15 Loss:0.01256\n",
      "Epoch:120 Batch:15 Loss:0.01264\n",
      "Epoch:140 Batch:15 Loss:0.01155\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.195\n",
      "Epoch:10 Batch:10 Loss:0.184\n",
      "Epoch:20 Batch:10 Loss:0.189\n",
      "Epoch:30 Batch:10 Loss:0.184\n",
      "Epoch:40 Batch:10 Loss:0.184\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 301.7669741035891 setps: 800 count: 800\n",
      "avg rewards: 301.7669741035891\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.24481\n",
      "Epoch:20 Batch:16 Loss:0.02252\n",
      "Epoch:40 Batch:16 Loss:0.01638\n",
      "Epoch:60 Batch:16 Loss:0.01490\n",
      "Epoch:80 Batch:16 Loss:0.01509\n",
      "Epoch:100 Batch:16 Loss:0.01133\n",
      "Epoch:120 Batch:16 Loss:0.01184\n",
      "Epoch:140 Batch:16 Loss:0.01133\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.160\n",
      "Epoch:10 Batch:10 Loss:0.155\n",
      "Epoch:20 Batch:10 Loss:0.153\n",
      "Epoch:30 Batch:10 Loss:0.153\n",
      "Epoch:40 Batch:10 Loss:0.151\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -209.81294261272703 setps: 800 count: 800\n",
      "avg rewards: -209.81294261272703\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.27504\n",
      "Epoch:20 Batch:17 Loss:0.01993\n",
      "Epoch:40 Batch:17 Loss:0.01671\n",
      "Epoch:60 Batch:17 Loss:0.01514\n",
      "Epoch:80 Batch:17 Loss:0.01298\n",
      "Epoch:100 Batch:17 Loss:0.01178\n",
      "Epoch:120 Batch:17 Loss:0.01419\n",
      "Epoch:140 Batch:17 Loss:0.01097\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.116\n",
      "Epoch:10 Batch:10 Loss:0.108\n",
      "Epoch:20 Batch:10 Loss:0.105\n",
      "Epoch:30 Batch:10 Loss:0.107\n",
      "Epoch:40 Batch:10 Loss:0.107\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -262.9046642157848 setps: 800 count: 800\n",
      "avg rewards: -262.9046642157848\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.22958\n",
      "Epoch:20 Batch:18 Loss:0.02019\n",
      "Epoch:40 Batch:18 Loss:0.01597\n",
      "Epoch:60 Batch:18 Loss:0.01592\n",
      "Epoch:80 Batch:18 Loss:0.01261\n",
      "Epoch:100 Batch:18 Loss:0.01221\n",
      "Epoch:120 Batch:18 Loss:0.01217\n",
      "Epoch:140 Batch:18 Loss:0.01132\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.111\n",
      "Epoch:10 Batch:10 Loss:0.108\n",
      "Epoch:20 Batch:10 Loss:0.109\n",
      "Epoch:30 Batch:10 Loss:0.105\n",
      "Epoch:40 Batch:10 Loss:0.106\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -282.5642606721612 setps: 800 count: 800\n",
      "avg rewards: -282.5642606721612\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.21413\n",
      "Epoch:20 Batch:19 Loss:0.02080\n",
      "Epoch:40 Batch:19 Loss:0.01515\n",
      "Epoch:60 Batch:19 Loss:0.01511\n",
      "Epoch:80 Batch:19 Loss:0.01287\n",
      "Epoch:100 Batch:19 Loss:0.01231\n",
      "Epoch:120 Batch:19 Loss:0.01175\n",
      "Epoch:140 Batch:19 Loss:0.01176\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.117\n",
      "Epoch:10 Batch:10 Loss:0.116\n",
      "Epoch:20 Batch:10 Loss:0.110\n",
      "Epoch:30 Batch:10 Loss:0.108\n",
      "Epoch:40 Batch:10 Loss:0.109\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 77.12368380511901 setps: 800 count: 800\n",
      "avg rewards: 77.12368380511901\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.19800\n",
      "Epoch:20 Batch:20 Loss:0.02139\n",
      "Epoch:40 Batch:20 Loss:0.01653\n",
      "Epoch:60 Batch:20 Loss:0.01364\n",
      "Epoch:80 Batch:20 Loss:0.01347\n",
      "Epoch:100 Batch:20 Loss:0.01525\n",
      "Epoch:120 Batch:20 Loss:0.01028\n",
      "Epoch:140 Batch:20 Loss:0.01089\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.106\n",
      "Epoch:10 Batch:10 Loss:0.103\n",
      "Epoch:20 Batch:10 Loss:0.104\n",
      "Epoch:30 Batch:10 Loss:0.101\n",
      "Epoch:40 Batch:10 Loss:0.101\n",
      "Done!\n",
      "############# start HumanoidBulletEnv-v0 training ###################\n",
      "(49,)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -32.59327098235517 setps: 22 count: 22\n",
      "reward: -20.025285138403706 setps: 20 count: 42\n",
      "reward: -20.49645852637623 setps: 20 count: 62\n",
      "reward: -25.700962189897837 setps: 19 count: 81\n",
      "reward: -18.788999088015405 setps: 19 count: 100\n",
      "reward: -22.790281978456186 setps: 19 count: 119\n",
      "reward: -31.185122736895575 setps: 18 count: 137\n",
      "reward: -29.665487870843208 setps: 22 count: 159\n",
      "reward: -12.346137957232713 setps: 18 count: 177\n",
      "reward: -36.271735022617214 setps: 18 count: 195\n",
      "reward: -29.163276184146525 setps: 20 count: 215\n",
      "reward: -28.285630425611448 setps: 24 count: 239\n",
      "reward: -28.61326972336537 setps: 19 count: 258\n",
      "reward: -28.023421662834885 setps: 19 count: 277\n",
      "reward: -31.298959614802158 setps: 20 count: 297\n",
      "reward: -39.98181828571687 setps: 24 count: 321\n",
      "reward: -25.35084381789202 setps: 19 count: 340\n",
      "reward: -18.6547190843441 setps: 19 count: 359\n",
      "reward: -29.4562628077474 setps: 22 count: 381\n",
      "reward: -22.268161394484927 setps: 19 count: 400\n",
      "reward: -29.36021647918678 setps: 22 count: 422\n",
      "reward: -33.95250150139473 setps: 18 count: 440\n",
      "reward: -22.44168341341428 setps: 19 count: 459\n",
      "reward: -40.260897488791656 setps: 25 count: 484\n",
      "reward: -41.470481391105565 setps: 18 count: 502\n",
      "reward: -20.20329166664451 setps: 17 count: 519\n",
      "reward: -34.32584393924917 setps: 19 count: 538\n",
      "reward: -31.343376773374622 setps: 18 count: 556\n",
      "reward: -35.648598603361464 setps: 19 count: 575\n",
      "reward: -23.357746788808438 setps: 18 count: 593\n",
      "reward: -25.497490061561983 setps: 17 count: 610\n",
      "reward: -39.738364452582026 setps: 21 count: 631\n",
      "reward: -37.0995766303502 setps: 14 count: 645\n",
      "reward: -33.31335272940923 setps: 21 count: 666\n",
      "reward: -29.650835852792078 setps: 21 count: 687\n",
      "reward: -26.70514389798627 setps: 18 count: 705\n",
      "reward: -30.882442079905015 setps: 20 count: 725\n",
      "reward: -31.567458384926432 setps: 19 count: 744\n",
      "reward: -39.22092470549688 setps: 18 count: 762\n",
      "reward: -21.634807421127334 setps: 19 count: 781\n",
      "reward: -25.851094098725298 setps: 19 count: 800\n",
      "reward: -40.10702863238257 setps: 20 count: 820\n",
      "reward: -30.30569269703265 setps: 20 count: 840\n",
      "reward: -20.469640096045623 setps: 20 count: 860\n",
      "reward: -14.706564948265438 setps: 18 count: 878\n",
      "reward: -31.243943485678756 setps: 22 count: 900\n",
      "reward: -26.16057961039478 setps: 18 count: 918\n",
      "reward: -20.239103065265223 setps: 19 count: 937\n",
      "reward: -36.009881602830134 setps: 19 count: 956\n",
      "reward: -35.112843998239256 setps: 17 count: 973\n",
      "reward: -35.9117987034042 setps: 19 count: 992\n",
      "avg rewards: -28.916731562583752\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:1.59224\n",
      "Epoch:20 Batch:1 Loss:0.52230\n",
      "Epoch:40 Batch:1 Loss:0.43913\n",
      "Epoch:60 Batch:1 Loss:0.37481\n",
      "Epoch:80 Batch:1 Loss:0.31751\n",
      "Epoch:100 Batch:1 Loss:0.27680\n",
      "Epoch:120 Batch:1 Loss:0.24457\n",
      "Epoch:140 Batch:1 Loss:0.22297\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.229\n",
      "Epoch:10 Batch:8 Loss:0.218\n",
      "Epoch:20 Batch:8 Loss:0.214\n",
      "Epoch:30 Batch:8 Loss:0.214\n",
      "Epoch:40 Batch:8 Loss:0.211\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 15.593602803059913 setps: 20 count: 20\n",
      "reward: 27.6711266001439 setps: 37 count: 57\n",
      "reward: 16.667303766336406 setps: 18 count: 75\n",
      "reward: 33.45526381865491 setps: 53 count: 128\n",
      "reward: 25.21614770923917 setps: 78 count: 206\n",
      "reward: 40.18153410149097 setps: 62 count: 268\n",
      "reward: 47.79136112829 setps: 45 count: 313\n",
      "reward: 24.24062154611748 setps: 32 count: 345\n",
      "reward: 45.942398655469866 setps: 58 count: 403\n",
      "reward: 25.242973032004365 setps: 46 count: 449\n",
      "reward: 39.594066198130896 setps: 49 count: 498\n",
      "reward: 26.694171859527707 setps: 29 count: 527\n",
      "reward: 57.15352182312926 setps: 57 count: 584\n",
      "reward: 15.06800310646358 setps: 18 count: 602\n",
      "reward: 29.962315178714924 setps: 36 count: 638\n",
      "reward: 28.65736763819586 setps: 38 count: 676\n",
      "reward: 43.40338191196642 setps: 47 count: 723\n",
      "reward: 27.775233363361615 setps: 40 count: 763\n",
      "reward: 32.28945194627158 setps: 47 count: 810\n",
      "reward: 17.72463308816223 setps: 26 count: 836\n",
      "reward: 39.183596385864085 setps: 49 count: 885\n",
      "reward: 78.86798555558639 setps: 74 count: 959\n",
      "avg rewards: 33.562548237099165\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:1.41216\n",
      "Epoch:20 Batch:2 Loss:0.34695\n",
      "Epoch:40 Batch:2 Loss:0.25789\n",
      "Epoch:60 Batch:2 Loss:0.20812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:80 Batch:2 Loss:0.17919\n",
      "Epoch:100 Batch:2 Loss:0.15944\n",
      "Epoch:120 Batch:2 Loss:0.14337\n",
      "Epoch:140 Batch:2 Loss:0.14001\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.130\n",
      "Epoch:10 Batch:8 Loss:0.126\n",
      "Epoch:20 Batch:8 Loss:0.127\n",
      "Epoch:30 Batch:8 Loss:0.126\n",
      "Epoch:40 Batch:8 Loss:0.124\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 18.182958121699627 setps: 22 count: 22\n",
      "reward: 25.5284197975212 setps: 27 count: 49\n",
      "reward: 20.341758543391187 setps: 24 count: 73\n",
      "reward: 27.59966983403428 setps: 28 count: 101\n",
      "reward: 6.3126966739582695 setps: 16 count: 117\n",
      "reward: 31.92447723319638 setps: 30 count: 147\n",
      "reward: 13.504838608582213 setps: 18 count: 165\n",
      "reward: 24.066008712968323 setps: 28 count: 193\n",
      "reward: 12.86064326237829 setps: 18 count: 211\n",
      "reward: 34.05382569474313 setps: 29 count: 240\n",
      "reward: 18.790021626155063 setps: 22 count: 262\n",
      "reward: 18.777570534736153 setps: 23 count: 285\n",
      "reward: 18.201505298000114 setps: 19 count: 304\n",
      "reward: 41.01955775066599 setps: 34 count: 338\n",
      "reward: 30.932476305738966 setps: 30 count: 368\n",
      "reward: 23.10037166850961 setps: 24 count: 392\n",
      "reward: 12.556567110135806 setps: 20 count: 412\n",
      "reward: 13.534625203294853 setps: 21 count: 433\n",
      "reward: 14.470093140742394 setps: 19 count: 452\n",
      "reward: 24.123276462274948 setps: 26 count: 478\n",
      "reward: 28.524404496130582 setps: 29 count: 507\n",
      "reward: 18.970670845136922 setps: 21 count: 528\n",
      "reward: 17.286474051917317 setps: 23 count: 551\n",
      "reward: 18.827873048702898 setps: 24 count: 575\n",
      "reward: 14.682117819385894 setps: 21 count: 596\n",
      "reward: 22.871061860831098 setps: 28 count: 624\n",
      "reward: 12.695462895689706 setps: 20 count: 644\n",
      "reward: 24.2848950579355 setps: 26 count: 670\n",
      "reward: -0.5984784538406536 setps: 16 count: 686\n",
      "reward: 24.242263129193447 setps: 27 count: 713\n",
      "reward: 45.56484287880302 setps: 35 count: 748\n",
      "reward: 15.001880768977571 setps: 21 count: 769\n",
      "reward: 18.886990226525818 setps: 22 count: 791\n",
      "reward: 21.586037527468577 setps: 24 count: 815\n",
      "reward: 35.86032602666383 setps: 32 count: 847\n",
      "reward: 26.998846323670293 setps: 27 count: 874\n",
      "reward: 32.7163131807727 setps: 30 count: 904\n",
      "reward: 16.2029473357994 setps: 18 count: 922\n",
      "reward: 16.214124539957268 setps: 22 count: 944\n",
      "reward: 33.249371618824085 setps: 33 count: 977\n",
      "reward: -1.7913800670023203 setps: 14 count: 991\n",
      "avg rewards: 21.272156260835846\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:1.36878\n",
      "Epoch:20 Batch:3 Loss:0.26510\n",
      "Epoch:40 Batch:3 Loss:0.19755\n",
      "Epoch:60 Batch:3 Loss:0.15196\n",
      "Epoch:80 Batch:3 Loss:0.13302\n",
      "Epoch:100 Batch:3 Loss:0.12361\n",
      "Epoch:120 Batch:3 Loss:0.11997\n",
      "Epoch:140 Batch:3 Loss:0.11330\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.144\n",
      "Epoch:10 Batch:8 Loss:0.140\n",
      "Epoch:20 Batch:8 Loss:0.140\n",
      "Epoch:30 Batch:8 Loss:0.141\n",
      "Epoch:40 Batch:8 Loss:0.138\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 49.575409654427496 setps: 52 count: 52\n",
      "reward: 16.553291841887397 setps: 18 count: 70\n",
      "reward: 57.45141631553998 setps: 46 count: 116\n",
      "reward: 21.392627702292515 setps: 22 count: 138\n",
      "reward: 79.39882158908732 setps: 56 count: 194\n",
      "reward: 74.32515980003954 setps: 58 count: 252\n",
      "reward: 65.40971965619248 setps: 46 count: 298\n",
      "reward: 62.1590528648565 setps: 43 count: 341\n",
      "reward: 72.22845732580025 setps: 49 count: 390\n",
      "reward: 47.88272985620133 setps: 56 count: 446\n",
      "reward: 79.86433504206359 setps: 58 count: 504\n",
      "reward: 38.51070347664209 setps: 36 count: 540\n",
      "reward: 61.278745478940266 setps: 64 count: 604\n",
      "reward: 78.58901608899035 setps: 68 count: 672\n",
      "reward: 56.12232079716195 setps: 57 count: 729\n",
      "reward: 60.69587835324202 setps: 51 count: 780\n",
      "reward: 83.81176049398638 setps: 64 count: 844\n",
      "reward: 28.76474504625658 setps: 26 count: 870\n",
      "reward: 30.081424652902932 setps: 29 count: 899\n",
      "reward: 66.97796708933018 setps: 64 count: 963\n",
      "avg rewards: 56.55367915629206\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:1.24538\n",
      "Epoch:20 Batch:4 Loss:0.20393\n",
      "Epoch:40 Batch:4 Loss:0.14842\n",
      "Epoch:60 Batch:4 Loss:0.12642\n",
      "Epoch:80 Batch:4 Loss:0.10345\n",
      "Epoch:100 Batch:4 Loss:0.09543\n",
      "Epoch:120 Batch:4 Loss:0.09491\n",
      "Epoch:140 Batch:4 Loss:0.09420\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.120\n",
      "Epoch:10 Batch:8 Loss:0.118\n",
      "Epoch:20 Batch:8 Loss:0.116\n",
      "Epoch:30 Batch:8 Loss:0.120\n",
      "Epoch:40 Batch:8 Loss:0.119\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 28.993853291861882 setps: 32 count: 32\n",
      "reward: 53.60547449463193 setps: 58 count: 90\n",
      "reward: 58.97898172049319 setps: 55 count: 145\n",
      "reward: 43.04895173425902 setps: 42 count: 187\n",
      "reward: 42.68325751939411 setps: 35 count: 222\n",
      "reward: 39.24587992701126 setps: 35 count: 257\n",
      "reward: 62.00853841284553 setps: 55 count: 312\n",
      "reward: 47.90694205667095 setps: 39 count: 351\n",
      "reward: 58.546872918367455 setps: 45 count: 396\n",
      "reward: 47.00079788638541 setps: 39 count: 435\n",
      "reward: 70.60427550553287 setps: 63 count: 498\n",
      "reward: 39.25675901448267 setps: 34 count: 532\n",
      "reward: 54.20611101011163 setps: 48 count: 580\n",
      "reward: 39.401030188811994 setps: 38 count: 618\n",
      "reward: 49.519657150037524 setps: 39 count: 657\n",
      "reward: 36.53159490417602 setps: 34 count: 691\n",
      "reward: 52.08883074167534 setps: 51 count: 742\n",
      "reward: 65.34836240081204 setps: 60 count: 802\n",
      "reward: 36.181437212675526 setps: 33 count: 835\n",
      "reward: 46.580212238943204 setps: 41 count: 876\n",
      "reward: 34.540767086265255 setps: 36 count: 912\n",
      "reward: 31.516743985864743 setps: 33 count: 945\n",
      "reward: 68.29167757368852 setps: 51 count: 996\n",
      "avg rewards: 48.09073952065208\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:1.14081\n",
      "Epoch:20 Batch:5 Loss:0.18613\n",
      "Epoch:40 Batch:5 Loss:0.12697\n",
      "Epoch:60 Batch:5 Loss:0.11584\n",
      "Epoch:80 Batch:5 Loss:0.10071\n",
      "Epoch:100 Batch:5 Loss:0.09317\n",
      "Epoch:120 Batch:5 Loss:0.08154\n",
      "Epoch:140 Batch:5 Loss:0.08342\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.126\n",
      "Epoch:10 Batch:8 Loss:0.123\n",
      "Epoch:20 Batch:8 Loss:0.120\n",
      "Epoch:30 Batch:8 Loss:0.123\n",
      "Epoch:40 Batch:8 Loss:0.121\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 22.266260322427847 setps: 23 count: 23\n",
      "reward: 26.555338795098944 setps: 43 count: 66\n",
      "reward: 18.97593312437385 setps: 37 count: 103\n",
      "reward: 26.05036594327684 setps: 38 count: 141\n",
      "reward: 24.490617459986247 setps: 38 count: 179\n",
      "reward: 19.377490419142124 setps: 35 count: 214\n",
      "reward: 26.514809163838805 setps: 42 count: 256\n",
      "reward: 16.469478560845886 setps: 35 count: 291\n",
      "reward: 17.279459603299625 setps: 21 count: 312\n",
      "reward: 15.777153195028962 setps: 31 count: 343\n",
      "reward: 31.222106373183497 setps: 44 count: 387\n",
      "reward: 48.08676704127284 setps: 40 count: 427\n",
      "reward: 23.04845070241281 setps: 40 count: 467\n",
      "reward: 20.05301768412028 setps: 41 count: 508\n",
      "reward: 11.808196743094596 setps: 36 count: 544\n",
      "reward: 19.17493197841686 setps: 37 count: 581\n",
      "reward: 15.501249618215663 setps: 17 count: 598\n",
      "reward: 15.37317063887749 setps: 41 count: 639\n",
      "reward: 38.278697646263744 setps: 44 count: 683\n",
      "reward: 28.479632750825832 setps: 44 count: 727\n",
      "reward: 24.685134302498767 setps: 42 count: 769\n",
      "reward: 16.666968950495352 setps: 21 count: 790\n",
      "reward: 26.882695743240767 setps: 39 count: 829\n",
      "reward: 32.40782975485635 setps: 44 count: 873\n",
      "reward: 15.583389942595385 setps: 21 count: 894\n",
      "reward: 18.964264449283654 setps: 36 count: 930\n",
      "reward: 22.83461242207268 setps: 39 count: 969\n",
      "avg rewards: 23.066963827001697\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:1.06066\n",
      "Epoch:20 Batch:6 Loss:0.16065\n",
      "Epoch:40 Batch:6 Loss:0.11245\n",
      "Epoch:60 Batch:6 Loss:0.09602\n",
      "Epoch:80 Batch:6 Loss:0.08644\n",
      "Epoch:100 Batch:6 Loss:0.08214\n",
      "Epoch:120 Batch:6 Loss:0.07665\n",
      "Epoch:140 Batch:6 Loss:0.07215\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.117\n",
      "Epoch:10 Batch:8 Loss:0.115\n",
      "Epoch:20 Batch:8 Loss:0.117\n",
      "Epoch:30 Batch:8 Loss:0.112\n",
      "Epoch:40 Batch:8 Loss:0.112\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 26.820237688488856 setps: 27 count: 27\n",
      "reward: 18.128168891240787 setps: 19 count: 46\n",
      "reward: 31.348131355753868 setps: 27 count: 73\n",
      "reward: 22.14117561848834 setps: 22 count: 95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 29.605774232560357 setps: 30 count: 125\n",
      "reward: 30.24476614241575 setps: 26 count: 151\n",
      "reward: 30.769984594731067 setps: 34 count: 185\n",
      "reward: 55.86771671250462 setps: 47 count: 232\n",
      "reward: 30.808293859950208 setps: 29 count: 261\n",
      "reward: 27.30258632123004 setps: 26 count: 287\n",
      "reward: 32.81276885320112 setps: 30 count: 317\n",
      "reward: 29.91082882390183 setps: 28 count: 345\n",
      "reward: 33.42745007781341 setps: 31 count: 376\n",
      "reward: 17.9448436389066 setps: 19 count: 395\n",
      "reward: 30.019879508343003 setps: 31 count: 426\n",
      "reward: 35.56347063972207 setps: 31 count: 457\n",
      "reward: 34.81711290869862 setps: 33 count: 490\n",
      "reward: 27.890343358374956 setps: 24 count: 514\n",
      "reward: 36.83767113067298 setps: 33 count: 547\n",
      "reward: 36.24908764318243 setps: 33 count: 580\n",
      "reward: 30.287563129543564 setps: 26 count: 606\n",
      "reward: 13.240082290799181 setps: 19 count: 625\n",
      "reward: 34.88884698413166 setps: 28 count: 653\n",
      "reward: 39.350595454989524 setps: 36 count: 689\n",
      "reward: 18.128726736189854 setps: 20 count: 709\n",
      "reward: 26.196723338216547 setps: 25 count: 734\n",
      "reward: 19.362755227308657 setps: 20 count: 754\n",
      "reward: 22.45574862576322 setps: 24 count: 778\n",
      "reward: 27.483169248278145 setps: 27 count: 805\n",
      "reward: 26.096707904018693 setps: 24 count: 829\n",
      "reward: 28.48554439983272 setps: 27 count: 856\n",
      "reward: 30.349136273300974 setps: 26 count: 882\n",
      "reward: 27.271220871375416 setps: 33 count: 915\n",
      "reward: 33.98443089477805 setps: 33 count: 948\n",
      "reward: 31.317031476141704 setps: 28 count: 976\n",
      "avg rewards: 29.35453071013854\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:1.12886\n",
      "Epoch:20 Batch:7 Loss:0.13055\n",
      "Epoch:40 Batch:7 Loss:0.09938\n",
      "Epoch:60 Batch:7 Loss:0.08675\n",
      "Epoch:80 Batch:7 Loss:0.07813\n",
      "Epoch:100 Batch:7 Loss:0.07027\n",
      "Epoch:120 Batch:7 Loss:0.06686\n",
      "Epoch:140 Batch:7 Loss:0.06831\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.112\n",
      "Epoch:10 Batch:8 Loss:0.109\n",
      "Epoch:20 Batch:8 Loss:0.110\n",
      "Epoch:30 Batch:8 Loss:0.109\n",
      "Epoch:40 Batch:8 Loss:0.109\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 31.47896855356521 setps: 26 count: 26\n",
      "reward: 32.045350267227334 setps: 27 count: 53\n",
      "reward: 31.87787891367189 setps: 27 count: 80\n",
      "reward: 31.62932504245546 setps: 27 count: 107\n",
      "reward: 29.574660633223544 setps: 26 count: 133\n",
      "reward: 19.298892154029456 setps: 19 count: 152\n",
      "reward: 32.1724448147943 setps: 27 count: 179\n",
      "reward: 16.996840123229777 setps: 19 count: 198\n",
      "reward: 29.635036139529262 setps: 25 count: 223\n",
      "reward: 40.03665276724642 setps: 31 count: 254\n",
      "reward: 30.610368702358393 setps: 25 count: 279\n",
      "reward: 22.2793202492263 setps: 20 count: 299\n",
      "reward: 29.409351968299593 setps: 26 count: 325\n",
      "reward: 33.39196273556881 setps: 27 count: 352\n",
      "reward: 35.53219624894118 setps: 28 count: 380\n",
      "reward: 42.50196143888461 setps: 35 count: 415\n",
      "reward: 33.61372215834272 setps: 28 count: 443\n",
      "reward: 34.70874476803438 setps: 30 count: 473\n",
      "reward: 32.53706740881753 setps: 26 count: 499\n",
      "reward: 34.11905887629691 setps: 28 count: 527\n",
      "reward: 31.946254149643934 setps: 27 count: 554\n",
      "reward: 18.008378047318544 setps: 21 count: 575\n",
      "reward: 30.246124925919872 setps: 27 count: 602\n",
      "reward: 32.66560002098558 setps: 27 count: 629\n",
      "reward: 32.10960799643509 setps: 28 count: 657\n",
      "reward: 32.03045111774845 setps: 28 count: 685\n",
      "reward: 37.348938909018756 setps: 28 count: 713\n",
      "reward: 32.77295712146879 setps: 30 count: 743\n",
      "reward: 32.42880258908845 setps: 27 count: 770\n",
      "reward: 37.84266573932546 setps: 29 count: 799\n",
      "reward: 29.734786514629377 setps: 29 count: 828\n",
      "reward: 20.577232758651366 setps: 20 count: 848\n",
      "reward: 30.445261395035782 setps: 26 count: 874\n",
      "reward: 33.651115528444656 setps: 27 count: 901\n",
      "reward: 19.187505841685926 setps: 19 count: 920\n",
      "reward: 23.700483290356356 setps: 22 count: 942\n",
      "reward: 29.389120363540133 setps: 29 count: 971\n",
      "avg rewards: 30.4739213587308\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:1.05527\n",
      "Epoch:20 Batch:8 Loss:0.13660\n",
      "Epoch:40 Batch:8 Loss:0.09378\n",
      "Epoch:60 Batch:8 Loss:0.08488\n",
      "Epoch:80 Batch:8 Loss:0.07414\n",
      "Epoch:100 Batch:8 Loss:0.07441\n",
      "Epoch:120 Batch:8 Loss:0.06586\n",
      "Epoch:140 Batch:8 Loss:0.06962\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.109\n",
      "Epoch:10 Batch:8 Loss:0.110\n",
      "Epoch:20 Batch:8 Loss:0.107\n",
      "Epoch:30 Batch:8 Loss:0.106\n",
      "Epoch:40 Batch:8 Loss:0.105\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 48.49967743246088 setps: 37 count: 37\n",
      "reward: 46.38590029409823 setps: 42 count: 79\n",
      "reward: 52.58781092547142 setps: 39 count: 118\n",
      "reward: 35.55429275588831 setps: 36 count: 154\n",
      "reward: 20.03003495629091 setps: 18 count: 172\n",
      "reward: 43.48821597395437 setps: 42 count: 214\n",
      "reward: 38.32675310188934 setps: 33 count: 247\n",
      "reward: 58.259819526289355 setps: 41 count: 288\n",
      "reward: 48.74946975566417 setps: 40 count: 328\n",
      "reward: 35.36439177680586 setps: 35 count: 363\n",
      "reward: 32.532716283292395 setps: 31 count: 394\n",
      "reward: 36.07978141201893 setps: 33 count: 427\n",
      "reward: 61.03627204634833 setps: 44 count: 471\n",
      "reward: 43.844229519488 setps: 40 count: 511\n",
      "reward: 22.699254860766814 setps: 20 count: 531\n",
      "reward: 36.81353599969007 setps: 30 count: 561\n",
      "reward: 45.1521895629441 setps: 37 count: 598\n",
      "reward: 50.10648429640862 setps: 40 count: 638\n",
      "reward: 43.214437292567155 setps: 38 count: 676\n",
      "reward: 47.21173087774659 setps: 37 count: 713\n",
      "reward: 41.81578053450358 setps: 35 count: 748\n",
      "reward: 66.38317849933375 setps: 45 count: 793\n",
      "reward: 45.27798614030617 setps: 40 count: 833\n",
      "reward: 3.844404116213263 setps: 16 count: 849\n",
      "reward: -3.811081038073462 setps: 11 count: 860\n",
      "reward: 41.942668892983036 setps: 39 count: 899\n",
      "reward: 28.48129296879778 setps: 23 count: 922\n",
      "reward: 39.50828654827347 setps: 35 count: 957\n",
      "reward: 19.49226454470772 setps: 18 count: 975\n",
      "avg rewards: 38.9266130985217\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.93662\n",
      "Epoch:20 Batch:9 Loss:0.13054\n",
      "Epoch:40 Batch:9 Loss:0.08979\n",
      "Epoch:60 Batch:9 Loss:0.08226\n",
      "Epoch:80 Batch:9 Loss:0.07568\n",
      "Epoch:100 Batch:9 Loss:0.07110\n",
      "Epoch:120 Batch:9 Loss:0.06769\n",
      "Epoch:140 Batch:9 Loss:0.06439\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.095\n",
      "Epoch:10 Batch:8 Loss:0.095\n",
      "Epoch:20 Batch:8 Loss:0.095\n",
      "Epoch:30 Batch:8 Loss:0.094\n",
      "Epoch:40 Batch:8 Loss:0.093\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 35.962826729322835 setps: 28 count: 28\n",
      "reward: 16.22970296526473 setps: 17 count: 45\n",
      "reward: 22.188247364856945 setps: 20 count: 65\n",
      "reward: 39.93317774724856 setps: 30 count: 95\n",
      "reward: 36.185425450331245 setps: 28 count: 123\n",
      "reward: 45.22286052895797 setps: 33 count: 156\n",
      "reward: 36.40833648084953 setps: 31 count: 187\n",
      "reward: 37.019674412677695 setps: 29 count: 216\n",
      "reward: 23.824279854302585 setps: 20 count: 236\n",
      "reward: 43.730216476622445 setps: 32 count: 268\n",
      "reward: 47.33724801853968 setps: 33 count: 301\n",
      "reward: 32.63606739333481 setps: 31 count: 332\n",
      "reward: 55.51123486238211 setps: 40 count: 372\n",
      "reward: 52.75903081551371 setps: 37 count: 409\n",
      "reward: 33.9839516402455 setps: 27 count: 436\n",
      "reward: 55.6452070480067 setps: 44 count: 480\n",
      "reward: 39.48344012780724 setps: 31 count: 511\n",
      "reward: 37.49541072975262 setps: 30 count: 541\n",
      "reward: 36.47017437861505 setps: 30 count: 571\n",
      "reward: 44.64815994357487 setps: 35 count: 606\n",
      "reward: 57.458243511040926 setps: 39 count: 645\n",
      "reward: 48.8291900656317 setps: 34 count: 679\n",
      "reward: 44.55155938088428 setps: 32 count: 711\n",
      "reward: 39.072482753645446 setps: 28 count: 739\n",
      "reward: 38.48592496560595 setps: 33 count: 772\n",
      "reward: 38.76971040571516 setps: 28 count: 800\n",
      "reward: 35.143208540965865 setps: 32 count: 832\n",
      "reward: 33.472886160298366 setps: 26 count: 858\n",
      "reward: 28.84621075855539 setps: 24 count: 882\n",
      "reward: 37.882746179713294 setps: 29 count: 911\n",
      "reward: 16.645031637346253 setps: 18 count: 929\n",
      "reward: 18.967559600218376 setps: 18 count: 947\n",
      "reward: 33.41465827635838 setps: 28 count: 975\n",
      "avg rewards: 37.70345712739958\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.59146\n",
      "Epoch:20 Batch:10 Loss:0.10306\n",
      "Epoch:40 Batch:10 Loss:0.08640\n",
      "Epoch:60 Batch:10 Loss:0.07529\n",
      "Epoch:80 Batch:10 Loss:0.06950\n",
      "Epoch:100 Batch:10 Loss:0.06764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:120 Batch:10 Loss:0.06095\n",
      "Epoch:140 Batch:10 Loss:0.05965\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.097\n",
      "Epoch:10 Batch:8 Loss:0.095\n",
      "Epoch:20 Batch:8 Loss:0.095\n",
      "Epoch:30 Batch:8 Loss:0.095\n",
      "Epoch:40 Batch:8 Loss:0.095\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 50.62454274680494 setps: 38 count: 38\n",
      "reward: 43.16163931077754 setps: 32 count: 70\n",
      "reward: 37.71519421005941 setps: 31 count: 101\n",
      "reward: 54.51038669272963 setps: 43 count: 144\n",
      "reward: 46.74966945642373 setps: 35 count: 179\n",
      "reward: 20.333177971604158 setps: 20 count: 199\n",
      "reward: 48.02704915882204 setps: 37 count: 236\n",
      "reward: 24.10494028956018 setps: 21 count: 257\n",
      "reward: 55.55766238597573 setps: 42 count: 299\n",
      "reward: 41.74566105340055 setps: 35 count: 334\n",
      "reward: 17.194059662423385 setps: 19 count: 353\n",
      "reward: 40.89816939163429 setps: 33 count: 386\n",
      "reward: 42.45617054496834 setps: 39 count: 425\n",
      "reward: 54.464048910822015 setps: 39 count: 464\n",
      "reward: 44.629887529632846 setps: 35 count: 499\n",
      "reward: 56.60828019840993 setps: 45 count: 544\n",
      "reward: 44.28947312314558 setps: 38 count: 582\n",
      "reward: 42.04815144322055 setps: 36 count: 618\n",
      "reward: 44.93624813994249 setps: 41 count: 659\n",
      "reward: 68.7156707047863 setps: 46 count: 705\n",
      "reward: 60.95316525064409 setps: 54 count: 759\n",
      "reward: 43.710427632543706 setps: 36 count: 795\n",
      "reward: 57.01927902245805 setps: 44 count: 839\n",
      "reward: 19.297245036214004 setps: 20 count: 859\n",
      "reward: 60.8291737661726 setps: 47 count: 906\n",
      "reward: 36.71570042851527 setps: 31 count: 937\n",
      "reward: 61.64028772696183 setps: 43 count: 980\n",
      "reward: 8.209121969470287 setps: 18 count: 998\n",
      "avg rewards: 43.82658870564727\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.77990\n",
      "Epoch:20 Batch:11 Loss:0.11290\n",
      "Epoch:40 Batch:11 Loss:0.08062\n",
      "Epoch:60 Batch:11 Loss:0.07412\n",
      "Epoch:80 Batch:11 Loss:0.07163\n",
      "Epoch:100 Batch:11 Loss:0.06654\n",
      "Epoch:120 Batch:11 Loss:0.06689\n",
      "Epoch:140 Batch:11 Loss:0.06384\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.097\n",
      "Epoch:10 Batch:8 Loss:0.096\n",
      "Epoch:20 Batch:8 Loss:0.092\n",
      "Epoch:30 Batch:8 Loss:0.093\n",
      "Epoch:40 Batch:8 Loss:0.095\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 88.64589299586078 setps: 63 count: 63\n",
      "reward: 73.31974357061407 setps: 66 count: 129\n",
      "reward: 95.41692678599064 setps: 84 count: 213\n",
      "reward: 81.88550428514863 setps: 75 count: 288\n",
      "reward: 92.02180739388277 setps: 64 count: 352\n",
      "reward: 94.03644964758136 setps: 69 count: 421\n",
      "reward: 87.42526912211326 setps: 54 count: 475\n",
      "reward: 73.11783028538629 setps: 48 count: 523\n",
      "reward: 29.50654045547562 setps: 23 count: 546\n",
      "reward: 72.10598665888539 setps: 56 count: 602\n",
      "reward: 85.69764331934275 setps: 63 count: 665\n",
      "reward: 101.22697036168391 setps: 68 count: 733\n",
      "reward: 102.42620482567578 setps: 77 count: 810\n",
      "reward: 88.05457901037443 setps: 51 count: 861\n",
      "reward: 69.12027648086952 setps: 51 count: 912\n",
      "reward: 56.49670346561325 setps: 68 count: 980\n",
      "reward: 16.5464214052874 setps: 18 count: 998\n",
      "avg rewards: 76.88533823939918\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.71548\n",
      "Epoch:20 Batch:12 Loss:0.09378\n",
      "Epoch:40 Batch:12 Loss:0.07460\n",
      "Epoch:60 Batch:12 Loss:0.06881\n",
      "Epoch:80 Batch:12 Loss:0.06415\n",
      "Epoch:100 Batch:12 Loss:0.06127\n",
      "Epoch:120 Batch:12 Loss:0.06066\n",
      "Epoch:140 Batch:12 Loss:0.05860\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.086\n",
      "Epoch:10 Batch:8 Loss:0.087\n",
      "Epoch:20 Batch:8 Loss:0.085\n",
      "Epoch:30 Batch:8 Loss:0.085\n",
      "Epoch:40 Batch:8 Loss:0.084\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 20.727393283328276 setps: 18 count: 18\n",
      "reward: 75.70237369973329 setps: 51 count: 69\n",
      "reward: 105.95149758640183 setps: 68 count: 137\n",
      "reward: 45.136789082615 setps: 31 count: 168\n",
      "reward: 82.66013293652505 setps: 58 count: 226\n",
      "reward: 78.85442055201273 setps: 55 count: 281\n",
      "reward: 62.84963292067985 setps: 47 count: 328\n",
      "reward: 47.28080789109809 setps: 32 count: 360\n",
      "reward: 80.74111698796041 setps: 50 count: 410\n",
      "reward: 23.880023951068875 setps: 19 count: 429\n",
      "reward: 33.53418002259132 setps: 25 count: 454\n",
      "reward: 93.40407497934791 setps: 57 count: 511\n",
      "reward: 77.74295831598427 setps: 46 count: 557\n",
      "reward: 81.96473060428832 setps: 47 count: 604\n",
      "reward: 88.4167058788822 setps: 55 count: 659\n",
      "reward: 73.26399325238309 setps: 43 count: 702\n",
      "reward: 76.44368599318115 setps: 46 count: 748\n",
      "reward: 16.998465992462297 setps: 17 count: 765\n",
      "reward: 24.640448509155245 setps: 20 count: 785\n",
      "reward: 56.42726705373498 setps: 74 count: 859\n",
      "reward: 90.92958906620626 setps: 47 count: 906\n",
      "reward: 73.44736581179167 setps: 44 count: 950\n",
      "avg rewards: 64.13625701688328\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.63756\n",
      "Epoch:20 Batch:13 Loss:0.09310\n",
      "Epoch:40 Batch:13 Loss:0.07123\n",
      "Epoch:60 Batch:13 Loss:0.06950\n",
      "Epoch:80 Batch:13 Loss:0.06040\n",
      "Epoch:100 Batch:13 Loss:0.05842\n",
      "Epoch:120 Batch:13 Loss:0.05600\n",
      "Epoch:140 Batch:13 Loss:0.05762\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.079\n",
      "Epoch:10 Batch:8 Loss:0.076\n",
      "Epoch:20 Batch:8 Loss:0.074\n",
      "Epoch:30 Batch:8 Loss:0.075\n",
      "Epoch:40 Batch:8 Loss:0.075\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 36.40068758865528 setps: 30 count: 30\n",
      "reward: 51.2376104378258 setps: 48 count: 78\n",
      "reward: 34.91426069515436 setps: 32 count: 110\n",
      "reward: 65.30876727571741 setps: 59 count: 169\n",
      "reward: 30.099467034700506 setps: 27 count: 196\n",
      "reward: 30.184542147352477 setps: 27 count: 223\n",
      "reward: 59.98571958747925 setps: 45 count: 268\n",
      "reward: 60.92181865472375 setps: 41 count: 309\n",
      "reward: 36.10791045060469 setps: 33 count: 342\n",
      "reward: 31.801884053897815 setps: 27 count: 369\n",
      "reward: 43.00860063914879 setps: 34 count: 403\n",
      "reward: 35.229440360066654 setps: 28 count: 431\n",
      "reward: 26.541941528439928 setps: 25 count: 456\n",
      "reward: 15.43913046115922 setps: 19 count: 475\n",
      "reward: 44.829660823768066 setps: 37 count: 512\n",
      "reward: 56.86919608867611 setps: 44 count: 556\n",
      "reward: 50.83449392224867 setps: 37 count: 593\n",
      "reward: 52.11880037213413 setps: 40 count: 633\n",
      "reward: 36.53454178349494 setps: 34 count: 667\n",
      "reward: 25.453756410523788 setps: 24 count: 691\n",
      "reward: 37.72735467503517 setps: 34 count: 725\n",
      "reward: 52.377244443260146 setps: 46 count: 771\n",
      "reward: 39.00988975490035 setps: 35 count: 806\n",
      "reward: 35.10787585803773 setps: 33 count: 839\n",
      "reward: 57.46455959764572 setps: 52 count: 891\n",
      "reward: 36.11044977351848 setps: 35 count: 926\n",
      "reward: 55.677128214383266 setps: 54 count: 980\n",
      "avg rewards: 42.12210120861305\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.51131\n",
      "Epoch:20 Batch:14 Loss:0.09408\n",
      "Epoch:40 Batch:14 Loss:0.07684\n",
      "Epoch:60 Batch:14 Loss:0.06542\n",
      "Epoch:80 Batch:14 Loss:0.06216\n",
      "Epoch:100 Batch:14 Loss:0.06189\n",
      "Epoch:120 Batch:14 Loss:0.05811\n",
      "Epoch:140 Batch:14 Loss:0.05387\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.075\n",
      "Epoch:10 Batch:8 Loss:0.076\n",
      "Epoch:20 Batch:8 Loss:0.074\n",
      "Epoch:30 Batch:8 Loss:0.076\n",
      "Epoch:40 Batch:8 Loss:0.074\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 13.524914913889369 setps: 17 count: 17\n",
      "reward: 62.11339585697133 setps: 56 count: 73\n",
      "reward: 49.784256866714 setps: 43 count: 116\n",
      "reward: 28.184312854215385 setps: 25 count: 141\n",
      "reward: 53.32384287758468 setps: 39 count: 180\n",
      "reward: 24.178098934139413 setps: 24 count: 204\n",
      "reward: 64.68856834952312 setps: 42 count: 246\n",
      "reward: 31.779422127704304 setps: 27 count: 273\n",
      "reward: 53.95740105238364 setps: 36 count: 309\n",
      "reward: 51.20776587928847 setps: 41 count: 350\n",
      "reward: 29.412835076458574 setps: 25 count: 375\n",
      "reward: 52.306055809628745 setps: 35 count: 410\n",
      "reward: 39.32783711324883 setps: 36 count: 446\n",
      "reward: 66.88473126464523 setps: 44 count: 490\n",
      "reward: 66.54602112437132 setps: 51 count: 541\n",
      "reward: 11.671699502642149 setps: 21 count: 562\n",
      "reward: 72.88470599136342 setps: 49 count: 611\n",
      "reward: 32.57942005438963 setps: 27 count: 638\n",
      "reward: 57.9219867353022 setps: 37 count: 675\n",
      "reward: 46.92858515823317 setps: 34 count: 709\n",
      "reward: 35.87226675095008 setps: 29 count: 738\n",
      "reward: 31.587105142042855 setps: 28 count: 766\n",
      "reward: 36.335661015337976 setps: 29 count: 795\n",
      "reward: 31.047647277943913 setps: 27 count: 822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 72.2070512851613 setps: 52 count: 874\n",
      "reward: 46.95794751956857 setps: 35 count: 909\n",
      "reward: 53.64453118411183 setps: 38 count: 947\n",
      "reward: 37.51634469043492 setps: 37 count: 984\n",
      "avg rewards: 44.799086157437436\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.37038\n",
      "Epoch:20 Batch:15 Loss:0.08508\n",
      "Epoch:40 Batch:15 Loss:0.07303\n",
      "Epoch:60 Batch:15 Loss:0.06590\n",
      "Epoch:80 Batch:15 Loss:0.06626\n",
      "Epoch:100 Batch:15 Loss:0.06108\n",
      "Epoch:120 Batch:15 Loss:0.05105\n",
      "Epoch:140 Batch:15 Loss:0.05075\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.075\n",
      "Epoch:10 Batch:8 Loss:0.071\n",
      "Epoch:20 Batch:8 Loss:0.069\n",
      "Epoch:30 Batch:8 Loss:0.070\n",
      "Epoch:40 Batch:8 Loss:0.068\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 44.99272341532196 setps: 31 count: 31\n",
      "reward: 57.545984746477906 setps: 40 count: 71\n",
      "reward: 50.02312444864511 setps: 39 count: 110\n",
      "reward: 37.4570194439526 setps: 30 count: 140\n",
      "reward: 44.729074300920175 setps: 33 count: 173\n",
      "reward: 43.67858098096185 setps: 32 count: 205\n",
      "reward: 34.82584295413981 setps: 26 count: 231\n",
      "reward: 56.34676663812861 setps: 37 count: 268\n",
      "reward: 67.69424657225463 setps: 45 count: 313\n",
      "reward: 51.85051943663568 setps: 35 count: 348\n",
      "reward: 50.71420633861271 setps: 33 count: 381\n",
      "reward: 58.3247343328563 setps: 39 count: 420\n",
      "reward: 41.46696563501318 setps: 28 count: 448\n",
      "reward: 38.83283145927271 setps: 33 count: 481\n",
      "reward: 31.298784146404067 setps: 24 count: 505\n",
      "reward: 52.510414311211214 setps: 34 count: 539\n",
      "reward: 44.24812506371672 setps: 33 count: 572\n",
      "reward: 40.51269546539844 setps: 31 count: 603\n",
      "reward: 42.079606696839605 setps: 31 count: 634\n",
      "reward: 65.90069971135236 setps: 43 count: 677\n",
      "reward: 50.03465803328725 setps: 33 count: 710\n",
      "reward: 46.52998012226454 setps: 37 count: 747\n",
      "reward: 46.90197465477833 setps: 32 count: 779\n",
      "reward: 46.53395912573031 setps: 33 count: 812\n",
      "reward: 52.116855509423466 setps: 34 count: 846\n",
      "reward: 37.48287958276051 setps: 32 count: 878\n",
      "reward: 47.29812456805376 setps: 32 count: 910\n",
      "reward: 48.13558974720799 setps: 34 count: 944\n",
      "reward: 47.68028762275208 setps: 32 count: 976\n",
      "avg rewards: 47.50852603670255\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.37054\n",
      "Epoch:20 Batch:16 Loss:0.08421\n",
      "Epoch:40 Batch:16 Loss:0.06639\n",
      "Epoch:60 Batch:16 Loss:0.05931\n",
      "Epoch:80 Batch:16 Loss:0.05846\n",
      "Epoch:100 Batch:16 Loss:0.05595\n",
      "Epoch:120 Batch:16 Loss:0.05560\n",
      "Epoch:140 Batch:16 Loss:0.05349\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.076\n",
      "Epoch:10 Batch:8 Loss:0.075\n",
      "Epoch:20 Batch:8 Loss:0.075\n",
      "Epoch:30 Batch:8 Loss:0.074\n",
      "Epoch:40 Batch:8 Loss:0.073\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 81.69003947669114 setps: 54 count: 54\n",
      "reward: 63.87791918031871 setps: 40 count: 94\n",
      "reward: 75.8304751746895 setps: 54 count: 148\n",
      "reward: 72.8085517801941 setps: 47 count: 195\n",
      "reward: 84.7549900329017 setps: 55 count: 250\n",
      "reward: 65.11068610822696 setps: 52 count: 302\n",
      "reward: 90.10686487835481 setps: 105 count: 407\n",
      "reward: 65.0757714690175 setps: 42 count: 449\n",
      "reward: 64.64022459404514 setps: 47 count: 496\n",
      "reward: 50.92445277664955 setps: 44 count: 540\n",
      "reward: 41.288765868842894 setps: 32 count: 572\n",
      "reward: 52.939779582107455 setps: 43 count: 615\n",
      "reward: 51.289558629541716 setps: 43 count: 658\n",
      "reward: 48.409291356477475 setps: 41 count: 699\n",
      "reward: 46.933912662795045 setps: 38 count: 737\n",
      "reward: 56.414325108396575 setps: 63 count: 800\n",
      "reward: 81.42442701170657 setps: 52 count: 852\n",
      "reward: 42.83785508466827 setps: 40 count: 892\n",
      "reward: 67.67306750090066 setps: 49 count: 941\n",
      "reward: 57.291048897813 setps: 48 count: 989\n",
      "avg rewards: 63.06610035871694\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.32137\n",
      "Epoch:20 Batch:17 Loss:0.08721\n",
      "Epoch:40 Batch:17 Loss:0.06592\n",
      "Epoch:60 Batch:17 Loss:0.06143\n",
      "Epoch:80 Batch:17 Loss:0.06045\n",
      "Epoch:100 Batch:17 Loss:0.05248\n",
      "Epoch:120 Batch:17 Loss:0.05115\n",
      "Epoch:140 Batch:17 Loss:0.04769\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.072\n",
      "Epoch:10 Batch:8 Loss:0.072\n",
      "Epoch:20 Batch:8 Loss:0.070\n",
      "Epoch:30 Batch:8 Loss:0.069\n",
      "Epoch:40 Batch:8 Loss:0.069\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 72.77287204352322 setps: 64 count: 64\n",
      "reward: 54.24743186703562 setps: 43 count: 107\n",
      "reward: 55.92875928021822 setps: 47 count: 154\n",
      "reward: 30.23234619918075 setps: 32 count: 186\n",
      "reward: 15.756587337989181 setps: 19 count: 205\n",
      "reward: 18.498216543912715 setps: 21 count: 226\n",
      "reward: 15.225087283430916 setps: 18 count: 244\n",
      "reward: 52.73865283183404 setps: 46 count: 290\n",
      "reward: 54.100986664155805 setps: 43 count: 333\n",
      "reward: 43.048691058284014 setps: 41 count: 374\n",
      "reward: 74.84727279736687 setps: 60 count: 434\n",
      "reward: 42.87175454186799 setps: 42 count: 476\n",
      "reward: 43.72627298173466 setps: 36 count: 512\n",
      "reward: 60.20053684413287 setps: 51 count: 563\n",
      "reward: 28.02047814807156 setps: 26 count: 589\n",
      "reward: 60.311066607623054 setps: 52 count: 641\n",
      "reward: 14.800972899483165 setps: 20 count: 661\n",
      "reward: 22.433518018703037 setps: 23 count: 684\n",
      "reward: 56.67556021105699 setps: 47 count: 731\n",
      "reward: 47.66592418243672 setps: 51 count: 782\n",
      "reward: 45.33462578356704 setps: 42 count: 824\n",
      "reward: 38.938600666999996 setps: 43 count: 867\n",
      "reward: 33.72461343069735 setps: 33 count: 900\n",
      "reward: 51.55945976586518 setps: 47 count: 947\n",
      "reward: 34.91560540302889 setps: 37 count: 984\n",
      "avg rewards: 42.74303573568799\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.30852\n",
      "Epoch:20 Batch:18 Loss:0.08116\n",
      "Epoch:40 Batch:18 Loss:0.06513\n",
      "Epoch:60 Batch:18 Loss:0.05722\n",
      "Epoch:80 Batch:18 Loss:0.05762\n",
      "Epoch:100 Batch:18 Loss:0.05576\n",
      "Epoch:120 Batch:18 Loss:0.05199\n",
      "Epoch:140 Batch:18 Loss:0.04922\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.066\n",
      "Epoch:10 Batch:8 Loss:0.065\n",
      "Epoch:20 Batch:8 Loss:0.066\n",
      "Epoch:30 Batch:8 Loss:0.064\n",
      "Epoch:40 Batch:8 Loss:0.064\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 16.476134878098676 setps: 19 count: 19\n",
      "reward: 27.98337563943496 setps: 29 count: 48\n",
      "reward: 28.3413974148847 setps: 27 count: 75\n",
      "reward: 36.90144405479222 setps: 30 count: 105\n",
      "reward: 19.211683334677947 setps: 20 count: 125\n",
      "reward: 33.33358471818209 setps: 31 count: 156\n",
      "reward: 25.240774576518735 setps: 25 count: 181\n",
      "reward: 21.648961451346988 setps: 22 count: 203\n",
      "reward: 33.656713966470846 setps: 29 count: 232\n",
      "reward: 25.860413543299362 setps: 24 count: 256\n",
      "reward: 29.197363422560734 setps: 29 count: 285\n",
      "reward: 38.02719615005043 setps: 31 count: 316\n",
      "reward: 28.325365986899122 setps: 30 count: 346\n",
      "reward: 27.98042210905115 setps: 25 count: 371\n",
      "reward: 28.512352701443888 setps: 25 count: 396\n",
      "reward: 32.698932299620346 setps: 32 count: 428\n",
      "reward: 30.342294873509672 setps: 28 count: 456\n",
      "reward: 57.089609969338916 setps: 45 count: 501\n",
      "reward: 25.457824454740333 setps: 29 count: 530\n",
      "reward: 16.193920481915118 setps: 18 count: 548\n",
      "reward: 32.47240778393607 setps: 32 count: 580\n",
      "reward: 33.14203154056595 setps: 30 count: 610\n",
      "reward: 51.730893935299534 setps: 37 count: 647\n",
      "reward: 19.791658217922667 setps: 19 count: 666\n",
      "reward: 30.127148695563662 setps: 31 count: 697\n",
      "reward: 27.27875704292383 setps: 26 count: 723\n",
      "reward: 30.48001801093487 setps: 27 count: 750\n",
      "reward: 17.23526018114353 setps: 19 count: 769\n",
      "reward: 29.338306991846178 setps: 29 count: 798\n",
      "reward: 36.59667123339895 setps: 35 count: 833\n",
      "reward: 48.69548569937178 setps: 40 count: 873\n",
      "reward: 23.414559722284316 setps: 24 count: 897\n",
      "reward: 33.560445456666635 setps: 29 count: 926\n",
      "reward: 34.48871927459114 setps: 32 count: 958\n",
      "reward: 29.25640152609413 setps: 29 count: 987\n",
      "avg rewards: 30.288243752553704\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.29114\n",
      "Epoch:20 Batch:19 Loss:0.07892\n",
      "Epoch:40 Batch:19 Loss:0.06962\n",
      "Epoch:60 Batch:19 Loss:0.05954\n",
      "Epoch:80 Batch:19 Loss:0.04982\n",
      "Epoch:100 Batch:19 Loss:0.05018\n",
      "Epoch:120 Batch:19 Loss:0.05147\n",
      "Epoch:140 Batch:19 Loss:0.04763\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.070\n",
      "Epoch:10 Batch:8 Loss:0.069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 Batch:8 Loss:0.068\n",
      "Epoch:30 Batch:8 Loss:0.066\n",
      "Epoch:40 Batch:8 Loss:0.066\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 42.075766288292655 setps: 33 count: 33\n",
      "reward: 60.40677320979157 setps: 41 count: 74\n",
      "reward: 60.21201174097949 setps: 44 count: 118\n",
      "reward: 30.828616771091752 setps: 29 count: 147\n",
      "reward: 52.08879713101341 setps: 42 count: 189\n",
      "reward: 22.125451641055403 setps: 21 count: 210\n",
      "reward: 60.284959208339565 setps: 43 count: 253\n",
      "reward: 50.644129717977194 setps: 36 count: 289\n",
      "reward: 58.604715908660715 setps: 41 count: 330\n",
      "reward: 44.3238525959736 setps: 34 count: 364\n",
      "reward: 47.60085338726785 setps: 37 count: 401\n",
      "reward: 18.009384366012817 setps: 19 count: 420\n",
      "reward: 36.44720433382754 setps: 36 count: 456\n",
      "reward: 18.664219308549942 setps: 19 count: 475\n",
      "reward: 17.52737741297606 setps: 18 count: 493\n",
      "reward: 38.837227873379014 setps: 41 count: 534\n",
      "reward: 28.01798033979721 setps: 24 count: 558\n",
      "reward: 42.222835728271455 setps: 37 count: 595\n",
      "reward: 32.4705951225682 setps: 30 count: 625\n",
      "reward: 33.51303497900953 setps: 30 count: 655\n",
      "reward: 51.872300463667486 setps: 46 count: 701\n",
      "reward: 41.574855625919014 setps: 34 count: 735\n",
      "reward: 48.07976750155503 setps: 37 count: 772\n",
      "reward: 22.40195712954009 setps: 21 count: 793\n",
      "reward: 61.60988334544236 setps: 46 count: 839\n",
      "reward: 13.65119428274047 setps: 17 count: 856\n",
      "reward: 59.32910702318184 setps: 41 count: 897\n",
      "reward: 54.588031152935585 setps: 41 count: 938\n",
      "reward: 59.64008125994442 setps: 43 count: 981\n",
      "avg rewards: 41.64320568447453\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.25778\n",
      "Epoch:20 Batch:20 Loss:0.07482\n",
      "Epoch:40 Batch:20 Loss:0.05699\n",
      "Epoch:60 Batch:20 Loss:0.05962\n",
      "Epoch:80 Batch:20 Loss:0.06047\n",
      "Epoch:100 Batch:20 Loss:0.04833\n",
      "Epoch:120 Batch:20 Loss:0.05182\n",
      "Epoch:140 Batch:20 Loss:0.04961\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.067\n",
      "Epoch:10 Batch:8 Loss:0.066\n",
      "Epoch:20 Batch:8 Loss:0.067\n",
      "Epoch:30 Batch:8 Loss:0.069\n",
      "Epoch:40 Batch:8 Loss:0.067\n",
      "Done!\n",
      "############# start BipedalWalker-v3 training ###################\n",
      "(49,)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -44.24513472215952 setps: 800 count: 800\n",
      "reward: -100.77605780164983 setps: 73 count: 873\n",
      "reward: -101.72889737528749 setps: 70 count: 943\n",
      "reward: -118.75422500360696 setps: 56 count: 999\n",
      "avg rewards: -91.37607872567595\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:1.35586\n",
      "Epoch:20 Batch:1 Loss:0.14338\n",
      "Epoch:40 Batch:1 Loss:0.12653\n",
      "Epoch:60 Batch:1 Loss:0.10818\n",
      "Epoch:80 Batch:1 Loss:0.09212\n",
      "Epoch:100 Batch:1 Loss:0.08128\n",
      "Epoch:120 Batch:1 Loss:0.07478\n",
      "Epoch:140 Batch:1 Loss:0.07113\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.150\n",
      "Epoch:10 Batch:9 Loss:0.145\n",
      "Epoch:20 Batch:9 Loss:0.136\n",
      "Epoch:30 Batch:9 Loss:0.131\n",
      "Epoch:40 Batch:9 Loss:0.130\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -112.9781126505776 setps: 62 count: 62\n",
      "reward: -113.89502698712238 setps: 65 count: 127\n",
      "reward: -116.77762725556951 setps: 64 count: 191\n",
      "reward: -113.50711274135921 setps: 61 count: 252\n",
      "reward: -111.71377248238275 setps: 69 count: 321\n",
      "reward: -116.70819556525412 setps: 73 count: 394\n",
      "reward: -116.30061685341286 setps: 67 count: 461\n",
      "reward: -108.30652085586885 setps: 70 count: 531\n",
      "reward: -113.59554466946298 setps: 63 count: 594\n",
      "reward: -114.26755939142531 setps: 65 count: 659\n",
      "reward: -116.96743661673366 setps: 61 count: 720\n",
      "reward: -113.27548678031005 setps: 57 count: 777\n",
      "reward: -117.48527392368516 setps: 63 count: 840\n",
      "reward: -116.48195831414188 setps: 64 count: 904\n",
      "reward: -113.13428508793874 setps: 63 count: 967\n",
      "avg rewards: -114.35963534501633\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:1.28482\n",
      "Epoch:20 Batch:2 Loss:0.11358\n",
      "Epoch:40 Batch:2 Loss:0.07858\n",
      "Epoch:60 Batch:2 Loss:0.06350\n",
      "Epoch:80 Batch:2 Loss:0.05548\n",
      "Epoch:100 Batch:2 Loss:0.05057\n",
      "Epoch:120 Batch:2 Loss:0.04621\n",
      "Epoch:140 Batch:2 Loss:0.04478\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.103\n",
      "Epoch:10 Batch:9 Loss:0.091\n",
      "Epoch:20 Batch:9 Loss:0.092\n",
      "Epoch:30 Batch:9 Loss:0.091\n",
      "Epoch:40 Batch:9 Loss:0.094\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -109.8094417353229 setps: 68 count: 68\n",
      "reward: -113.70984023662098 setps: 61 count: 129\n",
      "reward: -105.27723082708877 setps: 49 count: 178\n",
      "reward: -113.82051840963152 setps: 57 count: 235\n",
      "reward: -111.14430354354965 setps: 81 count: 316\n",
      "reward: -113.85848738338115 setps: 65 count: 381\n",
      "reward: -107.83771808702872 setps: 55 count: 436\n",
      "reward: -105.94224430951414 setps: 47 count: 483\n",
      "reward: -108.94113657565788 setps: 53 count: 536\n",
      "reward: -108.45395536891681 setps: 55 count: 591\n",
      "reward: -105.03550992388651 setps: 48 count: 639\n",
      "reward: -104.38651078339603 setps: 50 count: 689\n",
      "reward: -107.37493562273805 setps: 58 count: 747\n",
      "reward: -106.06451477571142 setps: 55 count: 802\n",
      "reward: -106.59027492498483 setps: 53 count: 855\n",
      "reward: -107.68247921211459 setps: 61 count: 916\n",
      "reward: -111.05407163430067 setps: 60 count: 976\n",
      "avg rewards: -108.6460690208144\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:1.15209\n",
      "Epoch:20 Batch:3 Loss:0.09459\n",
      "Epoch:40 Batch:3 Loss:0.06508\n",
      "Epoch:60 Batch:3 Loss:0.05280\n",
      "Epoch:80 Batch:3 Loss:0.04748\n",
      "Epoch:100 Batch:3 Loss:0.04410\n",
      "Epoch:120 Batch:3 Loss:0.04318\n",
      "Epoch:140 Batch:3 Loss:0.04135\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.085\n",
      "Epoch:10 Batch:9 Loss:0.091\n",
      "Epoch:20 Batch:9 Loss:0.081\n",
      "Epoch:30 Batch:9 Loss:0.087\n",
      "Epoch:40 Batch:9 Loss:0.083\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -109.88507829518554 setps: 54 count: 54\n",
      "reward: -107.41400147541736 setps: 59 count: 113\n",
      "reward: -111.01256923375092 setps: 57 count: 170\n",
      "reward: -115.142067595593 setps: 66 count: 236\n",
      "reward: -108.82963747926988 setps: 54 count: 290\n",
      "reward: -109.27820867143758 setps: 55 count: 345\n",
      "reward: -110.23272999237665 setps: 55 count: 400\n",
      "reward: -109.51616734436404 setps: 58 count: 458\n",
      "reward: -105.71797024773744 setps: 51 count: 509\n",
      "reward: -106.07997752259485 setps: 53 count: 562\n",
      "reward: -111.23198426505054 setps: 51 count: 613\n",
      "reward: -111.61640688308266 setps: 56 count: 669\n",
      "reward: -105.9917448452295 setps: 53 count: 722\n",
      "reward: -107.57958481519731 setps: 58 count: 780\n",
      "reward: -112.97230092443215 setps: 60 count: 840\n",
      "reward: -107.40986190581694 setps: 54 count: 894\n",
      "reward: -105.85265904766074 setps: 60 count: 954\n",
      "avg rewards: -109.16252650259983\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.89710\n",
      "Epoch:20 Batch:4 Loss:0.07912\n",
      "Epoch:40 Batch:4 Loss:0.05701\n",
      "Epoch:60 Batch:4 Loss:0.04753\n",
      "Epoch:80 Batch:4 Loss:0.04341\n",
      "Epoch:100 Batch:4 Loss:0.03923\n",
      "Epoch:120 Batch:4 Loss:0.03741\n",
      "Epoch:140 Batch:4 Loss:0.03531\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.081\n",
      "Epoch:10 Batch:9 Loss:0.083\n",
      "Epoch:20 Batch:9 Loss:0.077\n",
      "Epoch:30 Batch:9 Loss:0.082\n",
      "Epoch:40 Batch:9 Loss:0.079\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -116.88921173508092 setps: 66 count: 66\n",
      "reward: -118.97105684011305 setps: 68 count: 134\n",
      "reward: -109.14135590328152 setps: 58 count: 192\n",
      "reward: -112.94082934268874 setps: 59 count: 251\n",
      "reward: -129.5941597730536 setps: 88 count: 339\n",
      "reward: -129.0755578611046 setps: 86 count: 425\n",
      "reward: -112.56587421082581 setps: 89 count: 514\n",
      "reward: -124.33901820469274 setps: 79 count: 593\n",
      "reward: -123.90417393204507 setps: 77 count: 670\n",
      "reward: -110.86094065932991 setps: 61 count: 731\n",
      "reward: -108.80727066043205 setps: 57 count: 788\n",
      "reward: -109.7509555079223 setps: 57 count: 845\n",
      "reward: -110.62172768371303 setps: 57 count: 902\n",
      "reward: -107.9107912040328 setps: 61 count: 963\n",
      "avg rewards: -116.09806596559403\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.78800\n",
      "Epoch:20 Batch:5 Loss:0.06824\n",
      "Epoch:40 Batch:5 Loss:0.04967\n",
      "Epoch:60 Batch:5 Loss:0.04190\n",
      "Epoch:80 Batch:5 Loss:0.03819\n",
      "Epoch:100 Batch:5 Loss:0.03508\n",
      "Epoch:120 Batch:5 Loss:0.03419\n",
      "Epoch:140 Batch:5 Loss:0.03225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.074\n",
      "Epoch:10 Batch:9 Loss:0.077\n",
      "Epoch:20 Batch:9 Loss:0.077\n",
      "Epoch:30 Batch:9 Loss:0.077\n",
      "Epoch:40 Batch:9 Loss:0.079\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -119.51646398219592 setps: 84 count: 84\n",
      "reward: -106.29456585114387 setps: 117 count: 201\n",
      "reward: -116.6189799659634 setps: 77 count: 278\n",
      "reward: -113.38780137080606 setps: 65 count: 343\n",
      "reward: -118.08514043294142 setps: 78 count: 421\n",
      "reward: -120.95050396006243 setps: 77 count: 498\n",
      "reward: -116.2316522002928 setps: 93 count: 591\n",
      "reward: -117.29903880760197 setps: 83 count: 674\n",
      "reward: -117.43840138333229 setps: 80 count: 754\n",
      "reward: -120.59279138492451 setps: 79 count: 833\n",
      "reward: -126.4169201602942 setps: 82 count: 915\n",
      "reward: -117.65888875578159 setps: 83 count: 998\n",
      "avg rewards: -117.54092902127839\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.54347\n",
      "Epoch:20 Batch:6 Loss:0.06564\n",
      "Epoch:40 Batch:6 Loss:0.04896\n",
      "Epoch:60 Batch:6 Loss:0.04261\n",
      "Epoch:80 Batch:6 Loss:0.03868\n",
      "Epoch:100 Batch:6 Loss:0.03712\n",
      "Epoch:120 Batch:6 Loss:0.03317\n",
      "Epoch:140 Batch:6 Loss:0.03267\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.075\n",
      "Epoch:10 Batch:9 Loss:0.081\n",
      "Epoch:20 Batch:9 Loss:0.079\n",
      "Epoch:30 Batch:9 Loss:0.077\n",
      "Epoch:40 Batch:9 Loss:0.078\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -16.339352169565228 setps: 800 count: 800\n",
      "avg rewards: -16.339352169565228\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.76664\n",
      "Epoch:20 Batch:7 Loss:0.05815\n",
      "Epoch:40 Batch:7 Loss:0.04257\n",
      "Epoch:60 Batch:7 Loss:0.03654\n",
      "Epoch:80 Batch:7 Loss:0.03234\n",
      "Epoch:100 Batch:7 Loss:0.03075\n",
      "Epoch:120 Batch:7 Loss:0.03094\n",
      "Epoch:140 Batch:7 Loss:0.02896\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.076\n",
      "Epoch:10 Batch:9 Loss:0.071\n",
      "Epoch:20 Batch:9 Loss:0.075\n",
      "Epoch:30 Batch:9 Loss:0.070\n",
      "Epoch:40 Batch:9 Loss:0.076\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -10.402522296755526 setps: 800 count: 800\n",
      "avg rewards: -10.402522296755526\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.79422\n",
      "Epoch:20 Batch:8 Loss:0.05292\n",
      "Epoch:40 Batch:8 Loss:0.03953\n",
      "Epoch:60 Batch:8 Loss:0.03494\n",
      "Epoch:80 Batch:8 Loss:0.03215\n",
      "Epoch:100 Batch:8 Loss:0.02988\n",
      "Epoch:120 Batch:8 Loss:0.02827\n",
      "Epoch:140 Batch:8 Loss:0.02555\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.069\n",
      "Epoch:10 Batch:9 Loss:0.069\n",
      "Epoch:20 Batch:9 Loss:0.072\n",
      "Epoch:30 Batch:9 Loss:0.074\n",
      "Epoch:40 Batch:9 Loss:0.071\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -127.3999009023048 setps: 95 count: 95\n",
      "reward: -120.83611244503037 setps: 84 count: 179\n",
      "reward: -120.62370915271218 setps: 68 count: 247\n",
      "reward: -114.5682258567003 setps: 69 count: 316\n",
      "reward: -123.61990255215764 setps: 89 count: 405\n",
      "reward: -121.85189119145771 setps: 76 count: 481\n",
      "reward: -111.4507457522191 setps: 66 count: 547\n",
      "reward: -111.10569920621067 setps: 67 count: 614\n",
      "reward: -119.13617584119861 setps: 69 count: 683\n",
      "reward: -127.10207025976355 setps: 82 count: 765\n",
      "reward: -124.72976596082995 setps: 83 count: 848\n",
      "reward: -120.8693420301918 setps: 66 count: 914\n",
      "reward: -115.91921356769093 setps: 69 count: 983\n",
      "avg rewards: -119.93944267065136\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.40550\n",
      "Epoch:20 Batch:9 Loss:0.05136\n",
      "Epoch:40 Batch:9 Loss:0.03572\n",
      "Epoch:60 Batch:9 Loss:0.03371\n",
      "Epoch:80 Batch:9 Loss:0.02864\n",
      "Epoch:100 Batch:9 Loss:0.02808\n",
      "Epoch:120 Batch:9 Loss:0.02656\n",
      "Epoch:140 Batch:9 Loss:0.02564\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.076\n",
      "Epoch:10 Batch:9 Loss:0.069\n",
      "Epoch:20 Batch:9 Loss:0.074\n",
      "Epoch:30 Batch:9 Loss:0.074\n",
      "Epoch:40 Batch:9 Loss:0.066\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -115.48935733110706 setps: 121 count: 121\n",
      "reward: -109.16144878602897 setps: 51 count: 172\n",
      "reward: -112.85302122134219 setps: 46 count: 218\n",
      "reward: -110.52621690389203 setps: 60 count: 278\n",
      "reward: -110.03176863110625 setps: 53 count: 331\n",
      "reward: -115.00822268554506 setps: 49 count: 380\n",
      "reward: -118.2293535613492 setps: 150 count: 530\n",
      "reward: -113.75224273101924 setps: 56 count: 586\n",
      "reward: -109.7188803290253 setps: 53 count: 639\n",
      "reward: -110.0872329684409 setps: 61 count: 700\n",
      "reward: -117.84901464024188 setps: 144 count: 844\n",
      "reward: -116.58964958397362 setps: 64 count: 908\n",
      "reward: -109.16199780084752 setps: 57 count: 965\n",
      "avg rewards: -112.9583390133784\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.29792\n",
      "Epoch:20 Batch:10 Loss:0.05046\n",
      "Epoch:40 Batch:10 Loss:0.03525\n",
      "Epoch:60 Batch:10 Loss:0.03257\n",
      "Epoch:80 Batch:10 Loss:0.03073\n",
      "Epoch:100 Batch:10 Loss:0.02786\n",
      "Epoch:120 Batch:10 Loss:0.02691\n",
      "Epoch:140 Batch:10 Loss:0.02713\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.069\n",
      "Epoch:10 Batch:9 Loss:0.069\n",
      "Epoch:20 Batch:9 Loss:0.070\n",
      "Epoch:30 Batch:9 Loss:0.070\n",
      "Epoch:40 Batch:9 Loss:0.066\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -126.6699137979777 setps: 86 count: 86\n",
      "reward: -109.32432652484128 setps: 57 count: 143\n",
      "reward: -109.06404326792868 setps: 52 count: 195\n",
      "reward: -105.99049801300714 setps: 53 count: 248\n",
      "reward: -117.09924662709119 setps: 61 count: 309\n",
      "reward: -115.42653571678511 setps: 90 count: 399\n",
      "reward: -106.08799608914244 setps: 50 count: 449\n",
      "reward: -114.86046197935443 setps: 71 count: 520\n",
      "reward: -107.25801622669088 setps: 54 count: 574\n",
      "reward: -120.76910717970132 setps: 70 count: 644\n",
      "reward: -112.9736359782846 setps: 106 count: 750\n",
      "reward: -109.74668502540203 setps: 78 count: 828\n",
      "reward: -110.73760568913973 setps: 85 count: 913\n",
      "reward: -109.48043653350882 setps: 56 count: 969\n",
      "avg rewards: -112.53489347491822\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.29205\n",
      "Epoch:20 Batch:11 Loss:0.04712\n",
      "Epoch:40 Batch:11 Loss:0.03382\n",
      "Epoch:60 Batch:11 Loss:0.03179\n",
      "Epoch:80 Batch:11 Loss:0.02825\n",
      "Epoch:100 Batch:11 Loss:0.02734\n",
      "Epoch:120 Batch:11 Loss:0.02611\n",
      "Epoch:140 Batch:11 Loss:0.02582\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.070\n",
      "Epoch:10 Batch:9 Loss:0.074\n",
      "Epoch:20 Batch:9 Loss:0.070\n",
      "Epoch:30 Batch:9 Loss:0.069\n",
      "Epoch:40 Batch:9 Loss:0.071\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 7.484584777546657 setps: 800 count: 800\n",
      "avg rewards: 7.484584777546657\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.31032\n",
      "Epoch:20 Batch:12 Loss:0.04795\n",
      "Epoch:40 Batch:12 Loss:0.03629\n",
      "Epoch:60 Batch:12 Loss:0.03358\n",
      "Epoch:80 Batch:12 Loss:0.02925\n",
      "Epoch:100 Batch:12 Loss:0.02939\n",
      "Epoch:120 Batch:12 Loss:0.02841\n",
      "Epoch:140 Batch:12 Loss:0.02568\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.064\n",
      "Epoch:10 Batch:9 Loss:0.064\n",
      "Epoch:20 Batch:9 Loss:0.064\n",
      "Epoch:30 Batch:9 Loss:0.064\n",
      "Epoch:40 Batch:9 Loss:0.061\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -117.5872331297826 setps: 64 count: 64\n",
      "reward: -121.84221496958969 setps: 63 count: 127\n",
      "reward: -119.2008185985405 setps: 66 count: 193\n",
      "reward: -110.96618355517897 setps: 54 count: 247\n",
      "reward: -113.19089134390902 setps: 53 count: 300\n",
      "reward: -119.96219710690963 setps: 65 count: 365\n",
      "reward: -119.21465558785448 setps: 72 count: 437\n",
      "reward: -119.43738348304419 setps: 62 count: 499\n",
      "reward: -115.31677314806979 setps: 56 count: 555\n",
      "reward: -109.39988967923261 setps: 58 count: 613\n",
      "reward: -119.01189951523207 setps: 60 count: 673\n",
      "reward: -119.98794755128027 setps: 73 count: 746\n",
      "reward: -106.36882410377636 setps: 58 count: 804\n",
      "reward: -106.67353291604171 setps: 46 count: 850\n",
      "reward: -121.6232990447941 setps: 62 count: 912\n",
      "reward: -115.66856631366413 setps: 69 count: 981\n",
      "avg rewards: -115.96576937793127\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.25471\n",
      "Epoch:20 Batch:13 Loss:0.04424\n",
      "Epoch:40 Batch:13 Loss:0.03518\n",
      "Epoch:60 Batch:13 Loss:0.03280\n",
      "Epoch:80 Batch:13 Loss:0.03059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:100 Batch:13 Loss:0.02898\n",
      "Epoch:120 Batch:13 Loss:0.02820\n",
      "Epoch:140 Batch:13 Loss:0.02675\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.062\n",
      "Epoch:10 Batch:9 Loss:0.062\n",
      "Epoch:20 Batch:9 Loss:0.062\n",
      "Epoch:30 Batch:9 Loss:0.061\n",
      "Epoch:40 Batch:9 Loss:0.063\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -114.08893076284292 setps: 74 count: 74\n",
      "reward: -115.38439914322582 setps: 66 count: 140\n",
      "reward: -121.00003107837091 setps: 64 count: 204\n",
      "reward: -120.50760502327171 setps: 67 count: 271\n",
      "reward: -119.94273050349207 setps: 65 count: 336\n",
      "reward: -116.19556063948014 setps: 63 count: 399\n",
      "reward: -125.20085630017084 setps: 82 count: 481\n",
      "reward: -112.90587930198883 setps: 53 count: 534\n",
      "reward: -115.48802683325732 setps: 67 count: 601\n",
      "reward: -118.23535268860559 setps: 65 count: 666\n",
      "reward: -118.4551941528724 setps: 69 count: 735\n",
      "reward: -118.09876041250303 setps: 78 count: 813\n",
      "reward: -114.7883960891509 setps: 61 count: 874\n",
      "reward: -120.57333796855683 setps: 75 count: 949\n",
      "avg rewards: -117.91893292127067\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.21247\n",
      "Epoch:20 Batch:14 Loss:0.04410\n",
      "Epoch:40 Batch:14 Loss:0.03493\n",
      "Epoch:60 Batch:14 Loss:0.02984\n",
      "Epoch:80 Batch:14 Loss:0.02601\n",
      "Epoch:100 Batch:14 Loss:0.02751\n",
      "Epoch:120 Batch:14 Loss:0.02557\n",
      "Epoch:140 Batch:14 Loss:0.02400\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.052\n",
      "Epoch:10 Batch:9 Loss:0.059\n",
      "Epoch:20 Batch:9 Loss:0.056\n",
      "Epoch:30 Batch:9 Loss:0.056\n",
      "Epoch:40 Batch:9 Loss:0.057\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -117.48835393243594 setps: 102 count: 102\n",
      "reward: -115.12077028646941 setps: 62 count: 164\n",
      "reward: -23.798405164240563 setps: 800 count: 964\n",
      "avg rewards: -85.46917646104863\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.23780\n",
      "Epoch:20 Batch:15 Loss:0.04802\n",
      "Epoch:40 Batch:15 Loss:0.03606\n",
      "Epoch:60 Batch:15 Loss:0.03086\n",
      "Epoch:80 Batch:15 Loss:0.03086\n",
      "Epoch:100 Batch:15 Loss:0.02858\n",
      "Epoch:120 Batch:15 Loss:0.02681\n",
      "Epoch:140 Batch:15 Loss:0.02468\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.056\n",
      "Epoch:10 Batch:9 Loss:0.061\n",
      "Epoch:20 Batch:9 Loss:0.056\n",
      "Epoch:30 Batch:9 Loss:0.056\n",
      "Epoch:40 Batch:9 Loss:0.058\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -36.243341557194455 setps: 800 count: 800\n",
      "avg rewards: -36.243341557194455\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.20507\n",
      "Epoch:20 Batch:16 Loss:0.04264\n",
      "Epoch:40 Batch:16 Loss:0.03533\n",
      "Epoch:60 Batch:16 Loss:0.03195\n",
      "Epoch:80 Batch:16 Loss:0.02969\n",
      "Epoch:100 Batch:16 Loss:0.02894\n",
      "Epoch:120 Batch:16 Loss:0.02705\n",
      "Epoch:140 Batch:16 Loss:0.02557\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.058\n",
      "Epoch:10 Batch:9 Loss:0.056\n",
      "Epoch:20 Batch:9 Loss:0.052\n",
      "Epoch:30 Batch:9 Loss:0.054\n",
      "Epoch:40 Batch:9 Loss:0.055\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -117.94468186747282 setps: 93 count: 93\n",
      "reward: -114.2542003301736 setps: 97 count: 190\n",
      "reward: -14.883869883492272 setps: 800 count: 990\n",
      "avg rewards: -82.36091736037956\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.19363\n",
      "Epoch:20 Batch:17 Loss:0.04543\n",
      "Epoch:40 Batch:17 Loss:0.03379\n",
      "Epoch:60 Batch:17 Loss:0.03105\n",
      "Epoch:80 Batch:17 Loss:0.02952\n",
      "Epoch:100 Batch:17 Loss:0.03075\n",
      "Epoch:120 Batch:17 Loss:0.02477\n",
      "Epoch:140 Batch:17 Loss:0.02599\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.058\n",
      "Epoch:10 Batch:9 Loss:0.053\n",
      "Epoch:20 Batch:9 Loss:0.056\n",
      "Epoch:30 Batch:9 Loss:0.052\n",
      "Epoch:40 Batch:9 Loss:0.057\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -109.19963076530583 setps: 74 count: 74\n",
      "reward: -4.402368410353243 setps: 800 count: 874\n",
      "reward: -94.92364335472634 setps: 114 count: 988\n",
      "avg rewards: -69.50854751012848\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.19481\n",
      "Epoch:20 Batch:18 Loss:0.04151\n",
      "Epoch:40 Batch:18 Loss:0.03546\n",
      "Epoch:60 Batch:18 Loss:0.02932\n",
      "Epoch:80 Batch:18 Loss:0.03041\n",
      "Epoch:100 Batch:18 Loss:0.02739\n",
      "Epoch:120 Batch:18 Loss:0.02795\n",
      "Epoch:140 Batch:18 Loss:0.02615\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.056\n",
      "Epoch:10 Batch:9 Loss:0.051\n",
      "Epoch:20 Batch:9 Loss:0.055\n",
      "Epoch:30 Batch:9 Loss:0.049\n",
      "Epoch:40 Batch:9 Loss:0.051\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -95.39865727590893 setps: 118 count: 118\n",
      "reward: -121.05993486713308 setps: 78 count: 196\n",
      "reward: -112.83046217157505 setps: 81 count: 277\n",
      "reward: -119.27712411585823 setps: 74 count: 351\n",
      "avg rewards: -112.14154460761881\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.17028\n",
      "Epoch:20 Batch:19 Loss:0.04784\n",
      "Epoch:40 Batch:19 Loss:0.03722\n",
      "Epoch:60 Batch:19 Loss:0.03327\n",
      "Epoch:80 Batch:19 Loss:0.03039\n",
      "Epoch:100 Batch:19 Loss:0.02887\n",
      "Epoch:120 Batch:19 Loss:0.02603\n",
      "Epoch:140 Batch:19 Loss:0.02984\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.050\n",
      "Epoch:10 Batch:9 Loss:0.050\n",
      "Epoch:20 Batch:9 Loss:0.054\n",
      "Epoch:30 Batch:9 Loss:0.048\n",
      "Epoch:40 Batch:9 Loss:0.052\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -14.000808147139214 setps: 800 count: 800\n",
      "avg rewards: -14.000808147139214\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.16975\n",
      "Epoch:20 Batch:20 Loss:0.03818\n",
      "Epoch:40 Batch:20 Loss:0.03679\n",
      "Epoch:60 Batch:20 Loss:0.03160\n",
      "Epoch:80 Batch:20 Loss:0.03025\n",
      "Epoch:100 Batch:20 Loss:0.02898\n",
      "Epoch:120 Batch:20 Loss:0.02647\n",
      "Epoch:140 Batch:20 Loss:0.02686\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.047\n",
      "Epoch:10 Batch:9 Loss:0.046\n",
      "Epoch:20 Batch:9 Loss:0.046\n",
      "Epoch:30 Batch:9 Loss:0.046\n",
      "Epoch:40 Batch:9 Loss:0.045\n",
      "Done!\n",
      "############# start Walker2DBulletEnv-v0 training ###################\n",
      "(49, 1000, 22)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 17.628482255975538 setps: 13 count: 13\n",
      "reward: 15.295545780273098 setps: 12 count: 25\n",
      "reward: 14.60132241650281 setps: 9 count: 34\n",
      "reward: 16.80427010738058 setps: 14 count: 48\n",
      "reward: 13.327445565631205 setps: 8 count: 56\n",
      "reward: 10.250701255020976 setps: 8 count: 64\n",
      "reward: 15.136430656119773 setps: 13 count: 77\n",
      "reward: 11.756486017561109 setps: 7 count: 84\n",
      "reward: 16.824948083671917 setps: 12 count: 96\n",
      "reward: 19.49326231560699 setps: 17 count: 113\n",
      "reward: 27.06296331273333 setps: 26 count: 139\n",
      "reward: 25.434329342700945 setps: 27 count: 166\n",
      "reward: 14.202319174558214 setps: 8 count: 174\n",
      "reward: 16.054074167239012 setps: 17 count: 191\n",
      "reward: 20.110619779252737 setps: 18 count: 209\n",
      "reward: 15.33067732019408 setps: 12 count: 221\n",
      "reward: 19.559810573591676 setps: 21 count: 242\n",
      "reward: 22.055368437802823 setps: 23 count: 265\n",
      "reward: 16.328318490931995 setps: 16 count: 281\n",
      "reward: 18.166326788424335 setps: 18 count: 299\n",
      "reward: 14.575148098405041 setps: 14 count: 313\n",
      "reward: 22.141886228630028 setps: 22 count: 335\n",
      "reward: 11.09307995052659 setps: 6 count: 341\n",
      "reward: 16.746675539205896 setps: 14 count: 355\n",
      "reward: 14.491405984517769 setps: 10 count: 365\n",
      "reward: 12.139916922258271 setps: 8 count: 373\n",
      "reward: 27.976969660820036 setps: 30 count: 403\n",
      "reward: 15.532611891254785 setps: 12 count: 415\n",
      "reward: 16.336029688832056 setps: 13 count: 428\n",
      "reward: 17.608039007104523 setps: 14 count: 442\n",
      "reward: 17.625913934643904 setps: 21 count: 463\n",
      "reward: 24.651218557333053 setps: 24 count: 487\n",
      "reward: 16.44466511144419 setps: 12 count: 499\n",
      "reward: 14.41406574047578 setps: 10 count: 509\n",
      "reward: 17.58412711729761 setps: 18 count: 527\n",
      "reward: 20.553306586608233 setps: 16 count: 543\n",
      "reward: 15.736016320492489 setps: 11 count: 554\n",
      "reward: 17.819669316033828 setps: 15 count: 569\n",
      "reward: 25.819218799834196 setps: 29 count: 598\n",
      "reward: 14.98679081998998 setps: 12 count: 610\n",
      "reward: 13.74165762893972 setps: 9 count: 619\n",
      "reward: 13.458624790895556 setps: 11 count: 630\n",
      "reward: 15.374212534478284 setps: 13 count: 643\n",
      "reward: 19.89335080527089 setps: 19 count: 662\n",
      "reward: 15.841128521163771 setps: 13 count: 675\n",
      "reward: 18.11115311489557 setps: 16 count: 691\n",
      "reward: 15.432769860027474 setps: 15 count: 706\n",
      "reward: 22.455822799970335 setps: 19 count: 725\n",
      "reward: 18.785966558349898 setps: 19 count: 744\n",
      "reward: 16.5050942114176 setps: 12 count: 756\n",
      "reward: 20.38701897207065 setps: 19 count: 775\n",
      "reward: 13.945260211777349 setps: 11 count: 786\n",
      "reward: 19.516826823129666 setps: 17 count: 803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 13.20895221870014 setps: 9 count: 812\n",
      "reward: 19.348339649602714 setps: 20 count: 832\n",
      "reward: 14.918982877806412 setps: 10 count: 842\n",
      "reward: 22.118211751247873 setps: 20 count: 862\n",
      "reward: 15.251755501983283 setps: 11 count: 873\n",
      "reward: 14.787219608628948 setps: 12 count: 885\n",
      "reward: 18.87202530757495 setps: 18 count: 903\n",
      "reward: 14.7494128306731 setps: 11 count: 914\n",
      "reward: 15.395560012491478 setps: 15 count: 929\n",
      "reward: 16.313154007004048 setps: 11 count: 940\n",
      "reward: 17.376869249681473 setps: 14 count: 954\n",
      "reward: 20.31432603583671 setps: 17 count: 971\n",
      "reward: 14.990424769197123 setps: 10 count: 981\n",
      "reward: 19.57561119704624 setps: 18 count: 999\n",
      "avg rewards: 17.31895804427974\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:1.38377\n",
      "Epoch:20 Batch:1 Loss:0.13184\n",
      "Epoch:40 Batch:1 Loss:0.09703\n",
      "Epoch:60 Batch:1 Loss:0.07087\n",
      "Epoch:80 Batch:1 Loss:0.05891\n",
      "Epoch:100 Batch:1 Loss:0.05353\n",
      "Epoch:120 Batch:1 Loss:0.04902\n",
      "Epoch:140 Batch:1 Loss:0.04576\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.109\n",
      "Epoch:10 Batch:10 Loss:0.108\n",
      "Epoch:20 Batch:10 Loss:0.104\n",
      "Epoch:30 Batch:10 Loss:0.098\n",
      "Epoch:40 Batch:10 Loss:0.098\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 51.62690931349789 setps: 92 count: 92\n",
      "reward: 51.93857245518592 setps: 91 count: 183\n",
      "reward: 73.01323820610708 setps: 136 count: 319\n",
      "reward: 60.30779588780571 setps: 103 count: 422\n",
      "reward: 52.447723665158286 setps: 82 count: 504\n",
      "reward: 46.09501966808457 setps: 72 count: 576\n",
      "reward: 54.975914377144335 setps: 91 count: 667\n",
      "avg rewards: 55.77216765328338\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:1.17368\n",
      "Epoch:20 Batch:2 Loss:0.08268\n",
      "Epoch:40 Batch:2 Loss:0.05332\n",
      "Epoch:60 Batch:2 Loss:0.04054\n",
      "Epoch:80 Batch:2 Loss:0.03530\n",
      "Epoch:100 Batch:2 Loss:0.03174\n",
      "Epoch:120 Batch:2 Loss:0.02901\n",
      "Epoch:140 Batch:2 Loss:0.02538\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.093\n",
      "Epoch:10 Batch:10 Loss:0.089\n",
      "Epoch:20 Batch:10 Loss:0.090\n",
      "Epoch:30 Batch:10 Loss:0.088\n",
      "Epoch:40 Batch:10 Loss:0.087\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 24.090567012301474 setps: 73 count: 73\n",
      "reward: 23.038554019112787 setps: 80 count: 153\n",
      "reward: 16.72458500694919 setps: 64 count: 217\n",
      "reward: 55.36863617054039 setps: 70 count: 287\n",
      "reward: 23.162532927916608 setps: 81 count: 368\n",
      "reward: 27.055559318020816 setps: 90 count: 458\n",
      "reward: 30.348786543682227 setps: 93 count: 551\n",
      "reward: 17.683588401437742 setps: 68 count: 619\n",
      "reward: 25.78022487295675 setps: 70 count: 689\n",
      "reward: 15.28553886117151 setps: 64 count: 753\n",
      "reward: 20.683946639433284 setps: 75 count: 828\n",
      "reward: 46.83581115688867 setps: 68 count: 896\n",
      "avg rewards: 27.171527577534288\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.99399\n",
      "Epoch:20 Batch:3 Loss:0.05567\n",
      "Epoch:40 Batch:3 Loss:0.03901\n",
      "Epoch:60 Batch:3 Loss:0.02912\n",
      "Epoch:80 Batch:3 Loss:0.02661\n",
      "Epoch:100 Batch:3 Loss:0.02329\n",
      "Epoch:120 Batch:3 Loss:0.02151\n",
      "Epoch:140 Batch:3 Loss:0.01915\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.092\n",
      "Epoch:10 Batch:10 Loss:0.089\n",
      "Epoch:20 Batch:10 Loss:0.088\n",
      "Epoch:30 Batch:10 Loss:0.088\n",
      "Epoch:40 Batch:10 Loss:0.087\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 474.2490845917916 setps: 800 count: 800\n",
      "reward: 62.65908469500428 setps: 81 count: 881\n",
      "reward: 37.170915740140494 setps: 84 count: 965\n",
      "avg rewards: 191.35969500897878\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.79141\n",
      "Epoch:20 Batch:4 Loss:0.04394\n",
      "Epoch:40 Batch:4 Loss:0.02500\n",
      "Epoch:60 Batch:4 Loss:0.02053\n",
      "Epoch:80 Batch:4 Loss:0.01818\n",
      "Epoch:100 Batch:4 Loss:0.01580\n",
      "Epoch:120 Batch:4 Loss:0.01577\n",
      "Epoch:140 Batch:4 Loss:0.01376\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.081\n",
      "Epoch:10 Batch:10 Loss:0.077\n",
      "Epoch:20 Batch:10 Loss:0.078\n",
      "Epoch:30 Batch:10 Loss:0.078\n",
      "Epoch:40 Batch:10 Loss:0.078\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 536.1795079303831 setps: 800 count: 800\n",
      "avg rewards: 536.1795079303831\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.53036\n",
      "Epoch:20 Batch:5 Loss:0.03280\n",
      "Epoch:40 Batch:5 Loss:0.02267\n",
      "Epoch:60 Batch:5 Loss:0.01681\n",
      "Epoch:80 Batch:5 Loss:0.01463\n",
      "Epoch:100 Batch:5 Loss:0.01325\n",
      "Epoch:120 Batch:5 Loss:0.01326\n",
      "Epoch:140 Batch:5 Loss:0.01139\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.093\n",
      "Epoch:10 Batch:10 Loss:0.090\n",
      "Epoch:20 Batch:10 Loss:0.091\n",
      "Epoch:30 Batch:10 Loss:0.093\n",
      "Epoch:40 Batch:10 Loss:0.091\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 553.7967284348296 setps: 800 count: 800\n",
      "reward: 41.583567126229156 setps: 102 count: 902\n",
      "avg rewards: 297.69014778052934\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.50770\n",
      "Epoch:20 Batch:6 Loss:0.03035\n",
      "Epoch:40 Batch:6 Loss:0.01925\n",
      "Epoch:60 Batch:6 Loss:0.01517\n",
      "Epoch:80 Batch:6 Loss:0.01316\n",
      "Epoch:100 Batch:6 Loss:0.01180\n",
      "Epoch:120 Batch:6 Loss:0.01153\n",
      "Epoch:140 Batch:6 Loss:0.00981\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.081\n",
      "Epoch:10 Batch:10 Loss:0.079\n",
      "Epoch:20 Batch:10 Loss:0.079\n",
      "Epoch:30 Batch:10 Loss:0.079\n",
      "Epoch:40 Batch:10 Loss:0.080\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 78.54799985637074 setps: 147 count: 147\n",
      "reward: 49.78687551947077 setps: 106 count: 253\n",
      "avg rewards: 64.16743768792075\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.73079\n",
      "Epoch:20 Batch:7 Loss:0.02421\n",
      "Epoch:40 Batch:7 Loss:0.01598\n",
      "Epoch:60 Batch:7 Loss:0.01202\n",
      "Epoch:80 Batch:7 Loss:0.00983\n",
      "Epoch:100 Batch:7 Loss:0.01085\n",
      "Epoch:120 Batch:7 Loss:0.00944\n",
      "Epoch:140 Batch:7 Loss:0.00898\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.082\n",
      "Epoch:10 Batch:10 Loss:0.081\n",
      "Epoch:20 Batch:10 Loss:0.080\n",
      "Epoch:30 Batch:10 Loss:0.080\n",
      "Epoch:40 Batch:10 Loss:0.080\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 23.20386190698191 setps: 66 count: 66\n",
      "reward: 551.4159381028778 setps: 800 count: 866\n",
      "reward: 47.64862268364522 setps: 106 count: 972\n",
      "avg rewards: 207.42280756450168\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.44079\n",
      "Epoch:20 Batch:8 Loss:0.02420\n",
      "Epoch:40 Batch:8 Loss:0.01515\n",
      "Epoch:60 Batch:8 Loss:0.01058\n",
      "Epoch:80 Batch:8 Loss:0.00946\n",
      "Epoch:100 Batch:8 Loss:0.00877\n",
      "Epoch:120 Batch:8 Loss:0.00844\n",
      "Epoch:140 Batch:8 Loss:0.00690\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.081\n",
      "Epoch:10 Batch:10 Loss:0.081\n",
      "Epoch:20 Batch:10 Loss:0.081\n",
      "Epoch:30 Batch:10 Loss:0.080\n",
      "Epoch:40 Batch:10 Loss:0.078\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 490.31320470481364 setps: 800 count: 800\n",
      "reward: 37.32325577204755 setps: 93 count: 893\n",
      "avg rewards: 263.8182302384306\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.20239\n",
      "Epoch:20 Batch:9 Loss:0.01943\n",
      "Epoch:40 Batch:9 Loss:0.01207\n",
      "Epoch:60 Batch:9 Loss:0.01053\n",
      "Epoch:80 Batch:9 Loss:0.00872\n",
      "Epoch:100 Batch:9 Loss:0.00789\n",
      "Epoch:120 Batch:9 Loss:0.00788\n",
      "Epoch:140 Batch:9 Loss:0.00704\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.085\n",
      "Epoch:10 Batch:10 Loss:0.087\n",
      "Epoch:20 Batch:10 Loss:0.084\n",
      "Epoch:30 Batch:10 Loss:0.084\n",
      "Epoch:40 Batch:10 Loss:0.084\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 16.338556938197865 setps: 67 count: 67\n",
      "reward: 43.00929188276061 setps: 108 count: 175\n",
      "reward: 21.529808800951287 setps: 74 count: 249\n",
      "reward: 13.682969884923661 setps: 60 count: 309\n",
      "reward: 30.571797019759828 setps: 87 count: 396\n",
      "reward: 20.47719783601496 setps: 63 count: 459\n",
      "reward: 21.23560703295516 setps: 73 count: 532\n",
      "reward: 26.66458203149815 setps: 76 count: 608\n",
      "reward: 24.1562646148348 setps: 73 count: 681\n",
      "reward: 16.586108680532195 setps: 65 count: 746\n",
      "reward: 13.309493000058872 setps: 56 count: 802\n",
      "reward: 24.75908105412964 setps: 79 count: 881\n",
      "reward: 20.43255235538235 setps: 69 count: 950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg rewards: 22.51948547169226\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.19308\n",
      "Epoch:20 Batch:10 Loss:0.01986\n",
      "Epoch:40 Batch:10 Loss:0.01288\n",
      "Epoch:60 Batch:10 Loss:0.01022\n",
      "Epoch:80 Batch:10 Loss:0.00969\n",
      "Epoch:100 Batch:10 Loss:0.00845\n",
      "Epoch:120 Batch:10 Loss:0.00827\n",
      "Epoch:140 Batch:10 Loss:0.00765\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.079\n",
      "Epoch:10 Batch:10 Loss:0.079\n",
      "Epoch:20 Batch:10 Loss:0.078\n",
      "Epoch:30 Batch:10 Loss:0.078\n",
      "Epoch:40 Batch:10 Loss:0.079\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 537.5488617508366 setps: 800 count: 800\n",
      "reward: 60.988353387803386 setps: 136 count: 936\n",
      "avg rewards: 299.26860756932\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.29657\n",
      "Epoch:20 Batch:11 Loss:0.01738\n",
      "Epoch:40 Batch:11 Loss:0.01177\n",
      "Epoch:60 Batch:11 Loss:0.00781\n",
      "Epoch:80 Batch:11 Loss:0.00744\n",
      "Epoch:100 Batch:11 Loss:0.00755\n",
      "Epoch:120 Batch:11 Loss:0.00613\n",
      "Epoch:140 Batch:11 Loss:0.00574\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.083\n",
      "Epoch:10 Batch:10 Loss:0.083\n",
      "Epoch:20 Batch:10 Loss:0.081\n",
      "Epoch:30 Batch:10 Loss:0.083\n",
      "Epoch:40 Batch:10 Loss:0.081\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 85.0925960524037 setps: 92 count: 92\n",
      "reward: 87.09603035188626 setps: 68 count: 160\n",
      "reward: 25.787035188860322 setps: 57 count: 217\n",
      "reward: 35.74169289564161 setps: 69 count: 286\n",
      "reward: 28.128665341135637 setps: 34 count: 320\n",
      "reward: 117.6968500784278 setps: 97 count: 417\n",
      "reward: 38.6834651460871 setps: 62 count: 479\n",
      "reward: 33.51687372183369 setps: 26 count: 505\n",
      "reward: 65.11287723175192 setps: 107 count: 612\n",
      "reward: 72.36424022066637 setps: 66 count: 678\n",
      "reward: 85.09182944595085 setps: 68 count: 746\n",
      "reward: 71.32196230633996 setps: 56 count: 802\n",
      "reward: 30.11597325970216 setps: 62 count: 864\n",
      "reward: 25.893502304219865 setps: 44 count: 908\n",
      "reward: 24.92968395861972 setps: 43 count: 951\n",
      "reward: 26.196075778134396 setps: 44 count: 995\n",
      "avg rewards: 53.29808458010384\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.19549\n",
      "Epoch:20 Batch:12 Loss:0.01668\n",
      "Epoch:40 Batch:12 Loss:0.01228\n",
      "Epoch:60 Batch:12 Loss:0.00874\n",
      "Epoch:80 Batch:12 Loss:0.00876\n",
      "Epoch:100 Batch:12 Loss:0.00804\n",
      "Epoch:120 Batch:12 Loss:0.00758\n",
      "Epoch:140 Batch:12 Loss:0.00659\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.069\n",
      "Epoch:10 Batch:10 Loss:0.068\n",
      "Epoch:20 Batch:10 Loss:0.068\n",
      "Epoch:30 Batch:10 Loss:0.067\n",
      "Epoch:40 Batch:10 Loss:0.067\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 29.54541914267613 setps: 68 count: 68\n",
      "reward: 33.99941387244328 setps: 75 count: 143\n",
      "reward: 21.899351741520515 setps: 59 count: 202\n",
      "reward: 15.146403708605796 setps: 44 count: 246\n",
      "reward: 27.79470734603529 setps: 47 count: 293\n",
      "reward: 26.002179224287094 setps: 67 count: 360\n",
      "reward: 30.913639759948992 setps: 72 count: 432\n",
      "reward: 24.21360870205826 setps: 61 count: 493\n",
      "reward: 26.570979430564332 setps: 62 count: 555\n",
      "reward: 30.154033455764875 setps: 70 count: 625\n",
      "reward: 57.97762902728866 setps: 103 count: 728\n",
      "reward: 24.879535141990342 setps: 62 count: 790\n",
      "reward: 25.65410766683781 setps: 63 count: 853\n",
      "reward: 31.92333242426974 setps: 52 count: 905\n",
      "reward: 28.150608155320516 setps: 47 count: 952\n",
      "reward: 20.390402058168547 setps: 43 count: 995\n",
      "avg rewards: 28.450959428611263\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.11792\n",
      "Epoch:20 Batch:13 Loss:0.01484\n",
      "Epoch:40 Batch:13 Loss:0.01262\n",
      "Epoch:60 Batch:13 Loss:0.00916\n",
      "Epoch:80 Batch:13 Loss:0.00769\n",
      "Epoch:100 Batch:13 Loss:0.00686\n",
      "Epoch:120 Batch:13 Loss:0.00625\n",
      "Epoch:140 Batch:13 Loss:0.00606\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.068\n",
      "Epoch:10 Batch:10 Loss:0.066\n",
      "Epoch:20 Batch:10 Loss:0.066\n",
      "Epoch:30 Batch:10 Loss:0.064\n",
      "Epoch:40 Batch:10 Loss:0.065\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 28.663875306106647 setps: 82 count: 82\n",
      "reward: 26.56875143658689 setps: 78 count: 160\n",
      "reward: 555.793148547361 setps: 800 count: 960\n",
      "avg rewards: 203.6752584300182\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.09543\n",
      "Epoch:20 Batch:14 Loss:0.01533\n",
      "Epoch:40 Batch:14 Loss:0.01272\n",
      "Epoch:60 Batch:14 Loss:0.00924\n",
      "Epoch:80 Batch:14 Loss:0.00828\n",
      "Epoch:100 Batch:14 Loss:0.00705\n",
      "Epoch:120 Batch:14 Loss:0.00651\n",
      "Epoch:140 Batch:14 Loss:0.00602\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.064\n",
      "Epoch:10 Batch:10 Loss:0.065\n",
      "Epoch:20 Batch:10 Loss:0.064\n",
      "Epoch:30 Batch:10 Loss:0.064\n",
      "Epoch:40 Batch:10 Loss:0.065\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 24.160044692103106 setps: 74 count: 74\n",
      "reward: 30.89775127484171 setps: 86 count: 160\n",
      "reward: 31.076947066019038 setps: 77 count: 237\n",
      "reward: 13.589606787996312 setps: 56 count: 293\n",
      "reward: 33.516248996660565 setps: 87 count: 380\n",
      "avg rewards: 26.648119763524146\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.13030\n",
      "Epoch:20 Batch:15 Loss:0.01410\n",
      "Epoch:40 Batch:15 Loss:0.01131\n",
      "Epoch:60 Batch:15 Loss:0.00923\n",
      "Epoch:80 Batch:15 Loss:0.00834\n",
      "Epoch:100 Batch:15 Loss:0.00644\n",
      "Epoch:120 Batch:15 Loss:0.00687\n",
      "Epoch:140 Batch:15 Loss:0.00585\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.069\n",
      "Epoch:10 Batch:10 Loss:0.067\n",
      "Epoch:20 Batch:10 Loss:0.065\n",
      "Epoch:30 Batch:10 Loss:0.066\n",
      "Epoch:40 Batch:10 Loss:0.063\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 63.36059548367487 setps: 116 count: 116\n",
      "reward: 72.3510686259906 setps: 80 count: 196\n",
      "reward: 87.16598953429205 setps: 94 count: 290\n",
      "reward: 31.192079744354125 setps: 68 count: 358\n",
      "reward: 43.36913009509882 setps: 89 count: 447\n",
      "reward: 23.648873236855422 setps: 59 count: 506\n",
      "reward: 34.34626270796726 setps: 79 count: 585\n",
      "reward: 20.48578100609738 setps: 60 count: 645\n",
      "reward: 69.14341660351784 setps: 75 count: 720\n",
      "reward: 73.84283284904524 setps: 81 count: 801\n",
      "reward: 23.20872839738878 setps: 63 count: 864\n",
      "reward: 51.35919781808917 setps: 100 count: 964\n",
      "avg rewards: 49.45616300853097\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.06469\n",
      "Epoch:20 Batch:16 Loss:0.01363\n",
      "Epoch:40 Batch:16 Loss:0.00984\n",
      "Epoch:60 Batch:16 Loss:0.00744\n",
      "Epoch:80 Batch:16 Loss:0.00840\n",
      "Epoch:100 Batch:16 Loss:0.00627\n",
      "Epoch:120 Batch:16 Loss:0.00617\n",
      "Epoch:140 Batch:16 Loss:0.00523\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.066\n",
      "Epoch:10 Batch:10 Loss:0.064\n",
      "Epoch:20 Batch:10 Loss:0.064\n",
      "Epoch:30 Batch:10 Loss:0.063\n",
      "Epoch:40 Batch:10 Loss:0.065\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 42.22160734619828 setps: 81 count: 81\n",
      "reward: 26.523254604810795 setps: 61 count: 142\n",
      "reward: 34.03390451371089 setps: 65 count: 207\n",
      "reward: 113.84776814712671 setps: 119 count: 326\n",
      "reward: 98.74193750465638 setps: 95 count: 421\n",
      "reward: 137.03915141336762 setps: 149 count: 570\n",
      "reward: 56.111961130166314 setps: 96 count: 666\n",
      "reward: 79.0075784349741 setps: 75 count: 741\n",
      "reward: 139.3686768845946 setps: 142 count: 883\n",
      "reward: 36.64052399916544 setps: 59 count: 942\n",
      "avg rewards: 76.35363639787712\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.08587\n",
      "Epoch:20 Batch:17 Loss:0.01395\n",
      "Epoch:40 Batch:17 Loss:0.00919\n",
      "Epoch:60 Batch:17 Loss:0.00766\n",
      "Epoch:80 Batch:17 Loss:0.00761\n",
      "Epoch:100 Batch:17 Loss:0.00530\n",
      "Epoch:120 Batch:17 Loss:0.00642\n",
      "Epoch:140 Batch:17 Loss:0.00518\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.066\n",
      "Epoch:10 Batch:10 Loss:0.065\n",
      "Epoch:20 Batch:10 Loss:0.064\n",
      "Epoch:30 Batch:10 Loss:0.063\n",
      "Epoch:40 Batch:10 Loss:0.064\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 15.34264864893048 setps: 56 count: 56\n",
      "reward: 469.1452220902532 setps: 651 count: 707\n",
      "reward: 18.564300267799997 setps: 61 count: 768\n",
      "reward: 26.20487759356037 setps: 68 count: 836\n",
      "reward: 46.47646925727022 setps: 98 count: 934\n",
      "reward: 18.203012183675305 setps: 60 count: 994\n",
      "avg rewards: 98.9894216735816\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:18 Loss:0.06668\n",
      "Epoch:20 Batch:18 Loss:0.01262\n",
      "Epoch:40 Batch:18 Loss:0.00950\n",
      "Epoch:60 Batch:18 Loss:0.00732\n",
      "Epoch:80 Batch:18 Loss:0.00714\n",
      "Epoch:100 Batch:18 Loss:0.00678\n",
      "Epoch:120 Batch:18 Loss:0.00520\n",
      "Epoch:140 Batch:18 Loss:0.00566\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.063\n",
      "Epoch:10 Batch:10 Loss:0.061\n",
      "Epoch:20 Batch:10 Loss:0.060\n",
      "Epoch:30 Batch:10 Loss:0.060\n",
      "Epoch:40 Batch:10 Loss:0.061\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 24.429032949182144 setps: 71 count: 71\n",
      "reward: 40.221226300850674 setps: 99 count: 170\n",
      "reward: 554.8479348608672 setps: 800 count: 970\n",
      "avg rewards: 206.49939803696668\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.06703\n",
      "Epoch:20 Batch:19 Loss:0.01221\n",
      "Epoch:40 Batch:19 Loss:0.00915\n",
      "Epoch:60 Batch:19 Loss:0.00802\n",
      "Epoch:80 Batch:19 Loss:0.00682\n",
      "Epoch:100 Batch:19 Loss:0.00624\n",
      "Epoch:120 Batch:19 Loss:0.00554\n",
      "Epoch:140 Batch:19 Loss:0.00539\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.070\n",
      "Epoch:10 Batch:10 Loss:0.068\n",
      "Epoch:20 Batch:10 Loss:0.067\n",
      "Epoch:30 Batch:10 Loss:0.067\n",
      "Epoch:40 Batch:10 Loss:0.066\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 551.5458335648501 setps: 800 count: 800\n",
      "avg rewards: 551.5458335648501\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.05108\n",
      "Epoch:20 Batch:20 Loss:0.01216\n",
      "Epoch:40 Batch:20 Loss:0.00850\n",
      "Epoch:60 Batch:20 Loss:0.00751\n",
      "Epoch:80 Batch:20 Loss:0.00653\n",
      "Epoch:100 Batch:20 Loss:0.00531\n",
      "Epoch:120 Batch:20 Loss:0.00473\n",
      "Epoch:140 Batch:20 Loss:0.00480\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.067\n",
      "Epoch:10 Batch:10 Loss:0.062\n",
      "Epoch:20 Batch:10 Loss:0.062\n",
      "Epoch:30 Batch:10 Loss:0.062\n",
      "Epoch:40 Batch:10 Loss:0.063\n",
      "Done!\n",
      "############# start HopperBulletEnv-v0 training ###################\n",
      "(49,)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 18.64622586793121 setps: 17 count: 17\n",
      "reward: 17.40818309659371 setps: 7 count: 24\n",
      "reward: 29.16112769078754 setps: 19 count: 43\n",
      "reward: 19.706662755335856 setps: 6 count: 49\n",
      "reward: 17.834163890368654 setps: 11 count: 60\n",
      "reward: 15.414542757795425 setps: 5 count: 65\n",
      "reward: 22.750892984151143 setps: 12 count: 77\n",
      "reward: 21.80796264240198 setps: 12 count: 89\n",
      "reward: 19.568133132855294 setps: 6 count: 95\n",
      "reward: 18.128844645718345 setps: 8 count: 103\n",
      "reward: 17.84680500954273 setps: 7 count: 110\n",
      "reward: 19.310564171876468 setps: 10 count: 120\n",
      "reward: 19.86305069273367 setps: 6 count: 126\n",
      "reward: 19.241151433065532 setps: 8 count: 134\n",
      "reward: 20.308980393574164 setps: 15 count: 149\n",
      "reward: 20.214840676484165 setps: 9 count: 158\n",
      "reward: 16.789200878505653 setps: 5 count: 163\n",
      "reward: 21.39787101899128 setps: 7 count: 170\n",
      "reward: 19.60174560581654 setps: 9 count: 179\n",
      "reward: 17.827535326604266 setps: 9 count: 188\n",
      "reward: 24.887277609622107 setps: 11 count: 199\n",
      "reward: 21.393940396537072 setps: 14 count: 213\n",
      "reward: 18.002829718199795 setps: 11 count: 224\n",
      "reward: 20.501069520310555 setps: 10 count: 234\n",
      "reward: 16.844627722518634 setps: 18 count: 252\n",
      "reward: 25.165394061084953 setps: 15 count: 267\n",
      "reward: 21.528627748241703 setps: 9 count: 276\n",
      "reward: 20.395198861430984 setps: 11 count: 287\n",
      "reward: 21.722299501718958 setps: 17 count: 304\n",
      "reward: 22.09144142128935 setps: 21 count: 325\n",
      "reward: 21.126231244151132 setps: 14 count: 339\n",
      "reward: 21.787212841423752 setps: 7 count: 346\n",
      "reward: 20.266550721086972 setps: 11 count: 357\n",
      "reward: 17.30798618016561 setps: 12 count: 369\n",
      "reward: 18.20786339586193 setps: 12 count: 381\n",
      "reward: 31.364673500433856 setps: 27 count: 408\n",
      "reward: 27.293263943678177 setps: 22 count: 430\n",
      "reward: 17.19448577382427 setps: 11 count: 441\n",
      "reward: 21.545916226369446 setps: 11 count: 452\n",
      "reward: 25.312593153957277 setps: 18 count: 470\n",
      "reward: 20.23581227164104 setps: 16 count: 486\n",
      "reward: 24.591501283783874 setps: 24 count: 510\n",
      "reward: 23.234750680532308 setps: 16 count: 526\n",
      "reward: 19.825682194056572 setps: 8 count: 534\n",
      "reward: 17.99982078739849 setps: 12 count: 546\n",
      "reward: 21.059514472233424 setps: 12 count: 558\n",
      "reward: 21.487991855837755 setps: 12 count: 570\n",
      "reward: 21.60700323674537 setps: 13 count: 583\n",
      "reward: 19.794143930661086 setps: 12 count: 595\n",
      "reward: 20.992879474215442 setps: 14 count: 609\n",
      "reward: 18.884208956078506 setps: 12 count: 621\n",
      "reward: 16.720906671618287 setps: 5 count: 626\n",
      "reward: 23.02920197494241 setps: 10 count: 636\n",
      "reward: 22.633363741256467 setps: 10 count: 646\n",
      "reward: 16.348403305921238 setps: 14 count: 660\n",
      "reward: 20.74376247452456 setps: 8 count: 668\n",
      "reward: 18.945885027166515 setps: 7 count: 675\n",
      "reward: 17.206041958720018 setps: 7 count: 682\n",
      "reward: 26.136647380510112 setps: 18 count: 700\n",
      "reward: 17.05585997849121 setps: 6 count: 706\n",
      "reward: 19.712622070475483 setps: 11 count: 717\n",
      "reward: 17.732156160479647 setps: 8 count: 725\n",
      "reward: 15.238716689778084 setps: 7 count: 732\n",
      "reward: 17.194037004568962 setps: 7 count: 739\n",
      "reward: 16.19925266330392 setps: 6 count: 745\n",
      "reward: 29.07830599426961 setps: 20 count: 765\n",
      "reward: 19.41357452069642 setps: 9 count: 774\n",
      "reward: 20.653941987246796 setps: 13 count: 787\n",
      "reward: 25.181756244999995 setps: 13 count: 800\n",
      "reward: 19.679301787279833 setps: 15 count: 815\n",
      "reward: 24.50541133024963 setps: 12 count: 827\n",
      "reward: 17.203685440489792 setps: 5 count: 832\n",
      "reward: 22.56045434886182 setps: 12 count: 844\n",
      "reward: 14.628545155499886 setps: 12 count: 856\n",
      "reward: 24.96474900062022 setps: 16 count: 872\n",
      "reward: 16.888685974825055 setps: 7 count: 879\n",
      "reward: 19.432054585524025 setps: 16 count: 895\n",
      "reward: 21.920430831357955 setps: 15 count: 910\n",
      "reward: 17.444797522101723 setps: 17 count: 927\n",
      "reward: 26.620541610402867 setps: 20 count: 947\n",
      "reward: 26.090298064738462 setps: 20 count: 967\n",
      "reward: 18.495343158606556 setps: 13 count: 980\n",
      "reward: 28.108907147754508 setps: 20 count: 1000\n",
      "avg rewards: 20.653601435704893\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:1.36481\n",
      "Epoch:20 Batch:1 Loss:0.14424\n",
      "Epoch:40 Batch:1 Loss:0.09695\n",
      "Epoch:60 Batch:1 Loss:0.07087\n",
      "Epoch:80 Batch:1 Loss:0.06321\n",
      "Epoch:100 Batch:1 Loss:0.05671\n",
      "Epoch:120 Batch:1 Loss:0.05274\n",
      "Epoch:140 Batch:1 Loss:0.04956\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.115\n",
      "Epoch:10 Batch:10 Loss:0.116\n",
      "Epoch:20 Batch:10 Loss:0.105\n",
      "Epoch:30 Batch:10 Loss:0.111\n",
      "Epoch:40 Batch:10 Loss:0.107\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 5.790841591790375 setps: 30 count: 30\n",
      "reward: 6.562548156775299 setps: 31 count: 61\n",
      "reward: 6.3116457372307195 setps: 30 count: 91\n",
      "reward: 3.7613871534602343 setps: 28 count: 119\n",
      "reward: 10.169207687806919 setps: 32 count: 151\n",
      "reward: 4.640809807642654 setps: 30 count: 181\n",
      "reward: 5.767085461599343 setps: 29 count: 210\n",
      "reward: 2.8148474192692436 setps: 28 count: 238\n",
      "reward: 6.324153873843896 setps: 31 count: 269\n",
      "reward: 5.016691834920493 setps: 31 count: 300\n",
      "reward: 6.382090275865631 setps: 31 count: 331\n",
      "reward: 9.91593681207014 setps: 32 count: 363\n",
      "reward: 1.107594685506773 setps: 30 count: 393\n",
      "reward: 0.33233875769947097 setps: 29 count: 422\n",
      "reward: -0.41374038363428656 setps: 30 count: 452\n",
      "reward: 5.514946447232795 setps: 29 count: 481\n",
      "reward: 1.297494478154839 setps: 28 count: 509\n",
      "reward: 5.38372595193359 setps: 29 count: 538\n",
      "reward: 7.640720273388435 setps: 32 count: 570\n",
      "reward: 7.641860728849132 setps: 30 count: 600\n",
      "reward: 7.8413809282195865 setps: 32 count: 632\n",
      "reward: 6.009291884599951 setps: 29 count: 661\n",
      "reward: 1.2968667651861319 setps: 31 count: 692\n",
      "reward: 4.098245570894507 setps: 30 count: 722\n",
      "reward: 1.7242365537009385 setps: 30 count: 752\n",
      "reward: 5.630583691420906 setps: 31 count: 783\n",
      "reward: 8.248594509204848 setps: 32 count: 815\n",
      "reward: -1.3951149084532526 setps: 30 count: 845\n",
      "reward: 8.215560494906278 setps: 31 count: 876\n",
      "reward: 2.606722784440716 setps: 29 count: 905\n",
      "reward: 1.2994412216183262 setps: 30 count: 935\n",
      "reward: 5.691829859769494 setps: 29 count: 964\n",
      "reward: 6.102347427196216 setps: 31 count: 995\n",
      "avg rewards: 4.828247682851828\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:2 Loss:1.20936\n",
      "Epoch:20 Batch:2 Loss:0.09016\n",
      "Epoch:40 Batch:2 Loss:0.05508\n",
      "Epoch:60 Batch:2 Loss:0.04600\n",
      "Epoch:80 Batch:2 Loss:0.03882\n",
      "Epoch:100 Batch:2 Loss:0.03512\n",
      "Epoch:120 Batch:2 Loss:0.03331\n",
      "Epoch:140 Batch:2 Loss:0.03124\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.086\n",
      "Epoch:10 Batch:10 Loss:0.090\n",
      "Epoch:20 Batch:10 Loss:0.081\n",
      "Epoch:30 Batch:10 Loss:0.081\n",
      "Epoch:40 Batch:10 Loss:0.083\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 30.054785748400903 setps: 37 count: 37\n",
      "reward: 40.764085291822276 setps: 47 count: 84\n",
      "reward: 26.944685171268056 setps: 35 count: 119\n",
      "reward: 31.422050299093833 setps: 38 count: 157\n",
      "reward: 35.511726846019165 setps: 41 count: 198\n",
      "reward: 37.64538561402296 setps: 43 count: 241\n",
      "reward: 29.36253619672207 setps: 37 count: 278\n",
      "reward: 29.3238769827527 setps: 35 count: 313\n",
      "reward: 37.30585530909448 setps: 43 count: 356\n",
      "reward: 32.39926157745357 setps: 39 count: 395\n",
      "reward: 36.582662031704984 setps: 40 count: 435\n",
      "reward: 36.52211641919274 setps: 42 count: 477\n",
      "reward: 29.635010154491468 setps: 36 count: 513\n",
      "reward: 29.49362563427276 setps: 35 count: 548\n",
      "reward: 30.192786044249086 setps: 37 count: 585\n",
      "reward: 31.88055234112835 setps: 38 count: 623\n",
      "reward: 35.40405155057089 setps: 39 count: 662\n",
      "reward: 36.1738560829428 setps: 40 count: 702\n",
      "reward: 37.70980514537368 setps: 41 count: 743\n",
      "reward: 39.4163142159916 setps: 43 count: 786\n",
      "reward: 39.02945907387039 setps: 44 count: 830\n",
      "reward: 30.497838277580744 setps: 37 count: 867\n",
      "reward: 37.98122788815235 setps: 42 count: 909\n",
      "reward: 29.677459414987243 setps: 35 count: 944\n",
      "reward: 30.25340184654633 setps: 37 count: 981\n",
      "avg rewards: 33.647376606308214\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:1.13277\n",
      "Epoch:20 Batch:3 Loss:0.06491\n",
      "Epoch:40 Batch:3 Loss:0.03891\n",
      "Epoch:60 Batch:3 Loss:0.03055\n",
      "Epoch:80 Batch:3 Loss:0.02906\n",
      "Epoch:100 Batch:3 Loss:0.02586\n",
      "Epoch:120 Batch:3 Loss:0.02607\n",
      "Epoch:140 Batch:3 Loss:0.02404\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.073\n",
      "Epoch:10 Batch:10 Loss:0.062\n",
      "Epoch:20 Batch:10 Loss:0.069\n",
      "Epoch:30 Batch:10 Loss:0.071\n",
      "Epoch:40 Batch:10 Loss:0.066\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 23.896475786186056 setps: 16 count: 16\n",
      "reward: 25.44768969554571 setps: 18 count: 34\n",
      "reward: 27.050345254594863 setps: 18 count: 52\n",
      "reward: 22.08942199701415 setps: 18 count: 70\n",
      "reward: 18.075932765867037 setps: 15 count: 85\n",
      "reward: 25.96167497627903 setps: 19 count: 104\n",
      "reward: 24.25586227899039 setps: 17 count: 121\n",
      "reward: 26.947338737330575 setps: 19 count: 140\n",
      "reward: 25.408822455714102 setps: 18 count: 158\n",
      "reward: 26.57829310892557 setps: 19 count: 177\n",
      "reward: 25.63521290045901 setps: 19 count: 196\n",
      "reward: 26.045885061191804 setps: 19 count: 215\n",
      "reward: 25.58917898871587 setps: 17 count: 232\n",
      "reward: 25.379789316182723 setps: 17 count: 249\n",
      "reward: 25.839683525481085 setps: 17 count: 266\n",
      "reward: 20.87285874174704 setps: 17 count: 283\n",
      "reward: 25.052238178001424 setps: 18 count: 301\n",
      "reward: 24.431847230995483 setps: 17 count: 318\n",
      "reward: 26.276802422298353 setps: 18 count: 336\n",
      "reward: 24.344754888194437 setps: 18 count: 354\n",
      "reward: 24.403697949333583 setps: 17 count: 371\n",
      "reward: 24.82343664287328 setps: 17 count: 388\n",
      "reward: 27.20457559379138 setps: 19 count: 407\n",
      "reward: 25.87707137531397 setps: 18 count: 425\n",
      "reward: 26.271303004886434 setps: 19 count: 444\n",
      "reward: 27.47321656732238 setps: 19 count: 463\n",
      "reward: 25.53289368013794 setps: 18 count: 481\n",
      "reward: 26.20442798283038 setps: 19 count: 500\n",
      "reward: 26.09011478722968 setps: 18 count: 518\n",
      "reward: 25.440308848560377 setps: 18 count: 536\n",
      "reward: 23.98791237360856 setps: 18 count: 554\n",
      "reward: 23.128605376525957 setps: 16 count: 570\n",
      "reward: 28.641366301428935 setps: 20 count: 590\n",
      "reward: 22.398431001024434 setps: 16 count: 606\n",
      "reward: 27.118395141573274 setps: 18 count: 624\n",
      "reward: 23.724157962617756 setps: 17 count: 641\n",
      "reward: 28.41850684814708 setps: 18 count: 659\n",
      "reward: 22.31584158988262 setps: 17 count: 676\n",
      "reward: 25.265031874524713 setps: 17 count: 693\n",
      "reward: 25.803183022976732 setps: 17 count: 710\n",
      "reward: 27.254157975500863 setps: 19 count: 729\n",
      "reward: 22.56818750908278 setps: 16 count: 745\n",
      "reward: 23.634596226271245 setps: 18 count: 763\n",
      "reward: 18.059904367032868 setps: 15 count: 778\n",
      "reward: 26.560381985607094 setps: 18 count: 796\n",
      "reward: 26.907310569408583 setps: 17 count: 813\n",
      "reward: 25.395319298122196 setps: 17 count: 830\n",
      "reward: 27.323438593049648 setps: 19 count: 849\n",
      "reward: 26.49032605069806 setps: 19 count: 868\n",
      "reward: 26.703075941077262 setps: 19 count: 887\n",
      "reward: 25.609908613700824 setps: 18 count: 905\n",
      "reward: 25.805621039673866 setps: 18 count: 923\n",
      "reward: 25.798581605794606 setps: 18 count: 941\n",
      "reward: 27.17996118669834 setps: 18 count: 959\n",
      "reward: 26.666217375572884 setps: 17 count: 976\n",
      "reward: 25.602637485507877 setps: 17 count: 993\n",
      "avg rewards: 25.229682358162556\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.94509\n",
      "Epoch:20 Batch:4 Loss:0.05508\n",
      "Epoch:40 Batch:4 Loss:0.03565\n",
      "Epoch:60 Batch:4 Loss:0.03266\n",
      "Epoch:80 Batch:4 Loss:0.02457\n",
      "Epoch:100 Batch:4 Loss:0.02415\n",
      "Epoch:120 Batch:4 Loss:0.02209\n",
      "Epoch:140 Batch:4 Loss:0.02270\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.077\n",
      "Epoch:10 Batch:10 Loss:0.069\n",
      "Epoch:20 Batch:10 Loss:0.066\n",
      "Epoch:30 Batch:10 Loss:0.067\n",
      "Epoch:40 Batch:10 Loss:0.070\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 32.43749878416128 setps: 38 count: 38\n",
      "reward: 60.479938013406354 setps: 53 count: 91\n",
      "reward: 56.74853438877618 setps: 42 count: 133\n",
      "reward: 39.09268263614795 setps: 39 count: 172\n",
      "reward: 41.0547717510155 setps: 45 count: 217\n",
      "reward: 37.23950173346238 setps: 42 count: 259\n",
      "reward: 46.9342436164996 setps: 38 count: 297\n",
      "reward: 50.892334848847554 setps: 43 count: 340\n",
      "reward: 43.59388029133844 setps: 40 count: 380\n",
      "reward: 30.170817911201457 setps: 37 count: 417\n",
      "reward: 37.95358696755865 setps: 41 count: 458\n",
      "reward: 42.57439513741118 setps: 44 count: 502\n",
      "reward: 48.99656090934295 setps: 36 count: 538\n",
      "reward: 41.58575320412492 setps: 41 count: 579\n",
      "reward: 66.30177662331845 setps: 51 count: 630\n",
      "reward: 38.0041367291895 setps: 42 count: 672\n",
      "reward: 58.09881849441009 setps: 45 count: 717\n",
      "reward: 46.05560373840273 setps: 51 count: 768\n",
      "reward: 34.93534073144984 setps: 41 count: 809\n",
      "reward: 46.78815076761675 setps: 43 count: 852\n",
      "reward: 37.94724926446796 setps: 41 count: 893\n",
      "reward: 66.3242993296022 setps: 54 count: 947\n",
      "reward: 37.49689218125131 setps: 41 count: 988\n",
      "avg rewards: 45.29159861100015\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.55566\n",
      "Epoch:20 Batch:5 Loss:0.04518\n",
      "Epoch:40 Batch:5 Loss:0.03363\n",
      "Epoch:60 Batch:5 Loss:0.02676\n",
      "Epoch:80 Batch:5 Loss:0.02424\n",
      "Epoch:100 Batch:5 Loss:0.02240\n",
      "Epoch:120 Batch:5 Loss:0.02153\n",
      "Epoch:140 Batch:5 Loss:0.02113\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.052\n",
      "Epoch:10 Batch:10 Loss:0.051\n",
      "Epoch:20 Batch:10 Loss:0.054\n",
      "Epoch:30 Batch:10 Loss:0.064\n",
      "Epoch:40 Batch:10 Loss:0.061\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 46.543394448123586 setps: 56 count: 56\n",
      "reward: 46.42066810651594 setps: 46 count: 102\n",
      "reward: 51.41288239077693 setps: 55 count: 157\n",
      "reward: 51.9303302798784 setps: 45 count: 202\n",
      "reward: 56.55397848221617 setps: 54 count: 256\n",
      "reward: 51.271953319363845 setps: 57 count: 313\n",
      "reward: 47.50083212123135 setps: 48 count: 361\n",
      "reward: 43.862093625617845 setps: 49 count: 410\n",
      "reward: 56.03104494598376 setps: 55 count: 465\n",
      "reward: 47.105238412025216 setps: 50 count: 515\n",
      "reward: 57.27059514033025 setps: 50 count: 565\n",
      "reward: 53.249435939319625 setps: 56 count: 621\n",
      "reward: 52.03693729056286 setps: 52 count: 673\n",
      "reward: 40.2106268930016 setps: 51 count: 724\n",
      "reward: 45.21036937361204 setps: 43 count: 767\n",
      "reward: 42.81610140382982 setps: 42 count: 809\n",
      "reward: 56.83701320698165 setps: 52 count: 861\n",
      "reward: 55.85577397748565 setps: 52 count: 913\n",
      "reward: 43.51915286348814 setps: 45 count: 958\n",
      "avg rewards: 49.77044327475498\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:6 Loss:0.41052\n",
      "Epoch:20 Batch:6 Loss:0.04278\n",
      "Epoch:40 Batch:6 Loss:0.02808\n",
      "Epoch:60 Batch:6 Loss:0.02425\n",
      "Epoch:80 Batch:6 Loss:0.02311\n",
      "Epoch:100 Batch:6 Loss:0.01989\n",
      "Epoch:120 Batch:6 Loss:0.02055\n",
      "Epoch:140 Batch:6 Loss:0.02038\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.051\n",
      "Epoch:10 Batch:10 Loss:0.047\n",
      "Epoch:20 Batch:10 Loss:0.052\n",
      "Epoch:30 Batch:10 Loss:0.050\n",
      "Epoch:40 Batch:10 Loss:0.048\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 32.339215469056214 setps: 41 count: 41\n",
      "reward: 20.363705526512053 setps: 35 count: 76\n",
      "reward: 25.36290478928858 setps: 36 count: 112\n",
      "reward: 30.470610364348968 setps: 39 count: 151\n",
      "reward: 29.030783631278613 setps: 41 count: 192\n",
      "reward: 20.646692156451174 setps: 38 count: 230\n",
      "reward: 27.05087302715401 setps: 37 count: 267\n",
      "reward: 27.393873292430367 setps: 39 count: 306\n",
      "reward: 26.908007413227462 setps: 36 count: 342\n",
      "reward: 29.288238588157405 setps: 40 count: 382\n",
      "reward: 35.81356632491953 setps: 43 count: 425\n",
      "reward: 22.25022482917557 setps: 36 count: 461\n",
      "reward: 24.93362991858157 setps: 37 count: 498\n",
      "reward: 24.929915951691513 setps: 37 count: 535\n",
      "reward: 31.977855336214994 setps: 42 count: 577\n",
      "reward: 24.105902644390877 setps: 37 count: 614\n",
      "reward: 26.518593755054464 setps: 37 count: 651\n",
      "reward: 28.09627628143207 setps: 36 count: 687\n",
      "reward: 21.075595261827402 setps: 35 count: 722\n",
      "reward: 11.7756202147837 setps: 33 count: 755\n",
      "reward: 28.789691827175556 setps: 37 count: 792\n",
      "reward: 25.39152827582439 setps: 40 count: 832\n",
      "reward: 19.95596502678381 setps: 35 count: 867\n",
      "reward: 28.910853955651692 setps: 40 count: 907\n",
      "reward: 28.627526656172993 setps: 38 count: 945\n",
      "reward: 33.684566246072066 setps: 41 count: 986\n",
      "avg rewards: 26.372777567832966\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.67467\n",
      "Epoch:20 Batch:7 Loss:0.04105\n",
      "Epoch:40 Batch:7 Loss:0.02656\n",
      "Epoch:60 Batch:7 Loss:0.02522\n",
      "Epoch:80 Batch:7 Loss:0.02018\n",
      "Epoch:100 Batch:7 Loss:0.01965\n",
      "Epoch:120 Batch:7 Loss:0.01916\n",
      "Epoch:140 Batch:7 Loss:0.01764\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.045\n",
      "Epoch:10 Batch:10 Loss:0.042\n",
      "Epoch:20 Batch:10 Loss:0.044\n",
      "Epoch:30 Batch:10 Loss:0.047\n",
      "Epoch:40 Batch:10 Loss:0.045\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 52.79474709124188 setps: 82 count: 82\n",
      "reward: 17.59479896057601 setps: 10 count: 92\n",
      "reward: 79.47815702697697 setps: 78 count: 170\n",
      "reward: 38.23587215144944 setps: 67 count: 237\n",
      "reward: 30.32783879141425 setps: 61 count: 298\n",
      "reward: 14.142880194417376 setps: 54 count: 352\n",
      "reward: 15.482057602738497 setps: 10 count: 362\n",
      "reward: 5.877172686895937 setps: 46 count: 408\n",
      "reward: 15.929562458173315 setps: 13 count: 421\n",
      "reward: 19.25832144462037 setps: 62 count: 483\n",
      "reward: 47.035849560111835 setps: 53 count: 536\n",
      "reward: 18.004289533858532 setps: 26 count: 562\n",
      "reward: 60.179693905463495 setps: 77 count: 639\n",
      "reward: 29.423048752640895 setps: 37 count: 676\n",
      "reward: -4.1841503486328175 setps: 75 count: 751\n",
      "reward: 28.425257543199404 setps: 26 count: 777\n",
      "reward: 27.330031872034308 setps: 14 count: 791\n",
      "reward: 20.264863865518414 setps: 29 count: 820\n",
      "reward: 26.08885421363375 setps: 24 count: 844\n",
      "reward: 82.69328226543581 setps: 106 count: 950\n",
      "reward: 13.847743121275565 setps: 50 count: 1000\n",
      "avg rewards: 30.391912985383016\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.44361\n",
      "Epoch:20 Batch:8 Loss:0.03270\n",
      "Epoch:40 Batch:8 Loss:0.02479\n",
      "Epoch:60 Batch:8 Loss:0.02301\n",
      "Epoch:80 Batch:8 Loss:0.02069\n",
      "Epoch:100 Batch:8 Loss:0.01853\n",
      "Epoch:120 Batch:8 Loss:0.01861\n",
      "Epoch:140 Batch:8 Loss:0.02033\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.047\n",
      "Epoch:10 Batch:10 Loss:0.043\n",
      "Epoch:20 Batch:10 Loss:0.041\n",
      "Epoch:30 Batch:10 Loss:0.041\n",
      "Epoch:40 Batch:10 Loss:0.044\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 24.6729904720225 setps: 38 count: 38\n",
      "reward: 42.22584274136461 setps: 44 count: 82\n",
      "reward: 34.35028051483241 setps: 37 count: 119\n",
      "reward: 41.29617326166772 setps: 43 count: 162\n",
      "reward: 35.28875632976559 setps: 40 count: 202\n",
      "reward: 35.43701617800398 setps: 41 count: 243\n",
      "reward: 38.740033681129 setps: 41 count: 284\n",
      "reward: 32.356036909176325 setps: 39 count: 323\n",
      "reward: 37.57410255377472 setps: 39 count: 362\n",
      "reward: 52.587391063150434 setps: 83 count: 445\n",
      "reward: 29.725612256859314 setps: 37 count: 482\n",
      "reward: 35.6186157729404 setps: 38 count: 520\n",
      "reward: 53.32678117415926 setps: 51 count: 571\n",
      "reward: 34.4544156252712 setps: 40 count: 611\n",
      "reward: 18.7246438890259 setps: 43 count: 654\n",
      "reward: 19.207979057340705 setps: 31 count: 685\n",
      "reward: 25.650568633707 setps: 53 count: 738\n",
      "reward: 35.49942827556516 setps: 39 count: 777\n",
      "reward: 17.49219749316108 setps: 44 count: 821\n",
      "reward: 32.48142471358733 setps: 40 count: 861\n",
      "reward: 18.919967733598607 setps: 44 count: 905\n",
      "reward: 18.591749577662263 setps: 36 count: 941\n",
      "reward: 39.107558365633295 setps: 40 count: 981\n",
      "avg rewards: 32.75345940319125\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.22150\n",
      "Epoch:20 Batch:9 Loss:0.03377\n",
      "Epoch:40 Batch:9 Loss:0.02455\n",
      "Epoch:60 Batch:9 Loss:0.02016\n",
      "Epoch:80 Batch:9 Loss:0.02448\n",
      "Epoch:100 Batch:9 Loss:0.01786\n",
      "Epoch:120 Batch:9 Loss:0.01803\n",
      "Epoch:140 Batch:9 Loss:0.01724\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.034\n",
      "Epoch:10 Batch:10 Loss:0.037\n",
      "Epoch:20 Batch:10 Loss:0.034\n",
      "Epoch:30 Batch:10 Loss:0.034\n",
      "Epoch:40 Batch:10 Loss:0.033\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 52.120650470907265 setps: 54 count: 54\n",
      "reward: 70.98311466172161 setps: 61 count: 115\n",
      "reward: 82.91779407739526 setps: 78 count: 193\n",
      "reward: 71.99811771452076 setps: 63 count: 256\n",
      "reward: 60.48481111935978 setps: 51 count: 307\n",
      "reward: 69.90567162773947 setps: 62 count: 369\n",
      "reward: 58.945754496987504 setps: 51 count: 420\n",
      "reward: 118.84610154125879 setps: 125 count: 545\n",
      "reward: 68.56771904063356 setps: 57 count: 602\n",
      "reward: 19.497394750951205 setps: 46 count: 648\n",
      "reward: 85.09739963076863 setps: 80 count: 728\n",
      "reward: 64.86589630638191 setps: 75 count: 803\n",
      "reward: 65.08543386994057 setps: 58 count: 861\n",
      "reward: 76.03490114183953 setps: 62 count: 923\n",
      "reward: 20.92216391355323 setps: 48 count: 971\n",
      "avg rewards: 65.75152829093062\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.24530\n",
      "Epoch:20 Batch:10 Loss:0.02976\n",
      "Epoch:40 Batch:10 Loss:0.02363\n",
      "Epoch:60 Batch:10 Loss:0.01974\n",
      "Epoch:80 Batch:10 Loss:0.01774\n",
      "Epoch:100 Batch:10 Loss:0.01855\n",
      "Epoch:120 Batch:10 Loss:0.01709\n",
      "Epoch:140 Batch:10 Loss:0.01618\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.032\n",
      "Epoch:10 Batch:10 Loss:0.029\n",
      "Epoch:20 Batch:10 Loss:0.032\n",
      "Epoch:30 Batch:10 Loss:0.029\n",
      "Epoch:40 Batch:10 Loss:0.029\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 26.640584559287518 setps: 57 count: 57\n",
      "reward: 56.34250659099636 setps: 50 count: 107\n",
      "reward: 31.331230255287668 setps: 60 count: 167\n",
      "reward: 64.36240366575073 setps: 56 count: 223\n",
      "reward: 82.19190871465547 setps: 77 count: 300\n",
      "reward: 66.40737035427301 setps: 58 count: 358\n",
      "reward: 61.74046331841238 setps: 50 count: 408\n",
      "reward: 68.50703630191853 setps: 55 count: 463\n",
      "reward: 71.31366471314396 setps: 64 count: 527\n",
      "reward: 59.49362764902764 setps: 49 count: 576\n",
      "reward: 59.4589492889427 setps: 51 count: 627\n",
      "reward: 27.544551737065188 setps: 57 count: 684\n",
      "reward: 71.10719651296533 setps: 55 count: 739\n",
      "reward: 66.20974483593164 setps: 56 count: 795\n",
      "reward: 39.61437122837814 setps: 66 count: 861\n",
      "reward: 24.077985174456263 setps: 46 count: 907\n",
      "reward: 25.61497068137105 setps: 52 count: 959\n",
      "avg rewards: 53.05638621069785\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.30064\n",
      "Epoch:20 Batch:11 Loss:0.02680\n",
      "Epoch:40 Batch:11 Loss:0.02034\n",
      "Epoch:60 Batch:11 Loss:0.02180\n",
      "Epoch:80 Batch:11 Loss:0.01944\n",
      "Epoch:100 Batch:11 Loss:0.01556\n",
      "Epoch:120 Batch:11 Loss:0.01703\n",
      "Epoch:140 Batch:11 Loss:0.01531\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10 Batch:10 Loss:0.033\n",
      "Epoch:20 Batch:10 Loss:0.033\n",
      "Epoch:30 Batch:10 Loss:0.033\n",
      "Epoch:40 Batch:10 Loss:0.031\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 70.71811477985175 setps: 58 count: 58\n",
      "reward: 17.48901102485397 setps: 44 count: 102\n",
      "reward: 61.86240863847196 setps: 59 count: 161\n",
      "reward: 53.93576184142294 setps: 49 count: 210\n",
      "reward: 60.29715633051235 setps: 56 count: 266\n",
      "reward: 100.55493320669152 setps: 153 count: 419\n",
      "reward: 68.50807124205748 setps: 61 count: 480\n",
      "reward: 23.748354205118087 setps: 53 count: 533\n",
      "reward: 71.31764565909106 setps: 63 count: 596\n",
      "reward: 68.51633590917662 setps: 64 count: 660\n",
      "reward: 66.62301660871164 setps: 61 count: 721\n",
      "reward: 25.558737317218043 setps: 55 count: 776\n",
      "reward: 22.70248128341919 setps: 50 count: 826\n",
      "reward: 23.339303201339497 setps: 52 count: 878\n",
      "reward: 31.088281552102128 setps: 62 count: 940\n",
      "reward: 19.54404109739408 setps: 46 count: 986\n",
      "avg rewards: 49.112728368589515\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.21259\n",
      "Epoch:20 Batch:12 Loss:0.02612\n",
      "Epoch:40 Batch:12 Loss:0.02002\n",
      "Epoch:60 Batch:12 Loss:0.01790\n",
      "Epoch:80 Batch:12 Loss:0.01413\n",
      "Epoch:100 Batch:12 Loss:0.01680\n",
      "Epoch:120 Batch:12 Loss:0.01369\n",
      "Epoch:140 Batch:12 Loss:0.01401\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.034\n",
      "Epoch:10 Batch:10 Loss:0.034\n",
      "Epoch:20 Batch:10 Loss:0.033\n",
      "Epoch:30 Batch:10 Loss:0.032\n",
      "Epoch:40 Batch:10 Loss:0.033\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 19.404296289250482 setps: 42 count: 42\n",
      "reward: 33.75964925735752 setps: 62 count: 104\n",
      "reward: 48.06045506533846 setps: 44 count: 148\n",
      "reward: 110.58507983584639 setps: 122 count: 270\n",
      "reward: 53.37393681924296 setps: 52 count: 322\n",
      "reward: 23.35883348685892 setps: 50 count: 372\n",
      "reward: 62.08494217067052 setps: 64 count: 436\n",
      "reward: 21.268987683059827 setps: 44 count: 480\n",
      "reward: 67.66037900219669 setps: 66 count: 546\n",
      "reward: 42.45416814820928 setps: 42 count: 588\n",
      "reward: 70.1522646849975 setps: 75 count: 663\n",
      "reward: 36.19512239789763 setps: 67 count: 730\n",
      "reward: 41.716846915567295 setps: 42 count: 772\n",
      "reward: 35.101328562930576 setps: 45 count: 817\n",
      "reward: 60.67787405934212 setps: 60 count: 877\n",
      "reward: 47.421685094499836 setps: 46 count: 923\n",
      "reward: 15.28065767349763 setps: 40 count: 963\n",
      "avg rewards: 46.3856768909861\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.14502\n",
      "Epoch:20 Batch:13 Loss:0.02571\n",
      "Epoch:40 Batch:13 Loss:0.01966\n",
      "Epoch:60 Batch:13 Loss:0.01829\n",
      "Epoch:80 Batch:13 Loss:0.01744\n",
      "Epoch:100 Batch:13 Loss:0.01503\n",
      "Epoch:120 Batch:13 Loss:0.01203\n",
      "Epoch:140 Batch:13 Loss:0.01336\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.035\n",
      "Epoch:10 Batch:10 Loss:0.039\n",
      "Epoch:20 Batch:10 Loss:0.034\n",
      "Epoch:30 Batch:10 Loss:0.033\n",
      "Epoch:40 Batch:10 Loss:0.034\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 20.929880536375276 setps: 47 count: 47\n",
      "reward: 57.81260879850305 setps: 51 count: 98\n",
      "reward: 21.44112386733905 setps: 41 count: 139\n",
      "reward: 17.35756280747591 setps: 44 count: 183\n",
      "reward: 28.4643624023025 setps: 51 count: 234\n",
      "reward: 28.62309448128507 setps: 59 count: 293\n",
      "reward: 26.052259505058455 setps: 44 count: 337\n",
      "reward: 67.33282966488042 setps: 61 count: 398\n",
      "reward: 53.23471316835495 setps: 51 count: 449\n",
      "reward: 18.642286842127216 setps: 41 count: 490\n",
      "reward: 29.05321005377482 setps: 58 count: 548\n",
      "reward: 69.47579624096397 setps: 64 count: 612\n",
      "reward: 32.60379283649672 setps: 41 count: 653\n",
      "reward: 59.94705845952557 setps: 54 count: 707\n",
      "reward: 25.504180193274795 setps: 54 count: 761\n",
      "reward: 25.864910627603233 setps: 55 count: 816\n",
      "reward: 52.29979104534432 setps: 49 count: 865\n",
      "reward: 15.26633413279779 setps: 40 count: 905\n",
      "reward: 47.209388468317044 setps: 47 count: 952\n",
      "avg rewards: 36.69027284904211\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.15249\n",
      "Epoch:20 Batch:14 Loss:0.02310\n",
      "Epoch:40 Batch:14 Loss:0.01952\n",
      "Epoch:60 Batch:14 Loss:0.01741\n",
      "Epoch:80 Batch:14 Loss:0.01338\n",
      "Epoch:100 Batch:14 Loss:0.01384\n",
      "Epoch:120 Batch:14 Loss:0.01442\n",
      "Epoch:140 Batch:14 Loss:0.01184\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.031\n",
      "Epoch:10 Batch:10 Loss:0.030\n",
      "Epoch:20 Batch:10 Loss:0.029\n",
      "Epoch:30 Batch:10 Loss:0.031\n",
      "Epoch:40 Batch:10 Loss:0.029\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 18.090763484063793 setps: 36 count: 36\n",
      "reward: 18.81775610516924 setps: 42 count: 78\n",
      "reward: 35.160976144847524 setps: 31 count: 109\n",
      "reward: 38.717253710885416 setps: 25 count: 134\n",
      "reward: 19.768520589142284 setps: 48 count: 182\n",
      "reward: 18.606856788430008 setps: 37 count: 219\n",
      "reward: 42.11418120809541 setps: 29 count: 248\n",
      "reward: 32.8859018402596 setps: 55 count: 303\n",
      "reward: 13.771345898424626 setps: 36 count: 339\n",
      "reward: 44.93460230559577 setps: 38 count: 377\n",
      "reward: 18.65813298393769 setps: 42 count: 419\n",
      "reward: 19.706667076675508 setps: 43 count: 462\n",
      "reward: 24.84416202165448 setps: 52 count: 514\n",
      "reward: 50.16261344891099 setps: 40 count: 554\n",
      "reward: 23.68684549129248 setps: 36 count: 590\n",
      "reward: 36.04689099595707 setps: 20 count: 610\n",
      "reward: 28.027606548483888 setps: 15 count: 625\n",
      "reward: 17.361327474516287 setps: 39 count: 664\n",
      "reward: 20.894748043669097 setps: 46 count: 710\n",
      "reward: 43.98190563061943 setps: 34 count: 744\n",
      "reward: 18.417286137690827 setps: 40 count: 784\n",
      "reward: 47.8724984972825 setps: 35 count: 819\n",
      "reward: 50.391782954864915 setps: 41 count: 860\n",
      "reward: 27.723036540520837 setps: 45 count: 905\n",
      "reward: 18.2810600149518 setps: 39 count: 944\n",
      "reward: 20.498276332473324 setps: 40 count: 984\n",
      "avg rewards: 28.82396147186211\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.13423\n",
      "Epoch:20 Batch:15 Loss:0.02402\n",
      "Epoch:40 Batch:15 Loss:0.01839\n",
      "Epoch:60 Batch:15 Loss:0.01640\n",
      "Epoch:80 Batch:15 Loss:0.01413\n",
      "Epoch:100 Batch:15 Loss:0.01451\n",
      "Epoch:120 Batch:15 Loss:0.01383\n",
      "Epoch:140 Batch:15 Loss:0.01126\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.035\n",
      "Epoch:10 Batch:10 Loss:0.032\n",
      "Epoch:20 Batch:10 Loss:0.033\n",
      "Epoch:30 Batch:10 Loss:0.032\n",
      "Epoch:40 Batch:10 Loss:0.030\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 18.804307440796407 setps: 43 count: 43\n",
      "reward: 26.542099173537284 setps: 53 count: 96\n",
      "reward: 32.59988422306926 setps: 65 count: 161\n",
      "reward: 49.8125825056064 setps: 38 count: 199\n",
      "reward: 54.16677703853056 setps: 50 count: 249\n",
      "reward: 58.44426151666848 setps: 46 count: 295\n",
      "reward: 82.07698671970287 setps: 68 count: 363\n",
      "reward: 25.893290459188567 setps: 55 count: 418\n",
      "reward: 19.66729137157817 setps: 42 count: 460\n",
      "reward: 36.375307740198345 setps: 67 count: 527\n",
      "reward: 48.18919371652737 setps: 36 count: 563\n",
      "reward: 44.09593879679743 setps: 31 count: 594\n",
      "reward: 23.239936339062115 setps: 50 count: 644\n",
      "reward: 25.629421724445997 setps: 53 count: 697\n",
      "reward: 63.582982332259434 setps: 53 count: 750\n",
      "reward: 56.92808116329689 setps: 43 count: 793\n",
      "reward: 48.3541037433315 setps: 33 count: 826\n",
      "reward: 70.22182481098135 setps: 68 count: 894\n",
      "reward: 24.26873962657555 setps: 40 count: 934\n",
      "reward: 21.21669646659575 setps: 48 count: 982\n",
      "avg rewards: 41.50548534543749\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.11033\n",
      "Epoch:20 Batch:16 Loss:0.02086\n",
      "Epoch:40 Batch:16 Loss:0.01739\n",
      "Epoch:60 Batch:16 Loss:0.01440\n",
      "Epoch:80 Batch:16 Loss:0.01502\n",
      "Epoch:100 Batch:16 Loss:0.01376\n",
      "Epoch:120 Batch:16 Loss:0.01389\n",
      "Epoch:140 Batch:16 Loss:0.01324\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.031\n",
      "Epoch:10 Batch:10 Loss:0.030\n",
      "Epoch:20 Batch:10 Loss:0.028\n",
      "Epoch:30 Batch:10 Loss:0.029\n",
      "Epoch:40 Batch:10 Loss:0.027\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 24.26389028153063 setps: 56 count: 56\n",
      "reward: 16.67251710367272 setps: 37 count: 93\n",
      "reward: 18.530649021203864 setps: 45 count: 138\n",
      "reward: 36.06397645052202 setps: 67 count: 205\n",
      "reward: 15.830217805813305 setps: 41 count: 246\n",
      "reward: 24.39815344879608 setps: 40 count: 286\n",
      "reward: 22.232182342177836 setps: 49 count: 335\n",
      "reward: 15.695246975103512 setps: 41 count: 376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 21.631399188200888 setps: 41 count: 417\n",
      "reward: 20.944711624247425 setps: 48 count: 465\n",
      "reward: 34.950525610281325 setps: 40 count: 505\n",
      "reward: 16.933982878332603 setps: 42 count: 547\n",
      "reward: 15.176127961552996 setps: 37 count: 584\n",
      "reward: 20.59404736984579 setps: 48 count: 632\n",
      "reward: 17.03785523113765 setps: 43 count: 675\n",
      "reward: 18.83641629507183 setps: 41 count: 716\n",
      "reward: 20.329636978608328 setps: 49 count: 765\n",
      "reward: 17.462484079944268 setps: 38 count: 803\n",
      "reward: 25.464636356347185 setps: 52 count: 855\n",
      "reward: 13.97527543807082 setps: 36 count: 891\n",
      "reward: 14.64473704643897 setps: 39 count: 930\n",
      "reward: 34.276743967966475 setps: 68 count: 998\n",
      "avg rewards: 21.179336975221204\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.12561\n",
      "Epoch:20 Batch:17 Loss:0.01846\n",
      "Epoch:40 Batch:17 Loss:0.01671\n",
      "Epoch:60 Batch:17 Loss:0.01442\n",
      "Epoch:80 Batch:17 Loss:0.01249\n",
      "Epoch:100 Batch:17 Loss:0.01229\n",
      "Epoch:120 Batch:17 Loss:0.01279\n",
      "Epoch:140 Batch:17 Loss:0.01271\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.032\n",
      "Epoch:10 Batch:10 Loss:0.028\n",
      "Epoch:20 Batch:10 Loss:0.030\n",
      "Epoch:30 Batch:10 Loss:0.028\n",
      "Epoch:40 Batch:10 Loss:0.027\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 14.82321580696589 setps: 38 count: 38\n",
      "reward: 31.817908054044537 setps: 63 count: 101\n",
      "reward: 20.043573323125013 setps: 48 count: 149\n",
      "reward: 20.34202644279431 setps: 46 count: 195\n",
      "reward: 38.124600708698566 setps: 73 count: 268\n",
      "reward: 17.88471112372353 setps: 48 count: 316\n",
      "reward: 20.539833072182955 setps: 44 count: 360\n",
      "reward: 16.051031258948203 setps: 42 count: 402\n",
      "reward: 17.500427219655833 setps: 45 count: 447\n",
      "reward: 16.088522677829307 setps: 43 count: 490\n",
      "reward: 16.151106152999272 setps: 42 count: 532\n",
      "reward: 14.285372055121112 setps: 42 count: 574\n",
      "reward: 15.606435127559244 setps: 44 count: 618\n",
      "reward: 14.972368733111939 setps: 36 count: 654\n",
      "reward: 14.69022068616177 setps: 39 count: 693\n",
      "reward: 14.064135287811224 setps: 37 count: 730\n",
      "reward: 18.970577469884297 setps: 48 count: 778\n",
      "reward: 20.47241412966833 setps: 48 count: 826\n",
      "reward: 27.656944034277696 setps: 44 count: 870\n",
      "reward: 31.81999882208765 setps: 42 count: 912\n",
      "reward: 15.624709817314578 setps: 40 count: 952\n",
      "reward: 20.433531317190496 setps: 48 count: 1000\n",
      "avg rewards: 19.907439241870716\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.11263\n",
      "Epoch:20 Batch:18 Loss:0.01982\n",
      "Epoch:40 Batch:18 Loss:0.01821\n",
      "Epoch:60 Batch:18 Loss:0.01529\n",
      "Epoch:80 Batch:18 Loss:0.01467\n",
      "Epoch:100 Batch:18 Loss:0.01240\n",
      "Epoch:120 Batch:18 Loss:0.01112\n",
      "Epoch:140 Batch:18 Loss:0.01116\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.035\n",
      "Epoch:10 Batch:10 Loss:0.032\n",
      "Epoch:20 Batch:10 Loss:0.031\n",
      "Epoch:30 Batch:10 Loss:0.033\n",
      "Epoch:40 Batch:10 Loss:0.030\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 59.77507206961163 setps: 56 count: 56\n",
      "reward: 99.74333561570673 setps: 100 count: 156\n",
      "reward: 35.828256325336405 setps: 47 count: 203\n",
      "reward: 31.65917920758656 setps: 60 count: 263\n",
      "reward: 54.43326824404792 setps: 52 count: 315\n",
      "reward: 52.05428202297625 setps: 56 count: 371\n",
      "reward: 64.8328167242318 setps: 61 count: 432\n",
      "reward: 54.879653600082385 setps: 93 count: 525\n",
      "reward: 60.06952144285022 setps: 55 count: 580\n",
      "reward: 55.35181407825146 setps: 55 count: 635\n",
      "reward: 61.953916549097634 setps: 58 count: 693\n",
      "reward: 24.557575390754202 setps: 50 count: 743\n",
      "reward: 28.932906406745307 setps: 53 count: 796\n",
      "reward: 51.9430806559787 setps: 52 count: 848\n",
      "reward: 64.36518533814233 setps: 65 count: 913\n",
      "reward: 82.0856010916992 setps: 82 count: 995\n",
      "avg rewards: 55.15409154769367\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.10436\n",
      "Epoch:20 Batch:19 Loss:0.01900\n",
      "Epoch:40 Batch:19 Loss:0.01485\n",
      "Epoch:60 Batch:19 Loss:0.01191\n",
      "Epoch:80 Batch:19 Loss:0.01184\n",
      "Epoch:100 Batch:19 Loss:0.01122\n",
      "Epoch:120 Batch:19 Loss:0.01218\n",
      "Epoch:140 Batch:19 Loss:0.01203\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.030\n",
      "Epoch:10 Batch:10 Loss:0.030\n",
      "Epoch:20 Batch:10 Loss:0.029\n",
      "Epoch:30 Batch:10 Loss:0.029\n",
      "Epoch:40 Batch:10 Loss:0.033\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 56.42796914578358 setps: 41 count: 41\n",
      "reward: 34.883258360295436 setps: 62 count: 103\n",
      "reward: 29.22353171943104 setps: 58 count: 161\n",
      "reward: 76.12931166272611 setps: 61 count: 222\n",
      "reward: 35.4153653810863 setps: 60 count: 282\n",
      "reward: 90.85776611900508 setps: 80 count: 362\n",
      "reward: 57.423937672286435 setps: 44 count: 406\n",
      "reward: 67.44014795253895 setps: 59 count: 465\n",
      "reward: 51.53576921737403 setps: 38 count: 503\n",
      "reward: 58.32959036989922 setps: 49 count: 552\n",
      "reward: 22.59317447220965 setps: 51 count: 603\n",
      "reward: 21.161796220589885 setps: 46 count: 649\n",
      "reward: 48.84262913832646 setps: 41 count: 690\n",
      "reward: 18.386217532104638 setps: 42 count: 732\n",
      "reward: 71.87975345538433 setps: 69 count: 801\n",
      "reward: 39.37130865609652 setps: 35 count: 836\n",
      "reward: 30.033128770029002 setps: 60 count: 896\n",
      "reward: 72.3826758019888 setps: 59 count: 955\n",
      "avg rewards: 49.017629535953084\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.09553\n",
      "Epoch:20 Batch:20 Loss:0.01837\n",
      "Epoch:40 Batch:20 Loss:0.01678\n",
      "Epoch:60 Batch:20 Loss:0.01201\n",
      "Epoch:80 Batch:20 Loss:0.01089\n",
      "Epoch:100 Batch:20 Loss:0.01251\n",
      "Epoch:120 Batch:20 Loss:0.01106\n",
      "Epoch:140 Batch:20 Loss:0.00995\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.030\n",
      "Epoch:10 Batch:10 Loss:0.027\n",
      "Epoch:20 Batch:10 Loss:0.029\n",
      "Epoch:30 Batch:10 Loss:0.028\n",
      "Epoch:40 Batch:10 Loss:0.027\n",
      "Done!\n",
      "############# start HalfCheetahBulletEnv-v0 training ###################\n",
      "(50, 1000, 26)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -980.7297681761808 setps: 800 count: 800\n",
      "avg rewards: -980.7297681761808\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:1.40480\n",
      "Epoch:20 Batch:1 Loss:0.23700\n",
      "Epoch:40 Batch:1 Loss:0.17743\n",
      "Epoch:60 Batch:1 Loss:0.14371\n",
      "Epoch:80 Batch:1 Loss:0.12292\n",
      "Epoch:100 Batch:1 Loss:0.10862\n",
      "Epoch:120 Batch:1 Loss:0.09891\n",
      "Epoch:140 Batch:1 Loss:0.09493\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.156\n",
      "Epoch:10 Batch:10 Loss:0.158\n",
      "Epoch:20 Batch:10 Loss:0.165\n",
      "Epoch:30 Batch:10 Loss:0.158\n",
      "Epoch:40 Batch:10 Loss:0.160\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1167.568048985574 setps: 800 count: 800\n",
      "avg rewards: -1167.568048985574\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:1.25585\n",
      "Epoch:20 Batch:2 Loss:0.14654\n",
      "Epoch:40 Batch:2 Loss:0.09491\n",
      "Epoch:60 Batch:2 Loss:0.08111\n",
      "Epoch:80 Batch:2 Loss:0.07211\n",
      "Epoch:100 Batch:2 Loss:0.05866\n",
      "Epoch:120 Batch:2 Loss:0.05542\n",
      "Epoch:140 Batch:2 Loss:0.05272\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.137\n",
      "Epoch:10 Batch:10 Loss:0.138\n",
      "Epoch:20 Batch:10 Loss:0.133\n",
      "Epoch:30 Batch:10 Loss:0.134\n",
      "Epoch:40 Batch:10 Loss:0.128\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1039.1958057165966 setps: 800 count: 800\n",
      "avg rewards: -1039.1958057165966\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:1.25120\n",
      "Epoch:20 Batch:3 Loss:0.09511\n",
      "Epoch:40 Batch:3 Loss:0.07179\n",
      "Epoch:60 Batch:3 Loss:0.05603\n",
      "Epoch:80 Batch:3 Loss:0.05046\n",
      "Epoch:100 Batch:3 Loss:0.04500\n",
      "Epoch:120 Batch:3 Loss:0.04457\n",
      "Epoch:140 Batch:3 Loss:0.04144\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.115\n",
      "Epoch:10 Batch:10 Loss:0.115\n",
      "Epoch:20 Batch:10 Loss:0.110\n",
      "Epoch:30 Batch:10 Loss:0.106\n",
      "Epoch:40 Batch:10 Loss:0.107\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1154.0929641557116 setps: 800 count: 800\n",
      "avg rewards: -1154.0929641557116\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:1.05764\n",
      "Epoch:20 Batch:4 Loss:0.08347\n",
      "Epoch:40 Batch:4 Loss:0.05628\n",
      "Epoch:60 Batch:4 Loss:0.04784\n",
      "Epoch:80 Batch:4 Loss:0.03992\n",
      "Epoch:100 Batch:4 Loss:0.03824\n",
      "Epoch:120 Batch:4 Loss:0.03476\n",
      "Epoch:140 Batch:4 Loss:0.03181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.115\n",
      "Epoch:10 Batch:10 Loss:0.112\n",
      "Epoch:20 Batch:10 Loss:0.110\n",
      "Epoch:30 Batch:10 Loss:0.111\n",
      "Epoch:40 Batch:10 Loss:0.111\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -979.6155296045852 setps: 800 count: 800\n",
      "avg rewards: -979.6155296045852\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.85037\n",
      "Epoch:20 Batch:5 Loss:0.06641\n",
      "Epoch:40 Batch:5 Loss:0.04617\n",
      "Epoch:60 Batch:5 Loss:0.04097\n",
      "Epoch:80 Batch:5 Loss:0.03273\n",
      "Epoch:100 Batch:5 Loss:0.03292\n",
      "Epoch:120 Batch:5 Loss:0.02692\n",
      "Epoch:140 Batch:5 Loss:0.02707\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.107\n",
      "Epoch:10 Batch:10 Loss:0.106\n",
      "Epoch:20 Batch:10 Loss:0.102\n",
      "Epoch:30 Batch:10 Loss:0.105\n",
      "Epoch:40 Batch:10 Loss:0.106\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1031.3478923910152 setps: 800 count: 800\n",
      "avg rewards: -1031.3478923910152\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.82449\n",
      "Epoch:20 Batch:6 Loss:0.05592\n",
      "Epoch:40 Batch:6 Loss:0.04102\n",
      "Epoch:60 Batch:6 Loss:0.03366\n",
      "Epoch:80 Batch:6 Loss:0.03186\n",
      "Epoch:100 Batch:6 Loss:0.02844\n",
      "Epoch:120 Batch:6 Loss:0.02691\n",
      "Epoch:140 Batch:6 Loss:0.02596\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.113\n",
      "Epoch:10 Batch:10 Loss:0.113\n",
      "Epoch:20 Batch:10 Loss:0.103\n",
      "Epoch:30 Batch:10 Loss:0.105\n",
      "Epoch:40 Batch:10 Loss:0.104\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1237.4255743825036 setps: 800 count: 800\n",
      "avg rewards: -1237.4255743825036\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.81124\n",
      "Epoch:20 Batch:7 Loss:0.05057\n",
      "Epoch:40 Batch:7 Loss:0.03552\n",
      "Epoch:60 Batch:7 Loss:0.02899\n",
      "Epoch:80 Batch:7 Loss:0.02674\n",
      "Epoch:100 Batch:7 Loss:0.02377\n",
      "Epoch:120 Batch:7 Loss:0.02597\n",
      "Epoch:140 Batch:7 Loss:0.02395\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.096\n",
      "Epoch:10 Batch:10 Loss:0.090\n",
      "Epoch:20 Batch:10 Loss:0.095\n",
      "Epoch:30 Batch:10 Loss:0.093\n",
      "Epoch:40 Batch:10 Loss:0.090\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -987.7398589759348 setps: 800 count: 800\n",
      "avg rewards: -987.7398589759348\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.71870\n",
      "Epoch:20 Batch:8 Loss:0.04732\n",
      "Epoch:40 Batch:8 Loss:0.03408\n",
      "Epoch:60 Batch:8 Loss:0.02828\n",
      "Epoch:80 Batch:8 Loss:0.02651\n",
      "Epoch:100 Batch:8 Loss:0.02419\n",
      "Epoch:120 Batch:8 Loss:0.02224\n",
      "Epoch:140 Batch:8 Loss:0.02326\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.095\n",
      "Epoch:10 Batch:10 Loss:0.094\n",
      "Epoch:20 Batch:10 Loss:0.094\n",
      "Epoch:30 Batch:10 Loss:0.095\n",
      "Epoch:40 Batch:10 Loss:0.091\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1009.0939642088498 setps: 800 count: 800\n",
      "avg rewards: -1009.0939642088498\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.56245\n",
      "Epoch:20 Batch:9 Loss:0.04686\n",
      "Epoch:40 Batch:9 Loss:0.03486\n",
      "Epoch:60 Batch:9 Loss:0.02930\n",
      "Epoch:80 Batch:9 Loss:0.02454\n",
      "Epoch:100 Batch:9 Loss:0.02467\n",
      "Epoch:120 Batch:9 Loss:0.02340\n",
      "Epoch:140 Batch:9 Loss:0.02402\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.105\n",
      "Epoch:10 Batch:10 Loss:0.100\n",
      "Epoch:20 Batch:10 Loss:0.095\n",
      "Epoch:30 Batch:10 Loss:0.094\n",
      "Epoch:40 Batch:10 Loss:0.094\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1013.7015319725022 setps: 800 count: 800\n",
      "avg rewards: -1013.7015319725022\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.37004\n",
      "Epoch:20 Batch:10 Loss:0.04165\n",
      "Epoch:40 Batch:10 Loss:0.03202\n",
      "Epoch:60 Batch:10 Loss:0.02781\n",
      "Epoch:80 Batch:10 Loss:0.02525\n",
      "Epoch:100 Batch:10 Loss:0.02321\n",
      "Epoch:120 Batch:10 Loss:0.02122\n",
      "Epoch:140 Batch:10 Loss:0.01960\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.091\n",
      "Epoch:10 Batch:10 Loss:0.091\n",
      "Epoch:20 Batch:10 Loss:0.090\n",
      "Epoch:30 Batch:10 Loss:0.087\n",
      "Epoch:40 Batch:10 Loss:0.085\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1151.316871897553 setps: 800 count: 800\n",
      "avg rewards: -1151.316871897553\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.35475\n",
      "Epoch:20 Batch:11 Loss:0.04346\n",
      "Epoch:40 Batch:11 Loss:0.03437\n",
      "Epoch:60 Batch:11 Loss:0.03068\n",
      "Epoch:80 Batch:11 Loss:0.02583\n",
      "Epoch:100 Batch:11 Loss:0.02768\n",
      "Epoch:120 Batch:11 Loss:0.02487\n",
      "Epoch:140 Batch:11 Loss:0.02385\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.098\n",
      "Epoch:10 Batch:10 Loss:0.091\n",
      "Epoch:20 Batch:10 Loss:0.088\n",
      "Epoch:30 Batch:10 Loss:0.086\n",
      "Epoch:40 Batch:10 Loss:0.085\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -866.7689345043449 setps: 800 count: 800\n",
      "avg rewards: -866.7689345043449\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.40188\n",
      "Epoch:20 Batch:12 Loss:0.04892\n",
      "Epoch:40 Batch:12 Loss:0.03052\n",
      "Epoch:60 Batch:12 Loss:0.02811\n",
      "Epoch:80 Batch:12 Loss:0.02489\n",
      "Epoch:100 Batch:12 Loss:0.02348\n",
      "Epoch:120 Batch:12 Loss:0.02519\n",
      "Epoch:140 Batch:12 Loss:0.02330\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.078\n",
      "Epoch:10 Batch:10 Loss:0.070\n",
      "Epoch:20 Batch:10 Loss:0.070\n",
      "Epoch:30 Batch:10 Loss:0.070\n",
      "Epoch:40 Batch:10 Loss:0.068\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1150.7157517773314 setps: 800 count: 800\n",
      "avg rewards: -1150.7157517773314\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.31562\n",
      "Epoch:20 Batch:13 Loss:0.04145\n",
      "Epoch:40 Batch:13 Loss:0.03155\n",
      "Epoch:60 Batch:13 Loss:0.03066\n",
      "Epoch:80 Batch:13 Loss:0.02895\n",
      "Epoch:100 Batch:13 Loss:0.02660\n",
      "Epoch:120 Batch:13 Loss:0.02558\n",
      "Epoch:140 Batch:13 Loss:0.02297\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.076\n",
      "Epoch:10 Batch:10 Loss:0.073\n",
      "Epoch:20 Batch:10 Loss:0.071\n",
      "Epoch:30 Batch:10 Loss:0.072\n",
      "Epoch:40 Batch:10 Loss:0.071\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 218.91956170033208 setps: 800 count: 800\n",
      "avg rewards: 218.91956170033208\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.28480\n",
      "Epoch:20 Batch:14 Loss:0.03944\n",
      "Epoch:40 Batch:14 Loss:0.03112\n",
      "Epoch:60 Batch:14 Loss:0.02969\n",
      "Epoch:80 Batch:14 Loss:0.02665\n",
      "Epoch:100 Batch:14 Loss:0.02685\n",
      "Epoch:120 Batch:14 Loss:0.02349\n",
      "Epoch:140 Batch:14 Loss:0.02365\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.067\n",
      "Epoch:10 Batch:10 Loss:0.066\n",
      "Epoch:20 Batch:10 Loss:0.067\n",
      "Epoch:30 Batch:10 Loss:0.065\n",
      "Epoch:40 Batch:10 Loss:0.064\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -973.1775763227994 setps: 800 count: 800\n",
      "avg rewards: -973.1775763227994\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.25155\n",
      "Epoch:20 Batch:15 Loss:0.03724\n",
      "Epoch:40 Batch:15 Loss:0.03178\n",
      "Epoch:60 Batch:15 Loss:0.02630\n",
      "Epoch:80 Batch:15 Loss:0.02574\n",
      "Epoch:100 Batch:15 Loss:0.02536\n",
      "Epoch:120 Batch:15 Loss:0.02197\n",
      "Epoch:140 Batch:15 Loss:0.02335\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.065\n",
      "Epoch:10 Batch:10 Loss:0.067\n",
      "Epoch:20 Batch:10 Loss:0.068\n",
      "Epoch:30 Batch:10 Loss:0.067\n",
      "Epoch:40 Batch:10 Loss:0.065\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1214.7489370708843 setps: 800 count: 800\n",
      "avg rewards: -1214.7489370708843\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.32433\n",
      "Epoch:20 Batch:16 Loss:0.04104\n",
      "Epoch:40 Batch:16 Loss:0.02767\n",
      "Epoch:60 Batch:16 Loss:0.02777\n",
      "Epoch:80 Batch:16 Loss:0.02712\n",
      "Epoch:100 Batch:16 Loss:0.02390\n",
      "Epoch:120 Batch:16 Loss:0.02423\n",
      "Epoch:140 Batch:16 Loss:0.02161\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.064\n",
      "Epoch:10 Batch:10 Loss:0.065\n",
      "Epoch:20 Batch:10 Loss:0.065\n",
      "Epoch:30 Batch:10 Loss:0.062\n",
      "Epoch:40 Batch:10 Loss:0.066\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -940.1863698820606 setps: 800 count: 800\n",
      "avg rewards: -940.1863698820606\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.23672\n",
      "Epoch:20 Batch:17 Loss:0.03996\n",
      "Epoch:40 Batch:17 Loss:0.02881\n",
      "Epoch:60 Batch:17 Loss:0.02776\n",
      "Epoch:80 Batch:17 Loss:0.02674\n",
      "Epoch:100 Batch:17 Loss:0.02492\n",
      "Epoch:120 Batch:17 Loss:0.02379\n",
      "Epoch:140 Batch:17 Loss:0.02209\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.062\n",
      "Epoch:10 Batch:10 Loss:0.061\n",
      "Epoch:20 Batch:10 Loss:0.060\n",
      "Epoch:30 Batch:10 Loss:0.061\n",
      "Epoch:40 Batch:10 Loss:0.062\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1043.4418963541737 setps: 800 count: 800\n",
      "avg rewards: -1043.4418963541737\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.21236\n",
      "Epoch:20 Batch:18 Loss:0.03588\n",
      "Epoch:40 Batch:18 Loss:0.02928\n",
      "Epoch:60 Batch:18 Loss:0.02594\n",
      "Epoch:80 Batch:18 Loss:0.02211\n",
      "Epoch:100 Batch:18 Loss:0.02529\n",
      "Epoch:120 Batch:18 Loss:0.02172\n",
      "Epoch:140 Batch:18 Loss:0.02177\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.062\n",
      "Epoch:10 Batch:10 Loss:0.062\n",
      "Epoch:20 Batch:10 Loss:0.059\n",
      "Epoch:30 Batch:10 Loss:0.059\n",
      "Epoch:40 Batch:10 Loss:0.060\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1091.4273733003497 setps: 800 count: 800\n",
      "avg rewards: -1091.4273733003497\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.19783\n",
      "Epoch:20 Batch:19 Loss:0.03932\n",
      "Epoch:40 Batch:19 Loss:0.02942\n",
      "Epoch:60 Batch:19 Loss:0.02436\n",
      "Epoch:80 Batch:19 Loss:0.02369\n",
      "Epoch:100 Batch:19 Loss:0.02440\n",
      "Epoch:120 Batch:19 Loss:0.02165\n",
      "Epoch:140 Batch:19 Loss:0.02109\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.057\n",
      "Epoch:10 Batch:10 Loss:0.057\n",
      "Epoch:20 Batch:10 Loss:0.056\n",
      "Epoch:30 Batch:10 Loss:0.054\n",
      "Epoch:40 Batch:10 Loss:0.057\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1072.9201812803326 setps: 800 count: 800\n",
      "avg rewards: -1072.9201812803326\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.19863\n",
      "Epoch:20 Batch:20 Loss:0.03728\n",
      "Epoch:40 Batch:20 Loss:0.02472\n",
      "Epoch:60 Batch:20 Loss:0.02505\n",
      "Epoch:80 Batch:20 Loss:0.02311\n",
      "Epoch:100 Batch:20 Loss:0.01891\n",
      "Epoch:120 Batch:20 Loss:0.02085\n",
      "Epoch:140 Batch:20 Loss:0.02277\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.053\n",
      "Epoch:10 Batch:10 Loss:0.054\n",
      "Epoch:20 Batch:10 Loss:0.054\n",
      "Epoch:30 Batch:10 Loss:0.053\n",
      "Epoch:40 Batch:10 Loss:0.053\n",
      "Done!\n",
      "############# start AntBulletEnv-v0 training ###################\n",
      "(50, 1000, 28)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 398.48683142906646 setps: 800 count: 800\n",
      "avg rewards: 398.48683142906646\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:1.46101\n",
      "Epoch:20 Batch:1 Loss:0.07814\n",
      "Epoch:40 Batch:1 Loss:0.05916\n",
      "Epoch:60 Batch:1 Loss:0.04574\n",
      "Epoch:80 Batch:1 Loss:0.03355\n",
      "Epoch:100 Batch:1 Loss:0.02783\n",
      "Epoch:120 Batch:1 Loss:0.02296\n",
      "Epoch:140 Batch:1 Loss:0.02009\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.345\n",
      "Epoch:10 Batch:10 Loss:0.346\n",
      "Epoch:20 Batch:10 Loss:0.335\n",
      "Epoch:30 Batch:10 Loss:0.337\n",
      "Epoch:40 Batch:10 Loss:0.329\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 364.1598635516636 setps: 800 count: 800\n",
      "avg rewards: 364.1598635516636\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:1.21983\n",
      "Epoch:20 Batch:2 Loss:0.04788\n",
      "Epoch:40 Batch:2 Loss:0.03635\n",
      "Epoch:60 Batch:2 Loss:0.02564\n",
      "Epoch:80 Batch:2 Loss:0.01910\n",
      "Epoch:100 Batch:2 Loss:0.01597\n",
      "Epoch:120 Batch:2 Loss:0.01366\n",
      "Epoch:140 Batch:2 Loss:0.01237\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.230\n",
      "Epoch:10 Batch:10 Loss:0.227\n",
      "Epoch:20 Batch:10 Loss:0.224\n",
      "Epoch:30 Batch:10 Loss:0.221\n",
      "Epoch:40 Batch:10 Loss:0.221\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 345.35885607788845 setps: 800 count: 800\n",
      "avg rewards: 345.35885607788845\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:1.26775\n",
      "Epoch:20 Batch:3 Loss:0.03627\n",
      "Epoch:40 Batch:3 Loss:0.02513\n",
      "Epoch:60 Batch:3 Loss:0.01822\n",
      "Epoch:80 Batch:3 Loss:0.01469\n",
      "Epoch:100 Batch:3 Loss:0.01280\n",
      "Epoch:120 Batch:3 Loss:0.01174\n",
      "Epoch:140 Batch:3 Loss:0.01079\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.218\n",
      "Epoch:10 Batch:10 Loss:0.214\n",
      "Epoch:20 Batch:10 Loss:0.215\n",
      "Epoch:30 Batch:10 Loss:0.216\n",
      "Epoch:40 Batch:10 Loss:0.213\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 337.3551099622507 setps: 800 count: 800\n",
      "avg rewards: 337.3551099622507\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.95517\n",
      "Epoch:20 Batch:4 Loss:0.03230\n",
      "Epoch:40 Batch:4 Loss:0.01898\n",
      "Epoch:60 Batch:4 Loss:0.01536\n",
      "Epoch:80 Batch:4 Loss:0.01178\n",
      "Epoch:100 Batch:4 Loss:0.01143\n",
      "Epoch:120 Batch:4 Loss:0.01020\n",
      "Epoch:140 Batch:4 Loss:0.01041\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.213\n",
      "Epoch:10 Batch:10 Loss:0.211\n",
      "Epoch:20 Batch:10 Loss:0.211\n",
      "Epoch:30 Batch:10 Loss:0.212\n",
      "Epoch:40 Batch:10 Loss:0.210\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 445.2533409944282 setps: 800 count: 800\n",
      "avg rewards: 445.2533409944282\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.75083\n",
      "Epoch:20 Batch:5 Loss:0.02963\n",
      "Epoch:40 Batch:5 Loss:0.01732\n",
      "Epoch:60 Batch:5 Loss:0.01432\n",
      "Epoch:80 Batch:5 Loss:0.01143\n",
      "Epoch:100 Batch:5 Loss:0.01046\n",
      "Epoch:120 Batch:5 Loss:0.01030\n",
      "Epoch:140 Batch:5 Loss:0.00919\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.217\n",
      "Epoch:10 Batch:10 Loss:0.213\n",
      "Epoch:20 Batch:10 Loss:0.211\n",
      "Epoch:30 Batch:10 Loss:0.208\n",
      "Epoch:40 Batch:10 Loss:0.212\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 533.4798612324978 setps: 800 count: 800\n",
      "avg rewards: 533.4798612324978\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.63415\n",
      "Epoch:20 Batch:6 Loss:0.02762\n",
      "Epoch:40 Batch:6 Loss:0.01540\n",
      "Epoch:60 Batch:6 Loss:0.01236\n",
      "Epoch:80 Batch:6 Loss:0.01097\n",
      "Epoch:100 Batch:6 Loss:0.00901\n",
      "Epoch:120 Batch:6 Loss:0.00881\n",
      "Epoch:140 Batch:6 Loss:0.00778\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.201\n",
      "Epoch:10 Batch:10 Loss:0.197\n",
      "Epoch:20 Batch:10 Loss:0.197\n",
      "Epoch:30 Batch:10 Loss:0.197\n",
      "Epoch:40 Batch:10 Loss:0.193\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 440.6467211014459 setps: 800 count: 800\n",
      "avg rewards: 440.6467211014459\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.81506\n",
      "Epoch:20 Batch:7 Loss:0.02516\n",
      "Epoch:40 Batch:7 Loss:0.01686\n",
      "Epoch:60 Batch:7 Loss:0.01198\n",
      "Epoch:80 Batch:7 Loss:0.01163\n",
      "Epoch:100 Batch:7 Loss:0.01045\n",
      "Epoch:120 Batch:7 Loss:0.00997\n",
      "Epoch:140 Batch:7 Loss:0.00874\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.176\n",
      "Epoch:10 Batch:10 Loss:0.172\n",
      "Epoch:20 Batch:10 Loss:0.172\n",
      "Epoch:30 Batch:10 Loss:0.170\n",
      "Epoch:40 Batch:10 Loss:0.172\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 511.21782773756496 setps: 800 count: 800\n",
      "avg rewards: 511.21782773756496\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.65082\n",
      "Epoch:20 Batch:8 Loss:0.02220\n",
      "Epoch:40 Batch:8 Loss:0.01499\n",
      "Epoch:60 Batch:8 Loss:0.01125\n",
      "Epoch:80 Batch:8 Loss:0.00982\n",
      "Epoch:100 Batch:8 Loss:0.00947\n",
      "Epoch:120 Batch:8 Loss:0.00928\n",
      "Epoch:140 Batch:8 Loss:0.00836\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.172\n",
      "Epoch:10 Batch:10 Loss:0.174\n",
      "Epoch:20 Batch:10 Loss:0.172\n",
      "Epoch:30 Batch:10 Loss:0.170\n",
      "Epoch:40 Batch:10 Loss:0.168\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 254.08994275721074 setps: 800 count: 800\n",
      "avg rewards: 254.08994275721074\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.54945\n",
      "Epoch:20 Batch:9 Loss:0.02112\n",
      "Epoch:40 Batch:9 Loss:0.01389\n",
      "Epoch:60 Batch:9 Loss:0.01189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:80 Batch:9 Loss:0.01042\n",
      "Epoch:100 Batch:9 Loss:0.00946\n",
      "Epoch:120 Batch:9 Loss:0.00856\n",
      "Epoch:140 Batch:9 Loss:0.00832\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.185\n",
      "Epoch:10 Batch:10 Loss:0.176\n",
      "Epoch:20 Batch:10 Loss:0.169\n",
      "Epoch:30 Batch:10 Loss:0.170\n",
      "Epoch:40 Batch:10 Loss:0.173\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 455.4490079972476 setps: 800 count: 800\n",
      "avg rewards: 455.4490079972476\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.35405\n",
      "Epoch:20 Batch:10 Loss:0.01937\n",
      "Epoch:40 Batch:10 Loss:0.01162\n",
      "Epoch:60 Batch:10 Loss:0.00968\n",
      "Epoch:80 Batch:10 Loss:0.00906\n",
      "Epoch:100 Batch:10 Loss:0.00924\n",
      "Epoch:120 Batch:10 Loss:0.00760\n",
      "Epoch:140 Batch:10 Loss:0.00703\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.175\n",
      "Epoch:10 Batch:10 Loss:0.173\n",
      "Epoch:20 Batch:10 Loss:0.169\n",
      "Epoch:30 Batch:10 Loss:0.172\n",
      "Epoch:40 Batch:10 Loss:0.170\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 244.34778311926377 setps: 800 count: 800\n",
      "avg rewards: 244.34778311926377\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.40635\n",
      "Epoch:20 Batch:11 Loss:0.01763\n",
      "Epoch:40 Batch:11 Loss:0.01133\n",
      "Epoch:60 Batch:11 Loss:0.01056\n",
      "Epoch:80 Batch:11 Loss:0.00998\n",
      "Epoch:100 Batch:11 Loss:0.00864\n",
      "Epoch:120 Batch:11 Loss:0.00893\n",
      "Epoch:140 Batch:11 Loss:0.00831\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.157\n",
      "Epoch:10 Batch:10 Loss:0.152\n",
      "Epoch:20 Batch:10 Loss:0.153\n",
      "Epoch:30 Batch:10 Loss:0.149\n",
      "Epoch:40 Batch:10 Loss:0.149\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 374.81331003843013 setps: 800 count: 800\n",
      "avg rewards: 374.81331003843013\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.51171\n",
      "Epoch:20 Batch:12 Loss:0.01793\n",
      "Epoch:40 Batch:12 Loss:0.01158\n",
      "Epoch:60 Batch:12 Loss:0.01082\n",
      "Epoch:80 Batch:12 Loss:0.00955\n",
      "Epoch:100 Batch:12 Loss:0.00948\n",
      "Epoch:120 Batch:12 Loss:0.00861\n",
      "Epoch:140 Batch:12 Loss:0.00842\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.156\n",
      "Epoch:10 Batch:10 Loss:0.151\n",
      "Epoch:20 Batch:10 Loss:0.150\n",
      "Epoch:30 Batch:10 Loss:0.148\n",
      "Epoch:40 Batch:10 Loss:0.148\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -163.9996638854557 setps: 800 count: 800\n",
      "avg rewards: -163.9996638854557\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.30648\n",
      "Epoch:20 Batch:13 Loss:0.01759\n",
      "Epoch:40 Batch:13 Loss:0.01261\n",
      "Epoch:60 Batch:13 Loss:0.00996\n",
      "Epoch:80 Batch:13 Loss:0.01008\n",
      "Epoch:100 Batch:13 Loss:0.00934\n",
      "Epoch:120 Batch:13 Loss:0.00863\n",
      "Epoch:140 Batch:13 Loss:0.00775\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.141\n",
      "Epoch:10 Batch:10 Loss:0.136\n",
      "Epoch:20 Batch:10 Loss:0.133\n",
      "Epoch:30 Batch:10 Loss:0.133\n",
      "Epoch:40 Batch:10 Loss:0.128\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 350.82522846132383 setps: 800 count: 800\n",
      "avg rewards: 350.82522846132383\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.24812\n",
      "Epoch:20 Batch:14 Loss:0.01615\n",
      "Epoch:40 Batch:14 Loss:0.01213\n",
      "Epoch:60 Batch:14 Loss:0.01074\n",
      "Epoch:80 Batch:14 Loss:0.00924\n",
      "Epoch:100 Batch:14 Loss:0.00896\n",
      "Epoch:120 Batch:14 Loss:0.00846\n",
      "Epoch:140 Batch:14 Loss:0.00757\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.158\n",
      "Epoch:10 Batch:10 Loss:0.148\n",
      "Epoch:20 Batch:10 Loss:0.146\n",
      "Epoch:30 Batch:10 Loss:0.143\n",
      "Epoch:40 Batch:10 Loss:0.143\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 232.52596948067955 setps: 800 count: 800\n",
      "avg rewards: 232.52596948067955\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.24696\n",
      "Epoch:20 Batch:15 Loss:0.01620\n",
      "Epoch:40 Batch:15 Loss:0.01142\n",
      "Epoch:60 Batch:15 Loss:0.01142\n",
      "Epoch:80 Batch:15 Loss:0.00898\n",
      "Epoch:100 Batch:15 Loss:0.00898\n",
      "Epoch:120 Batch:15 Loss:0.00851\n",
      "Epoch:140 Batch:15 Loss:0.00848\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.142\n",
      "Epoch:10 Batch:10 Loss:0.137\n",
      "Epoch:20 Batch:10 Loss:0.136\n",
      "Epoch:30 Batch:10 Loss:0.133\n",
      "Epoch:40 Batch:10 Loss:0.132\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 312.66407332828095 setps: 800 count: 800\n",
      "avg rewards: 312.66407332828095\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.21981\n",
      "Epoch:20 Batch:16 Loss:0.01498\n",
      "Epoch:40 Batch:16 Loss:0.01194\n",
      "Epoch:60 Batch:16 Loss:0.01063\n",
      "Epoch:80 Batch:16 Loss:0.01015\n",
      "Epoch:100 Batch:16 Loss:0.00897\n",
      "Epoch:120 Batch:16 Loss:0.00845\n",
      "Epoch:140 Batch:16 Loss:0.00785\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.136\n",
      "Epoch:10 Batch:10 Loss:0.127\n",
      "Epoch:20 Batch:10 Loss:0.128\n",
      "Epoch:30 Batch:10 Loss:0.125\n",
      "Epoch:40 Batch:10 Loss:0.126\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 161.39762350437965 setps: 800 count: 800\n",
      "avg rewards: 161.39762350437965\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.22729\n",
      "Epoch:20 Batch:17 Loss:0.01463\n",
      "Epoch:40 Batch:17 Loss:0.01165\n",
      "Epoch:60 Batch:17 Loss:0.00984\n",
      "Epoch:80 Batch:17 Loss:0.00933\n",
      "Epoch:100 Batch:17 Loss:0.00898\n",
      "Epoch:120 Batch:17 Loss:0.00684\n",
      "Epoch:140 Batch:17 Loss:0.00903\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.140\n",
      "Epoch:10 Batch:10 Loss:0.135\n",
      "Epoch:20 Batch:10 Loss:0.135\n",
      "Epoch:30 Batch:10 Loss:0.131\n",
      "Epoch:40 Batch:10 Loss:0.130\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 219.51072504360351 setps: 800 count: 800\n",
      "avg rewards: 219.51072504360351\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.20354\n",
      "Epoch:20 Batch:18 Loss:0.01613\n",
      "Epoch:40 Batch:18 Loss:0.01257\n",
      "Epoch:60 Batch:18 Loss:0.01022\n",
      "Epoch:80 Batch:18 Loss:0.00886\n",
      "Epoch:100 Batch:18 Loss:0.00802\n",
      "Epoch:120 Batch:18 Loss:0.00868\n",
      "Epoch:140 Batch:18 Loss:0.00742\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.127\n",
      "Epoch:10 Batch:10 Loss:0.119\n",
      "Epoch:20 Batch:10 Loss:0.116\n",
      "Epoch:30 Batch:10 Loss:0.112\n",
      "Epoch:40 Batch:10 Loss:0.111\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -263.3013477644389 setps: 800 count: 800\n",
      "avg rewards: -263.3013477644389\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.19528\n",
      "Epoch:20 Batch:19 Loss:0.01498\n",
      "Epoch:40 Batch:19 Loss:0.01136\n",
      "Epoch:60 Batch:19 Loss:0.00967\n",
      "Epoch:80 Batch:19 Loss:0.00916\n",
      "Epoch:100 Batch:19 Loss:0.00810\n",
      "Epoch:120 Batch:19 Loss:0.00895\n",
      "Epoch:140 Batch:19 Loss:0.00751\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.151\n",
      "Epoch:10 Batch:10 Loss:0.131\n",
      "Epoch:20 Batch:10 Loss:0.126\n",
      "Epoch:30 Batch:10 Loss:0.122\n",
      "Epoch:40 Batch:10 Loss:0.122\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -189.1075098895003 setps: 800 count: 800\n",
      "reward: -3.7616181375124174 setps: 81 count: 881\n",
      "avg rewards: -96.43456401350636\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.19411\n",
      "Epoch:20 Batch:20 Loss:0.01537\n",
      "Epoch:40 Batch:20 Loss:0.01060\n",
      "Epoch:60 Batch:20 Loss:0.01089\n",
      "Epoch:80 Batch:20 Loss:0.00947\n",
      "Epoch:100 Batch:20 Loss:0.00898\n",
      "Epoch:120 Batch:20 Loss:0.00838\n",
      "Epoch:140 Batch:20 Loss:0.00875\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.117\n",
      "Epoch:10 Batch:10 Loss:0.111\n",
      "Epoch:20 Batch:10 Loss:0.110\n",
      "Epoch:30 Batch:10 Loss:0.107\n",
      "Epoch:40 Batch:10 Loss:0.107\n",
      "Done!\n",
      "############# start HumanoidBulletEnv-v0 training ###################\n",
      "(49,)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -31.670611619416743 setps: 19 count: 19\n",
      "reward: -31.887152813519066 setps: 19 count: 38\n",
      "reward: -28.64125677267438 setps: 18 count: 56\n",
      "reward: -24.909012063659613 setps: 19 count: 75\n",
      "reward: -41.29290771828528 setps: 20 count: 95\n",
      "reward: -51.99106353911048 setps: 20 count: 115\n",
      "reward: -39.31853355359345 setps: 20 count: 135\n",
      "reward: -28.79578133157338 setps: 18 count: 153\n",
      "reward: -31.628607562232357 setps: 20 count: 173\n",
      "reward: -17.75812217401544 setps: 19 count: 192\n",
      "reward: -35.16423495947384 setps: 18 count: 210\n",
      "reward: -30.055821837983967 setps: 21 count: 231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -25.62942155986966 setps: 19 count: 250\n",
      "reward: -32.761298733825974 setps: 20 count: 270\n",
      "reward: -38.50627379091165 setps: 19 count: 289\n",
      "reward: -21.899687603708298 setps: 19 count: 308\n",
      "reward: -32.51787980311201 setps: 20 count: 328\n",
      "reward: -37.147549563650685 setps: 19 count: 347\n",
      "reward: -36.271225149756354 setps: 20 count: 367\n",
      "reward: -28.97792792861874 setps: 19 count: 386\n",
      "reward: -43.6678331704199 setps: 20 count: 406\n",
      "reward: -32.592590161877155 setps: 20 count: 426\n",
      "reward: -30.57825719642569 setps: 19 count: 445\n",
      "reward: -25.044538317006662 setps: 21 count: 466\n",
      "reward: -32.29310693947482 setps: 19 count: 485\n",
      "reward: -44.71703968606743 setps: 19 count: 504\n",
      "reward: -32.2655731237668 setps: 31 count: 535\n",
      "reward: -29.846799830935193 setps: 20 count: 555\n",
      "reward: -33.898694592404354 setps: 23 count: 578\n",
      "reward: -30.640793769559238 setps: 18 count: 596\n",
      "reward: -38.273901971895256 setps: 20 count: 616\n",
      "reward: -31.58877532303595 setps: 18 count: 634\n",
      "reward: -20.958476304473873 setps: 24 count: 658\n",
      "reward: -26.97668673825101 setps: 17 count: 675\n",
      "reward: -20.09804180001374 setps: 18 count: 693\n",
      "reward: -19.265679680260654 setps: 19 count: 712\n",
      "reward: -36.24132778493222 setps: 20 count: 732\n",
      "reward: -35.27037726789567 setps: 19 count: 751\n",
      "reward: -23.978927737307096 setps: 20 count: 771\n",
      "reward: -21.421806157306122 setps: 18 count: 789\n",
      "reward: -36.311412511651 setps: 19 count: 808\n",
      "reward: -40.74404378553736 setps: 23 count: 831\n",
      "reward: -26.159285747693506 setps: 19 count: 850\n",
      "reward: -31.721262937643044 setps: 19 count: 869\n",
      "reward: -29.72960160078656 setps: 19 count: 888\n",
      "reward: -41.219347612689305 setps: 20 count: 908\n",
      "reward: -12.25031010333478 setps: 20 count: 928\n",
      "reward: -23.72108948078385 setps: 20 count: 948\n",
      "reward: -36.536707819423455 setps: 23 count: 971\n",
      "reward: -38.589419054178876 setps: 25 count: 996\n",
      "avg rewards: -31.46852156572044\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:1.57436\n",
      "Epoch:20 Batch:1 Loss:0.54310\n",
      "Epoch:40 Batch:1 Loss:0.45579\n",
      "Epoch:60 Batch:1 Loss:0.38155\n",
      "Epoch:80 Batch:1 Loss:0.32594\n",
      "Epoch:100 Batch:1 Loss:0.28483\n",
      "Epoch:120 Batch:1 Loss:0.25450\n",
      "Epoch:140 Batch:1 Loss:0.23614\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.246\n",
      "Epoch:10 Batch:8 Loss:0.233\n",
      "Epoch:20 Batch:8 Loss:0.228\n",
      "Epoch:30 Batch:8 Loss:0.220\n",
      "Epoch:40 Batch:8 Loss:0.215\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -3.466062679936295 setps: 17 count: 17\n",
      "reward: -7.619120845384897 setps: 28 count: 45\n",
      "reward: -12.78107005050552 setps: 40 count: 85\n",
      "reward: 33.09751319641072 setps: 85 count: 170\n",
      "reward: -6.275379447135494 setps: 17 count: 187\n",
      "reward: -6.658367013408865 setps: 33 count: 220\n",
      "reward: -18.066640973147873 setps: 35 count: 255\n",
      "reward: -36.200487775907085 setps: 46 count: 301\n",
      "reward: 4.978840789977401 setps: 61 count: 362\n",
      "reward: -19.24138806296687 setps: 53 count: 415\n",
      "reward: -15.276128459266337 setps: 45 count: 460\n",
      "reward: 14.87632270861359 setps: 76 count: 536\n",
      "reward: -26.932647320869727 setps: 34 count: 570\n",
      "reward: -6.244625186211495 setps: 64 count: 634\n",
      "reward: -27.763145912556507 setps: 54 count: 688\n",
      "reward: -25.66700097750144 setps: 62 count: 750\n",
      "reward: -14.823129805934151 setps: 38 count: 788\n",
      "reward: -21.939389833100726 setps: 43 count: 831\n",
      "reward: -19.762402254001067 setps: 41 count: 872\n",
      "reward: -15.003861592496104 setps: 39 count: 911\n",
      "reward: -33.951674068566355 setps: 56 count: 967\n",
      "avg rewards: -12.605706931614051\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:1.45958\n",
      "Epoch:20 Batch:2 Loss:0.33637\n",
      "Epoch:40 Batch:2 Loss:0.26938\n",
      "Epoch:60 Batch:2 Loss:0.21655\n",
      "Epoch:80 Batch:2 Loss:0.18311\n",
      "Epoch:100 Batch:2 Loss:0.16955\n",
      "Epoch:120 Batch:2 Loss:0.16084\n",
      "Epoch:140 Batch:2 Loss:0.14252\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.187\n",
      "Epoch:10 Batch:8 Loss:0.159\n",
      "Epoch:20 Batch:8 Loss:0.158\n",
      "Epoch:30 Batch:8 Loss:0.156\n",
      "Epoch:40 Batch:8 Loss:0.154\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 54.77593262638984 setps: 48 count: 48\n",
      "reward: 58.92794667035923 setps: 55 count: 103\n",
      "reward: 30.757476048795795 setps: 33 count: 136\n",
      "reward: 58.1196948912344 setps: 48 count: 184\n",
      "reward: 67.14301973226831 setps: 67 count: 251\n",
      "reward: 49.19243512233661 setps: 41 count: 292\n",
      "reward: 56.24662985465839 setps: 49 count: 341\n",
      "reward: 54.05292982004174 setps: 49 count: 390\n",
      "reward: 50.19768458082691 setps: 45 count: 435\n",
      "reward: 58.66576626102322 setps: 52 count: 487\n",
      "reward: 43.851176996067814 setps: 68 count: 555\n",
      "reward: 57.25188528543512 setps: 60 count: 615\n",
      "reward: 60.37143270830128 setps: 57 count: 672\n",
      "reward: 65.58952837198156 setps: 56 count: 728\n",
      "reward: 57.47270514666453 setps: 49 count: 777\n",
      "reward: 52.22034941430465 setps: 43 count: 820\n",
      "reward: 72.01265393607756 setps: 63 count: 883\n",
      "reward: 45.79651615231851 setps: 46 count: 929\n",
      "reward: 59.94589739902004 setps: 53 count: 982\n",
      "avg rewards: 55.399561106216076\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:1.38931\n",
      "Epoch:20 Batch:3 Loss:0.24067\n",
      "Epoch:40 Batch:3 Loss:0.18864\n",
      "Epoch:60 Batch:3 Loss:0.15805\n",
      "Epoch:80 Batch:3 Loss:0.13259\n",
      "Epoch:100 Batch:3 Loss:0.12158\n",
      "Epoch:120 Batch:3 Loss:0.12012\n",
      "Epoch:140 Batch:3 Loss:0.11004\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.157\n",
      "Epoch:10 Batch:8 Loss:0.148\n",
      "Epoch:20 Batch:8 Loss:0.147\n",
      "Epoch:30 Batch:8 Loss:0.144\n",
      "Epoch:40 Batch:8 Loss:0.145\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 17.950819753513496 setps: 21 count: 21\n",
      "reward: 48.339793237352545 setps: 49 count: 70\n",
      "reward: 9.338358277275987 setps: 17 count: 87\n",
      "reward: 50.4879804263328 setps: 51 count: 138\n",
      "reward: 27.71025672035757 setps: 30 count: 168\n",
      "reward: 89.08090449636802 setps: 74 count: 242\n",
      "reward: 27.77829812594136 setps: 27 count: 269\n",
      "reward: 36.99661712186207 setps: 33 count: 302\n",
      "reward: 34.398119526260416 setps: 33 count: 335\n",
      "reward: 28.70829111609491 setps: 31 count: 366\n",
      "reward: 67.87535198459956 setps: 51 count: 417\n",
      "reward: 17.664821154761007 setps: 20 count: 437\n",
      "reward: 59.297771636964185 setps: 44 count: 481\n",
      "reward: 61.74130850542135 setps: 84 count: 565\n",
      "reward: 7.072096668808079 setps: 16 count: 581\n",
      "reward: 43.91370614624757 setps: 38 count: 619\n",
      "reward: 59.05932677627425 setps: 48 count: 667\n",
      "reward: 10.54666728900629 setps: 17 count: 684\n",
      "reward: 41.735952006305155 setps: 33 count: 717\n",
      "reward: 84.42696120344189 setps: 56 count: 773\n",
      "reward: 56.522734297612615 setps: 46 count: 819\n",
      "reward: 27.06497648050718 setps: 32 count: 851\n",
      "reward: 40.132698593454556 setps: 38 count: 889\n",
      "reward: 73.17250383291102 setps: 53 count: 942\n",
      "reward: 78.74629927152854 setps: 58 count: 1000\n",
      "avg rewards: 43.990504585968104\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:1.26405\n",
      "Epoch:20 Batch:4 Loss:0.20983\n",
      "Epoch:40 Batch:4 Loss:0.15279\n",
      "Epoch:60 Batch:4 Loss:0.13046\n",
      "Epoch:80 Batch:4 Loss:0.11774\n",
      "Epoch:100 Batch:4 Loss:0.10866\n",
      "Epoch:120 Batch:4 Loss:0.09837\n",
      "Epoch:140 Batch:4 Loss:0.09048\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.119\n",
      "Epoch:10 Batch:8 Loss:0.117\n",
      "Epoch:20 Batch:8 Loss:0.116\n",
      "Epoch:30 Batch:8 Loss:0.116\n",
      "Epoch:40 Batch:8 Loss:0.114\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 10.92092874549271 setps: 17 count: 17\n",
      "reward: 71.00402834544074 setps: 41 count: 58\n",
      "reward: 73.90672162807022 setps: 52 count: 110\n",
      "reward: 64.36125566832634 setps: 41 count: 151\n",
      "reward: 17.19014155415061 setps: 17 count: 168\n",
      "reward: 77.17158821155755 setps: 54 count: 222\n",
      "reward: 43.25414574242896 setps: 37 count: 259\n",
      "reward: 52.26717320224415 setps: 36 count: 295\n",
      "reward: 43.92547662423749 setps: 31 count: 326\n",
      "reward: 4.413878031812782 setps: 13 count: 339\n",
      "reward: 28.209582188414064 setps: 26 count: 365\n",
      "reward: 63.87806390489423 setps: 43 count: 408\n",
      "reward: 50.781409289094164 setps: 46 count: 454\n",
      "reward: 53.681513593801355 setps: 37 count: 491\n",
      "reward: 76.3331185447314 setps: 46 count: 537\n",
      "reward: 62.80992999909502 setps: 46 count: 583\n",
      "reward: 49.383902471636254 setps: 41 count: 624\n",
      "reward: 37.43814108151564 setps: 31 count: 655\n",
      "reward: 76.07847638467358 setps: 57 count: 712\n",
      "reward: 29.99284203837887 setps: 28 count: 740\n",
      "reward: 17.051537385667324 setps: 17 count: 757\n",
      "reward: 33.32419786847312 setps: 29 count: 786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 52.46350066446757 setps: 39 count: 825\n",
      "reward: 112.30267582254746 setps: 75 count: 900\n",
      "reward: 57.665903828320715 setps: 42 count: 942\n",
      "reward: 21.59942516318988 setps: 19 count: 961\n",
      "reward: 40.205777575590766 setps: 31 count: 992\n",
      "avg rewards: 48.948716131787144\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:1.15105\n",
      "Epoch:20 Batch:5 Loss:0.17294\n",
      "Epoch:40 Batch:5 Loss:0.13292\n",
      "Epoch:60 Batch:5 Loss:0.11703\n",
      "Epoch:80 Batch:5 Loss:0.09303\n",
      "Epoch:100 Batch:5 Loss:0.09479\n",
      "Epoch:120 Batch:5 Loss:0.08528\n",
      "Epoch:140 Batch:5 Loss:0.08850\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.105\n",
      "Epoch:10 Batch:8 Loss:0.104\n",
      "Epoch:20 Batch:8 Loss:0.103\n",
      "Epoch:30 Batch:8 Loss:0.102\n",
      "Epoch:40 Batch:8 Loss:0.103\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 22.241088867276265 setps: 22 count: 22\n",
      "reward: 22.14166328370629 setps: 23 count: 45\n",
      "reward: 20.335676818292992 setps: 23 count: 68\n",
      "reward: 19.985127563285637 setps: 22 count: 90\n",
      "reward: 20.34136066984938 setps: 19 count: 109\n",
      "reward: 19.521302235382603 setps: 23 count: 132\n",
      "reward: 19.448821928320104 setps: 21 count: 153\n",
      "reward: 15.705154639478128 setps: 22 count: 175\n",
      "reward: 19.483275960403265 setps: 21 count: 196\n",
      "reward: 23.046850742919197 setps: 23 count: 219\n",
      "reward: 19.379481418068465 setps: 23 count: 242\n",
      "reward: 21.406150814912685 setps: 22 count: 264\n",
      "reward: 20.747723019564003 setps: 22 count: 286\n",
      "reward: 24.562767700408585 setps: 24 count: 310\n",
      "reward: 11.389884448671364 setps: 17 count: 327\n",
      "reward: 16.13588246686704 setps: 21 count: 348\n",
      "reward: 17.7407840568485 setps: 20 count: 368\n",
      "reward: 22.554578363522886 setps: 23 count: 391\n",
      "reward: 21.761559280469374 setps: 22 count: 413\n",
      "reward: 17.276554860471514 setps: 22 count: 435\n",
      "reward: 16.490358532419485 setps: 17 count: 452\n",
      "reward: 21.460756286382093 setps: 22 count: 474\n",
      "reward: 18.636492320100658 setps: 22 count: 496\n",
      "reward: 23.55212264647416 setps: 23 count: 519\n",
      "reward: 16.825228685604817 setps: 23 count: 542\n",
      "reward: 19.28537852482987 setps: 21 count: 563\n",
      "reward: 20.047486256501 setps: 21 count: 584\n",
      "reward: 20.285185444750823 setps: 22 count: 606\n",
      "reward: 15.028867792728121 setps: 18 count: 624\n",
      "reward: 18.24644914729288 setps: 23 count: 647\n",
      "reward: 17.0485257016393 setps: 21 count: 668\n",
      "reward: 14.779702822308169 setps: 20 count: 688\n",
      "reward: 16.21583437187073 setps: 21 count: 709\n",
      "reward: 20.486084835811923 setps: 22 count: 731\n",
      "reward: 20.851820867056084 setps: 21 count: 752\n",
      "reward: 17.966267424674875 setps: 21 count: 773\n",
      "reward: 15.450960369491074 setps: 20 count: 793\n",
      "reward: 18.763976473631917 setps: 21 count: 814\n",
      "reward: 25.422732725074454 setps: 24 count: 838\n",
      "reward: 18.986527004958774 setps: 22 count: 860\n",
      "reward: 25.49575470626587 setps: 24 count: 884\n",
      "reward: 20.748146488307974 setps: 20 count: 904\n",
      "reward: 22.135124103345152 setps: 23 count: 927\n",
      "reward: 21.62477444910619 setps: 22 count: 949\n",
      "reward: 25.500319339738056 setps: 23 count: 972\n",
      "reward: 19.95902274873806 setps: 20 count: 992\n",
      "avg rewards: 19.70651280886567\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:1.08792\n",
      "Epoch:20 Batch:6 Loss:0.15729\n",
      "Epoch:40 Batch:6 Loss:0.12211\n",
      "Epoch:60 Batch:6 Loss:0.10228\n",
      "Epoch:80 Batch:6 Loss:0.09293\n",
      "Epoch:100 Batch:6 Loss:0.08046\n",
      "Epoch:120 Batch:6 Loss:0.08365\n",
      "Epoch:140 Batch:6 Loss:0.07552\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.102\n",
      "Epoch:10 Batch:8 Loss:0.101\n",
      "Epoch:20 Batch:8 Loss:0.101\n",
      "Epoch:30 Batch:8 Loss:0.099\n",
      "Epoch:40 Batch:8 Loss:0.098\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 21.59931546665321 setps: 21 count: 21\n",
      "reward: 40.38854623098013 setps: 29 count: 50\n",
      "reward: 46.84270109254139 setps: 36 count: 86\n",
      "reward: 41.164842971315366 setps: 29 count: 115\n",
      "reward: 49.58824736431851 setps: 39 count: 154\n",
      "reward: 26.136880982344152 setps: 24 count: 178\n",
      "reward: 36.928737207019005 setps: 29 count: 207\n",
      "reward: 18.292791567824313 setps: 19 count: 226\n",
      "reward: 45.413598946933064 setps: 31 count: 257\n",
      "reward: 40.2887649078024 setps: 30 count: 287\n",
      "reward: 38.628888655775526 setps: 29 count: 316\n",
      "reward: 15.713083193519559 setps: 18 count: 334\n",
      "reward: 38.456823116874155 setps: 28 count: 362\n",
      "reward: 40.008048863793384 setps: 29 count: 391\n",
      "reward: 43.818522445048444 setps: 31 count: 422\n",
      "reward: 42.654706250299945 setps: 30 count: 452\n",
      "reward: 40.24867700042523 setps: 27 count: 479\n",
      "reward: 33.76506754481088 setps: 29 count: 508\n",
      "reward: 17.866754898107313 setps: 17 count: 525\n",
      "reward: 40.09288791088037 setps: 31 count: 556\n",
      "reward: 40.572703268637994 setps: 29 count: 585\n",
      "reward: 38.13055404879705 setps: 29 count: 614\n",
      "reward: 37.424544328177575 setps: 27 count: 641\n",
      "reward: 54.65225415756576 setps: 36 count: 677\n",
      "reward: 21.939013899698327 setps: 19 count: 696\n",
      "reward: 25.240022933462754 setps: 22 count: 718\n",
      "reward: 39.080573590390856 setps: 30 count: 748\n",
      "reward: 35.06493990720628 setps: 28 count: 776\n",
      "reward: 36.861809975124196 setps: 27 count: 803\n",
      "reward: 14.60103111782955 setps: 18 count: 821\n",
      "reward: 50.96903110894492 setps: 34 count: 855\n",
      "reward: 35.54334041529073 setps: 27 count: 882\n",
      "reward: 37.60565342225163 setps: 27 count: 909\n",
      "reward: 42.34386894620111 setps: 29 count: 938\n",
      "reward: 42.94552570863016 setps: 30 count: 968\n",
      "reward: 34.951257515251925 setps: 26 count: 994\n",
      "avg rewards: 36.272889193353535\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:1.15789\n",
      "Epoch:20 Batch:7 Loss:0.14964\n",
      "Epoch:40 Batch:7 Loss:0.11179\n",
      "Epoch:60 Batch:7 Loss:0.09390\n",
      "Epoch:80 Batch:7 Loss:0.07931\n",
      "Epoch:100 Batch:7 Loss:0.07967\n",
      "Epoch:120 Batch:7 Loss:0.07630\n",
      "Epoch:140 Batch:7 Loss:0.06677\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.097\n",
      "Epoch:10 Batch:8 Loss:0.097\n",
      "Epoch:20 Batch:8 Loss:0.097\n",
      "Epoch:30 Batch:8 Loss:0.096\n",
      "Epoch:40 Batch:8 Loss:0.093\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 54.56246490490447 setps: 69 count: 69\n",
      "reward: 58.218506655728575 setps: 56 count: 125\n",
      "reward: 71.16593540196773 setps: 44 count: 169\n",
      "reward: 21.965220410143957 setps: 19 count: 188\n",
      "reward: 72.66319070899627 setps: 59 count: 247\n",
      "reward: 131.621853278493 setps: 97 count: 344\n",
      "reward: 43.93899442522961 setps: 32 count: 376\n",
      "reward: 48.58642975371477 setps: 45 count: 421\n",
      "reward: 56.40177152538237 setps: 44 count: 465\n",
      "reward: 63.154766822884284 setps: 50 count: 515\n",
      "reward: 58.99969326829305 setps: 40 count: 555\n",
      "reward: 46.12857141625718 setps: 45 count: 600\n",
      "reward: 64.15823046764854 setps: 43 count: 643\n",
      "reward: 89.50242553403223 setps: 54 count: 697\n",
      "reward: 89.12807313936209 setps: 68 count: 765\n",
      "reward: 61.12324745063961 setps: 51 count: 816\n",
      "reward: 61.82080858104018 setps: 39 count: 855\n",
      "reward: 65.19435076711088 setps: 48 count: 903\n",
      "reward: 56.84606850290874 setps: 38 count: 941\n",
      "reward: 61.352346705739926 setps: 42 count: 983\n",
      "avg rewards: 63.826647486023866\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.96174\n",
      "Epoch:20 Batch:8 Loss:0.13107\n",
      "Epoch:40 Batch:8 Loss:0.09526\n",
      "Epoch:60 Batch:8 Loss:0.08388\n",
      "Epoch:80 Batch:8 Loss:0.08121\n",
      "Epoch:100 Batch:8 Loss:0.07814\n",
      "Epoch:120 Batch:8 Loss:0.06866\n",
      "Epoch:140 Batch:8 Loss:0.06642\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.081\n",
      "Epoch:10 Batch:8 Loss:0.082\n",
      "Epoch:20 Batch:8 Loss:0.078\n",
      "Epoch:30 Batch:8 Loss:0.079\n",
      "Epoch:40 Batch:8 Loss:0.080\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 44.884390710073056 setps: 32 count: 32\n",
      "reward: 63.962658478878446 setps: 46 count: 78\n",
      "reward: 52.83048841485287 setps: 41 count: 119\n",
      "reward: 46.28054808025917 setps: 31 count: 150\n",
      "reward: 48.031433545662736 setps: 33 count: 183\n",
      "reward: 44.4963264733291 setps: 31 count: 214\n",
      "reward: 48.49689589132467 setps: 32 count: 246\n",
      "reward: 44.41603706512105 setps: 34 count: 280\n",
      "reward: 55.178228398371715 setps: 37 count: 317\n",
      "reward: 89.80706691481173 setps: 50 count: 367\n",
      "reward: 80.50720372697688 setps: 51 count: 418\n",
      "reward: 16.8348200841021 setps: 20 count: 438\n",
      "reward: 75.18566369688779 setps: 41 count: 479\n",
      "reward: 48.80384778974694 setps: 34 count: 513\n",
      "reward: 64.36509256213031 setps: 46 count: 559\n",
      "reward: 53.99626707701392 setps: 46 count: 605\n",
      "reward: 90.64249900692379 setps: 61 count: 666\n",
      "reward: 14.952207389722751 setps: 17 count: 683\n",
      "reward: 37.29704658992414 setps: 27 count: 710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 29.483604778224255 setps: 28 count: 738\n",
      "reward: 50.273223142874485 setps: 34 count: 772\n",
      "reward: 47.37934774998284 setps: 33 count: 805\n",
      "reward: 45.383259402500705 setps: 31 count: 836\n",
      "reward: 71.1589711817229 setps: 46 count: 882\n",
      "reward: 51.05029451260634 setps: 39 count: 921\n",
      "reward: 68.97920747559401 setps: 49 count: 970\n",
      "avg rewards: 53.25679346690841\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.83385\n",
      "Epoch:20 Batch:9 Loss:0.12266\n",
      "Epoch:40 Batch:9 Loss:0.09401\n",
      "Epoch:60 Batch:9 Loss:0.08413\n",
      "Epoch:80 Batch:9 Loss:0.07158\n",
      "Epoch:100 Batch:9 Loss:0.06814\n",
      "Epoch:120 Batch:9 Loss:0.06761\n",
      "Epoch:140 Batch:9 Loss:0.06336\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.086\n",
      "Epoch:10 Batch:8 Loss:0.084\n",
      "Epoch:20 Batch:8 Loss:0.085\n",
      "Epoch:30 Batch:8 Loss:0.087\n",
      "Epoch:40 Batch:8 Loss:0.085\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 71.67277898585162 setps: 53 count: 53\n",
      "reward: 87.98458608233923 setps: 51 count: 104\n",
      "reward: 83.69195764171747 setps: 75 count: 179\n",
      "reward: 95.29106460574546 setps: 55 count: 234\n",
      "reward: 78.8724043461101 setps: 50 count: 284\n",
      "reward: 43.081069312644836 setps: 65 count: 349\n",
      "reward: 20.78551346568274 setps: 19 count: 368\n",
      "reward: 121.70607075233129 setps: 80 count: 448\n",
      "reward: 16.076519916921097 setps: 17 count: 465\n",
      "reward: 80.90371169680292 setps: 54 count: 519\n",
      "reward: 91.98346364912867 setps: 78 count: 597\n",
      "reward: 22.291642649073037 setps: 20 count: 617\n",
      "reward: 67.61884111812252 setps: 50 count: 667\n",
      "reward: 67.79057125149266 setps: 51 count: 718\n",
      "reward: 107.50276546603851 setps: 72 count: 790\n",
      "reward: 89.18700600683803 setps: 54 count: 844\n",
      "reward: 81.98468603397772 setps: 56 count: 900\n",
      "reward: 41.347826591442576 setps: 33 count: 933\n",
      "reward: 89.92365119332972 setps: 62 count: 995\n",
      "avg rewards: 71.56295425082055\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.57978\n",
      "Epoch:20 Batch:10 Loss:0.10624\n",
      "Epoch:40 Batch:10 Loss:0.08335\n",
      "Epoch:60 Batch:10 Loss:0.07071\n",
      "Epoch:80 Batch:10 Loss:0.07128\n",
      "Epoch:100 Batch:10 Loss:0.06912\n",
      "Epoch:120 Batch:10 Loss:0.06387\n",
      "Epoch:140 Batch:10 Loss:0.05561\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.083\n",
      "Epoch:10 Batch:8 Loss:0.082\n",
      "Epoch:20 Batch:8 Loss:0.080\n",
      "Epoch:30 Batch:8 Loss:0.081\n",
      "Epoch:40 Batch:8 Loss:0.083\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 31.64442753048933 setps: 26 count: 26\n",
      "reward: 49.6676142458542 setps: 32 count: 58\n",
      "reward: 16.869041398282572 setps: 17 count: 75\n",
      "reward: 92.41350471057957 setps: 56 count: 131\n",
      "reward: 99.8957080112945 setps: 69 count: 200\n",
      "reward: 3.6152065461719767 setps: 14 count: 214\n",
      "reward: 57.025702828327496 setps: 38 count: 252\n",
      "reward: 55.80526307405234 setps: 51 count: 303\n",
      "reward: 30.50816414097499 setps: 25 count: 328\n",
      "reward: 25.373645362343808 setps: 26 count: 354\n",
      "reward: 85.78971319149714 setps: 53 count: 407\n",
      "reward: 79.41073117870836 setps: 52 count: 459\n",
      "reward: 51.71377889745782 setps: 35 count: 494\n",
      "reward: 62.750322458369205 setps: 40 count: 534\n",
      "reward: 60.810965339165705 setps: 38 count: 572\n",
      "reward: 32.45695681681244 setps: 24 count: 596\n",
      "reward: 52.603536928039084 setps: 39 count: 635\n",
      "reward: 44.54481491487967 setps: 33 count: 668\n",
      "reward: 101.92715759777344 setps: 65 count: 733\n",
      "reward: 46.75257058387799 setps: 34 count: 767\n",
      "reward: 39.19592777871584 setps: 49 count: 816\n",
      "reward: 87.40729864547463 setps: 48 count: 864\n",
      "reward: 70.29883845333094 setps: 42 count: 906\n",
      "reward: 22.336679472788813 setps: 19 count: 925\n",
      "reward: 25.59085376115254 setps: 23 count: 948\n",
      "reward: 56.6623616762634 setps: 41 count: 989\n",
      "avg rewards: 53.19503021317992\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.56905\n",
      "Epoch:20 Batch:11 Loss:0.09359\n",
      "Epoch:40 Batch:11 Loss:0.08160\n",
      "Epoch:60 Batch:11 Loss:0.07325\n",
      "Epoch:80 Batch:11 Loss:0.05779\n",
      "Epoch:100 Batch:11 Loss:0.05969\n",
      "Epoch:120 Batch:11 Loss:0.05925\n",
      "Epoch:140 Batch:11 Loss:0.05416\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.084\n",
      "Epoch:10 Batch:8 Loss:0.082\n",
      "Epoch:20 Batch:8 Loss:0.081\n",
      "Epoch:30 Batch:8 Loss:0.083\n",
      "Epoch:40 Batch:8 Loss:0.081\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 70.75347309462957 setps: 60 count: 60\n",
      "reward: 37.77159425746504 setps: 51 count: 111\n",
      "reward: 47.31320343623958 setps: 39 count: 150\n",
      "reward: 47.499478987361364 setps: 36 count: 186\n",
      "reward: 67.02204952502652 setps: 44 count: 230\n",
      "reward: 51.27702036796836 setps: 38 count: 268\n",
      "reward: 66.62657644533027 setps: 74 count: 342\n",
      "reward: 65.82271421315673 setps: 48 count: 390\n",
      "reward: 69.49597755633731 setps: 85 count: 475\n",
      "reward: 55.985947398270945 setps: 45 count: 520\n",
      "reward: 11.987161388163804 setps: 17 count: 537\n",
      "reward: 54.19049422810931 setps: 45 count: 582\n",
      "reward: 53.374879813063316 setps: 39 count: 621\n",
      "reward: 45.52992596745753 setps: 57 count: 678\n",
      "reward: 34.24992709812068 setps: 55 count: 733\n",
      "reward: 46.49149592211033 setps: 46 count: 779\n",
      "reward: 56.84355087689476 setps: 48 count: 827\n",
      "reward: 44.64665033434867 setps: 46 count: 873\n",
      "reward: 56.830919883812044 setps: 43 count: 916\n",
      "reward: 44.08913497980975 setps: 48 count: 964\n",
      "avg rewards: 51.39010878868379\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.66779\n",
      "Epoch:20 Batch:12 Loss:0.10190\n",
      "Epoch:40 Batch:12 Loss:0.07145\n",
      "Epoch:60 Batch:12 Loss:0.07415\n",
      "Epoch:80 Batch:12 Loss:0.05978\n",
      "Epoch:100 Batch:12 Loss:0.05682\n",
      "Epoch:120 Batch:12 Loss:0.05925\n",
      "Epoch:140 Batch:12 Loss:0.05362\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.075\n",
      "Epoch:10 Batch:8 Loss:0.076\n",
      "Epoch:20 Batch:8 Loss:0.074\n",
      "Epoch:30 Batch:8 Loss:0.071\n",
      "Epoch:40 Batch:8 Loss:0.070\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 58.213904862676266 setps: 65 count: 65\n",
      "reward: 44.68956160087982 setps: 54 count: 119\n",
      "reward: 17.823528812089354 setps: 18 count: 137\n",
      "reward: 19.221419041072664 setps: 20 count: 157\n",
      "reward: 99.06163842091924 setps: 74 count: 231\n",
      "reward: 9.95196861300792 setps: 17 count: 248\n",
      "reward: 60.449138596813995 setps: 67 count: 315\n",
      "reward: 31.491683126105638 setps: 33 count: 348\n",
      "reward: 50.6840487225636 setps: 38 count: 386\n",
      "reward: 104.09098131005887 setps: 61 count: 447\n",
      "reward: 117.2576923191955 setps: 80 count: 527\n",
      "reward: 88.89434970693108 setps: 60 count: 587\n",
      "reward: 98.18340917714522 setps: 61 count: 648\n",
      "reward: 105.91007990401704 setps: 64 count: 712\n",
      "reward: 62.61189273764031 setps: 39 count: 751\n",
      "reward: 67.13812256154776 setps: 64 count: 815\n",
      "reward: 84.09794329305443 setps: 68 count: 883\n",
      "reward: 55.30158424723341 setps: 56 count: 939\n",
      "reward: 101.31139685928643 setps: 56 count: 995\n",
      "avg rewards: 67.17812336380202\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.48208\n",
      "Epoch:20 Batch:13 Loss:0.09566\n",
      "Epoch:40 Batch:13 Loss:0.07880\n",
      "Epoch:60 Batch:13 Loss:0.06630\n",
      "Epoch:80 Batch:13 Loss:0.05852\n",
      "Epoch:100 Batch:13 Loss:0.05726\n",
      "Epoch:120 Batch:13 Loss:0.05574\n",
      "Epoch:140 Batch:13 Loss:0.05685\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.075\n",
      "Epoch:10 Batch:8 Loss:0.074\n",
      "Epoch:20 Batch:8 Loss:0.074\n",
      "Epoch:30 Batch:8 Loss:0.074\n",
      "Epoch:40 Batch:8 Loss:0.074\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 21.391133653424912 setps: 18 count: 18\n",
      "reward: 37.907164292289245 setps: 34 count: 52\n",
      "reward: 39.58247997222496 setps: 52 count: 104\n",
      "reward: 43.89408707050022 setps: 39 count: 143\n",
      "reward: 32.173216555464016 setps: 35 count: 178\n",
      "reward: 21.441068937233645 setps: 19 count: 197\n",
      "reward: 46.333764950645836 setps: 55 count: 252\n",
      "reward: 4.924331497290405 setps: 14 count: 266\n",
      "reward: 12.722331194093567 setps: 17 count: 283\n",
      "reward: 49.31940510513086 setps: 56 count: 339\n",
      "reward: 40.4304413663369 setps: 35 count: 374\n",
      "reward: 36.47712525523966 setps: 35 count: 409\n",
      "reward: 37.05140395381312 setps: 36 count: 445\n",
      "reward: 57.43574335753221 setps: 50 count: 495\n",
      "reward: 66.86642926015774 setps: 57 count: 552\n",
      "reward: 32.81825110564095 setps: 35 count: 587\n",
      "reward: 38.055874351694364 setps: 41 count: 628\n",
      "reward: 77.8478597064532 setps: 85 count: 713\n",
      "reward: 46.25466375477408 setps: 39 count: 752\n",
      "reward: 37.46041784534609 setps: 31 count: 783\n",
      "reward: 37.11864663190208 setps: 40 count: 823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 48.38766632705084 setps: 46 count: 869\n",
      "reward: 44.7059578307715 setps: 37 count: 906\n",
      "reward: 63.87393101930212 setps: 54 count: 960\n",
      "reward: 32.227600004465785 setps: 35 count: 995\n",
      "avg rewards: 40.26803979995113\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.40181\n",
      "Epoch:20 Batch:14 Loss:0.08724\n",
      "Epoch:40 Batch:14 Loss:0.07359\n",
      "Epoch:60 Batch:14 Loss:0.06489\n",
      "Epoch:80 Batch:14 Loss:0.06101\n",
      "Epoch:100 Batch:14 Loss:0.05257\n",
      "Epoch:120 Batch:14 Loss:0.05682\n",
      "Epoch:140 Batch:14 Loss:0.06032\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.072\n",
      "Epoch:10 Batch:8 Loss:0.070\n",
      "Epoch:20 Batch:8 Loss:0.071\n",
      "Epoch:30 Batch:8 Loss:0.069\n",
      "Epoch:40 Batch:8 Loss:0.069\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 46.89252515504778 setps: 45 count: 45\n",
      "reward: 62.75678125242846 setps: 53 count: 98\n",
      "reward: 36.056476053135704 setps: 37 count: 135\n",
      "reward: 10.328466693751398 setps: 17 count: 152\n",
      "reward: 59.69852731225983 setps: 43 count: 195\n",
      "reward: 38.40735166861705 setps: 40 count: 235\n",
      "reward: 46.51791411507147 setps: 39 count: 274\n",
      "reward: 49.043048605664815 setps: 42 count: 316\n",
      "reward: 15.40014481122635 setps: 17 count: 333\n",
      "reward: 44.88627482310258 setps: 35 count: 368\n",
      "reward: 44.15979316072482 setps: 47 count: 415\n",
      "reward: 44.45456875611854 setps: 37 count: 452\n",
      "reward: 56.01396659133172 setps: 45 count: 497\n",
      "reward: 126.88089786729398 setps: 75 count: 572\n",
      "reward: 45.02734777309961 setps: 43 count: 615\n",
      "reward: 57.344154081562 setps: 41 count: 656\n",
      "reward: 40.76603063458168 setps: 36 count: 692\n",
      "reward: 18.774524698122693 setps: 18 count: 710\n",
      "reward: 60.98748238338596 setps: 42 count: 752\n",
      "reward: 86.30508682154468 setps: 62 count: 814\n",
      "reward: 39.55620121733228 setps: 37 count: 851\n",
      "reward: 46.978835091680224 setps: 54 count: 905\n",
      "reward: 44.810882585297804 setps: 43 count: 948\n",
      "reward: 55.18899676197471 setps: 38 count: 986\n",
      "avg rewards: 49.05151162143151\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.35317\n",
      "Epoch:20 Batch:15 Loss:0.08902\n",
      "Epoch:40 Batch:15 Loss:0.07040\n",
      "Epoch:60 Batch:15 Loss:0.06540\n",
      "Epoch:80 Batch:15 Loss:0.05612\n",
      "Epoch:100 Batch:15 Loss:0.05863\n",
      "Epoch:120 Batch:15 Loss:0.05404\n",
      "Epoch:140 Batch:15 Loss:0.05239\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.072\n",
      "Epoch:10 Batch:8 Loss:0.069\n",
      "Epoch:20 Batch:8 Loss:0.069\n",
      "Epoch:30 Batch:8 Loss:0.069\n",
      "Epoch:40 Batch:8 Loss:0.069\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 50.07043651567074 setps: 36 count: 36\n",
      "reward: 52.02386355585622 setps: 34 count: 70\n",
      "reward: 53.14937658612616 setps: 39 count: 109\n",
      "reward: 56.4121532175981 setps: 36 count: 145\n",
      "reward: 54.74674069661852 setps: 38 count: 183\n",
      "reward: 44.03569128358067 setps: 30 count: 213\n",
      "reward: 52.80182705510962 setps: 37 count: 250\n",
      "reward: 49.714387062321485 setps: 35 count: 285\n",
      "reward: 41.46034491306927 setps: 29 count: 314\n",
      "reward: 51.33306480512401 setps: 33 count: 347\n",
      "reward: 54.58138227183809 setps: 35 count: 382\n",
      "reward: 55.32185407062934 setps: 37 count: 419\n",
      "reward: 42.979518333869045 setps: 29 count: 448\n",
      "reward: 41.52642616477971 setps: 30 count: 478\n",
      "reward: 43.26033862785408 setps: 31 count: 509\n",
      "reward: 59.92964823544027 setps: 40 count: 549\n",
      "reward: 47.61256643402303 setps: 33 count: 582\n",
      "reward: 59.09854607992164 setps: 38 count: 620\n",
      "reward: 35.17332484594226 setps: 25 count: 645\n",
      "reward: 50.308640009186654 setps: 33 count: 678\n",
      "reward: 50.99743101659697 setps: 33 count: 711\n",
      "reward: 41.63547330556758 setps: 31 count: 742\n",
      "reward: 59.375194399275635 setps: 37 count: 779\n",
      "reward: 65.04604902495191 setps: 43 count: 822\n",
      "reward: 51.46009577122749 setps: 35 count: 857\n",
      "reward: 54.0362512457781 setps: 35 count: 892\n",
      "reward: 51.840979604839234 setps: 34 count: 926\n",
      "reward: 62.08306118662585 setps: 39 count: 965\n",
      "avg rewards: 51.14338093997935\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.36854\n",
      "Epoch:20 Batch:16 Loss:0.07635\n",
      "Epoch:40 Batch:16 Loss:0.07182\n",
      "Epoch:60 Batch:16 Loss:0.06192\n",
      "Epoch:80 Batch:16 Loss:0.05938\n",
      "Epoch:100 Batch:16 Loss:0.05622\n",
      "Epoch:120 Batch:16 Loss:0.05509\n",
      "Epoch:140 Batch:16 Loss:0.05144\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.073\n",
      "Epoch:10 Batch:8 Loss:0.072\n",
      "Epoch:20 Batch:8 Loss:0.072\n",
      "Epoch:30 Batch:8 Loss:0.072\n",
      "Epoch:40 Batch:8 Loss:0.073\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 33.272486102634865 setps: 26 count: 26\n",
      "reward: 89.3334847466831 setps: 61 count: 87\n",
      "reward: 78.38396857783809 setps: 44 count: 131\n",
      "reward: 44.47939260127462 setps: 43 count: 174\n",
      "reward: 22.227059589960845 setps: 19 count: 193\n",
      "reward: 35.915058917751594 setps: 32 count: 225\n",
      "reward: 49.495936528417225 setps: 36 count: 261\n",
      "reward: 43.529113562103895 setps: 34 count: 295\n",
      "reward: 59.20758094383489 setps: 76 count: 371\n",
      "reward: 114.57930415003646 setps: 70 count: 441\n",
      "reward: 48.08871403607335 setps: 35 count: 476\n",
      "reward: 12.340343829985068 setps: 17 count: 493\n",
      "reward: 45.42104980247824 setps: 37 count: 530\n",
      "reward: 49.50968171178684 setps: 42 count: 572\n",
      "reward: 23.429315797120218 setps: 20 count: 592\n",
      "reward: 52.83450696767397 setps: 40 count: 632\n",
      "reward: 86.96548067475668 setps: 49 count: 681\n",
      "reward: 43.64365535549151 setps: 36 count: 717\n",
      "reward: 45.30888318842918 setps: 38 count: 755\n",
      "reward: 63.449420365883256 setps: 42 count: 797\n",
      "reward: 52.2610733399386 setps: 44 count: 841\n",
      "reward: 88.59696427751041 setps: 52 count: 893\n",
      "reward: 33.16624862248136 setps: 32 count: 925\n",
      "reward: 24.045496782222475 setps: 20 count: 945\n",
      "reward: 72.76377581574494 setps: 48 count: 993\n",
      "avg rewards: 52.489919851524476\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.33256\n",
      "Epoch:20 Batch:17 Loss:0.08531\n",
      "Epoch:40 Batch:17 Loss:0.07328\n",
      "Epoch:60 Batch:17 Loss:0.06069\n",
      "Epoch:80 Batch:17 Loss:0.05570\n",
      "Epoch:100 Batch:17 Loss:0.05264\n",
      "Epoch:120 Batch:17 Loss:0.05286\n",
      "Epoch:140 Batch:17 Loss:0.04829\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.073\n",
      "Epoch:10 Batch:8 Loss:0.075\n",
      "Epoch:20 Batch:8 Loss:0.074\n",
      "Epoch:30 Batch:8 Loss:0.072\n",
      "Epoch:40 Batch:8 Loss:0.076\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 42.34151176813611 setps: 32 count: 32\n",
      "reward: 50.91540340710053 setps: 38 count: 70\n",
      "reward: 35.879525530793764 setps: 30 count: 100\n",
      "reward: 33.30530294876954 setps: 29 count: 129\n",
      "reward: 33.27721698565584 setps: 29 count: 158\n",
      "reward: 41.461932654996055 setps: 31 count: 189\n",
      "reward: 18.99921908409742 setps: 18 count: 207\n",
      "reward: 43.954832786195034 setps: 33 count: 240\n",
      "reward: 42.035833297630596 setps: 33 count: 273\n",
      "reward: 38.996737334912176 setps: 31 count: 304\n",
      "reward: 20.876929774261953 setps: 20 count: 324\n",
      "reward: 83.23331181006508 setps: 55 count: 379\n",
      "reward: 30.148510508448815 setps: 28 count: 407\n",
      "reward: 37.698786581252364 setps: 31 count: 438\n",
      "reward: 56.98781161355582 setps: 39 count: 477\n",
      "reward: 29.082468181321744 setps: 28 count: 505\n",
      "reward: 30.05032914969197 setps: 30 count: 535\n",
      "reward: 34.499422669458724 setps: 29 count: 564\n",
      "reward: 51.32330847710544 setps: 36 count: 600\n",
      "reward: 36.655308545440484 setps: 30 count: 630\n",
      "reward: 37.89452902038174 setps: 30 count: 660\n",
      "reward: 34.92272100688716 setps: 31 count: 691\n",
      "reward: 46.54205636019324 setps: 33 count: 724\n",
      "reward: 23.194863129391155 setps: 20 count: 744\n",
      "reward: 18.212578813494478 setps: 18 count: 762\n",
      "reward: 36.660221323139545 setps: 32 count: 794\n",
      "reward: 19.259970597737993 setps: 19 count: 813\n",
      "reward: 44.59694147560948 setps: 31 count: 844\n",
      "reward: 93.45384633784563 setps: 58 count: 902\n",
      "reward: 30.79604427716404 setps: 28 count: 930\n",
      "reward: 31.69394163853285 setps: 28 count: 958\n",
      "reward: 43.58839188314595 setps: 34 count: 992\n",
      "avg rewards: 39.141869030387895\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.32259\n",
      "Epoch:20 Batch:18 Loss:0.08690\n",
      "Epoch:40 Batch:18 Loss:0.06636\n",
      "Epoch:60 Batch:18 Loss:0.06158\n",
      "Epoch:80 Batch:18 Loss:0.05022\n",
      "Epoch:100 Batch:18 Loss:0.05150\n",
      "Epoch:120 Batch:18 Loss:0.04524\n",
      "Epoch:140 Batch:18 Loss:0.05013\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.075\n",
      "Epoch:10 Batch:8 Loss:0.074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 Batch:8 Loss:0.074\n",
      "Epoch:30 Batch:8 Loss:0.072\n",
      "Epoch:40 Batch:8 Loss:0.072\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 43.585949518901174 setps: 38 count: 38\n",
      "reward: 33.106673281743134 setps: 25 count: 63\n",
      "reward: 44.07779048430529 setps: 39 count: 102\n",
      "reward: 43.501293881238965 setps: 33 count: 135\n",
      "reward: 42.716502639184206 setps: 53 count: 188\n",
      "reward: 45.26670279820975 setps: 42 count: 230\n",
      "reward: 26.27150319763751 setps: 23 count: 253\n",
      "reward: 57.98890804741969 setps: 42 count: 295\n",
      "reward: 45.49464737191011 setps: 32 count: 327\n",
      "reward: 80.34257158827967 setps: 50 count: 377\n",
      "reward: 66.5992866565837 setps: 68 count: 445\n",
      "reward: 106.92929636495099 setps: 68 count: 513\n",
      "reward: 20.923348723517844 setps: 20 count: 533\n",
      "reward: 18.717793622113824 setps: 19 count: 552\n",
      "reward: 75.3414757214021 setps: 60 count: 612\n",
      "reward: 18.61729965000704 setps: 19 count: 631\n",
      "reward: 51.43465833039954 setps: 42 count: 673\n",
      "reward: 75.25080850398517 setps: 48 count: 721\n",
      "reward: 48.798715232266105 setps: 34 count: 755\n",
      "reward: 80.92135266882688 setps: 53 count: 808\n",
      "reward: 67.03233012377608 setps: 71 count: 879\n",
      "reward: 69.25735534699638 setps: 60 count: 939\n",
      "avg rewards: 52.82619380698432\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.29279\n",
      "Epoch:20 Batch:19 Loss:0.08540\n",
      "Epoch:40 Batch:19 Loss:0.05859\n",
      "Epoch:60 Batch:19 Loss:0.06025\n",
      "Epoch:80 Batch:19 Loss:0.05425\n",
      "Epoch:100 Batch:19 Loss:0.04868\n",
      "Epoch:120 Batch:19 Loss:0.05050\n",
      "Epoch:140 Batch:19 Loss:0.04593\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.070\n",
      "Epoch:10 Batch:8 Loss:0.069\n",
      "Epoch:20 Batch:8 Loss:0.070\n",
      "Epoch:30 Batch:8 Loss:0.069\n",
      "Epoch:40 Batch:8 Loss:0.070\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 48.8083495129802 setps: 32 count: 32\n",
      "reward: 63.40973191636876 setps: 40 count: 72\n",
      "reward: 48.6551248010801 setps: 32 count: 104\n",
      "reward: 38.84872749999776 setps: 28 count: 132\n",
      "reward: 70.02045034801267 setps: 46 count: 178\n",
      "reward: 44.45239136244636 setps: 31 count: 209\n",
      "reward: 51.299334153266685 setps: 33 count: 242\n",
      "reward: 43.656837888214795 setps: 31 count: 273\n",
      "reward: 51.391602124589554 setps: 38 count: 311\n",
      "reward: 35.199167632976604 setps: 28 count: 339\n",
      "reward: 69.98841881521803 setps: 43 count: 382\n",
      "reward: 60.86441117472423 setps: 48 count: 430\n",
      "reward: 49.55562343151395 setps: 33 count: 463\n",
      "reward: 44.36528211757686 setps: 32 count: 495\n",
      "reward: 44.253940733900535 setps: 31 count: 526\n",
      "reward: 42.502112359298906 setps: 29 count: 555\n",
      "reward: 20.912776898422457 setps: 19 count: 574\n",
      "reward: 20.149884938303146 setps: 19 count: 593\n",
      "reward: 59.83522735251754 setps: 40 count: 633\n",
      "reward: 39.68924381694377 setps: 30 count: 663\n",
      "reward: 84.68680862440961 setps: 53 count: 716\n",
      "reward: 56.37434669054928 setps: 39 count: 755\n",
      "reward: 43.031945972229 setps: 31 count: 786\n",
      "reward: 45.771476925563185 setps: 57 count: 843\n",
      "reward: 65.10756476408424 setps: 40 count: 883\n",
      "reward: 52.522815351805185 setps: 38 count: 921\n",
      "reward: 42.11811187041022 setps: 32 count: 953\n",
      "reward: 14.860734891713943 setps: 16 count: 969\n",
      "reward: 30.762469704581598 setps: 28 count: 997\n",
      "avg rewards: 47.69292805771376\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.27475\n",
      "Epoch:20 Batch:20 Loss:0.07205\n",
      "Epoch:40 Batch:20 Loss:0.06494\n",
      "Epoch:60 Batch:20 Loss:0.06095\n",
      "Epoch:80 Batch:20 Loss:0.04896\n",
      "Epoch:100 Batch:20 Loss:0.05114\n",
      "Epoch:120 Batch:20 Loss:0.04438\n",
      "Epoch:140 Batch:20 Loss:0.04690\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.070\n",
      "Epoch:10 Batch:8 Loss:0.070\n",
      "Epoch:20 Batch:8 Loss:0.069\n",
      "Epoch:30 Batch:8 Loss:0.065\n",
      "Epoch:40 Batch:8 Loss:0.070\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "env_list = [\"Pendulum-v0\", \"BipedalWalker-v3\", \"Walker2DBulletEnv-v0\", \"HopperBulletEnv-v0\", \"HalfCheetahBulletEnv-v0\", \"AntBulletEnv-v0\", \"HumanoidBulletEnv-v0\"]\n",
    "\n",
    "runs = 20\n",
    "inv_samples = 1000\n",
    "max_steps = 800\n",
    "expert_path='experts/'\n",
    "weight_path=\"weights/\"\n",
    "        \n",
    "test_rewards_envs = []\n",
    "record_folder = \"records/forward/\"\n",
    "init_seeds = [0,2,4,5]\n",
    "itr_per_env = len(init_seeds)\n",
    "\n",
    "for itr_id in range(itr_per_env):\n",
    "    seed = init_seeds[itr_id]\n",
    "    for en in env_list[1:]:\n",
    "        print(\"############# start \"+en+\" training ###################\")\n",
    "\n",
    "        ENV_NAME = en#env_list[3]\n",
    "        env=ENV_NAME\n",
    "        \n",
    "        DEMO_DIR = os.path.join(expert_path, env+'.pkl')\n",
    "        M = inv_samples\n",
    "\n",
    "        record_fn = record_folder + ENV_NAME + str(itr_id) + \".txt\"\n",
    "\n",
    "        \"\"\"load demonstrations\"\"\"\n",
    "        demos = load_demos(DEMO_DIR)\n",
    "\n",
    "        \"\"\"create environments\"\"\"\n",
    "        env = gym.make(ENV_NAME)\n",
    "        obs_dim, act_dim = env.observation_space.shape[0], env.action_space.shape[0]\n",
    "\n",
    "        \"\"\"init random seeds for reproduction\"\"\"\n",
    "        torch.manual_seed(seed)\n",
    "        env.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "\n",
    "        \"\"\"init models\"\"\"\n",
    "        policy = policy_multinomial(env.observation_space.shape[0],64,env.action_space.shape[0], n_heads=15, do_rate=0.04)#.cuda()\n",
    "        \n",
    "        #policy = MDN(obs_dim, out_features=act_dim, n_hidden=64,  num_gaussians=3)\n",
    "        #policy = policy_continuous(env.observation_space.shape[0],64,env.action_space.shape[0], uncertain=True)#.cuda()\n",
    "        #inv_model = MDN(in_features=obs_dim+act_dim, out_features=obs_dim, n_hidden=32,  num_gaussians=10)\n",
    "        inv_model = forward_dynamics_continuous(env.observation_space.shape[0],100,env.action_space.shape[0], uncertain=True, do_rate=0.08)#.cuda()\n",
    "        #inv_model = policy_multinomial(obs_dim+act_dim,100,obs_dim, n_heads=15)#.cuda()\n",
    "    \n",
    "        inv_model_best = None\n",
    "        reward_best = -1000\n",
    "\n",
    "        inv_dataset_list = []\n",
    "        use_policy = False\n",
    "\n",
    "        transitions = []\n",
    "        test_rewards = []\n",
    "        for steps in range(runs):\n",
    "            print('######## STEP %d #######'%(steps+1))\n",
    "            ### GET SAMPLES FOR LEARNING INVERSE MODEL\n",
    "            print('Collecting transitions for learning inverse model....')\n",
    "            if steps > 0:\n",
    "                use_policy = True\n",
    "\n",
    "\n",
    "            trans_samples, avg_reward = gen_inv_samples(env, policy.cpu(), M, 'continuous', use_policy, max_steps=max_steps)\n",
    "            transitions = transitions+trans_samples\n",
    "\n",
    "            f = open(record_fn, \"a+\")\n",
    "            f.write(str(avg_reward) + \"\\n\")\n",
    "            f.close()\n",
    "\n",
    "            \"\"\"\n",
    "            if len(transitions) > 92000:\n",
    "                transitions = random.sample(transitions,92000)\n",
    "            \"\"\"\n",
    "            test_rewards.append(avg_reward)\n",
    "            print('Done!', np.array(transitions).shape)\n",
    "\n",
    "            ### LEARN THE INVERSE MODEL\n",
    "            #inv_model = policy_multinomial(obs_dim+act_dim,100,obs_dim, n_heads=15)#.cuda()\n",
    "            inv_model = forward_dynamics_continuous(env.observation_space.shape[0],100,env.action_space.shape[0], uncertain=True, do_rate=0.08)#.cuda()\n",
    "            #inv_model = MDN(in_features=obs_dim+act_dim, out_features=obs_dim, n_hidden=32,  num_gaussians=10)#forward_dynamics_continuous(env.observation_space.shape[0],100,env.action_space.shape[0], uncertain=True)#.cuda()\n",
    "\n",
    "            print('Learning inverse model....')\n",
    "            inv_model = inv_model_training(transitions, inv_model,  ep_num=150)\n",
    "\n",
    "            ### GET ACTIONS FOR DEMOS\n",
    "            inv_model.cpu()\n",
    "            print('Getting labels for demos....')\n",
    "            trajs = get_state_labels(demos)\n",
    "            print('Done!')\n",
    "\n",
    "\n",
    "            ### PERFORM BEHAVIORAL CLONING\n",
    "            policy = train_bc(trajs, policy, inv_model, ep_num=50)\n",
    "\n",
    "        torch.save(policy, weight_path+ENV_NAME+str(itr_id)+'.pt')\n",
    "        test_rewards_envs.append(test_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cordless-principle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# start BipedalWalker-v3 training ###################\n",
      "(49,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:138: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -103.19613867788638 setps: 100 count: 100\n",
      "reward: -104.89378569665428 setps: 60 count: 160\n",
      "reward: -102.36979020965472 setps: 111 count: 271\n",
      "reward: -122.54565536505669 setps: 115 count: 386\n",
      "avg rewards: -108.25134248731301\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:78: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:1 Loss:0.33467\n",
      "Epoch:20 Batch:1 Loss:0.18029\n",
      "Epoch:40 Batch:1 Loss:0.15353\n",
      "Epoch:60 Batch:1 Loss:0.14662\n",
      "Epoch:80 Batch:1 Loss:0.13681\n",
      "Epoch:100 Batch:1 Loss:0.12147\n",
      "Epoch:120 Batch:1 Loss:0.10874\n",
      "Epoch:140 Batch:1 Loss:0.09751\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fafba648200>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1291, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:9 Loss:0.148\n",
      "Epoch:10 Batch:9 Loss:0.146\n",
      "Epoch:20 Batch:9 Loss:0.141\n",
      "Epoch:30 Batch:9 Loss:0.134\n",
      "Epoch:40 Batch:9 Loss:0.136\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -115.289026347061 setps: 62 count: 62\n",
      "reward: -114.86391151991486 setps: 62 count: 124\n",
      "reward: -113.78896209179237 setps: 61 count: 185\n",
      "reward: -113.57200884875655 setps: 63 count: 248\n",
      "reward: -113.9206204463467 setps: 65 count: 313\n",
      "reward: -113.64229823057727 setps: 62 count: 375\n",
      "reward: -113.88956239418127 setps: 60 count: 435\n",
      "reward: -113.01918142452526 setps: 60 count: 495\n",
      "reward: -114.24948945966301 setps: 63 count: 558\n",
      "reward: -114.53178908616304 setps: 61 count: 619\n",
      "reward: -113.77988213822866 setps: 62 count: 681\n",
      "reward: -114.52989998848489 setps: 59 count: 740\n",
      "reward: -113.91963853264475 setps: 61 count: 801\n",
      "reward: -113.93086992374373 setps: 62 count: 863\n",
      "reward: -114.93223042071921 setps: 61 count: 924\n",
      "reward: -113.80781915271841 setps: 62 count: 986\n",
      "avg rewards: -114.10419937534506\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:0.37176\n",
      "Epoch:20 Batch:2 Loss:0.16578\n",
      "Epoch:40 Batch:2 Loss:0.11863\n",
      "Epoch:60 Batch:2 Loss:0.08321\n",
      "Epoch:80 Batch:2 Loss:0.06710\n",
      "Epoch:100 Batch:2 Loss:0.05643\n",
      "Epoch:120 Batch:2 Loss:0.05053\n",
      "Epoch:140 Batch:2 Loss:0.04614\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.100\n",
      "Epoch:10 Batch:9 Loss:0.097\n",
      "Epoch:20 Batch:9 Loss:0.095\n",
      "Epoch:30 Batch:9 Loss:0.091\n",
      "Epoch:40 Batch:9 Loss:0.092\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -109.18977984342538 setps: 105 count: 105\n",
      "reward: -109.53430899815633 setps: 167 count: 272\n",
      "reward: -118.71932518137245 setps: 135 count: 407\n",
      "reward: -116.8783474985715 setps: 150 count: 557\n",
      "reward: -112.70119114204992 setps: 75 count: 632\n",
      "reward: -108.64941211711057 setps: 110 count: 742\n",
      "reward: -129.49320502856762 setps: 211 count: 953\n",
      "avg rewards: -115.02365282989341\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.37525\n",
      "Epoch:20 Batch:3 Loss:0.14204\n",
      "Epoch:40 Batch:3 Loss:0.08433\n",
      "Epoch:60 Batch:3 Loss:0.06334\n",
      "Epoch:80 Batch:3 Loss:0.05384\n",
      "Epoch:100 Batch:3 Loss:0.04844\n",
      "Epoch:120 Batch:3 Loss:0.04301\n",
      "Epoch:140 Batch:3 Loss:0.03887\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.085\n",
      "Epoch:10 Batch:9 Loss:0.085\n",
      "Epoch:20 Batch:9 Loss:0.082\n",
      "Epoch:30 Batch:9 Loss:0.080\n",
      "Epoch:40 Batch:9 Loss:0.083\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -26.548284686181674 setps: 800 count: 800\n",
      "avg rewards: -26.548284686181674\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.34043\n",
      "Epoch:20 Batch:4 Loss:0.11837\n",
      "Epoch:40 Batch:4 Loss:0.06334\n",
      "Epoch:60 Batch:4 Loss:0.04988\n",
      "Epoch:80 Batch:4 Loss:0.04140\n",
      "Epoch:100 Batch:4 Loss:0.03551\n",
      "Epoch:120 Batch:4 Loss:0.03456\n",
      "Epoch:140 Batch:4 Loss:0.02927\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.075\n",
      "Epoch:10 Batch:9 Loss:0.074\n",
      "Epoch:20 Batch:9 Loss:0.074\n",
      "Epoch:30 Batch:9 Loss:0.072\n",
      "Epoch:40 Batch:9 Loss:0.070\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -38.557295463218615 setps: 800 count: 800\n",
      "avg rewards: -38.557295463218615\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.32944\n",
      "Epoch:20 Batch:5 Loss:0.08056\n",
      "Epoch:40 Batch:5 Loss:0.05068\n",
      "Epoch:60 Batch:5 Loss:0.03764\n",
      "Epoch:80 Batch:5 Loss:0.03286\n",
      "Epoch:100 Batch:5 Loss:0.02825\n",
      "Epoch:120 Batch:5 Loss:0.02588\n",
      "Epoch:140 Batch:5 Loss:0.02562\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.067\n",
      "Epoch:10 Batch:9 Loss:0.066\n",
      "Epoch:20 Batch:9 Loss:0.064\n",
      "Epoch:30 Batch:9 Loss:0.061\n",
      "Epoch:40 Batch:9 Loss:0.061\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -125.28913995514189 setps: 64 count: 64\n",
      "reward: -119.50915921206027 setps: 60 count: 124\n",
      "reward: -119.36232296753488 setps: 54 count: 178\n",
      "reward: -119.81045938745513 setps: 57 count: 235\n",
      "reward: -121.3151077213871 setps: 58 count: 293\n",
      "reward: -122.00261803580884 setps: 56 count: 349\n",
      "reward: -107.00752034237857 setps: 49 count: 398\n",
      "reward: -118.65432725947474 setps: 57 count: 455\n",
      "reward: -115.16143542320094 setps: 58 count: 513\n",
      "reward: -122.78018177122064 setps: 93 count: 606\n",
      "reward: -115.57650155534533 setps: 59 count: 665\n",
      "reward: -114.63367578050679 setps: 55 count: 720\n",
      "reward: -120.4455900692058 setps: 57 count: 777\n",
      "reward: -121.0158641244173 setps: 83 count: 860\n",
      "reward: -121.81766310669792 setps: 61 count: 921\n",
      "reward: -119.59854994628579 setps: 71 count: 992\n",
      "avg rewards: -118.99875729113262\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.32019\n",
      "Epoch:20 Batch:6 Loss:0.07397\n",
      "Epoch:40 Batch:6 Loss:0.04402\n",
      "Epoch:60 Batch:6 Loss:0.03646\n",
      "Epoch:80 Batch:6 Loss:0.03238\n",
      "Epoch:100 Batch:6 Loss:0.02752\n",
      "Epoch:120 Batch:6 Loss:0.02629\n",
      "Epoch:140 Batch:6 Loss:0.02607\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.064\n",
      "Epoch:10 Batch:9 Loss:0.060\n",
      "Epoch:20 Batch:9 Loss:0.060\n",
      "Epoch:30 Batch:9 Loss:0.059\n",
      "Epoch:40 Batch:9 Loss:0.065\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -112.05893589409752 setps: 69 count: 69\n",
      "reward: -111.97249238436173 setps: 70 count: 139\n",
      "reward: -127.57308932578626 setps: 94 count: 233\n",
      "reward: -111.91819049946342 setps: 74 count: 307\n",
      "reward: -110.80163640441683 setps: 70 count: 377\n",
      "reward: -110.63488126507588 setps: 68 count: 445\n",
      "reward: -113.11489629791429 setps: 66 count: 511\n",
      "reward: -112.96700630566664 setps: 67 count: 578\n",
      "reward: -116.87178623830279 setps: 68 count: 646\n",
      "reward: -111.22354282015749 setps: 71 count: 717\n",
      "reward: -110.92235316170627 setps: 71 count: 788\n",
      "reward: -110.92048539926857 setps: 70 count: 858\n",
      "reward: -114.80709800838059 setps: 69 count: 927\n",
      "reward: -115.80471286980497 setps: 63 count: 990\n",
      "avg rewards: -113.68507906245736\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.33470\n",
      "Epoch:20 Batch:7 Loss:0.06896\n",
      "Epoch:40 Batch:7 Loss:0.04668\n",
      "Epoch:60 Batch:7 Loss:0.03484\n",
      "Epoch:80 Batch:7 Loss:0.03047\n",
      "Epoch:100 Batch:7 Loss:0.02746\n",
      "Epoch:120 Batch:7 Loss:0.02641\n",
      "Epoch:140 Batch:7 Loss:0.02599\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.067\n",
      "Epoch:10 Batch:9 Loss:0.062\n",
      "Epoch:20 Batch:9 Loss:0.064\n",
      "Epoch:30 Batch:9 Loss:0.062\n",
      "Epoch:40 Batch:9 Loss:0.064\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -112.11834809955086 setps: 68 count: 68\n",
      "reward: -116.74194371872643 setps: 71 count: 139\n",
      "reward: -113.44690716835608 setps: 74 count: 213\n",
      "reward: -118.56541088002268 setps: 80 count: 293\n",
      "reward: -128.0362406314357 setps: 91 count: 384\n",
      "reward: -111.21608051726719 setps: 68 count: 452\n",
      "reward: -112.95522359596876 setps: 69 count: 521\n",
      "reward: -115.95108499206603 setps: 74 count: 595\n",
      "reward: -120.0817931999856 setps: 79 count: 674\n",
      "reward: -113.41652958406819 setps: 72 count: 746\n",
      "reward: -110.18999069611728 setps: 55 count: 801\n",
      "reward: -115.58905746063839 setps: 81 count: 882\n",
      "reward: -127.59337351166394 setps: 87 count: 969\n",
      "avg rewards: -116.6078449273744\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.32392\n",
      "Epoch:20 Batch:8 Loss:0.06694\n",
      "Epoch:40 Batch:8 Loss:0.03838\n",
      "Epoch:60 Batch:8 Loss:0.02995\n",
      "Epoch:80 Batch:8 Loss:0.02844\n",
      "Epoch:100 Batch:8 Loss:0.02779\n",
      "Epoch:120 Batch:8 Loss:0.02501\n",
      "Epoch:140 Batch:8 Loss:0.02614\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.063\n",
      "Epoch:10 Batch:9 Loss:0.063\n",
      "Epoch:20 Batch:9 Loss:0.063\n",
      "Epoch:30 Batch:9 Loss:0.063\n",
      "Epoch:40 Batch:9 Loss:0.064\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -101.50287060887925 setps: 87 count: 87\n",
      "reward: -122.95035674692255 setps: 89 count: 176\n",
      "reward: -103.67679353876102 setps: 88 count: 264\n",
      "reward: -114.87663988364177 setps: 75 count: 339\n",
      "avg rewards: -110.75166519455114\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.31663\n",
      "Epoch:20 Batch:9 Loss:0.06106\n",
      "Epoch:40 Batch:9 Loss:0.04086\n",
      "Epoch:60 Batch:9 Loss:0.03416\n",
      "Epoch:80 Batch:9 Loss:0.02893\n",
      "Epoch:100 Batch:9 Loss:0.02763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:120 Batch:9 Loss:0.02694\n",
      "Epoch:140 Batch:9 Loss:0.02580\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.058\n",
      "Epoch:10 Batch:9 Loss:0.057\n",
      "Epoch:20 Batch:9 Loss:0.055\n",
      "Epoch:30 Batch:9 Loss:0.057\n",
      "Epoch:40 Batch:9 Loss:0.055\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -116.4617619377145 setps: 93 count: 93\n",
      "reward: -108.15232131116278 setps: 71 count: 164\n",
      "reward: -110.31024516583793 setps: 69 count: 233\n",
      "reward: -122.9642459450358 setps: 84 count: 317\n",
      "reward: -121.83705354645166 setps: 78 count: 395\n",
      "reward: -108.62232034457537 setps: 70 count: 465\n",
      "reward: -122.01839997730465 setps: 84 count: 549\n",
      "reward: -122.15998152402044 setps: 88 count: 637\n",
      "reward: -123.19304884310625 setps: 93 count: 730\n",
      "reward: -108.78905509166172 setps: 65 count: 795\n",
      "reward: -111.27757366246979 setps: 74 count: 869\n",
      "reward: -121.74329396560788 setps: 96 count: 965\n",
      "avg rewards: -116.46077510957907\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.31538\n",
      "Epoch:20 Batch:10 Loss:0.05286\n",
      "Epoch:40 Batch:10 Loss:0.03445\n",
      "Epoch:60 Batch:10 Loss:0.02877\n",
      "Epoch:80 Batch:10 Loss:0.02729\n",
      "Epoch:100 Batch:10 Loss:0.02588\n",
      "Epoch:120 Batch:10 Loss:0.02547\n",
      "Epoch:140 Batch:10 Loss:0.02446\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.058\n",
      "Epoch:10 Batch:9 Loss:0.053\n",
      "Epoch:20 Batch:9 Loss:0.056\n",
      "Epoch:30 Batch:9 Loss:0.055\n",
      "Epoch:40 Batch:9 Loss:0.054\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -94.25406004317787 setps: 138 count: 138\n",
      "reward: 28.617295052489265 setps: 800 count: 938\n",
      "avg rewards: -32.8183824953443\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.27977\n",
      "Epoch:20 Batch:11 Loss:0.04606\n",
      "Epoch:40 Batch:11 Loss:0.03505\n",
      "Epoch:60 Batch:11 Loss:0.02964\n",
      "Epoch:80 Batch:11 Loss:0.02647\n",
      "Epoch:100 Batch:11 Loss:0.02713\n",
      "Epoch:120 Batch:11 Loss:0.02574\n",
      "Epoch:140 Batch:11 Loss:0.02378\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.051\n",
      "Epoch:10 Batch:9 Loss:0.047\n",
      "Epoch:20 Batch:9 Loss:0.042\n",
      "Epoch:30 Batch:9 Loss:0.047\n",
      "Epoch:40 Batch:9 Loss:0.045\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -122.26777639648009 setps: 109 count: 109\n",
      "reward: -29.97383691639266 setps: 800 count: 909\n",
      "avg rewards: -76.12080665643637\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.29803\n",
      "Epoch:20 Batch:12 Loss:0.05090\n",
      "Epoch:40 Batch:12 Loss:0.03574\n",
      "Epoch:60 Batch:12 Loss:0.02997\n",
      "Epoch:80 Batch:12 Loss:0.02758\n",
      "Epoch:100 Batch:12 Loss:0.02609\n",
      "Epoch:120 Batch:12 Loss:0.02393\n",
      "Epoch:140 Batch:12 Loss:0.02450\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.047\n",
      "Epoch:10 Batch:9 Loss:0.046\n",
      "Epoch:20 Batch:9 Loss:0.045\n",
      "Epoch:30 Batch:9 Loss:0.045\n",
      "Epoch:40 Batch:9 Loss:0.045\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -120.9384053250514 setps: 94 count: 94\n",
      "reward: -55.61899178034067 setps: 249 count: 343\n",
      "reward: -100.28533077734708 setps: 131 count: 474\n",
      "reward: -119.64884067141823 setps: 67 count: 541\n",
      "reward: -109.91868775488373 setps: 48 count: 589\n",
      "reward: -107.06047342559572 setps: 46 count: 635\n",
      "avg rewards: -102.24512162243946\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.27488\n",
      "Epoch:20 Batch:13 Loss:0.04760\n",
      "Epoch:40 Batch:13 Loss:0.03293\n",
      "Epoch:60 Batch:13 Loss:0.02771\n",
      "Epoch:80 Batch:13 Loss:0.02829\n",
      "Epoch:100 Batch:13 Loss:0.02603\n",
      "Epoch:120 Batch:13 Loss:0.02445\n",
      "Epoch:140 Batch:13 Loss:0.02289\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.040\n",
      "Epoch:10 Batch:9 Loss:0.035\n",
      "Epoch:20 Batch:9 Loss:0.037\n",
      "Epoch:30 Batch:9 Loss:0.039\n",
      "Epoch:40 Batch:9 Loss:0.031\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -10.717307701212043 setps: 800 count: 800\n",
      "avg rewards: -10.717307701212043\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.28355\n",
      "Epoch:20 Batch:14 Loss:0.04646\n",
      "Epoch:40 Batch:14 Loss:0.03365\n",
      "Epoch:60 Batch:14 Loss:0.03052\n",
      "Epoch:80 Batch:14 Loss:0.02866\n",
      "Epoch:100 Batch:14 Loss:0.02618\n",
      "Epoch:120 Batch:14 Loss:0.02718\n",
      "Epoch:140 Batch:14 Loss:0.02436\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.038\n",
      "Epoch:10 Batch:9 Loss:0.038\n",
      "Epoch:20 Batch:9 Loss:0.040\n",
      "Epoch:30 Batch:9 Loss:0.038\n",
      "Epoch:40 Batch:9 Loss:0.038\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 92.83972603449226 setps: 800 count: 800\n",
      "avg rewards: 92.83972603449226\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.25855\n",
      "Epoch:20 Batch:15 Loss:0.04555\n",
      "Epoch:40 Batch:15 Loss:0.03402\n",
      "Epoch:60 Batch:15 Loss:0.03178\n",
      "Epoch:80 Batch:15 Loss:0.02823\n",
      "Epoch:100 Batch:15 Loss:0.02709\n",
      "Epoch:120 Batch:15 Loss:0.02662\n",
      "Epoch:140 Batch:15 Loss:0.02505\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.036\n",
      "Epoch:10 Batch:9 Loss:0.034\n",
      "Epoch:20 Batch:9 Loss:0.035\n",
      "Epoch:30 Batch:9 Loss:0.037\n",
      "Epoch:40 Batch:9 Loss:0.039\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 96.37792972495835 setps: 800 count: 800\n",
      "reward: -122.39291079543594 setps: 102 count: 902\n",
      "avg rewards: -13.007490535238794\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.24927\n",
      "Epoch:20 Batch:16 Loss:0.04286\n",
      "Epoch:40 Batch:16 Loss:0.03122\n",
      "Epoch:60 Batch:16 Loss:0.03152\n",
      "Epoch:80 Batch:16 Loss:0.02935\n",
      "Epoch:100 Batch:16 Loss:0.02545\n",
      "Epoch:120 Batch:16 Loss:0.02614\n",
      "Epoch:140 Batch:16 Loss:0.02351\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.037\n",
      "Epoch:10 Batch:9 Loss:0.034\n",
      "Epoch:20 Batch:9 Loss:0.034\n",
      "Epoch:30 Batch:9 Loss:0.033\n",
      "Epoch:40 Batch:9 Loss:0.031\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 39.51542819153614 setps: 800 count: 800\n",
      "avg rewards: 39.51542819153614\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.24947\n",
      "Epoch:20 Batch:17 Loss:0.04481\n",
      "Epoch:40 Batch:17 Loss:0.03166\n",
      "Epoch:60 Batch:17 Loss:0.02932\n",
      "Epoch:80 Batch:17 Loss:0.02602\n",
      "Epoch:100 Batch:17 Loss:0.02704\n",
      "Epoch:120 Batch:17 Loss:0.02331\n",
      "Epoch:140 Batch:17 Loss:0.02444\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.033\n",
      "Epoch:10 Batch:9 Loss:0.030\n",
      "Epoch:20 Batch:9 Loss:0.032\n",
      "Epoch:30 Batch:9 Loss:0.034\n",
      "Epoch:40 Batch:9 Loss:0.030\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 100.21274225374393 setps: 800 count: 800\n",
      "avg rewards: 100.21274225374393\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.24950\n",
      "Epoch:20 Batch:18 Loss:0.04286\n",
      "Epoch:40 Batch:18 Loss:0.03385\n",
      "Epoch:60 Batch:18 Loss:0.02860\n",
      "Epoch:80 Batch:18 Loss:0.02776\n",
      "Epoch:100 Batch:18 Loss:0.02504\n",
      "Epoch:120 Batch:18 Loss:0.02586\n",
      "Epoch:140 Batch:18 Loss:0.02353\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.032\n",
      "Epoch:10 Batch:9 Loss:0.030\n",
      "Epoch:20 Batch:9 Loss:0.037\n",
      "Epoch:30 Batch:9 Loss:0.032\n",
      "Epoch:40 Batch:9 Loss:0.031\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 119.37750054289086 setps: 800 count: 800\n",
      "avg rewards: 119.37750054289086\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.24985\n",
      "Epoch:20 Batch:19 Loss:0.04222\n",
      "Epoch:40 Batch:19 Loss:0.03202\n",
      "Epoch:60 Batch:19 Loss:0.03080\n",
      "Epoch:80 Batch:19 Loss:0.02789\n",
      "Epoch:100 Batch:19 Loss:0.02507\n",
      "Epoch:120 Batch:19 Loss:0.02531\n",
      "Epoch:140 Batch:19 Loss:0.02330\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.029\n",
      "Epoch:10 Batch:9 Loss:0.030\n",
      "Epoch:20 Batch:9 Loss:0.032\n",
      "Epoch:30 Batch:9 Loss:0.033\n",
      "Epoch:40 Batch:9 Loss:0.032\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 149.89636263277387 setps: 800 count: 800\n",
      "avg rewards: 149.89636263277387\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.21148\n",
      "Epoch:20 Batch:20 Loss:0.04173\n",
      "Epoch:40 Batch:20 Loss:0.02982\n",
      "Epoch:60 Batch:20 Loss:0.02713\n",
      "Epoch:80 Batch:20 Loss:0.02576\n",
      "Epoch:100 Batch:20 Loss:0.02474\n",
      "Epoch:120 Batch:20 Loss:0.02524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:140 Batch:20 Loss:0.02492\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.028\n",
      "Epoch:10 Batch:9 Loss:0.028\n",
      "Epoch:20 Batch:9 Loss:0.029\n",
      "Epoch:30 Batch:9 Loss:0.028\n",
      "Epoch:40 Batch:9 Loss:0.032\n",
      "Done!\n",
      "############# start Walker2DBulletEnv-v0 training ###################\n",
      "(49, 1000, 22)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 13.612366641615518 setps: 9 count: 9\n",
      "reward: 15.333661909894728 setps: 11 count: 20\n",
      "reward: 14.06518672312959 setps: 10 count: 30\n",
      "reward: 22.825537883289506 setps: 26 count: 56\n",
      "reward: 20.030772349816107 setps: 20 count: 76\n",
      "reward: 16.999050246212573 setps: 16 count: 92\n",
      "reward: 15.545413191551052 setps: 14 count: 106\n",
      "reward: 16.226392472333103 setps: 13 count: 119\n",
      "reward: 14.878472657156816 setps: 18 count: 137\n",
      "reward: 17.093303792305232 setps: 15 count: 152\n",
      "reward: 25.38579114184395 setps: 28 count: 180\n",
      "reward: 15.62311114539043 setps: 12 count: 192\n",
      "reward: 25.67762851235602 setps: 21 count: 213\n",
      "reward: 27.122905671574703 setps: 28 count: 241\n",
      "reward: 16.43561664125446 setps: 12 count: 253\n",
      "reward: 19.666378375614293 setps: 19 count: 272\n",
      "reward: 13.895775478085852 setps: 9 count: 281\n",
      "reward: 15.099087875016266 setps: 10 count: 291\n",
      "reward: 17.484739339732915 setps: 15 count: 306\n",
      "reward: 16.458311988427884 setps: 13 count: 319\n",
      "reward: 24.631510851769416 setps: 23 count: 342\n",
      "reward: 17.27214320818748 setps: 13 count: 355\n",
      "reward: 14.835133212765506 setps: 11 count: 366\n",
      "reward: 15.280416195635917 setps: 11 count: 377\n",
      "reward: 24.255114293211953 setps: 25 count: 402\n",
      "reward: 16.724412600457438 setps: 12 count: 414\n",
      "reward: 17.03025952757744 setps: 13 count: 427\n",
      "reward: 14.916645828922626 setps: 11 count: 438\n",
      "reward: 14.978043961331423 setps: 10 count: 448\n",
      "reward: 16.548853618519203 setps: 17 count: 465\n",
      "reward: 15.808945510665946 setps: 12 count: 477\n",
      "reward: 19.61333122198557 setps: 19 count: 496\n",
      "reward: 17.831681481759002 setps: 15 count: 511\n",
      "reward: 19.90174298195634 setps: 19 count: 530\n",
      "reward: 18.793621826905294 setps: 15 count: 545\n",
      "reward: 13.583168764971196 setps: 9 count: 554\n",
      "reward: 15.982339450970178 setps: 12 count: 566\n",
      "reward: 13.074903366122454 setps: 8 count: 574\n",
      "reward: 17.861683710719806 setps: 13 count: 587\n",
      "reward: 14.566726174604263 setps: 13 count: 600\n",
      "reward: 15.696694286954878 setps: 13 count: 613\n",
      "reward: 19.669800819159715 setps: 15 count: 628\n",
      "reward: 16.03928580185311 setps: 11 count: 639\n",
      "reward: 15.914378718014632 setps: 11 count: 650\n",
      "reward: 13.102440853048757 setps: 7 count: 657\n",
      "reward: 17.618380022801286 setps: 16 count: 673\n",
      "reward: 19.715804762375775 setps: 15 count: 688\n",
      "reward: 17.94201492472348 setps: 13 count: 701\n",
      "reward: 12.085757447723882 setps: 8 count: 709\n",
      "reward: 15.273487867224322 setps: 11 count: 720\n",
      "reward: 17.834171069120927 setps: 18 count: 738\n",
      "reward: 18.715817689345567 setps: 17 count: 755\n",
      "reward: 14.797437266143971 setps: 12 count: 767\n",
      "reward: 12.104907780767828 setps: 8 count: 775\n",
      "reward: 16.28500750227686 setps: 12 count: 787\n",
      "reward: 26.443078579950086 setps: 33 count: 820\n",
      "reward: 21.864974904285916 setps: 21 count: 841\n",
      "reward: 16.940205392897767 setps: 13 count: 854\n",
      "reward: 15.434112157528578 setps: 12 count: 866\n",
      "reward: 12.747652808242128 setps: 8 count: 874\n",
      "reward: 24.63804462880071 setps: 39 count: 913\n",
      "reward: 13.530255509416747 setps: 8 count: 921\n",
      "reward: 19.585226617201993 setps: 22 count: 943\n",
      "reward: 21.90765792617749 setps: 24 count: 967\n",
      "reward: 17.693504198580918 setps: 15 count: 982\n",
      "reward: 14.033769243680581 setps: 8 count: 990\n",
      "reward: 13.813665551440499 setps: 10 count: 1000\n",
      "avg rewards: 17.378771882916087\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:0.34094\n",
      "Epoch:20 Batch:1 Loss:0.12192\n",
      "Epoch:40 Batch:1 Loss:0.10348\n",
      "Epoch:60 Batch:1 Loss:0.09484\n",
      "Epoch:80 Batch:1 Loss:0.08193\n",
      "Epoch:100 Batch:1 Loss:0.06587\n",
      "Epoch:120 Batch:1 Loss:0.05323\n",
      "Epoch:140 Batch:1 Loss:0.04731\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.125\n",
      "Epoch:10 Batch:10 Loss:0.121\n",
      "Epoch:20 Batch:10 Loss:0.124\n",
      "Epoch:30 Batch:10 Loss:0.124\n",
      "Epoch:40 Batch:10 Loss:0.121\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 67.01229866425564 setps: 87 count: 87\n",
      "reward: 23.821809403611407 setps: 69 count: 156\n",
      "reward: 72.90737885503187 setps: 94 count: 250\n",
      "reward: 99.356371117587 setps: 138 count: 388\n",
      "reward: 85.02373600691647 setps: 110 count: 498\n",
      "avg rewards: 69.62431880948047\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:0.33288\n",
      "Epoch:20 Batch:2 Loss:0.07647\n",
      "Epoch:40 Batch:2 Loss:0.07065\n",
      "Epoch:60 Batch:2 Loss:0.05171\n",
      "Epoch:80 Batch:2 Loss:0.03814\n",
      "Epoch:100 Batch:2 Loss:0.02989\n",
      "Epoch:120 Batch:2 Loss:0.02493\n",
      "Epoch:140 Batch:2 Loss:0.02381\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.102\n",
      "Epoch:10 Batch:10 Loss:0.098\n",
      "Epoch:20 Batch:10 Loss:0.096\n",
      "Epoch:30 Batch:10 Loss:0.097\n",
      "Epoch:40 Batch:10 Loss:0.098\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 17.07327902412071 setps: 62 count: 62\n",
      "reward: 33.44954490248201 setps: 83 count: 145\n",
      "reward: 283.6322619179686 setps: 528 count: 673\n",
      "reward: 37.96639827884939 setps: 87 count: 760\n",
      "reward: 32.438154305086925 setps: 90 count: 850\n",
      "reward: 17.121724219580944 setps: 61 count: 911\n",
      "reward: 32.047063241677805 setps: 89 count: 1000\n",
      "avg rewards: 64.8183465556809\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.33187\n",
      "Epoch:20 Batch:3 Loss:0.05402\n",
      "Epoch:40 Batch:3 Loss:0.04266\n",
      "Epoch:60 Batch:3 Loss:0.03070\n",
      "Epoch:80 Batch:3 Loss:0.02303\n",
      "Epoch:100 Batch:3 Loss:0.01943\n",
      "Epoch:120 Batch:3 Loss:0.01641\n",
      "Epoch:140 Batch:3 Loss:0.01524\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.089\n",
      "Epoch:10 Batch:10 Loss:0.089\n",
      "Epoch:20 Batch:10 Loss:0.089\n",
      "Epoch:30 Batch:10 Loss:0.090\n",
      "Epoch:40 Batch:10 Loss:0.090\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 5.491541226282423 setps: 52 count: 52\n",
      "reward: 6.526923514126974 setps: 52 count: 104\n",
      "reward: 14.283578738842348 setps: 61 count: 165\n",
      "reward: 19.889347063607424 setps: 74 count: 239\n",
      "reward: 8.985805141518355 setps: 56 count: 295\n",
      "reward: 9.846694193266735 setps: 59 count: 354\n",
      "reward: 6.132974453085625 setps: 53 count: 407\n",
      "reward: 12.687606006980058 setps: 64 count: 471\n",
      "reward: 26.424412788110203 setps: 98 count: 569\n",
      "reward: 5.673770680729649 setps: 53 count: 622\n",
      "reward: 15.502958613063674 setps: 77 count: 699\n",
      "reward: 9.773455920811095 setps: 59 count: 758\n",
      "reward: 18.32861720622749 setps: 68 count: 826\n",
      "reward: 10.964807943894993 setps: 55 count: 881\n",
      "reward: 5.164985909342068 setps: 49 count: 930\n",
      "avg rewards: 11.71183195999261\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.30760\n",
      "Epoch:20 Batch:4 Loss:0.05087\n",
      "Epoch:40 Batch:4 Loss:0.03488\n",
      "Epoch:60 Batch:4 Loss:0.02469\n",
      "Epoch:80 Batch:4 Loss:0.01911\n",
      "Epoch:100 Batch:4 Loss:0.01546\n",
      "Epoch:120 Batch:4 Loss:0.01404\n",
      "Epoch:140 Batch:4 Loss:0.01333\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.089\n",
      "Epoch:10 Batch:10 Loss:0.087\n",
      "Epoch:20 Batch:10 Loss:0.087\n",
      "Epoch:30 Batch:10 Loss:0.087\n",
      "Epoch:40 Batch:10 Loss:0.086\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 7.462037709784626 setps: 57 count: 57\n",
      "reward: 13.75984184157277 setps: 61 count: 118\n",
      "reward: 9.967084344530303 setps: 50 count: 168\n",
      "reward: 10.607922371085436 setps: 60 count: 228\n",
      "reward: 11.081163104674488 setps: 64 count: 292\n",
      "reward: 8.711604263537453 setps: 56 count: 348\n",
      "reward: 11.65859515112679 setps: 66 count: 414\n",
      "reward: 8.169535612940674 setps: 57 count: 471\n",
      "reward: 8.093649133988944 setps: 58 count: 529\n",
      "reward: 12.326697135287386 setps: 65 count: 594\n",
      "reward: 11.650917716800183 setps: 65 count: 659\n",
      "reward: 11.674521847428693 setps: 51 count: 710\n",
      "reward: 11.081929732440035 setps: 64 count: 774\n",
      "reward: 14.765473279262363 setps: 64 count: 838\n",
      "reward: 15.421765747280736 setps: 74 count: 912\n",
      "reward: 7.176055104708942 setps: 59 count: 971\n",
      "avg rewards: 10.850549631028114\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.29851\n",
      "Epoch:20 Batch:5 Loss:0.04238\n",
      "Epoch:40 Batch:5 Loss:0.02582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:60 Batch:5 Loss:0.01977\n",
      "Epoch:80 Batch:5 Loss:0.01523\n",
      "Epoch:100 Batch:5 Loss:0.01309\n",
      "Epoch:120 Batch:5 Loss:0.01255\n",
      "Epoch:140 Batch:5 Loss:0.01266\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.087\n",
      "Epoch:10 Batch:10 Loss:0.083\n",
      "Epoch:20 Batch:10 Loss:0.085\n",
      "Epoch:30 Batch:10 Loss:0.086\n",
      "Epoch:40 Batch:10 Loss:0.084\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 30.740162689250422 setps: 85 count: 85\n",
      "reward: 43.12754418812985 setps: 106 count: 191\n",
      "reward: 16.30146709723922 setps: 62 count: 253\n",
      "reward: 31.83758047574519 setps: 87 count: 340\n",
      "reward: 29.49511345850188 setps: 82 count: 422\n",
      "avg rewards: 30.300373581773318\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.27329\n",
      "Epoch:20 Batch:6 Loss:0.03404\n",
      "Epoch:40 Batch:6 Loss:0.01900\n",
      "Epoch:60 Batch:6 Loss:0.01499\n",
      "Epoch:80 Batch:6 Loss:0.01195\n",
      "Epoch:100 Batch:6 Loss:0.01187\n",
      "Epoch:120 Batch:6 Loss:0.00949\n",
      "Epoch:140 Batch:6 Loss:0.00947\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.081\n",
      "Epoch:10 Batch:10 Loss:0.080\n",
      "Epoch:20 Batch:10 Loss:0.079\n",
      "Epoch:30 Batch:10 Loss:0.081\n",
      "Epoch:40 Batch:10 Loss:0.080\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 38.491365533173656 setps: 59 count: 59\n",
      "reward: 76.91798868327892 setps: 94 count: 153\n",
      "reward: 43.90359194850462 setps: 85 count: 238\n",
      "reward: 29.32165196318383 setps: 29 count: 267\n",
      "reward: 49.41534323985833 setps: 82 count: 349\n",
      "reward: 29.388151651216322 setps: 71 count: 420\n",
      "reward: 27.37470664382854 setps: 58 count: 478\n",
      "reward: 32.375103708459946 setps: 78 count: 556\n",
      "reward: 32.19293563384417 setps: 28 count: 584\n",
      "reward: 39.8849520367483 setps: 52 count: 636\n",
      "reward: 27.196596869177302 setps: 25 count: 661\n",
      "reward: 18.43707148277753 setps: 52 count: 713\n",
      "reward: 23.3998794787636 setps: 60 count: 773\n",
      "reward: 40.71163419897202 setps: 52 count: 825\n",
      "reward: 30.2181652533487 setps: 37 count: 862\n",
      "reward: 25.402907103253526 setps: 42 count: 904\n",
      "reward: 21.247167970419113 setps: 54 count: 958\n",
      "reward: 29.157429798707025 setps: 36 count: 994\n",
      "avg rewards: 34.16870239986197\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.27687\n",
      "Epoch:20 Batch:7 Loss:0.03452\n",
      "Epoch:40 Batch:7 Loss:0.01604\n",
      "Epoch:60 Batch:7 Loss:0.01385\n",
      "Epoch:80 Batch:7 Loss:0.01142\n",
      "Epoch:100 Batch:7 Loss:0.01061\n",
      "Epoch:120 Batch:7 Loss:0.00928\n",
      "Epoch:140 Batch:7 Loss:0.00940\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.078\n",
      "Epoch:10 Batch:10 Loss:0.079\n",
      "Epoch:20 Batch:10 Loss:0.075\n",
      "Epoch:30 Batch:10 Loss:0.076\n",
      "Epoch:40 Batch:10 Loss:0.076\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 36.32886887600036 setps: 86 count: 86\n",
      "reward: 18.784790388669347 setps: 64 count: 150\n",
      "reward: 27.20521177214105 setps: 79 count: 229\n",
      "reward: 21.78710020653816 setps: 73 count: 302\n",
      "reward: 24.427992029154844 setps: 67 count: 369\n",
      "reward: 28.79911493623222 setps: 83 count: 452\n",
      "avg rewards: 26.222179701455996\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.27317\n",
      "Epoch:20 Batch:8 Loss:0.02624\n",
      "Epoch:40 Batch:8 Loss:0.01443\n",
      "Epoch:60 Batch:8 Loss:0.01206\n",
      "Epoch:80 Batch:8 Loss:0.01150\n",
      "Epoch:100 Batch:8 Loss:0.00877\n",
      "Epoch:120 Batch:8 Loss:0.00844\n",
      "Epoch:140 Batch:8 Loss:0.00823\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.074\n",
      "Epoch:10 Batch:10 Loss:0.074\n",
      "Epoch:20 Batch:10 Loss:0.073\n",
      "Epoch:30 Batch:10 Loss:0.074\n",
      "Epoch:40 Batch:10 Loss:0.074\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 45.58465961757465 setps: 60 count: 60\n",
      "reward: 55.459273257873434 setps: 81 count: 141\n",
      "reward: 47.27392705614619 setps: 62 count: 203\n",
      "reward: 83.52503021657756 setps: 133 count: 336\n",
      "reward: 47.019239402306276 setps: 64 count: 400\n",
      "avg rewards: 55.77242591009563\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.26770\n",
      "Epoch:20 Batch:9 Loss:0.02292\n",
      "Epoch:40 Batch:9 Loss:0.01457\n",
      "Epoch:60 Batch:9 Loss:0.01285\n",
      "Epoch:80 Batch:9 Loss:0.00939\n",
      "Epoch:100 Batch:9 Loss:0.00848\n",
      "Epoch:120 Batch:9 Loss:0.00749\n",
      "Epoch:140 Batch:9 Loss:0.00720\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.073\n",
      "Epoch:10 Batch:10 Loss:0.075\n",
      "Epoch:20 Batch:10 Loss:0.072\n",
      "Epoch:30 Batch:10 Loss:0.074\n",
      "Epoch:40 Batch:10 Loss:0.074\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 54.525427469266276 setps: 74 count: 74\n",
      "reward: 49.89391736201796 setps: 74 count: 148\n",
      "reward: 58.06230889667202 setps: 80 count: 228\n",
      "reward: 52.885797056496095 setps: 128 count: 356\n",
      "avg rewards: 53.84186269611309\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.23902\n",
      "Epoch:20 Batch:10 Loss:0.02386\n",
      "Epoch:40 Batch:10 Loss:0.01371\n",
      "Epoch:60 Batch:10 Loss:0.01090\n",
      "Epoch:80 Batch:10 Loss:0.00886\n",
      "Epoch:100 Batch:10 Loss:0.00756\n",
      "Epoch:120 Batch:10 Loss:0.00731\n",
      "Epoch:140 Batch:10 Loss:0.00633\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.078\n",
      "Epoch:10 Batch:10 Loss:0.076\n",
      "Epoch:20 Batch:10 Loss:0.074\n",
      "Epoch:30 Batch:10 Loss:0.077\n",
      "Epoch:40 Batch:10 Loss:0.076\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 68.18588532877475 setps: 79 count: 79\n",
      "reward: 48.90287434137862 setps: 57 count: 136\n",
      "reward: 35.853591109209816 setps: 80 count: 216\n",
      "reward: 62.57775666442758 setps: 72 count: 288\n",
      "reward: 77.18484692518251 setps: 89 count: 377\n",
      "avg rewards: 58.540990873794655\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.24539\n",
      "Epoch:20 Batch:11 Loss:0.02136\n",
      "Epoch:40 Batch:11 Loss:0.01124\n",
      "Epoch:60 Batch:11 Loss:0.01037\n",
      "Epoch:80 Batch:11 Loss:0.00767\n",
      "Epoch:100 Batch:11 Loss:0.00658\n",
      "Epoch:120 Batch:11 Loss:0.00617\n",
      "Epoch:140 Batch:11 Loss:0.00663\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.078\n",
      "Epoch:10 Batch:10 Loss:0.078\n",
      "Epoch:20 Batch:10 Loss:0.076\n",
      "Epoch:30 Batch:10 Loss:0.077\n",
      "Epoch:40 Batch:10 Loss:0.077\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 73.5928242390859 setps: 82 count: 82\n",
      "reward: 101.98612308493318 setps: 120 count: 202\n",
      "reward: 72.40804560965915 setps: 82 count: 284\n",
      "reward: 25.416319025633857 setps: 67 count: 351\n",
      "reward: 73.84094772710374 setps: 85 count: 436\n",
      "reward: 74.97535164181028 setps: 83 count: 519\n",
      "reward: 60.169369026340405 setps: 65 count: 584\n",
      "avg rewards: 68.91271147922379\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.26807\n",
      "Epoch:20 Batch:12 Loss:0.02116\n",
      "Epoch:40 Batch:12 Loss:0.01156\n",
      "Epoch:60 Batch:12 Loss:0.00820\n",
      "Epoch:80 Batch:12 Loss:0.00803\n",
      "Epoch:100 Batch:12 Loss:0.00733\n",
      "Epoch:120 Batch:12 Loss:0.00642\n",
      "Epoch:140 Batch:12 Loss:0.00530\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.080\n",
      "Epoch:10 Batch:10 Loss:0.080\n",
      "Epoch:20 Batch:10 Loss:0.078\n",
      "Epoch:30 Batch:10 Loss:0.079\n",
      "Epoch:40 Batch:10 Loss:0.078\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 31.550777527151507 setps: 23 count: 23\n",
      "reward: 36.086800134436636 setps: 64 count: 87\n",
      "reward: 39.026516641647305 setps: 89 count: 176\n",
      "reward: 77.1632981307601 setps: 52 count: 228\n",
      "reward: 95.75337438335262 setps: 78 count: 306\n",
      "reward: 119.24203425371529 setps: 112 count: 418\n",
      "reward: 66.2111876140407 setps: 62 count: 480\n",
      "reward: 79.77413413718602 setps: 56 count: 536\n",
      "reward: 170.78433709335513 setps: 183 count: 719\n",
      "reward: 107.84550408736102 setps: 92 count: 811\n",
      "reward: 72.7162106666132 setps: 49 count: 860\n",
      "reward: 99.62852874268427 setps: 84 count: 944\n",
      "avg rewards: 82.98189195102532\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.18129\n",
      "Epoch:20 Batch:13 Loss:0.01898\n",
      "Epoch:40 Batch:13 Loss:0.01092\n",
      "Epoch:60 Batch:13 Loss:0.00953\n",
      "Epoch:80 Batch:13 Loss:0.00827\n",
      "Epoch:100 Batch:13 Loss:0.00647\n",
      "Epoch:120 Batch:13 Loss:0.00629\n",
      "Epoch:140 Batch:13 Loss:0.00648\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.047\n",
      "Epoch:10 Batch:10 Loss:0.046\n",
      "Epoch:20 Batch:10 Loss:0.046\n",
      "Epoch:30 Batch:10 Loss:0.045\n",
      "Epoch:40 Batch:10 Loss:0.046\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 58.34506242657664 setps: 55 count: 55\n",
      "reward: 36.57120538092132 setps: 73 count: 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 51.58075464305875 setps: 37 count: 165\n",
      "reward: 58.01187105111457 setps: 40 count: 205\n",
      "reward: 49.18613552034077 setps: 84 count: 289\n",
      "reward: 76.90787535178823 setps: 54 count: 343\n",
      "reward: 71.71687204200425 setps: 51 count: 394\n",
      "reward: 86.32209535147557 setps: 67 count: 461\n",
      "reward: 49.67954501536587 setps: 34 count: 495\n",
      "reward: 68.56654343193954 setps: 64 count: 559\n",
      "reward: 68.24740381747834 setps: 57 count: 616\n",
      "reward: 28.4677309951163 setps: 47 count: 663\n",
      "reward: 92.78299438520041 setps: 88 count: 751\n",
      "reward: 79.49910861980608 setps: 66 count: 817\n",
      "reward: 63.24526332441746 setps: 54 count: 871\n",
      "reward: 49.033609732287 setps: 36 count: 907\n",
      "reward: 75.99464330342744 setps: 58 count: 965\n",
      "reward: 36.653844008219316 setps: 26 count: 991\n",
      "avg rewards: 61.156253244474314\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.20557\n",
      "Epoch:20 Batch:14 Loss:0.01872\n",
      "Epoch:40 Batch:14 Loss:0.01182\n",
      "Epoch:60 Batch:14 Loss:0.00902\n",
      "Epoch:80 Batch:14 Loss:0.00761\n",
      "Epoch:100 Batch:14 Loss:0.00689\n",
      "Epoch:120 Batch:14 Loss:0.00625\n",
      "Epoch:140 Batch:14 Loss:0.00538\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.043\n",
      "Epoch:10 Batch:10 Loss:0.040\n",
      "Epoch:20 Batch:10 Loss:0.041\n",
      "Epoch:30 Batch:10 Loss:0.040\n",
      "Epoch:40 Batch:10 Loss:0.040\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 81.43575190361449 setps: 154 count: 154\n",
      "reward: 53.67833734193118 setps: 118 count: 272\n",
      "reward: 38.44137935790057 setps: 91 count: 363\n",
      "avg rewards: 57.85182286781541\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.14144\n",
      "Epoch:20 Batch:15 Loss:0.01780\n",
      "Epoch:40 Batch:15 Loss:0.01098\n",
      "Epoch:60 Batch:15 Loss:0.00751\n",
      "Epoch:80 Batch:15 Loss:0.00671\n",
      "Epoch:100 Batch:15 Loss:0.00566\n",
      "Epoch:120 Batch:15 Loss:0.00546\n",
      "Epoch:140 Batch:15 Loss:0.00624\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.043\n",
      "Epoch:10 Batch:10 Loss:0.042\n",
      "Epoch:20 Batch:10 Loss:0.043\n",
      "Epoch:30 Batch:10 Loss:0.042\n",
      "Epoch:40 Batch:10 Loss:0.043\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 53.128669465512225 setps: 112 count: 112\n",
      "reward: 72.7024269033951 setps: 68 count: 180\n",
      "reward: 76.79893899554591 setps: 71 count: 251\n",
      "reward: 70.89210833497202 setps: 64 count: 315\n",
      "reward: 38.627696476645355 setps: 89 count: 404\n",
      "avg rewards: 62.42996803521412\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.11284\n",
      "Epoch:20 Batch:16 Loss:0.01570\n",
      "Epoch:40 Batch:16 Loss:0.00984\n",
      "Epoch:60 Batch:16 Loss:0.00877\n",
      "Epoch:80 Batch:16 Loss:0.00639\n",
      "Epoch:100 Batch:16 Loss:0.00743\n",
      "Epoch:120 Batch:16 Loss:0.00590\n",
      "Epoch:140 Batch:16 Loss:0.00600\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.047\n",
      "Epoch:10 Batch:10 Loss:0.044\n",
      "Epoch:20 Batch:10 Loss:0.045\n",
      "Epoch:30 Batch:10 Loss:0.043\n",
      "Epoch:40 Batch:10 Loss:0.045\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 95.83314603388426 setps: 81 count: 81\n",
      "reward: 89.2221378635237 setps: 77 count: 158\n",
      "reward: 88.88674188261791 setps: 76 count: 234\n",
      "reward: 78.92235987529132 setps: 114 count: 348\n",
      "reward: 40.849656611291 setps: 64 count: 412\n",
      "reward: 93.97793610976946 setps: 68 count: 480\n",
      "reward: 30.257469054372642 setps: 58 count: 538\n",
      "reward: 79.47235546206066 setps: 57 count: 595\n",
      "reward: 54.32159340267389 setps: 92 count: 687\n",
      "reward: 37.74455950443515 setps: 50 count: 737\n",
      "reward: 51.84876568991312 setps: 43 count: 780\n",
      "reward: 42.752811665568146 setps: 33 count: 813\n",
      "reward: 43.30588522948238 setps: 61 count: 874\n",
      "reward: 67.18115101794973 setps: 48 count: 922\n",
      "reward: 94.91066107531516 setps: 78 count: 1000\n",
      "avg rewards: 65.96581536520989\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.16992\n",
      "Epoch:20 Batch:17 Loss:0.01528\n",
      "Epoch:40 Batch:17 Loss:0.01049\n",
      "Epoch:60 Batch:17 Loss:0.00730\n",
      "Epoch:80 Batch:17 Loss:0.00589\n",
      "Epoch:100 Batch:17 Loss:0.00642\n",
      "Epoch:120 Batch:17 Loss:0.00541\n",
      "Epoch:140 Batch:17 Loss:0.00559\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.040\n",
      "Epoch:10 Batch:10 Loss:0.039\n",
      "Epoch:20 Batch:10 Loss:0.039\n",
      "Epoch:30 Batch:10 Loss:0.039\n",
      "Epoch:40 Batch:10 Loss:0.039\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 55.228833716629026 setps: 54 count: 54\n",
      "reward: 30.602004622509405 setps: 51 count: 105\n",
      "reward: 56.73450847752681 setps: 43 count: 148\n",
      "reward: 43.10721639591212 setps: 42 count: 190\n",
      "reward: 66.0517089224726 setps: 47 count: 237\n",
      "reward: 50.19545674564143 setps: 36 count: 273\n",
      "reward: 46.930849237465004 setps: 32 count: 305\n",
      "reward: 62.10377956037555 setps: 42 count: 347\n",
      "reward: 70.95024092428937 setps: 55 count: 402\n",
      "reward: 37.39351628798178 setps: 59 count: 461\n",
      "reward: 41.787305774421846 setps: 34 count: 495\n",
      "reward: 41.00036111722729 setps: 65 count: 560\n",
      "reward: 99.89208730894028 setps: 77 count: 637\n",
      "reward: 111.98568589136701 setps: 88 count: 725\n",
      "reward: 54.7621843981862 setps: 39 count: 764\n",
      "reward: 39.681998665294664 setps: 51 count: 815\n",
      "reward: 60.07735934725934 setps: 48 count: 863\n",
      "reward: 63.37836242200427 setps: 42 count: 905\n",
      "reward: 52.46793961751389 setps: 44 count: 949\n",
      "reward: 46.8752203228054 setps: 34 count: 983\n",
      "avg rewards: 56.56033098779117\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.10465\n",
      "Epoch:20 Batch:18 Loss:0.01331\n",
      "Epoch:40 Batch:18 Loss:0.01102\n",
      "Epoch:60 Batch:18 Loss:0.00640\n",
      "Epoch:80 Batch:18 Loss:0.00696\n",
      "Epoch:100 Batch:18 Loss:0.00694\n",
      "Epoch:120 Batch:18 Loss:0.00647\n",
      "Epoch:140 Batch:18 Loss:0.00491\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.038\n",
      "Epoch:10 Batch:10 Loss:0.040\n",
      "Epoch:20 Batch:10 Loss:0.038\n",
      "Epoch:30 Batch:10 Loss:0.039\n",
      "Epoch:40 Batch:10 Loss:0.038\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 28.585364039537676 setps: 33 count: 33\n",
      "reward: 76.87405282229737 setps: 54 count: 87\n",
      "reward: 75.14420776935584 setps: 54 count: 141\n",
      "reward: 44.122704754391464 setps: 32 count: 173\n",
      "reward: 30.039048234048824 setps: 36 count: 209\n",
      "reward: 42.62768908756116 setps: 33 count: 242\n",
      "reward: 64.62821278990451 setps: 56 count: 298\n",
      "reward: 75.44450702120082 setps: 51 count: 349\n",
      "reward: 35.73016764972707 setps: 33 count: 382\n",
      "reward: 56.84134960840746 setps: 43 count: 425\n",
      "reward: 38.93144339198916 setps: 33 count: 458\n",
      "reward: 71.67760885600875 setps: 47 count: 505\n",
      "reward: 48.311816854402416 setps: 41 count: 546\n",
      "reward: 54.176762555880124 setps: 41 count: 587\n",
      "reward: 53.24883069977658 setps: 43 count: 630\n",
      "reward: 34.64157470208594 setps: 37 count: 667\n",
      "reward: 48.24402597868903 setps: 36 count: 703\n",
      "reward: 38.00860512774525 setps: 38 count: 741\n",
      "reward: 72.53253350127633 setps: 47 count: 788\n",
      "reward: 40.24483231902996 setps: 29 count: 817\n",
      "reward: 39.00540582759713 setps: 36 count: 853\n",
      "reward: 74.94239922251582 setps: 49 count: 902\n",
      "reward: 73.24007333287591 setps: 49 count: 951\n",
      "reward: 39.29697907095979 setps: 39 count: 990\n",
      "avg rewards: 52.35584146738601\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.08484\n",
      "Epoch:20 Batch:19 Loss:0.01453\n",
      "Epoch:40 Batch:19 Loss:0.00875\n",
      "Epoch:60 Batch:19 Loss:0.00788\n",
      "Epoch:80 Batch:19 Loss:0.00536\n",
      "Epoch:100 Batch:19 Loss:0.00541\n",
      "Epoch:120 Batch:19 Loss:0.00615\n",
      "Epoch:140 Batch:19 Loss:0.00534\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.034\n",
      "Epoch:10 Batch:10 Loss:0.035\n",
      "Epoch:20 Batch:10 Loss:0.034\n",
      "Epoch:30 Batch:10 Loss:0.034\n",
      "Epoch:40 Batch:10 Loss:0.033\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 94.59527692696865 setps: 75 count: 75\n",
      "reward: 46.001533892957376 setps: 40 count: 115\n",
      "reward: 49.49868799363319 setps: 43 count: 158\n",
      "reward: 33.15210717249721 setps: 57 count: 215\n",
      "reward: 35.62653299146623 setps: 36 count: 251\n",
      "reward: 79.58773325332587 setps: 62 count: 313\n",
      "reward: 37.60945189827472 setps: 49 count: 362\n",
      "reward: 46.968740909450574 setps: 41 count: 403\n",
      "reward: 24.787773385326727 setps: 41 count: 444\n",
      "reward: 75.34428178630334 setps: 51 count: 495\n",
      "reward: 65.08491397262841 setps: 47 count: 542\n",
      "reward: 41.29526957316994 setps: 38 count: 580\n",
      "reward: 54.89087832121003 setps: 38 count: 618\n",
      "reward: 47.40531062144436 setps: 76 count: 694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 60.09367522766262 setps: 43 count: 737\n",
      "reward: 30.107848190376533 setps: 38 count: 775\n",
      "reward: 75.96136070318025 setps: 53 count: 828\n",
      "reward: 49.59048264870216 setps: 77 count: 905\n",
      "reward: 59.39496431587614 setps: 38 count: 943\n",
      "reward: 34.05916687777207 setps: 54 count: 997\n",
      "avg rewards: 52.05279953311132\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.06675\n",
      "Epoch:20 Batch:20 Loss:0.01343\n",
      "Epoch:40 Batch:20 Loss:0.00857\n",
      "Epoch:60 Batch:20 Loss:0.00787\n",
      "Epoch:80 Batch:20 Loss:0.00636\n",
      "Epoch:100 Batch:20 Loss:0.00533\n",
      "Epoch:120 Batch:20 Loss:0.00651\n",
      "Epoch:140 Batch:20 Loss:0.00516\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.037\n",
      "Epoch:10 Batch:10 Loss:0.035\n",
      "Epoch:20 Batch:10 Loss:0.036\n",
      "Epoch:30 Batch:10 Loss:0.037\n",
      "Epoch:40 Batch:10 Loss:0.037\n",
      "Done!\n",
      "############# start HopperBulletEnv-v0 training ###################\n",
      "(49,)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 18.135717426698825 setps: 8 count: 8\n",
      "reward: 20.31004078197002 setps: 10 count: 18\n",
      "reward: 21.438978731670066 setps: 15 count: 33\n",
      "reward: 17.765539309385346 setps: 9 count: 42\n",
      "reward: 18.332904579787286 setps: 11 count: 53\n",
      "reward: 21.875437221436005 setps: 19 count: 72\n",
      "reward: 21.506266662621055 setps: 15 count: 87\n",
      "reward: 19.376771460867893 setps: 11 count: 98\n",
      "reward: 24.13838943464216 setps: 12 count: 110\n",
      "reward: 20.692785486340288 setps: 17 count: 127\n",
      "reward: 18.940834932563305 setps: 11 count: 138\n",
      "reward: 21.166305673065654 setps: 14 count: 152\n",
      "reward: 21.23852567662252 setps: 19 count: 171\n",
      "reward: 21.796875095865108 setps: 11 count: 182\n",
      "reward: 20.010833770128375 setps: 12 count: 194\n",
      "reward: 20.04875340626022 setps: 10 count: 204\n",
      "reward: 18.056655500577474 setps: 8 count: 212\n",
      "reward: 17.39600966670987 setps: 7 count: 219\n",
      "reward: 22.168040633604683 setps: 15 count: 234\n",
      "reward: 22.489486426544318 setps: 12 count: 246\n",
      "reward: 16.921681424420967 setps: 7 count: 253\n",
      "reward: 20.304812986515756 setps: 9 count: 262\n",
      "reward: 19.91294782147306 setps: 15 count: 277\n",
      "reward: 25.1151405354336 setps: 17 count: 294\n",
      "reward: 17.640768596230192 setps: 7 count: 301\n",
      "reward: 21.37131197315757 setps: 23 count: 324\n",
      "reward: 23.107516491040585 setps: 18 count: 342\n",
      "reward: 17.87113537335681 setps: 18 count: 360\n",
      "reward: 21.333143469640344 setps: 14 count: 374\n",
      "reward: 16.236367013251584 setps: 6 count: 380\n",
      "reward: 18.379200361853872 setps: 14 count: 394\n",
      "reward: 18.160572214784043 setps: 14 count: 408\n",
      "reward: 28.397211807865823 setps: 15 count: 423\n",
      "reward: 42.62005108907906 setps: 40 count: 463\n",
      "reward: 18.12718621087551 setps: 10 count: 473\n",
      "reward: 22.348446159492592 setps: 16 count: 489\n",
      "reward: 22.58490416053828 setps: 13 count: 502\n",
      "reward: 18.85577770489035 setps: 7 count: 509\n",
      "reward: 24.706906872590476 setps: 14 count: 523\n",
      "reward: 23.529601036265372 setps: 16 count: 539\n",
      "reward: 20.90636560224084 setps: 10 count: 549\n",
      "reward: 21.022759772661086 setps: 9 count: 558\n",
      "reward: 18.993325167467994 setps: 10 count: 568\n",
      "reward: 15.589718839761916 setps: 6 count: 574\n",
      "reward: 24.23771513060492 setps: 13 count: 587\n",
      "reward: 18.721909652430618 setps: 15 count: 602\n",
      "reward: 21.95135470764362 setps: 11 count: 613\n",
      "reward: 18.423491722314793 setps: 12 count: 625\n",
      "reward: 26.554142017479176 setps: 27 count: 652\n",
      "reward: 21.519148920968295 setps: 11 count: 663\n",
      "reward: 21.72804832597467 setps: 8 count: 671\n",
      "reward: 18.013207625600625 setps: 8 count: 679\n",
      "reward: 19.974427477087012 setps: 10 count: 689\n",
      "reward: 21.006524462794182 setps: 11 count: 700\n",
      "reward: 17.406095495457702 setps: 10 count: 710\n",
      "reward: 18.114781990027403 setps: 8 count: 718\n",
      "reward: 20.886644943838476 setps: 10 count: 728\n",
      "reward: 19.1949897245373 setps: 13 count: 741\n",
      "reward: 20.460972389401288 setps: 9 count: 750\n",
      "reward: 27.29091283071757 setps: 18 count: 768\n",
      "reward: 23.599417462351266 setps: 20 count: 788\n",
      "reward: 19.10651244682958 setps: 9 count: 797\n",
      "reward: 23.42654081135261 setps: 14 count: 811\n",
      "reward: 22.88648002753762 setps: 11 count: 822\n",
      "reward: 14.441897365625485 setps: 14 count: 836\n",
      "reward: 23.193719969296946 setps: 11 count: 847\n",
      "reward: 17.056251911837897 setps: 9 count: 856\n",
      "reward: 18.224917638469197 setps: 11 count: 867\n",
      "reward: 21.66502618997329 setps: 16 count: 883\n",
      "reward: 19.407696980983015 setps: 8 count: 891\n",
      "reward: 6.465537419653269 setps: 27 count: 918\n",
      "reward: 19.796564671721715 setps: 8 count: 926\n",
      "reward: 22.738228704645007 setps: 18 count: 944\n",
      "reward: 19.708290001975545 setps: 10 count: 954\n",
      "reward: 22.50526024093852 setps: 24 count: 978\n",
      "reward: 20.741775295831026 setps: 10 count: 988\n",
      "reward: 12.698206946720893 setps: 7 count: 995\n",
      "avg rewards: 20.572320728115173\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:0.37437\n",
      "Epoch:20 Batch:1 Loss:0.18127\n",
      "Epoch:40 Batch:1 Loss:0.15284\n",
      "Epoch:60 Batch:1 Loss:0.12903\n",
      "Epoch:80 Batch:1 Loss:0.09121\n",
      "Epoch:100 Batch:1 Loss:0.06908\n",
      "Epoch:120 Batch:1 Loss:0.06039\n",
      "Epoch:140 Batch:1 Loss:0.05458\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.113\n",
      "Epoch:10 Batch:10 Loss:0.127\n",
      "Epoch:20 Batch:10 Loss:0.123\n",
      "Epoch:30 Batch:10 Loss:0.109\n",
      "Epoch:40 Batch:10 Loss:0.107\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 14.369026398544793 setps: 33 count: 33\n",
      "reward: 10.689989762339978 setps: 31 count: 64\n",
      "reward: 9.838866787217556 setps: 32 count: 96\n",
      "reward: 10.118798829970181 setps: 31 count: 127\n",
      "reward: 5.818683820898877 setps: 31 count: 158\n",
      "reward: 9.645007075319153 setps: 31 count: 189\n",
      "reward: 8.337089611853296 setps: 31 count: 220\n",
      "reward: 10.915806969519323 setps: 32 count: 252\n",
      "reward: 8.09850983735232 setps: 29 count: 281\n",
      "reward: 6.171779008804876 setps: 29 count: 310\n",
      "reward: 9.51142854323989 setps: 32 count: 342\n",
      "reward: 6.67250554340717 setps: 33 count: 375\n",
      "reward: 8.151597295477405 setps: 32 count: 407\n",
      "reward: 10.196264250821878 setps: 31 count: 438\n",
      "reward: 10.276000672410008 setps: 31 count: 469\n",
      "reward: 11.650912842023535 setps: 31 count: 500\n",
      "reward: 10.990578589566574 setps: 30 count: 530\n",
      "reward: 4.865808812670002 setps: 28 count: 558\n",
      "reward: 8.645392864429596 setps: 30 count: 588\n",
      "reward: 6.723067619935316 setps: 32 count: 620\n",
      "reward: 6.899628241789468 setps: 31 count: 651\n",
      "reward: 13.800537315502877 setps: 35 count: 686\n",
      "reward: 7.797851716008154 setps: 33 count: 719\n",
      "reward: 13.466048246117133 setps: 33 count: 752\n",
      "reward: 8.662101185592473 setps: 30 count: 782\n",
      "reward: 11.447864665626545 setps: 33 count: 815\n",
      "reward: 5.932336894341278 setps: 32 count: 847\n",
      "reward: 7.5908421291765995 setps: 29 count: 876\n",
      "reward: 4.846193886227229 setps: 30 count: 906\n",
      "reward: 10.090060362764232 setps: 29 count: 935\n",
      "reward: 7.29288757991162 setps: 29 count: 964\n",
      "reward: 13.056358287904002 setps: 32 count: 996\n",
      "avg rewards: 9.142807051461354\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:0.37027\n",
      "Epoch:20 Batch:2 Loss:0.11637\n",
      "Epoch:40 Batch:2 Loss:0.08574\n",
      "Epoch:60 Batch:2 Loss:0.05552\n",
      "Epoch:80 Batch:2 Loss:0.04089\n",
      "Epoch:100 Batch:2 Loss:0.03363\n",
      "Epoch:120 Batch:2 Loss:0.03164\n",
      "Epoch:140 Batch:2 Loss:0.02968\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.072\n",
      "Epoch:10 Batch:10 Loss:0.071\n",
      "Epoch:20 Batch:10 Loss:0.069\n",
      "Epoch:30 Batch:10 Loss:0.071\n",
      "Epoch:40 Batch:10 Loss:0.075\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -2.263846114662011 setps: 27 count: 27\n",
      "reward: -0.42852105560459774 setps: 29 count: 56\n",
      "reward: -3.991634961152159 setps: 29 count: 85\n",
      "reward: 3.714183861555647 setps: 30 count: 115\n",
      "reward: -0.4374893093547154 setps: 26 count: 141\n",
      "reward: -2.023209257522832 setps: 29 count: 170\n",
      "reward: 0.44219673272454996 setps: 28 count: 198\n",
      "reward: -3.27192328037345 setps: 28 count: 226\n",
      "reward: -0.41297210160119047 setps: 30 count: 256\n",
      "reward: -2.3168306063817012 setps: 30 count: 286\n",
      "reward: -3.306488118923154 setps: 28 count: 314\n",
      "reward: 1.0128611501328113 setps: 29 count: 343\n",
      "reward: 4.954690259730342 setps: 32 count: 375\n",
      "reward: 1.9893572809640303 setps: 29 count: 404\n",
      "reward: 0.27978118244227446 setps: 28 count: 432\n",
      "reward: 1.4228899604175231 setps: 29 count: 461\n",
      "reward: 1.1862802000619908 setps: 29 count: 490\n",
      "reward: -4.413324082588952 setps: 28 count: 518\n",
      "reward: -3.1349015164465532 setps: 29 count: 547\n",
      "reward: -0.8482812477901471 setps: 29 count: 576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -0.4265178386202033 setps: 29 count: 605\n",
      "reward: -1.7572530940975422 setps: 28 count: 633\n",
      "reward: -0.07914920009971027 setps: 29 count: 662\n",
      "reward: -3.905305263117772 setps: 28 count: 690\n",
      "reward: 1.001364088434638 setps: 29 count: 719\n",
      "reward: -1.0904766933199435 setps: 29 count: 748\n",
      "reward: 0.6091273409110713 setps: 29 count: 777\n",
      "reward: 1.2293474946098275 setps: 30 count: 807\n",
      "reward: 0.9304200962505993 setps: 30 count: 837\n",
      "reward: -0.8920145132404298 setps: 28 count: 865\n",
      "reward: -1.6966724196929146 setps: 29 count: 894\n",
      "reward: 2.0688346085013096 setps: 30 count: 924\n",
      "reward: 0.6450603533459782 setps: 30 count: 954\n",
      "reward: -1.8773214058950554 setps: 30 count: 984\n",
      "avg rewards: -0.502580513835366\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.33681\n",
      "Epoch:20 Batch:3 Loss:0.08940\n",
      "Epoch:40 Batch:3 Loss:0.05051\n",
      "Epoch:60 Batch:3 Loss:0.03695\n",
      "Epoch:80 Batch:3 Loss:0.02820\n",
      "Epoch:100 Batch:3 Loss:0.02354\n",
      "Epoch:120 Batch:3 Loss:0.02266\n",
      "Epoch:140 Batch:3 Loss:0.02152\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.078\n",
      "Epoch:10 Batch:10 Loss:0.069\n",
      "Epoch:20 Batch:10 Loss:0.074\n",
      "Epoch:30 Batch:10 Loss:0.076\n",
      "Epoch:40 Batch:10 Loss:0.079\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 30.724800234436405 setps: 39 count: 39\n",
      "reward: 33.322297511775105 setps: 39 count: 78\n",
      "reward: 26.821442602348178 setps: 37 count: 115\n",
      "reward: 22.608288815713607 setps: 36 count: 151\n",
      "reward: 17.50880634893256 setps: 33 count: 184\n",
      "reward: 30.31638075814117 setps: 38 count: 222\n",
      "reward: 23.106949617981446 setps: 37 count: 259\n",
      "reward: 27.48373011056792 setps: 36 count: 295\n",
      "reward: 33.25551563385816 setps: 41 count: 336\n",
      "reward: 31.12142283237627 setps: 39 count: 375\n",
      "reward: 24.792860605369782 setps: 35 count: 410\n",
      "reward: 27.202256959097575 setps: 39 count: 449\n",
      "reward: 32.51616972435585 setps: 40 count: 489\n",
      "reward: 28.37389261710059 setps: 38 count: 527\n",
      "reward: 23.470508999227597 setps: 35 count: 562\n",
      "reward: 37.10746764462821 setps: 44 count: 606\n",
      "reward: 13.892611397653397 setps: 31 count: 637\n",
      "reward: 31.84429858314979 setps: 38 count: 675\n",
      "reward: 20.419914343974956 setps: 36 count: 711\n",
      "reward: 32.65796955967089 setps: 40 count: 751\n",
      "reward: 30.44808015069866 setps: 38 count: 789\n",
      "reward: 24.98512553258188 setps: 37 count: 826\n",
      "reward: 27.823669402750962 setps: 36 count: 862\n",
      "reward: 28.946841174495052 setps: 36 count: 898\n",
      "reward: 20.028727664863982 setps: 34 count: 932\n",
      "reward: 23.16863449286029 setps: 34 count: 966\n",
      "reward: 24.46570353676361 setps: 34 count: 1000\n",
      "avg rewards: 26.978309883532365\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.29648\n",
      "Epoch:20 Batch:4 Loss:0.06664\n",
      "Epoch:40 Batch:4 Loss:0.04211\n",
      "Epoch:60 Batch:4 Loss:0.02793\n",
      "Epoch:80 Batch:4 Loss:0.02207\n",
      "Epoch:100 Batch:4 Loss:0.02048\n",
      "Epoch:120 Batch:4 Loss:0.01966\n",
      "Epoch:140 Batch:4 Loss:0.01844\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.071\n",
      "Epoch:10 Batch:10 Loss:0.074\n",
      "Epoch:20 Batch:10 Loss:0.072\n",
      "Epoch:30 Batch:10 Loss:0.068\n",
      "Epoch:40 Batch:10 Loss:0.068\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 45.59582873433829 setps: 49 count: 49\n",
      "reward: 32.27242991938839 setps: 39 count: 88\n",
      "reward: 36.62562430496364 setps: 42 count: 130\n",
      "reward: 31.341298177223507 setps: 38 count: 168\n",
      "reward: 32.185235084738586 setps: 38 count: 206\n",
      "reward: 31.293521919532576 setps: 38 count: 244\n",
      "reward: 27.944657604239175 setps: 35 count: 279\n",
      "reward: 32.319335749869786 setps: 37 count: 316\n",
      "reward: 37.509416885292374 setps: 43 count: 359\n",
      "reward: 36.543439789479095 setps: 41 count: 400\n",
      "reward: 22.099347524980114 setps: 34 count: 434\n",
      "reward: 31.667642485508985 setps: 42 count: 476\n",
      "reward: 40.74265459492597 setps: 44 count: 520\n",
      "reward: 36.52298924255302 setps: 40 count: 560\n",
      "reward: 20.311400248685096 setps: 33 count: 593\n",
      "reward: 35.07754960578749 setps: 39 count: 632\n",
      "reward: 35.5297028691235 setps: 41 count: 673\n",
      "reward: 28.556916462910884 setps: 37 count: 710\n",
      "reward: 34.61953805200027 setps: 40 count: 750\n",
      "reward: 46.67206337600219 setps: 50 count: 800\n",
      "reward: 37.09453692943061 setps: 45 count: 845\n",
      "reward: 28.8297198168686 setps: 36 count: 881\n",
      "reward: 37.41249381211819 setps: 41 count: 922\n",
      "reward: 38.777047655524804 setps: 44 count: 966\n",
      "avg rewards: 34.06434961856188\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.31635\n",
      "Epoch:20 Batch:5 Loss:0.05827\n",
      "Epoch:40 Batch:5 Loss:0.02750\n",
      "Epoch:60 Batch:5 Loss:0.01958\n",
      "Epoch:80 Batch:5 Loss:0.01938\n",
      "Epoch:100 Batch:5 Loss:0.01814\n",
      "Epoch:120 Batch:5 Loss:0.01683\n",
      "Epoch:140 Batch:5 Loss:0.01633\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.071\n",
      "Epoch:10 Batch:10 Loss:0.069\n",
      "Epoch:20 Batch:10 Loss:0.064\n",
      "Epoch:30 Batch:10 Loss:0.072\n",
      "Epoch:40 Batch:10 Loss:0.065\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 36.515425716315804 setps: 20 count: 20\n",
      "reward: 44.13885732202761 setps: 30 count: 50\n",
      "reward: 32.32276957941358 setps: 18 count: 68\n",
      "reward: 45.99235612361081 setps: 31 count: 99\n",
      "reward: 34.41044196196307 setps: 18 count: 117\n",
      "reward: 34.4430429357104 setps: 18 count: 135\n",
      "reward: 34.67879991883237 setps: 19 count: 154\n",
      "reward: 33.69618124055705 setps: 19 count: 173\n",
      "reward: 41.654449757165274 setps: 27 count: 200\n",
      "reward: 40.785348141612495 setps: 22 count: 222\n",
      "reward: 34.98506846112432 setps: 19 count: 241\n",
      "reward: 35.407007395601255 setps: 19 count: 260\n",
      "reward: 42.202770403290934 setps: 28 count: 288\n",
      "reward: 42.29482124974893 setps: 28 count: 316\n",
      "reward: 38.410429238561484 setps: 25 count: 341\n",
      "reward: 35.91918730149482 setps: 19 count: 360\n",
      "reward: 35.10108156069619 setps: 18 count: 378\n",
      "reward: 34.75617609888141 setps: 19 count: 397\n",
      "reward: 34.53435361377051 setps: 18 count: 415\n",
      "reward: 40.63516126311298 setps: 27 count: 442\n",
      "reward: 42.78458383088146 setps: 28 count: 470\n",
      "reward: 36.808571275509884 setps: 20 count: 490\n",
      "reward: 32.65868053532758 setps: 18 count: 508\n",
      "reward: 33.2509918782438 setps: 18 count: 526\n",
      "reward: 31.28351078945125 setps: 17 count: 543\n",
      "reward: 43.34194614056177 setps: 29 count: 572\n",
      "reward: 36.74388255159865 setps: 20 count: 592\n",
      "reward: 32.066337555051724 setps: 18 count: 610\n",
      "reward: 37.05364033444348 setps: 20 count: 630\n",
      "reward: 40.03182304723014 setps: 26 count: 656\n",
      "reward: 45.307315859495425 setps: 30 count: 686\n",
      "reward: 43.481688859274435 setps: 28 count: 714\n",
      "reward: 43.05400809983838 setps: 29 count: 743\n",
      "reward: 33.77667960438849 setps: 19 count: 762\n",
      "reward: 34.97640175266278 setps: 19 count: 781\n",
      "reward: 29.218448664553584 setps: 16 count: 797\n",
      "reward: 36.13971980098868 setps: 24 count: 821\n",
      "reward: 36.126850981535966 setps: 20 count: 841\n",
      "reward: 32.517506803327706 setps: 18 count: 859\n",
      "reward: 41.40134085166209 setps: 27 count: 886\n",
      "reward: 33.980901747861935 setps: 18 count: 904\n",
      "reward: 39.71768411654775 setps: 26 count: 930\n",
      "reward: 34.66699183966994 setps: 19 count: 949\n",
      "reward: 40.755234366758664 setps: 27 count: 976\n",
      "reward: 36.09787678315042 setps: 20 count: 996\n",
      "avg rewards: 37.336141052300164\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.24446\n",
      "Epoch:20 Batch:6 Loss:0.04583\n",
      "Epoch:40 Batch:6 Loss:0.02475\n",
      "Epoch:60 Batch:6 Loss:0.01986\n",
      "Epoch:80 Batch:6 Loss:0.01829\n",
      "Epoch:100 Batch:6 Loss:0.01531\n",
      "Epoch:120 Batch:6 Loss:0.01475\n",
      "Epoch:140 Batch:6 Loss:0.01404\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.056\n",
      "Epoch:10 Batch:10 Loss:0.066\n",
      "Epoch:20 Batch:10 Loss:0.058\n",
      "Epoch:30 Batch:10 Loss:0.058\n",
      "Epoch:40 Batch:10 Loss:0.055\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 31.29027794403373 setps: 38 count: 38\n",
      "reward: 28.525487247011913 setps: 35 count: 73\n",
      "reward: 33.39524935235968 setps: 38 count: 111\n",
      "reward: 22.671618049610696 setps: 33 count: 144\n",
      "reward: 58.77428192134831 setps: 55 count: 199\n",
      "reward: 35.45657917128991 setps: 40 count: 239\n",
      "reward: 39.04299042093916 setps: 46 count: 285\n",
      "reward: 47.78306437437278 setps: 48 count: 333\n",
      "reward: 47.8728579811752 setps: 47 count: 380\n",
      "reward: 41.79354755665992 setps: 43 count: 423\n",
      "reward: 78.02634568119683 setps: 59 count: 482\n",
      "reward: 37.94240791170014 setps: 40 count: 522\n",
      "reward: 32.76348609096749 setps: 38 count: 560\n",
      "reward: 31.37949449075677 setps: 38 count: 598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 39.61313950431795 setps: 41 count: 639\n",
      "reward: 56.67984994124127 setps: 53 count: 692\n",
      "reward: 31.576987679883317 setps: 37 count: 729\n",
      "reward: 31.85008975035598 setps: 38 count: 767\n",
      "reward: 37.70430388726381 setps: 41 count: 808\n",
      "reward: 48.88395013387781 setps: 47 count: 855\n",
      "reward: 50.04238455712328 setps: 48 count: 903\n",
      "reward: 37.55349886626209 setps: 41 count: 944\n",
      "reward: 54.14826416376453 setps: 51 count: 995\n",
      "avg rewards: 41.51174594250055\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.31474\n",
      "Epoch:20 Batch:7 Loss:0.04468\n",
      "Epoch:40 Batch:7 Loss:0.02493\n",
      "Epoch:60 Batch:7 Loss:0.01739\n",
      "Epoch:80 Batch:7 Loss:0.01699\n",
      "Epoch:100 Batch:7 Loss:0.01554\n",
      "Epoch:120 Batch:7 Loss:0.01441\n",
      "Epoch:140 Batch:7 Loss:0.01312\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.051\n",
      "Epoch:10 Batch:10 Loss:0.055\n",
      "Epoch:20 Batch:10 Loss:0.052\n",
      "Epoch:30 Batch:10 Loss:0.052\n",
      "Epoch:40 Batch:10 Loss:0.051\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 34.46525857205997 setps: 39 count: 39\n",
      "reward: 49.02108536301093 setps: 47 count: 86\n",
      "reward: 38.27536141506134 setps: 42 count: 128\n",
      "reward: 48.278451359049356 setps: 49 count: 177\n",
      "reward: 45.30921622565366 setps: 45 count: 222\n",
      "reward: 42.8481864151312 setps: 45 count: 267\n",
      "reward: 46.19817557582572 setps: 47 count: 314\n",
      "reward: 41.34260297486763 setps: 44 count: 358\n",
      "reward: 32.7185261473438 setps: 38 count: 396\n",
      "reward: 43.63744339426048 setps: 46 count: 442\n",
      "reward: 31.897251377422073 setps: 37 count: 479\n",
      "reward: 46.58423054248705 setps: 46 count: 525\n",
      "reward: 36.49241792215181 setps: 40 count: 565\n",
      "reward: 42.56262081616443 setps: 45 count: 610\n",
      "reward: 42.116241204063414 setps: 43 count: 653\n",
      "reward: 40.28238462707814 setps: 43 count: 696\n",
      "reward: 35.010437589952204 setps: 39 count: 735\n",
      "reward: 31.577804158165236 setps: 39 count: 774\n",
      "reward: 43.77686415173668 setps: 45 count: 819\n",
      "reward: 37.35759425211436 setps: 42 count: 861\n",
      "reward: 40.76817437938007 setps: 44 count: 905\n",
      "reward: 50.73261632724607 setps: 49 count: 954\n",
      "reward: 37.73353394229052 setps: 41 count: 995\n",
      "avg rewards: 40.82549907532679\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.28742\n",
      "Epoch:20 Batch:8 Loss:0.03428\n",
      "Epoch:40 Batch:8 Loss:0.01890\n",
      "Epoch:60 Batch:8 Loss:0.01583\n",
      "Epoch:80 Batch:8 Loss:0.01502\n",
      "Epoch:100 Batch:8 Loss:0.01462\n",
      "Epoch:120 Batch:8 Loss:0.01201\n",
      "Epoch:140 Batch:8 Loss:0.01182\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.053\n",
      "Epoch:10 Batch:10 Loss:0.055\n",
      "Epoch:20 Batch:10 Loss:0.055\n",
      "Epoch:30 Batch:10 Loss:0.048\n",
      "Epoch:40 Batch:10 Loss:0.044\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 59.92910477377008 setps: 49 count: 49\n",
      "reward: 56.529850968472594 setps: 47 count: 96\n",
      "reward: 63.27058199080056 setps: 46 count: 142\n",
      "reward: 52.05696034092543 setps: 46 count: 188\n",
      "reward: 67.57182287991455 setps: 53 count: 241\n",
      "reward: 50.39307271041908 setps: 47 count: 288\n",
      "reward: 55.475593126181046 setps: 45 count: 333\n",
      "reward: 63.74362658012106 setps: 52 count: 385\n",
      "reward: 54.43456840332363 setps: 42 count: 427\n",
      "reward: 54.3575114058738 setps: 47 count: 474\n",
      "reward: 61.01399569022905 setps: 49 count: 523\n",
      "reward: 75.24688452507543 setps: 56 count: 579\n",
      "reward: 55.50018474556126 setps: 45 count: 624\n",
      "reward: 61.81199671991198 setps: 50 count: 674\n",
      "reward: 55.14452453193662 setps: 41 count: 715\n",
      "reward: 64.30506536825851 setps: 52 count: 767\n",
      "reward: 48.69363913284906 setps: 49 count: 816\n",
      "reward: 57.48477275956975 setps: 48 count: 864\n",
      "reward: 63.0058145185758 setps: 49 count: 913\n",
      "reward: 52.88749603741016 setps: 41 count: 954\n",
      "avg rewards: 58.64285336045897\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.25749\n",
      "Epoch:20 Batch:9 Loss:0.03235\n",
      "Epoch:40 Batch:9 Loss:0.01762\n",
      "Epoch:60 Batch:9 Loss:0.01642\n",
      "Epoch:80 Batch:9 Loss:0.01394\n",
      "Epoch:100 Batch:9 Loss:0.01432\n",
      "Epoch:120 Batch:9 Loss:0.01291\n",
      "Epoch:140 Batch:9 Loss:0.01079\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.044\n",
      "Epoch:10 Batch:10 Loss:0.044\n",
      "Epoch:20 Batch:10 Loss:0.045\n",
      "Epoch:30 Batch:10 Loss:0.041\n",
      "Epoch:40 Batch:10 Loss:0.040\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 17.399397627649886 setps: 32 count: 32\n",
      "reward: 38.27401419860108 setps: 45 count: 77\n",
      "reward: 24.35052068866898 setps: 39 count: 116\n",
      "reward: 28.883956099058565 setps: 39 count: 155\n",
      "reward: 27.330918405197732 setps: 37 count: 192\n",
      "reward: 27.532447317846522 setps: 37 count: 229\n",
      "reward: 33.182320797107245 setps: 43 count: 272\n",
      "reward: 39.00614823692741 setps: 44 count: 316\n",
      "reward: 14.53927632719133 setps: 33 count: 349\n",
      "reward: 17.748811456323896 setps: 34 count: 383\n",
      "reward: 35.00522975210334 setps: 40 count: 423\n",
      "reward: 27.15203264378797 setps: 38 count: 461\n",
      "reward: 21.498981523915425 setps: 38 count: 499\n",
      "reward: 34.8254722557016 setps: 42 count: 541\n",
      "reward: 21.34521493149077 setps: 35 count: 576\n",
      "reward: 18.61926297904865 setps: 37 count: 613\n",
      "reward: 12.003259369832087 setps: 33 count: 646\n",
      "reward: 20.281968130520546 setps: 38 count: 684\n",
      "reward: 21.691220923569926 setps: 36 count: 720\n",
      "reward: 36.8730188511865 setps: 41 count: 761\n",
      "reward: 46.89024855220487 setps: 50 count: 811\n",
      "reward: 25.792071880526777 setps: 39 count: 850\n",
      "reward: 31.00645127735188 setps: 39 count: 889\n",
      "reward: 23.117798063920052 setps: 35 count: 924\n",
      "reward: 33.382958642419545 setps: 40 count: 964\n",
      "reward: 20.398075818747746 setps: 35 count: 999\n",
      "avg rewards: 26.851195259650012\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.24552\n",
      "Epoch:20 Batch:10 Loss:0.03005\n",
      "Epoch:40 Batch:10 Loss:0.01905\n",
      "Epoch:60 Batch:10 Loss:0.01602\n",
      "Epoch:80 Batch:10 Loss:0.01416\n",
      "Epoch:100 Batch:10 Loss:0.01186\n",
      "Epoch:120 Batch:10 Loss:0.01123\n",
      "Epoch:140 Batch:10 Loss:0.01147\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.040\n",
      "Epoch:10 Batch:10 Loss:0.034\n",
      "Epoch:20 Batch:10 Loss:0.038\n",
      "Epoch:30 Batch:10 Loss:0.034\n",
      "Epoch:40 Batch:10 Loss:0.039\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 14.539451112425011 setps: 31 count: 31\n",
      "reward: 50.948809365808856 setps: 50 count: 81\n",
      "reward: 33.63332956823869 setps: 40 count: 121\n",
      "reward: 36.401787212600176 setps: 42 count: 163\n",
      "reward: 15.96586810065491 setps: 32 count: 195\n",
      "reward: 44.25957085439293 setps: 46 count: 241\n",
      "reward: 46.765710818943624 setps: 51 count: 292\n",
      "reward: 33.46599045923941 setps: 41 count: 333\n",
      "reward: 46.13755781006476 setps: 48 count: 381\n",
      "reward: 28.656312686698953 setps: 37 count: 418\n",
      "reward: 34.93519615802361 setps: 39 count: 457\n",
      "reward: 39.29927874680579 setps: 43 count: 500\n",
      "reward: 37.64002475537855 setps: 43 count: 543\n",
      "reward: 28.06906355274986 setps: 39 count: 582\n",
      "reward: 42.06117751278799 setps: 45 count: 627\n",
      "reward: 29.96387691283163 setps: 38 count: 665\n",
      "reward: 46.74531087614888 setps: 48 count: 713\n",
      "reward: 37.42612900881649 setps: 42 count: 755\n",
      "reward: 37.95421119964885 setps: 42 count: 797\n",
      "reward: 36.91258803260134 setps: 42 count: 839\n",
      "reward: 43.078402679841375 setps: 45 count: 884\n",
      "reward: 45.95639578232368 setps: 47 count: 931\n",
      "reward: 38.418711263097066 setps: 44 count: 975\n",
      "avg rewards: 36.92325019435315\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.14910\n",
      "Epoch:20 Batch:11 Loss:0.02909\n",
      "Epoch:40 Batch:11 Loss:0.01716\n",
      "Epoch:60 Batch:11 Loss:0.01298\n",
      "Epoch:80 Batch:11 Loss:0.01244\n",
      "Epoch:100 Batch:11 Loss:0.01213\n",
      "Epoch:120 Batch:11 Loss:0.01187\n",
      "Epoch:140 Batch:11 Loss:0.01110\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.040\n",
      "Epoch:10 Batch:10 Loss:0.037\n",
      "Epoch:20 Batch:10 Loss:0.038\n",
      "Epoch:30 Batch:10 Loss:0.038\n",
      "Epoch:40 Batch:10 Loss:0.037\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 43.90750471692008 setps: 44 count: 44\n",
      "reward: 50.68698940134053 setps: 47 count: 91\n",
      "reward: 43.64576542395807 setps: 47 count: 138\n",
      "reward: 48.06802330501087 setps: 48 count: 186\n",
      "reward: 54.386855234044205 setps: 54 count: 240\n",
      "reward: 41.30485721785082 setps: 44 count: 284\n",
      "reward: 51.397565970348666 setps: 50 count: 334\n",
      "reward: 33.40563427149027 setps: 42 count: 376\n",
      "reward: 53.57221718263027 setps: 51 count: 427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 44.02262566483842 setps: 46 count: 473\n",
      "reward: 41.74575940773355 setps: 45 count: 518\n",
      "reward: 38.00105123471439 setps: 43 count: 561\n",
      "reward: 38.34368665234797 setps: 45 count: 606\n",
      "reward: 32.58910536510666 setps: 42 count: 648\n",
      "reward: 54.74194380256957 setps: 52 count: 700\n",
      "reward: 34.02067888779711 setps: 40 count: 740\n",
      "reward: 41.61003198782272 setps: 46 count: 786\n",
      "reward: 49.628732862738254 setps: 49 count: 835\n",
      "reward: 49.060712149050964 setps: 49 count: 884\n",
      "reward: 35.83340701855778 setps: 44 count: 928\n",
      "reward: 50.92188846414211 setps: 50 count: 978\n",
      "avg rewards: 44.32833505814348\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.17769\n",
      "Epoch:20 Batch:12 Loss:0.02246\n",
      "Epoch:40 Batch:12 Loss:0.01495\n",
      "Epoch:60 Batch:12 Loss:0.01455\n",
      "Epoch:80 Batch:12 Loss:0.01162\n",
      "Epoch:100 Batch:12 Loss:0.01174\n",
      "Epoch:120 Batch:12 Loss:0.01021\n",
      "Epoch:140 Batch:12 Loss:0.01090\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.042\n",
      "Epoch:10 Batch:10 Loss:0.043\n",
      "Epoch:20 Batch:10 Loss:0.039\n",
      "Epoch:30 Batch:10 Loss:0.039\n",
      "Epoch:40 Batch:10 Loss:0.034\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 29.939856595660967 setps: 39 count: 39\n",
      "reward: 70.74630533914898 setps: 55 count: 94\n",
      "reward: 72.31454620579025 setps: 53 count: 147\n",
      "reward: 28.17949571549252 setps: 42 count: 189\n",
      "reward: 67.41442379558723 setps: 55 count: 244\n",
      "reward: 47.25591528889345 setps: 44 count: 288\n",
      "reward: 65.38396462939272 setps: 53 count: 341\n",
      "reward: 51.625110357256204 setps: 47 count: 388\n",
      "reward: 51.09410177687533 setps: 48 count: 436\n",
      "reward: 30.02030476415966 setps: 43 count: 479\n",
      "reward: 35.37029679641128 setps: 44 count: 523\n",
      "reward: 48.03434550249656 setps: 47 count: 570\n",
      "reward: 61.31604051579197 setps: 46 count: 616\n",
      "reward: 56.550157683028395 setps: 47 count: 663\n",
      "reward: 56.666770039807304 setps: 43 count: 706\n",
      "reward: 15.694237772014457 setps: 35 count: 741\n",
      "reward: 54.54590007226944 setps: 42 count: 783\n",
      "reward: 68.64770462499872 setps: 52 count: 835\n",
      "reward: 38.94793495647172 setps: 43 count: 878\n",
      "reward: 15.318433340780029 setps: 34 count: 912\n",
      "reward: 31.106142044861922 setps: 38 count: 950\n",
      "reward: 59.40838779814804 setps: 48 count: 998\n",
      "avg rewards: 47.9809261643335\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.20293\n",
      "Epoch:20 Batch:13 Loss:0.02411\n",
      "Epoch:40 Batch:13 Loss:0.01533\n",
      "Epoch:60 Batch:13 Loss:0.01388\n",
      "Epoch:80 Batch:13 Loss:0.01206\n",
      "Epoch:100 Batch:13 Loss:0.01047\n",
      "Epoch:120 Batch:13 Loss:0.00971\n",
      "Epoch:140 Batch:13 Loss:0.00898\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.033\n",
      "Epoch:10 Batch:10 Loss:0.036\n",
      "Epoch:20 Batch:10 Loss:0.036\n",
      "Epoch:30 Batch:10 Loss:0.035\n",
      "Epoch:40 Batch:10 Loss:0.034\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 44.2812633032343 setps: 44 count: 44\n",
      "reward: 42.03689597497141 setps: 45 count: 89\n",
      "reward: 57.86009631322084 setps: 55 count: 144\n",
      "reward: 41.00366213782109 setps: 45 count: 189\n",
      "reward: 38.49195467639511 setps: 44 count: 233\n",
      "reward: 30.38764906796132 setps: 38 count: 271\n",
      "reward: 24.434751059355037 setps: 35 count: 306\n",
      "reward: 41.77372441141341 setps: 43 count: 349\n",
      "reward: 15.765794720860141 setps: 33 count: 382\n",
      "reward: 33.24710171483311 setps: 40 count: 422\n",
      "reward: 38.0345262955001 setps: 43 count: 465\n",
      "reward: 22.17775608611118 setps: 36 count: 501\n",
      "reward: 28.10373053031508 setps: 38 count: 539\n",
      "reward: 28.04841649730515 setps: 37 count: 576\n",
      "reward: 40.419681688812844 setps: 44 count: 620\n",
      "reward: 41.48037163545088 setps: 45 count: 665\n",
      "reward: 27.44847500474134 setps: 38 count: 703\n",
      "reward: 45.41076974902535 setps: 48 count: 751\n",
      "reward: 51.014633447119564 setps: 53 count: 804\n",
      "reward: 43.79988389376958 setps: 45 count: 849\n",
      "reward: 49.95944678660889 setps: 48 count: 897\n",
      "reward: 36.33050276894355 setps: 43 count: 940\n",
      "reward: 51.80990505821682 setps: 51 count: 991\n",
      "avg rewards: 37.970477948782\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.13603\n",
      "Epoch:20 Batch:14 Loss:0.01935\n",
      "Epoch:40 Batch:14 Loss:0.01489\n",
      "Epoch:60 Batch:14 Loss:0.01238\n",
      "Epoch:80 Batch:14 Loss:0.01309\n",
      "Epoch:100 Batch:14 Loss:0.01064\n",
      "Epoch:120 Batch:14 Loss:0.01050\n",
      "Epoch:140 Batch:14 Loss:0.00778\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.044\n",
      "Epoch:10 Batch:10 Loss:0.038\n",
      "Epoch:20 Batch:10 Loss:0.043\n",
      "Epoch:30 Batch:10 Loss:0.040\n",
      "Epoch:40 Batch:10 Loss:0.035\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 25.552745646930994 setps: 46 count: 46\n",
      "reward: 63.47288257629116 setps: 66 count: 112\n",
      "reward: 15.075747035637322 setps: 37 count: 149\n",
      "reward: 15.418840757154978 setps: 42 count: 191\n",
      "reward: -7.170677462794991 setps: 29 count: 220\n",
      "reward: 9.229532102664232 setps: 35 count: 255\n",
      "reward: 45.853908956056685 setps: 47 count: 302\n",
      "reward: 51.80505086398043 setps: 52 count: 354\n",
      "reward: 45.026341242868504 setps: 47 count: 401\n",
      "reward: 28.382266209287625 setps: 41 count: 442\n",
      "reward: 16.60545313237962 setps: 40 count: 482\n",
      "reward: 41.72827732173755 setps: 47 count: 529\n",
      "reward: 22.87141869222396 setps: 40 count: 569\n",
      "reward: 60.698098549686286 setps: 68 count: 637\n",
      "reward: 36.18880535927165 setps: 47 count: 684\n",
      "reward: 21.39509295233147 setps: 41 count: 725\n",
      "reward: 44.13893139172432 setps: 46 count: 771\n",
      "reward: 12.73248392019013 setps: 36 count: 807\n",
      "reward: 19.60316363567399 setps: 39 count: 846\n",
      "reward: 64.02837577290339 setps: 50 count: 896\n",
      "reward: 5.857330842231748 setps: 34 count: 930\n",
      "reward: 8.25354651450034 setps: 36 count: 966\n",
      "reward: 6.262866436647528 setps: 33 count: 999\n",
      "avg rewards: 28.391760106503433\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.14195\n",
      "Epoch:20 Batch:15 Loss:0.02168\n",
      "Epoch:40 Batch:15 Loss:0.01421\n",
      "Epoch:60 Batch:15 Loss:0.01539\n",
      "Epoch:80 Batch:15 Loss:0.01161\n",
      "Epoch:100 Batch:15 Loss:0.01255\n",
      "Epoch:120 Batch:15 Loss:0.00979\n",
      "Epoch:140 Batch:15 Loss:0.01135\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.037\n",
      "Epoch:10 Batch:10 Loss:0.035\n",
      "Epoch:20 Batch:10 Loss:0.037\n",
      "Epoch:30 Batch:10 Loss:0.038\n",
      "Epoch:40 Batch:10 Loss:0.038\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 52.70609393566411 setps: 46 count: 46\n",
      "reward: 117.96659032462635 setps: 112 count: 158\n",
      "reward: 42.865521508196245 setps: 43 count: 201\n",
      "reward: 54.14692487922729 setps: 51 count: 252\n",
      "reward: -3.675630438931676 setps: 77 count: 329\n",
      "reward: 47.28337038815224 setps: 74 count: 403\n",
      "reward: 47.039165884551764 setps: 45 count: 448\n",
      "reward: 50.75583462345384 setps: 45 count: 493\n",
      "reward: 49.70487325955181 setps: 44 count: 537\n",
      "reward: 53.380857226373344 setps: 49 count: 586\n",
      "reward: 58.613923808008266 setps: 52 count: 638\n",
      "reward: 50.19647531460504 setps: 47 count: 685\n",
      "reward: 44.517653077858256 setps: 52 count: 737\n",
      "reward: 69.29306413710874 setps: 67 count: 804\n",
      "reward: 114.88181122447058 setps: 182 count: 986\n",
      "avg rewards: 56.645101943527756\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.09024\n",
      "Epoch:20 Batch:16 Loss:0.02205\n",
      "Epoch:40 Batch:16 Loss:0.01541\n",
      "Epoch:60 Batch:16 Loss:0.01594\n",
      "Epoch:80 Batch:16 Loss:0.01167\n",
      "Epoch:100 Batch:16 Loss:0.01440\n",
      "Epoch:120 Batch:16 Loss:0.01193\n",
      "Epoch:140 Batch:16 Loss:0.00993\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.027\n",
      "Epoch:10 Batch:10 Loss:0.027\n",
      "Epoch:20 Batch:10 Loss:0.025\n",
      "Epoch:30 Batch:10 Loss:0.025\n",
      "Epoch:40 Batch:10 Loss:0.023\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 18.572298999247142 setps: 37 count: 37\n",
      "reward: 13.504601490120695 setps: 35 count: 72\n",
      "reward: 51.62555319917446 setps: 37 count: 109\n",
      "reward: 28.873443802673126 setps: 28 count: 137\n",
      "reward: 94.13366798034698 setps: 86 count: 223\n",
      "reward: 44.321069820215065 setps: 33 count: 256\n",
      "reward: 57.94708886096195 setps: 42 count: 298\n",
      "reward: 61.57199955766409 setps: 86 count: 384\n",
      "reward: 51.13607117720095 setps: 43 count: 427\n",
      "reward: 65.93619619932285 setps: 51 count: 478\n",
      "reward: 127.69503232503489 setps: 128 count: 606\n",
      "reward: 42.68229861436702 setps: 30 count: 636\n",
      "reward: 6.5512928235824734 setps: 46 count: 682\n",
      "reward: 77.19966612395801 setps: 73 count: 755\n",
      "reward: 67.54613010701722 setps: 73 count: 828\n",
      "reward: 49.2676516834923 setps: 39 count: 867\n",
      "reward: 38.51472284476476 setps: 30 count: 897\n",
      "reward: 43.88202310750201 setps: 31 count: 928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 44.72439488123635 setps: 35 count: 963\n",
      "avg rewards: 51.87816861041486\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.14833\n",
      "Epoch:20 Batch:17 Loss:0.02010\n",
      "Epoch:40 Batch:17 Loss:0.01517\n",
      "Epoch:60 Batch:17 Loss:0.01387\n",
      "Epoch:80 Batch:17 Loss:0.01299\n",
      "Epoch:100 Batch:17 Loss:0.01083\n",
      "Epoch:120 Batch:17 Loss:0.01332\n",
      "Epoch:140 Batch:17 Loss:0.00972\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.020\n",
      "Epoch:10 Batch:10 Loss:0.020\n",
      "Epoch:20 Batch:10 Loss:0.017\n",
      "Epoch:30 Batch:10 Loss:0.016\n",
      "Epoch:40 Batch:10 Loss:0.019\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 21.37153713619482 setps: 33 count: 33\n",
      "reward: 18.812868994841114 setps: 34 count: 67\n",
      "reward: 25.95710514842504 setps: 43 count: 110\n",
      "reward: 22.733338577086396 setps: 36 count: 146\n",
      "reward: 22.22576118272119 setps: 37 count: 183\n",
      "reward: 14.492049990051601 setps: 35 count: 218\n",
      "reward: 17.511025442728716 setps: 33 count: 251\n",
      "reward: 15.908031056742766 setps: 33 count: 284\n",
      "reward: 13.48665607255534 setps: 32 count: 316\n",
      "reward: 15.22155142045958 setps: 31 count: 347\n",
      "reward: 18.045256603603775 setps: 34 count: 381\n",
      "reward: 117.28812899081677 setps: 112 count: 493\n",
      "reward: 16.601088984556558 setps: 34 count: 527\n",
      "reward: 24.946702582038412 setps: 39 count: 566\n",
      "reward: 14.948785147628222 setps: 34 count: 600\n",
      "reward: 21.775884156415124 setps: 37 count: 637\n",
      "reward: 19.415043013377 setps: 32 count: 669\n",
      "reward: 12.607090907293603 setps: 31 count: 700\n",
      "reward: 109.69618366586218 setps: 101 count: 801\n",
      "reward: 15.521395596470395 setps: 33 count: 834\n",
      "reward: 17.264396501425658 setps: 34 count: 868\n",
      "reward: 13.76548099083011 setps: 31 count: 899\n",
      "reward: 34.50055372160133 setps: 41 count: 940\n",
      "reward: 20.835871369941746 setps: 34 count: 974\n",
      "avg rewards: 26.872157802236146\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.09083\n",
      "Epoch:20 Batch:18 Loss:0.02026\n",
      "Epoch:40 Batch:18 Loss:0.01487\n",
      "Epoch:60 Batch:18 Loss:0.01284\n",
      "Epoch:80 Batch:18 Loss:0.01425\n",
      "Epoch:100 Batch:18 Loss:0.01257\n",
      "Epoch:120 Batch:18 Loss:0.01001\n",
      "Epoch:140 Batch:18 Loss:0.01122\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.021\n",
      "Epoch:10 Batch:10 Loss:0.018\n",
      "Epoch:20 Batch:10 Loss:0.020\n",
      "Epoch:30 Batch:10 Loss:0.021\n",
      "Epoch:40 Batch:10 Loss:0.017\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 21.16749268238636 setps: 38 count: 38\n",
      "reward: 88.69745505009342 setps: 84 count: 122\n",
      "reward: 30.17291693456937 setps: 39 count: 161\n",
      "reward: 13.408022377362068 setps: 37 count: 198\n",
      "reward: 22.460087250256034 setps: 36 count: 234\n",
      "reward: 160.07438888318876 setps: 161 count: 395\n",
      "reward: 54.217394795881404 setps: 49 count: 444\n",
      "reward: 16.79879755468719 setps: 36 count: 480\n",
      "reward: 27.42770367080666 setps: 40 count: 520\n",
      "reward: 19.78934356985264 setps: 35 count: 555\n",
      "reward: 85.15974698434145 setps: 79 count: 634\n",
      "reward: 86.29502563310237 setps: 82 count: 716\n",
      "reward: 87.33606474092373 setps: 82 count: 798\n",
      "reward: 20.85510365568043 setps: 36 count: 834\n",
      "reward: 15.090135564842788 setps: 34 count: 868\n",
      "reward: 79.04633093357695 setps: 74 count: 942\n",
      "avg rewards: 51.74975064259698\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.08416\n",
      "Epoch:20 Batch:19 Loss:0.01897\n",
      "Epoch:40 Batch:19 Loss:0.01475\n",
      "Epoch:60 Batch:19 Loss:0.01639\n",
      "Epoch:80 Batch:19 Loss:0.01208\n",
      "Epoch:100 Batch:19 Loss:0.01002\n",
      "Epoch:120 Batch:19 Loss:0.01059\n",
      "Epoch:140 Batch:19 Loss:0.00994\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.018\n",
      "Epoch:10 Batch:10 Loss:0.016\n",
      "Epoch:20 Batch:10 Loss:0.017\n",
      "Epoch:30 Batch:10 Loss:0.018\n",
      "Epoch:40 Batch:10 Loss:0.016\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 74.46814412863462 setps: 67 count: 67\n",
      "reward: 20.263848461328593 setps: 35 count: 102\n",
      "reward: 20.60203485479724 setps: 35 count: 137\n",
      "reward: 77.14992269153298 setps: 69 count: 206\n",
      "reward: 19.665502184524666 setps: 34 count: 240\n",
      "reward: 51.118548930167165 setps: 51 count: 291\n",
      "reward: 16.44730402165879 setps: 36 count: 327\n",
      "reward: 117.4024684249701 setps: 109 count: 436\n",
      "reward: 73.8407132371998 setps: 67 count: 503\n",
      "reward: 74.26172918467783 setps: 68 count: 571\n",
      "reward: 75.71084014621884 setps: 66 count: 637\n",
      "reward: 24.851105104318417 setps: 38 count: 675\n",
      "reward: 14.982532353371788 setps: 35 count: 710\n",
      "reward: 10.963206666974294 setps: 32 count: 742\n",
      "reward: 14.825873104264609 setps: 33 count: 775\n",
      "reward: 79.92871860758896 setps: 71 count: 846\n",
      "reward: 11.389756785733333 setps: 32 count: 878\n",
      "reward: 92.43302386648283 setps: 85 count: 963\n",
      "reward: 18.094052854066835 setps: 33 count: 996\n",
      "avg rewards: 46.757859242553245\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.08686\n",
      "Epoch:20 Batch:20 Loss:0.01763\n",
      "Epoch:40 Batch:20 Loss:0.01235\n",
      "Epoch:60 Batch:20 Loss:0.01172\n",
      "Epoch:80 Batch:20 Loss:0.01174\n",
      "Epoch:100 Batch:20 Loss:0.01061\n",
      "Epoch:120 Batch:20 Loss:0.00934\n",
      "Epoch:140 Batch:20 Loss:0.00969\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.020\n",
      "Epoch:10 Batch:10 Loss:0.016\n",
      "Epoch:20 Batch:10 Loss:0.017\n",
      "Epoch:30 Batch:10 Loss:0.016\n",
      "Epoch:40 Batch:10 Loss:0.016\n",
      "Done!\n",
      "############# start HalfCheetahBulletEnv-v0 training ###################\n",
      "(50, 1000, 26)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1128.1485240440795 setps: 800 count: 800\n",
      "avg rewards: -1128.1485240440795\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:0.34681\n",
      "Epoch:20 Batch:1 Loss:0.25166\n",
      "Epoch:40 Batch:1 Loss:0.20558\n",
      "Epoch:60 Batch:1 Loss:0.19216\n",
      "Epoch:80 Batch:1 Loss:0.17350\n",
      "Epoch:100 Batch:1 Loss:0.15118\n",
      "Epoch:120 Batch:1 Loss:0.13059\n",
      "Epoch:140 Batch:1 Loss:0.11182\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.283\n",
      "Epoch:10 Batch:10 Loss:0.282\n",
      "Epoch:20 Batch:10 Loss:0.280\n",
      "Epoch:30 Batch:10 Loss:0.278\n",
      "Epoch:40 Batch:10 Loss:0.270\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1083.131264137426 setps: 800 count: 800\n",
      "avg rewards: -1083.131264137426\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:0.34629\n",
      "Epoch:20 Batch:2 Loss:0.14404\n",
      "Epoch:40 Batch:2 Loss:0.09965\n",
      "Epoch:60 Batch:2 Loss:0.08831\n",
      "Epoch:80 Batch:2 Loss:0.07749\n",
      "Epoch:100 Batch:2 Loss:0.05960\n",
      "Epoch:120 Batch:2 Loss:0.05328\n",
      "Epoch:140 Batch:2 Loss:0.05239\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.231\n",
      "Epoch:10 Batch:10 Loss:0.224\n",
      "Epoch:20 Batch:10 Loss:0.224\n",
      "Epoch:30 Batch:10 Loss:0.226\n",
      "Epoch:40 Batch:10 Loss:0.218\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1036.8594810212421 setps: 800 count: 800\n",
      "avg rewards: -1036.8594810212421\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.36849\n",
      "Epoch:20 Batch:3 Loss:0.09850\n",
      "Epoch:40 Batch:3 Loss:0.07212\n",
      "Epoch:60 Batch:3 Loss:0.06085\n",
      "Epoch:80 Batch:3 Loss:0.05146\n",
      "Epoch:100 Batch:3 Loss:0.04290\n",
      "Epoch:120 Batch:3 Loss:0.03904\n",
      "Epoch:140 Batch:3 Loss:0.03275\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.167\n",
      "Epoch:10 Batch:10 Loss:0.163\n",
      "Epoch:20 Batch:10 Loss:0.163\n",
      "Epoch:30 Batch:10 Loss:0.161\n",
      "Epoch:40 Batch:10 Loss:0.155\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1200.917876178667 setps: 800 count: 800\n",
      "avg rewards: -1200.917876178667\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.37524\n",
      "Epoch:20 Batch:4 Loss:0.08046\n",
      "Epoch:40 Batch:4 Loss:0.06616\n",
      "Epoch:60 Batch:4 Loss:0.04647\n",
      "Epoch:80 Batch:4 Loss:0.03547\n",
      "Epoch:100 Batch:4 Loss:0.03199\n",
      "Epoch:120 Batch:4 Loss:0.02858\n",
      "Epoch:140 Batch:4 Loss:0.02411\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.161\n",
      "Epoch:10 Batch:10 Loss:0.149\n",
      "Epoch:20 Batch:10 Loss:0.154\n",
      "Epoch:30 Batch:10 Loss:0.153\n",
      "Epoch:40 Batch:10 Loss:0.147\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -282.2236454939632 setps: 800 count: 800\n",
      "avg rewards: -282.2236454939632\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.36958\n",
      "Epoch:20 Batch:5 Loss:0.08529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:40 Batch:5 Loss:0.05167\n",
      "Epoch:60 Batch:5 Loss:0.04258\n",
      "Epoch:80 Batch:5 Loss:0.03600\n",
      "Epoch:100 Batch:5 Loss:0.03145\n",
      "Epoch:120 Batch:5 Loss:0.02702\n",
      "Epoch:140 Batch:5 Loss:0.02569\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.094\n",
      "Epoch:10 Batch:10 Loss:0.087\n",
      "Epoch:20 Batch:10 Loss:0.087\n",
      "Epoch:30 Batch:10 Loss:0.085\n",
      "Epoch:40 Batch:10 Loss:0.084\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1710.605085608498 setps: 800 count: 800\n",
      "avg rewards: -1710.605085608498\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.39230\n",
      "Epoch:20 Batch:6 Loss:0.06281\n",
      "Epoch:40 Batch:6 Loss:0.04037\n",
      "Epoch:60 Batch:6 Loss:0.03124\n",
      "Epoch:80 Batch:6 Loss:0.02917\n",
      "Epoch:100 Batch:6 Loss:0.02711\n",
      "Epoch:120 Batch:6 Loss:0.02052\n",
      "Epoch:140 Batch:6 Loss:0.02035\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.082\n",
      "Epoch:10 Batch:10 Loss:0.076\n",
      "Epoch:20 Batch:10 Loss:0.078\n",
      "Epoch:30 Batch:10 Loss:0.071\n",
      "Epoch:40 Batch:10 Loss:0.080\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1389.3737635701962 setps: 800 count: 800\n",
      "avg rewards: -1389.3737635701962\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.40313\n",
      "Epoch:20 Batch:7 Loss:0.05916\n",
      "Epoch:40 Batch:7 Loss:0.04091\n",
      "Epoch:60 Batch:7 Loss:0.02971\n",
      "Epoch:80 Batch:7 Loss:0.02581\n",
      "Epoch:100 Batch:7 Loss:0.02306\n",
      "Epoch:120 Batch:7 Loss:0.02106\n",
      "Epoch:140 Batch:7 Loss:0.01863\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.065\n",
      "Epoch:10 Batch:10 Loss:0.068\n",
      "Epoch:20 Batch:10 Loss:0.060\n",
      "Epoch:30 Batch:10 Loss:0.063\n",
      "Epoch:40 Batch:10 Loss:0.062\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1298.6263008928229 setps: 800 count: 800\n",
      "avg rewards: -1298.6263008928229\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.35580\n",
      "Epoch:20 Batch:8 Loss:0.05198\n",
      "Epoch:40 Batch:8 Loss:0.03402\n",
      "Epoch:60 Batch:8 Loss:0.02798\n",
      "Epoch:80 Batch:8 Loss:0.02135\n",
      "Epoch:100 Batch:8 Loss:0.02058\n",
      "Epoch:120 Batch:8 Loss:0.01867\n",
      "Epoch:140 Batch:8 Loss:0.01713\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.072\n",
      "Epoch:10 Batch:10 Loss:0.067\n",
      "Epoch:20 Batch:10 Loss:0.068\n",
      "Epoch:30 Batch:10 Loss:0.067\n",
      "Epoch:40 Batch:10 Loss:0.062\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1714.620757265952 setps: 800 count: 800\n",
      "avg rewards: -1714.620757265952\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.36446\n",
      "Epoch:20 Batch:9 Loss:0.04961\n",
      "Epoch:40 Batch:9 Loss:0.03240\n",
      "Epoch:60 Batch:9 Loss:0.02531\n",
      "Epoch:80 Batch:9 Loss:0.02254\n",
      "Epoch:100 Batch:9 Loss:0.02187\n",
      "Epoch:120 Batch:9 Loss:0.02220\n",
      "Epoch:140 Batch:9 Loss:0.01760\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.072\n",
      "Epoch:10 Batch:10 Loss:0.067\n",
      "Epoch:20 Batch:10 Loss:0.066\n",
      "Epoch:30 Batch:10 Loss:0.064\n",
      "Epoch:40 Batch:10 Loss:0.064\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1856.7398606268875 setps: 800 count: 800\n",
      "avg rewards: -1856.7398606268875\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.32231\n",
      "Epoch:20 Batch:10 Loss:0.04780\n",
      "Epoch:40 Batch:10 Loss:0.03226\n",
      "Epoch:60 Batch:10 Loss:0.02428\n",
      "Epoch:80 Batch:10 Loss:0.02501\n",
      "Epoch:100 Batch:10 Loss:0.02118\n",
      "Epoch:120 Batch:10 Loss:0.02129\n",
      "Epoch:140 Batch:10 Loss:0.02099\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.063\n",
      "Epoch:10 Batch:10 Loss:0.062\n",
      "Epoch:20 Batch:10 Loss:0.060\n",
      "Epoch:30 Batch:10 Loss:0.060\n",
      "Epoch:40 Batch:10 Loss:0.061\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1307.9740059820786 setps: 800 count: 800\n",
      "avg rewards: -1307.9740059820786\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.26761\n",
      "Epoch:20 Batch:11 Loss:0.04456\n",
      "Epoch:40 Batch:11 Loss:0.02998\n",
      "Epoch:60 Batch:11 Loss:0.02507\n",
      "Epoch:80 Batch:11 Loss:0.02237\n",
      "Epoch:100 Batch:11 Loss:0.02072\n",
      "Epoch:120 Batch:11 Loss:0.01903\n",
      "Epoch:140 Batch:11 Loss:0.01587\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.060\n",
      "Epoch:10 Batch:10 Loss:0.059\n",
      "Epoch:20 Batch:10 Loss:0.055\n",
      "Epoch:30 Batch:10 Loss:0.055\n",
      "Epoch:40 Batch:10 Loss:0.054\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1192.9452006012634 setps: 800 count: 800\n",
      "avg rewards: -1192.9452006012634\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.30161\n",
      "Epoch:20 Batch:12 Loss:0.04455\n",
      "Epoch:40 Batch:12 Loss:0.03086\n",
      "Epoch:60 Batch:12 Loss:0.02305\n",
      "Epoch:80 Batch:12 Loss:0.02161\n",
      "Epoch:100 Batch:12 Loss:0.01865\n",
      "Epoch:120 Batch:12 Loss:0.01967\n",
      "Epoch:140 Batch:12 Loss:0.01777\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.054\n",
      "Epoch:10 Batch:10 Loss:0.050\n",
      "Epoch:20 Batch:10 Loss:0.049\n",
      "Epoch:30 Batch:10 Loss:0.051\n",
      "Epoch:40 Batch:10 Loss:0.050\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1183.5418879299698 setps: 800 count: 800\n",
      "avg rewards: -1183.5418879299698\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.28194\n",
      "Epoch:20 Batch:13 Loss:0.04105\n",
      "Epoch:40 Batch:13 Loss:0.03414\n",
      "Epoch:60 Batch:13 Loss:0.02480\n",
      "Epoch:80 Batch:13 Loss:0.02202\n",
      "Epoch:100 Batch:13 Loss:0.01963\n",
      "Epoch:120 Batch:13 Loss:0.01760\n",
      "Epoch:140 Batch:13 Loss:0.01761\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.055\n",
      "Epoch:10 Batch:10 Loss:0.053\n",
      "Epoch:20 Batch:10 Loss:0.051\n",
      "Epoch:30 Batch:10 Loss:0.049\n",
      "Epoch:40 Batch:10 Loss:0.050\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -654.6509017175055 setps: 800 count: 800\n",
      "avg rewards: -654.6509017175055\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.24150\n",
      "Epoch:20 Batch:14 Loss:0.03815\n",
      "Epoch:40 Batch:14 Loss:0.02597\n",
      "Epoch:60 Batch:14 Loss:0.02270\n",
      "Epoch:80 Batch:14 Loss:0.02246\n",
      "Epoch:100 Batch:14 Loss:0.02127\n",
      "Epoch:120 Batch:14 Loss:0.01843\n",
      "Epoch:140 Batch:14 Loss:0.01859\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.045\n",
      "Epoch:10 Batch:10 Loss:0.039\n",
      "Epoch:20 Batch:10 Loss:0.038\n",
      "Epoch:30 Batch:10 Loss:0.038\n",
      "Epoch:40 Batch:10 Loss:0.037\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -46.450371499436706 setps: 800 count: 800\n",
      "avg rewards: -46.450371499436706\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.23447\n",
      "Epoch:20 Batch:15 Loss:0.04316\n",
      "Epoch:40 Batch:15 Loss:0.03068\n",
      "Epoch:60 Batch:15 Loss:0.02707\n",
      "Epoch:80 Batch:15 Loss:0.02559\n",
      "Epoch:100 Batch:15 Loss:0.02522\n",
      "Epoch:120 Batch:15 Loss:0.02128\n",
      "Epoch:140 Batch:15 Loss:0.01771\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.034\n",
      "Epoch:10 Batch:10 Loss:0.030\n",
      "Epoch:20 Batch:10 Loss:0.030\n",
      "Epoch:30 Batch:10 Loss:0.028\n",
      "Epoch:40 Batch:10 Loss:0.027\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1079.8065650928904 setps: 800 count: 800\n",
      "avg rewards: -1079.8065650928904\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.24578\n",
      "Epoch:20 Batch:16 Loss:0.04447\n",
      "Epoch:40 Batch:16 Loss:0.02958\n",
      "Epoch:60 Batch:16 Loss:0.02535\n",
      "Epoch:80 Batch:16 Loss:0.02635\n",
      "Epoch:100 Batch:16 Loss:0.02170\n",
      "Epoch:120 Batch:16 Loss:0.02033\n",
      "Epoch:140 Batch:16 Loss:0.01931\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.030\n",
      "Epoch:10 Batch:10 Loss:0.028\n",
      "Epoch:20 Batch:10 Loss:0.028\n",
      "Epoch:30 Batch:10 Loss:0.028\n",
      "Epoch:40 Batch:10 Loss:0.028\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1048.8907979356027 setps: 800 count: 800\n",
      "avg rewards: -1048.8907979356027\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.28432\n",
      "Epoch:20 Batch:17 Loss:0.04403\n",
      "Epoch:40 Batch:17 Loss:0.02749\n",
      "Epoch:60 Batch:17 Loss:0.02232\n",
      "Epoch:80 Batch:17 Loss:0.02274\n",
      "Epoch:100 Batch:17 Loss:0.01835\n",
      "Epoch:120 Batch:17 Loss:0.01652\n",
      "Epoch:140 Batch:17 Loss:0.01934\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:10 Loss:0.031\n",
      "Epoch:10 Batch:10 Loss:0.030\n",
      "Epoch:20 Batch:10 Loss:0.029\n",
      "Epoch:30 Batch:10 Loss:0.027\n",
      "Epoch:40 Batch:10 Loss:0.028\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1063.9591225415465 setps: 800 count: 800\n",
      "avg rewards: -1063.9591225415465\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.24040\n",
      "Epoch:20 Batch:18 Loss:0.04053\n",
      "Epoch:40 Batch:18 Loss:0.02630\n",
      "Epoch:60 Batch:18 Loss:0.02281\n",
      "Epoch:80 Batch:18 Loss:0.02042\n",
      "Epoch:100 Batch:18 Loss:0.01874\n",
      "Epoch:120 Batch:18 Loss:0.01931\n",
      "Epoch:140 Batch:18 Loss:0.01985\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.029\n",
      "Epoch:10 Batch:10 Loss:0.027\n",
      "Epoch:20 Batch:10 Loss:0.026\n",
      "Epoch:30 Batch:10 Loss:0.026\n",
      "Epoch:40 Batch:10 Loss:0.027\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1096.8712376334188 setps: 800 count: 800\n",
      "avg rewards: -1096.8712376334188\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.22708\n",
      "Epoch:20 Batch:19 Loss:0.03406\n",
      "Epoch:40 Batch:19 Loss:0.02852\n",
      "Epoch:60 Batch:19 Loss:0.02388\n",
      "Epoch:80 Batch:19 Loss:0.02002\n",
      "Epoch:100 Batch:19 Loss:0.01913\n",
      "Epoch:120 Batch:19 Loss:0.02120\n",
      "Epoch:140 Batch:19 Loss:0.01932\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.029\n",
      "Epoch:10 Batch:10 Loss:0.027\n",
      "Epoch:20 Batch:10 Loss:0.026\n",
      "Epoch:30 Batch:10 Loss:0.027\n",
      "Epoch:40 Batch:10 Loss:0.027\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -892.7739148362338 setps: 800 count: 800\n",
      "avg rewards: -892.7739148362338\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.23538\n",
      "Epoch:20 Batch:20 Loss:0.04141\n",
      "Epoch:40 Batch:20 Loss:0.02805\n",
      "Epoch:60 Batch:20 Loss:0.02323\n",
      "Epoch:80 Batch:20 Loss:0.01991\n",
      "Epoch:100 Batch:20 Loss:0.02063\n",
      "Epoch:120 Batch:20 Loss:0.01733\n",
      "Epoch:140 Batch:20 Loss:0.02082\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.027\n",
      "Epoch:10 Batch:10 Loss:0.028\n",
      "Epoch:20 Batch:10 Loss:0.029\n",
      "Epoch:30 Batch:10 Loss:0.026\n",
      "Epoch:40 Batch:10 Loss:0.027\n",
      "Done!\n",
      "############# start AntBulletEnv-v0 training ###################\n",
      "(50, 1000, 28)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 425.9359149051115 setps: 800 count: 800\n",
      "reward: 33.65539614537992 setps: 65 count: 865\n",
      "avg rewards: 229.7956555252457\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:0.39167\n",
      "Epoch:20 Batch:1 Loss:0.12476\n",
      "Epoch:40 Batch:1 Loss:0.09535\n",
      "Epoch:60 Batch:1 Loss:0.07364\n",
      "Epoch:80 Batch:1 Loss:0.05120\n",
      "Epoch:100 Batch:1 Loss:0.04762\n",
      "Epoch:120 Batch:1 Loss:0.04541\n",
      "Epoch:140 Batch:1 Loss:0.04250\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.306\n",
      "Epoch:10 Batch:10 Loss:0.304\n",
      "Epoch:20 Batch:10 Loss:0.303\n",
      "Epoch:30 Batch:10 Loss:0.300\n",
      "Epoch:40 Batch:10 Loss:0.302\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 9.477918427917757 setps: 20 count: 20\n",
      "reward: 9.492471611958171 setps: 20 count: 40\n",
      "reward: 9.510213896622009 setps: 20 count: 60\n",
      "reward: 9.228278922448224 setps: 20 count: 80\n",
      "reward: 9.349108108208746 setps: 20 count: 100\n",
      "reward: 9.331658476448501 setps: 20 count: 120\n",
      "reward: 9.38459692632605 setps: 20 count: 140\n",
      "reward: 9.558091308352596 setps: 20 count: 160\n",
      "reward: 9.4051979402022 setps: 20 count: 180\n",
      "reward: 9.34563281285955 setps: 20 count: 200\n",
      "reward: 9.261251043566155 setps: 20 count: 220\n",
      "reward: 9.5320584909452 setps: 20 count: 240\n",
      "reward: 9.515090438652258 setps: 20 count: 260\n",
      "reward: 9.516551637863447 setps: 20 count: 280\n",
      "reward: 9.224315684873725 setps: 20 count: 300\n",
      "reward: 8.594690220910707 setps: 20 count: 320\n",
      "reward: 8.83510207815125 setps: 20 count: 340\n",
      "reward: 9.433701974553696 setps: 20 count: 360\n",
      "reward: 9.508810470577735 setps: 20 count: 380\n",
      "reward: 9.439557602556306 setps: 20 count: 400\n",
      "reward: 9.438459826307373 setps: 20 count: 420\n",
      "reward: 9.613129160823883 setps: 20 count: 440\n",
      "reward: 9.21555584703019 setps: 20 count: 460\n",
      "reward: 9.450734580356222 setps: 20 count: 480\n",
      "reward: 9.49087231977028 setps: 20 count: 500\n",
      "reward: 9.54105435320962 setps: 20 count: 520\n",
      "reward: 9.377300420260871 setps: 20 count: 540\n",
      "reward: 9.495030569359372 setps: 20 count: 560\n",
      "reward: 8.693815394364355 setps: 20 count: 580\n",
      "reward: 9.470011261604668 setps: 20 count: 600\n",
      "reward: 8.970340493580448 setps: 20 count: 620\n",
      "reward: 9.264062059730346 setps: 20 count: 640\n",
      "reward: 9.639853559700715 setps: 20 count: 660\n",
      "reward: 9.205667365633417 setps: 20 count: 680\n",
      "reward: 9.526073149569857 setps: 20 count: 700\n",
      "reward: 9.566614468130865 setps: 20 count: 720\n",
      "reward: 9.256672528503987 setps: 20 count: 740\n",
      "reward: 9.42446203175496 setps: 20 count: 760\n",
      "reward: 9.739699844407733 setps: 20 count: 780\n",
      "reward: 9.512187307990096 setps: 20 count: 800\n",
      "reward: 9.47917990966089 setps: 20 count: 820\n",
      "reward: 9.519960394114605 setps: 20 count: 840\n",
      "reward: 11.168004701103202 setps: 21 count: 861\n",
      "reward: 11.823352225533744 setps: 21 count: 882\n",
      "reward: 9.657404271808627 setps: 20 count: 902\n",
      "reward: 9.014252853147628 setps: 20 count: 922\n",
      "reward: 9.497578693281685 setps: 20 count: 942\n",
      "reward: 9.521660566626817 setps: 20 count: 962\n",
      "reward: 9.563284765575373 setps: 20 count: 982\n",
      "avg rewards: 9.471032101978288\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:0.46343\n",
      "Epoch:20 Batch:2 Loss:0.10492\n",
      "Epoch:40 Batch:2 Loss:0.05583\n",
      "Epoch:60 Batch:2 Loss:0.03071\n",
      "Epoch:80 Batch:2 Loss:0.02560\n",
      "Epoch:100 Batch:2 Loss:0.02441\n",
      "Epoch:120 Batch:2 Loss:0.02327\n",
      "Epoch:140 Batch:2 Loss:0.01976\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.275\n",
      "Epoch:10 Batch:10 Loss:0.272\n",
      "Epoch:20 Batch:10 Loss:0.267\n",
      "Epoch:30 Batch:10 Loss:0.265\n",
      "Epoch:40 Batch:10 Loss:0.267\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 7.144327152676123 setps: 20 count: 20\n",
      "reward: 6.854006200349249 setps: 20 count: 40\n",
      "reward: 7.061523500127077 setps: 20 count: 60\n",
      "reward: 7.0146567681775185 setps: 20 count: 80\n",
      "reward: 7.329892942419976 setps: 20 count: 100\n",
      "reward: 6.808094671397703 setps: 20 count: 120\n",
      "reward: 6.9568075433649925 setps: 20 count: 140\n",
      "reward: 7.081117940698459 setps: 20 count: 160\n",
      "reward: 7.194927910066326 setps: 20 count: 180\n",
      "reward: 6.906572823727037 setps: 20 count: 200\n",
      "reward: 7.2517770318212555 setps: 20 count: 220\n",
      "reward: 6.9997797872667435 setps: 20 count: 240\n",
      "reward: 6.932360722607699 setps: 20 count: 260\n",
      "reward: 7.182131333996951 setps: 20 count: 280\n",
      "reward: 6.98165398954734 setps: 20 count: 300\n",
      "reward: 9.049656936789688 setps: 22 count: 322\n",
      "reward: 6.8763383209327 setps: 20 count: 342\n",
      "reward: 7.245960508842836 setps: 20 count: 362\n",
      "reward: 6.983805514551933 setps: 20 count: 382\n",
      "reward: 7.329661224462322 setps: 20 count: 402\n",
      "reward: 7.1568704533827265 setps: 20 count: 422\n",
      "reward: 7.409069806148182 setps: 20 count: 442\n",
      "reward: 7.084808721809529 setps: 20 count: 462\n",
      "reward: 7.3418289389024745 setps: 20 count: 482\n",
      "reward: 7.2936443369617345 setps: 20 count: 502\n",
      "reward: 6.954992394847794 setps: 20 count: 522\n",
      "reward: 7.154820098198252 setps: 20 count: 542\n",
      "reward: 7.003482097342203 setps: 20 count: 562\n",
      "reward: 6.866996680165176 setps: 20 count: 582\n",
      "reward: 7.252637926120952 setps: 20 count: 602\n",
      "reward: 6.951195875012489 setps: 20 count: 622\n",
      "reward: 7.182262927229749 setps: 20 count: 642\n",
      "reward: 6.833372571384826 setps: 20 count: 662\n",
      "reward: 6.947254773011083 setps: 20 count: 682\n",
      "reward: 7.237650931053211 setps: 20 count: 702\n",
      "reward: 7.373747614135208 setps: 20 count: 722\n",
      "reward: 7.248022809225951 setps: 20 count: 742\n",
      "reward: 6.9439792745411975 setps: 20 count: 762\n",
      "reward: 7.077281928743469 setps: 20 count: 782\n",
      "reward: 6.994379492510052 setps: 20 count: 802\n",
      "reward: 7.018206502337126 setps: 20 count: 822\n",
      "reward: 7.030878186321933 setps: 20 count: 842\n",
      "reward: 6.940400824074457 setps: 20 count: 862\n",
      "reward: 7.136997042993608 setps: 20 count: 882\n",
      "reward: 7.161022596557452 setps: 20 count: 902\n",
      "reward: 7.217862014428828 setps: 20 count: 922\n",
      "reward: 7.008200261904857 setps: 20 count: 942\n",
      "reward: 7.255016604562115 setps: 20 count: 962\n",
      "reward: 6.8484478091748295 setps: 20 count: 982\n",
      "avg rewards: 7.124701720753172\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:3 Loss:0.46433\n",
      "Epoch:20 Batch:3 Loss:0.07200\n",
      "Epoch:40 Batch:3 Loss:0.03601\n",
      "Epoch:60 Batch:3 Loss:0.02641\n",
      "Epoch:80 Batch:3 Loss:0.02263\n",
      "Epoch:100 Batch:3 Loss:0.01920\n",
      "Epoch:120 Batch:3 Loss:0.01681\n",
      "Epoch:140 Batch:3 Loss:0.01593\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.277\n",
      "Epoch:10 Batch:10 Loss:0.270\n",
      "Epoch:20 Batch:10 Loss:0.267\n",
      "Epoch:30 Batch:10 Loss:0.267\n",
      "Epoch:40 Batch:10 Loss:0.266\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 462.3469161212556 setps: 800 count: 800\n",
      "avg rewards: 462.3469161212556\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.48572\n",
      "Epoch:20 Batch:4 Loss:0.05264\n",
      "Epoch:40 Batch:4 Loss:0.02600\n",
      "Epoch:60 Batch:4 Loss:0.01960\n",
      "Epoch:80 Batch:4 Loss:0.01500\n",
      "Epoch:100 Batch:4 Loss:0.01297\n",
      "Epoch:120 Batch:4 Loss:0.01281\n",
      "Epoch:140 Batch:4 Loss:0.01216\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.258\n",
      "Epoch:10 Batch:10 Loss:0.255\n",
      "Epoch:20 Batch:10 Loss:0.251\n",
      "Epoch:30 Batch:10 Loss:0.251\n",
      "Epoch:40 Batch:10 Loss:0.249\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 7.432066703362216 setps: 20 count: 20\n",
      "reward: 7.006831266233345 setps: 20 count: 40\n",
      "reward: 7.821174989770227 setps: 20 count: 60\n",
      "reward: 7.803238419211993 setps: 20 count: 80\n",
      "reward: 7.700758165071601 setps: 20 count: 100\n",
      "reward: 7.560186205506034 setps: 20 count: 120\n",
      "reward: 7.381136596234866 setps: 20 count: 140\n",
      "reward: 7.475113781899563 setps: 20 count: 160\n",
      "reward: 7.554235188303573 setps: 20 count: 180\n",
      "reward: 7.658396607908072 setps: 20 count: 200\n",
      "reward: 8.156285153284259 setps: 20 count: 220\n",
      "reward: 7.188577141195127 setps: 20 count: 240\n",
      "reward: 7.3874870142724856 setps: 20 count: 260\n",
      "reward: 7.378392452707341 setps: 20 count: 280\n",
      "reward: 7.754559671341851 setps: 20 count: 300\n",
      "reward: 7.836981539237604 setps: 20 count: 320\n",
      "reward: 7.475725010984753 setps: 20 count: 340\n",
      "reward: 7.632139333341909 setps: 20 count: 360\n",
      "reward: 7.381616531203326 setps: 20 count: 380\n",
      "reward: 7.70156188655528 setps: 20 count: 400\n",
      "reward: 7.507819380305591 setps: 20 count: 420\n",
      "reward: 7.281833891765563 setps: 20 count: 440\n",
      "reward: 7.562252728716704 setps: 20 count: 460\n",
      "reward: 7.322615181599393 setps: 20 count: 480\n",
      "reward: 7.805103373211751 setps: 20 count: 500\n",
      "reward: 7.873823842046838 setps: 20 count: 520\n",
      "reward: 7.39533188420901 setps: 20 count: 540\n",
      "reward: 7.191906284735887 setps: 20 count: 560\n",
      "reward: 7.720304712073994 setps: 20 count: 580\n",
      "reward: 7.730282055311546 setps: 20 count: 600\n",
      "reward: 7.989050675326142 setps: 20 count: 620\n",
      "reward: 7.6202531469811206 setps: 20 count: 640\n",
      "reward: 7.54640480602102 setps: 20 count: 660\n",
      "reward: 7.420873472811945 setps: 20 count: 680\n",
      "reward: 8.178222286296657 setps: 20 count: 700\n",
      "reward: 7.683077983908878 setps: 20 count: 720\n",
      "reward: 7.7146009292730024 setps: 20 count: 740\n",
      "reward: 7.296797952362976 setps: 20 count: 760\n",
      "reward: 7.861287369900673 setps: 20 count: 780\n",
      "reward: 7.902310093090636 setps: 20 count: 800\n",
      "reward: 7.809673401726467 setps: 20 count: 820\n",
      "reward: 7.86002150667773 setps: 20 count: 840\n",
      "reward: 7.830503401035093 setps: 20 count: 860\n",
      "reward: 7.222113748035917 setps: 20 count: 880\n",
      "reward: 7.77248935316602 setps: 20 count: 900\n",
      "reward: 7.4273760432741245 setps: 20 count: 920\n",
      "reward: 7.233309477631701 setps: 20 count: 940\n",
      "reward: 7.608499262289843 setps: 20 count: 960\n",
      "reward: 6.6994494924307215 setps: 20 count: 980\n",
      "reward: 7.157819862279573 setps: 20 count: 1000\n",
      "avg rewards: 7.570237425122439\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.47548\n",
      "Epoch:20 Batch:5 Loss:0.04111\n",
      "Epoch:40 Batch:5 Loss:0.02422\n",
      "Epoch:60 Batch:5 Loss:0.01820\n",
      "Epoch:80 Batch:5 Loss:0.01231\n",
      "Epoch:100 Batch:5 Loss:0.01204\n",
      "Epoch:120 Batch:5 Loss:0.01002\n",
      "Epoch:140 Batch:5 Loss:0.00962\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.268\n",
      "Epoch:10 Batch:10 Loss:0.260\n",
      "Epoch:20 Batch:10 Loss:0.260\n",
      "Epoch:30 Batch:10 Loss:0.258\n",
      "Epoch:40 Batch:10 Loss:0.258\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 424.08429278994174 setps: 800 count: 800\n",
      "avg rewards: 424.08429278994174\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.49625\n",
      "Epoch:20 Batch:6 Loss:0.02881\n",
      "Epoch:40 Batch:6 Loss:0.02213\n",
      "Epoch:60 Batch:6 Loss:0.01595\n",
      "Epoch:80 Batch:6 Loss:0.01267\n",
      "Epoch:100 Batch:6 Loss:0.01009\n",
      "Epoch:120 Batch:6 Loss:0.00956\n",
      "Epoch:140 Batch:6 Loss:0.00960\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.258\n",
      "Epoch:10 Batch:10 Loss:0.255\n",
      "Epoch:20 Batch:10 Loss:0.252\n",
      "Epoch:30 Batch:10 Loss:0.254\n",
      "Epoch:40 Batch:10 Loss:0.252\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 28.989796219085115 setps: 61 count: 61\n",
      "reward: 9.375578006565048 setps: 23 count: 84\n",
      "reward: 9.481554856726142 setps: 21 count: 105\n",
      "reward: 11.246116800094025 setps: 27 count: 132\n",
      "reward: 13.114814746679619 setps: 31 count: 163\n",
      "reward: 10.528960122354327 setps: 27 count: 190\n",
      "reward: 9.14535525782121 setps: 24 count: 214\n",
      "reward: 9.629757886075822 setps: 22 count: 236\n",
      "reward: 11.623368860635672 setps: 32 count: 268\n",
      "reward: 15.110339730963458 setps: 41 count: 309\n",
      "reward: 13.37161232936487 setps: 34 count: 343\n",
      "reward: 9.410760958965696 setps: 23 count: 366\n",
      "reward: 14.292601103070776 setps: 33 count: 399\n",
      "reward: 13.145400995493404 setps: 34 count: 433\n",
      "reward: 11.603880332609695 setps: 29 count: 462\n",
      "reward: 10.069667545302943 setps: 25 count: 487\n",
      "reward: 11.936823143764922 setps: 28 count: 515\n",
      "reward: 14.36217923962395 setps: 36 count: 551\n",
      "reward: 9.664999789452123 setps: 21 count: 572\n",
      "reward: 18.895326761028254 setps: 50 count: 622\n",
      "reward: 13.599697488955277 setps: 34 count: 656\n",
      "reward: 9.521310761869245 setps: 21 count: 677\n",
      "reward: 9.691790936829058 setps: 21 count: 698\n",
      "reward: 12.58855053687148 setps: 30 count: 728\n",
      "reward: 18.371740351385967 setps: 51 count: 779\n",
      "reward: 10.010984522507352 setps: 26 count: 805\n",
      "reward: 9.457123554623106 setps: 22 count: 827\n",
      "reward: 17.259828409402687 setps: 41 count: 868\n",
      "reward: 9.849826940418277 setps: 24 count: 892\n",
      "reward: 9.257298847105993 setps: 22 count: 914\n",
      "reward: 9.47583280799881 setps: 22 count: 936\n",
      "avg rewards: 12.389770317536911\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.50179\n",
      "Epoch:20 Batch:7 Loss:0.02834\n",
      "Epoch:40 Batch:7 Loss:0.02207\n",
      "Epoch:60 Batch:7 Loss:0.01548\n",
      "Epoch:80 Batch:7 Loss:0.01255\n",
      "Epoch:100 Batch:7 Loss:0.01165\n",
      "Epoch:120 Batch:7 Loss:0.00944\n",
      "Epoch:140 Batch:7 Loss:0.00951\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.232\n",
      "Epoch:10 Batch:10 Loss:0.227\n",
      "Epoch:20 Batch:10 Loss:0.226\n",
      "Epoch:30 Batch:10 Loss:0.226\n",
      "Epoch:40 Batch:10 Loss:0.225\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 15.026409456822146 setps: 24 count: 24\n",
      "reward: 14.919827230981902 setps: 24 count: 48\n",
      "reward: 17.893287206988315 setps: 26 count: 74\n",
      "reward: 12.233104577266204 setps: 22 count: 96\n",
      "reward: 11.368186548943049 setps: 21 count: 117\n",
      "reward: 12.20211732769385 setps: 22 count: 139\n",
      "reward: 12.529320886469213 setps: 22 count: 161\n",
      "reward: 11.017185843081096 setps: 21 count: 182\n",
      "reward: 9.927536761414375 setps: 20 count: 202\n",
      "reward: 11.374967239596298 setps: 21 count: 223\n",
      "reward: 14.84333966297563 setps: 23 count: 246\n",
      "reward: 20.167884910936117 setps: 28 count: 274\n",
      "reward: 9.692936100305817 setps: 20 count: 294\n",
      "reward: 20.194137281409354 setps: 30 count: 324\n",
      "reward: 14.2554061282819 setps: 23 count: 347\n",
      "reward: 10.72299374776194 setps: 21 count: 368\n",
      "reward: 11.40582631618163 setps: 21 count: 389\n",
      "reward: 13.394268797256519 setps: 22 count: 411\n",
      "reward: 9.823954464045528 setps: 20 count: 431\n",
      "reward: 11.50775762405974 setps: 21 count: 452\n",
      "reward: 19.68493922062771 setps: 27 count: 479\n",
      "reward: 11.143195061893493 setps: 21 count: 500\n",
      "reward: 9.703220929121015 setps: 20 count: 520\n",
      "reward: 26.979920182519706 setps: 40 count: 560\n",
      "reward: 10.917225891252746 setps: 21 count: 581\n",
      "reward: 12.147553841747868 setps: 22 count: 603\n",
      "reward: 11.160638771507365 setps: 21 count: 624\n",
      "reward: 10.959321631415513 setps: 21 count: 645\n",
      "reward: 11.43087763859512 setps: 21 count: 666\n",
      "reward: 19.229509425860307 setps: 29 count: 695\n",
      "reward: 11.451785967161413 setps: 21 count: 716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 12.908906662352095 setps: 22 count: 738\n",
      "reward: 20.4839474001099 setps: 29 count: 767\n",
      "reward: 11.22000871981727 setps: 21 count: 788\n",
      "reward: 10.26524745454226 setps: 20 count: 808\n",
      "reward: 11.516259580691985 setps: 21 count: 829\n",
      "reward: 10.63738288511522 setps: 21 count: 850\n",
      "reward: 11.38085390784545 setps: 21 count: 871\n",
      "reward: 12.9899356504975 setps: 22 count: 893\n",
      "reward: 11.036448278711758 setps: 21 count: 914\n",
      "reward: 10.051952164260726 setps: 20 count: 934\n",
      "reward: 11.431359104704462 setps: 21 count: 955\n",
      "reward: 12.886624694669443 setps: 22 count: 977\n",
      "reward: 9.465320399259507 setps: 20 count: 997\n",
      "avg rewards: 13.081429172198874\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.45275\n",
      "Epoch:20 Batch:8 Loss:0.02943\n",
      "Epoch:40 Batch:8 Loss:0.01993\n",
      "Epoch:60 Batch:8 Loss:0.01359\n",
      "Epoch:80 Batch:8 Loss:0.01042\n",
      "Epoch:100 Batch:8 Loss:0.00929\n",
      "Epoch:120 Batch:8 Loss:0.00918\n",
      "Epoch:140 Batch:8 Loss:0.00898\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.236\n",
      "Epoch:10 Batch:10 Loss:0.227\n",
      "Epoch:20 Batch:10 Loss:0.223\n",
      "Epoch:30 Batch:10 Loss:0.218\n",
      "Epoch:40 Batch:10 Loss:0.222\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 490.17930789639433 setps: 800 count: 800\n",
      "avg rewards: 490.17930789639433\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.42540\n",
      "Epoch:20 Batch:9 Loss:0.02271\n",
      "Epoch:40 Batch:9 Loss:0.01667\n",
      "Epoch:60 Batch:9 Loss:0.01185\n",
      "Epoch:80 Batch:9 Loss:0.00978\n",
      "Epoch:100 Batch:9 Loss:0.00973\n",
      "Epoch:120 Batch:9 Loss:0.00739\n",
      "Epoch:140 Batch:9 Loss:0.00818\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.258\n",
      "Epoch:10 Batch:10 Loss:0.242\n",
      "Epoch:20 Batch:10 Loss:0.241\n",
      "Epoch:30 Batch:10 Loss:0.242\n",
      "Epoch:40 Batch:10 Loss:0.240\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 8.520718452843722 setps: 20 count: 20\n",
      "reward: 8.183518026680396 setps: 20 count: 40\n",
      "reward: 8.42402453780087 setps: 20 count: 60\n",
      "reward: 8.49055656661076 setps: 20 count: 80\n",
      "reward: 8.323227161320393 setps: 20 count: 100\n",
      "reward: 8.655194210838818 setps: 20 count: 120\n",
      "reward: 8.501450520544314 setps: 20 count: 140\n",
      "reward: 9.657376666500936 setps: 20 count: 160\n",
      "reward: 8.410172174606123 setps: 20 count: 180\n",
      "reward: 8.205685369379351 setps: 20 count: 200\n",
      "reward: 8.923734354419867 setps: 20 count: 220\n",
      "reward: 8.510533572698478 setps: 20 count: 240\n",
      "reward: 8.463730376482998 setps: 20 count: 260\n",
      "reward: 8.141654105424825 setps: 20 count: 280\n",
      "reward: 8.488366100625717 setps: 20 count: 300\n",
      "reward: 8.4263920286583 setps: 20 count: 320\n",
      "reward: 8.510981071332935 setps: 20 count: 340\n",
      "reward: 8.631124092984827 setps: 20 count: 360\n",
      "reward: 8.488205331772043 setps: 20 count: 380\n",
      "reward: 8.292209770702177 setps: 20 count: 400\n",
      "reward: 8.829111167849625 setps: 20 count: 420\n",
      "reward: 8.38165160431963 setps: 20 count: 440\n",
      "reward: 8.368467642724863 setps: 20 count: 460\n",
      "reward: 8.639243687818816 setps: 20 count: 480\n",
      "reward: 8.486215993957012 setps: 20 count: 500\n",
      "reward: 8.247651522711385 setps: 20 count: 520\n",
      "reward: 8.44613123228337 setps: 20 count: 540\n",
      "reward: 8.432330766077213 setps: 20 count: 560\n",
      "reward: 8.482554196740965 setps: 20 count: 580\n",
      "reward: 8.535089355015955 setps: 20 count: 600\n",
      "reward: 8.361320964831974 setps: 20 count: 620\n",
      "reward: 8.702406974538462 setps: 20 count: 640\n",
      "reward: 8.497740929212886 setps: 20 count: 660\n",
      "reward: 8.318239876475127 setps: 20 count: 680\n",
      "reward: 8.373168314823122 setps: 20 count: 700\n",
      "reward: 8.544566456841132 setps: 20 count: 720\n",
      "reward: 8.44946930387814 setps: 20 count: 740\n",
      "reward: 8.163965375281986 setps: 20 count: 760\n",
      "reward: 8.550801790523108 setps: 20 count: 780\n",
      "reward: 28.412374599487514 setps: 57 count: 837\n",
      "reward: 8.322833632591935 setps: 20 count: 857\n",
      "reward: 9.038887124219036 setps: 20 count: 877\n",
      "reward: 8.299176689860179 setps: 20 count: 897\n",
      "reward: 8.422430287172029 setps: 20 count: 917\n",
      "reward: 8.416473570516972 setps: 20 count: 937\n",
      "reward: 8.274984443123685 setps: 20 count: 957\n",
      "reward: 8.534941362586688 setps: 20 count: 977\n",
      "reward: 8.423839805017632 setps: 20 count: 997\n",
      "avg rewards: 8.900102565889755\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.43844\n",
      "Epoch:20 Batch:10 Loss:0.02103\n",
      "Epoch:40 Batch:10 Loss:0.01545\n",
      "Epoch:60 Batch:10 Loss:0.01034\n",
      "Epoch:80 Batch:10 Loss:0.00966\n",
      "Epoch:100 Batch:10 Loss:0.00800\n",
      "Epoch:120 Batch:10 Loss:0.00852\n",
      "Epoch:140 Batch:10 Loss:0.00733\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.259\n",
      "Epoch:10 Batch:10 Loss:0.255\n",
      "Epoch:20 Batch:10 Loss:0.254\n",
      "Epoch:30 Batch:10 Loss:0.250\n",
      "Epoch:40 Batch:10 Loss:0.249\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 11.141140785328751 setps: 21 count: 21\n",
      "reward: 10.77904676200269 setps: 21 count: 42\n",
      "reward: 12.087982650157935 setps: 22 count: 64\n",
      "reward: 13.268713798307001 setps: 22 count: 86\n",
      "reward: 10.456469796135208 setps: 21 count: 107\n",
      "reward: 9.857048737924195 setps: 21 count: 128\n",
      "reward: 14.54133700067032 setps: 23 count: 151\n",
      "reward: 9.149048690607012 setps: 20 count: 171\n",
      "reward: 12.017682636971585 setps: 22 count: 193\n",
      "reward: 10.599509438392126 setps: 21 count: 214\n",
      "reward: 10.355585881095612 setps: 21 count: 235\n",
      "reward: 18.106289768801073 setps: 26 count: 261\n",
      "reward: 86.84464416602574 setps: 142 count: 403\n",
      "reward: 10.784548817249014 setps: 21 count: 424\n",
      "reward: 14.81168460998306 setps: 23 count: 447\n",
      "reward: 13.296084166645597 setps: 23 count: 470\n",
      "reward: 10.891054784014704 setps: 21 count: 491\n",
      "reward: 65.64351108111589 setps: 118 count: 609\n",
      "reward: 8.757976806205988 setps: 20 count: 629\n",
      "reward: 11.907920914948043 setps: 22 count: 651\n",
      "reward: 9.688083079984061 setps: 21 count: 672\n",
      "reward: 10.785686734959016 setps: 21 count: 693\n",
      "reward: 9.658130550511123 setps: 20 count: 713\n",
      "reward: 12.83009266996087 setps: 22 count: 735\n",
      "reward: 14.208037289309141 setps: 23 count: 758\n",
      "reward: 8.403425403944855 setps: 20 count: 778\n",
      "reward: 17.264628660281595 setps: 25 count: 803\n",
      "reward: 12.664446308712646 setps: 22 count: 825\n",
      "reward: 12.426035131173556 setps: 22 count: 847\n",
      "reward: 12.732091013138414 setps: 22 count: 869\n",
      "reward: 11.734842631846549 setps: 22 count: 891\n",
      "reward: 9.91567376495077 setps: 21 count: 912\n",
      "reward: 11.502331600910109 setps: 22 count: 934\n",
      "reward: 16.335788441810294 setps: 24 count: 958\n",
      "reward: 10.163750432568486 setps: 21 count: 979\n",
      "reward: 10.856583321829383 setps: 21 count: 1000\n",
      "avg rewards: 15.457414120235345\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.34274\n",
      "Epoch:20 Batch:11 Loss:0.02364\n",
      "Epoch:40 Batch:11 Loss:0.01398\n",
      "Epoch:60 Batch:11 Loss:0.01154\n",
      "Epoch:80 Batch:11 Loss:0.01011\n",
      "Epoch:100 Batch:11 Loss:0.00840\n",
      "Epoch:120 Batch:11 Loss:0.00785\n",
      "Epoch:140 Batch:11 Loss:0.00667\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.245\n",
      "Epoch:10 Batch:10 Loss:0.236\n",
      "Epoch:20 Batch:10 Loss:0.238\n",
      "Epoch:30 Batch:10 Loss:0.235\n",
      "Epoch:40 Batch:10 Loss:0.233\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 65.21294302663009 setps: 143 count: 143\n",
      "reward: 304.352216189125 setps: 632 count: 775\n",
      "avg rewards: 184.78257960787755\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.40839\n",
      "Epoch:20 Batch:12 Loss:0.01975\n",
      "Epoch:40 Batch:12 Loss:0.01478\n",
      "Epoch:60 Batch:12 Loss:0.00965\n",
      "Epoch:80 Batch:12 Loss:0.00926\n",
      "Epoch:100 Batch:12 Loss:0.00798\n",
      "Epoch:120 Batch:12 Loss:0.00681\n",
      "Epoch:140 Batch:12 Loss:0.00664\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.246\n",
      "Epoch:10 Batch:10 Loss:0.232\n",
      "Epoch:20 Batch:10 Loss:0.227\n",
      "Epoch:30 Batch:10 Loss:0.226\n",
      "Epoch:40 Batch:10 Loss:0.227\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 177.653878218995 setps: 800 count: 800\n",
      "avg rewards: 177.653878218995\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.40260\n",
      "Epoch:20 Batch:13 Loss:0.01979\n",
      "Epoch:40 Batch:13 Loss:0.01364\n",
      "Epoch:60 Batch:13 Loss:0.00953\n",
      "Epoch:80 Batch:13 Loss:0.00840\n",
      "Epoch:100 Batch:13 Loss:0.00805\n",
      "Epoch:120 Batch:13 Loss:0.00733\n",
      "Epoch:140 Batch:13 Loss:0.00639\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:10 Loss:0.208\n",
      "Epoch:10 Batch:10 Loss:0.204\n",
      "Epoch:20 Batch:10 Loss:0.204\n",
      "Epoch:30 Batch:10 Loss:0.202\n",
      "Epoch:40 Batch:10 Loss:0.201\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 368.03631117139304 setps: 800 count: 800\n",
      "avg rewards: 368.03631117139304\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.42652\n",
      "Epoch:20 Batch:14 Loss:0.01989\n",
      "Epoch:40 Batch:14 Loss:0.01281\n",
      "Epoch:60 Batch:14 Loss:0.00907\n",
      "Epoch:80 Batch:14 Loss:0.00803\n",
      "Epoch:100 Batch:14 Loss:0.00841\n",
      "Epoch:120 Batch:14 Loss:0.00704\n",
      "Epoch:140 Batch:14 Loss:0.00677\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.227\n",
      "Epoch:10 Batch:10 Loss:0.221\n",
      "Epoch:20 Batch:10 Loss:0.215\n",
      "Epoch:30 Batch:10 Loss:0.213\n",
      "Epoch:40 Batch:10 Loss:0.213\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 85.80462961037708 setps: 224 count: 224\n",
      "reward: 6.225765334462629 setps: 25 count: 249\n",
      "avg rewards: 46.01519747241986\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.33678\n",
      "Epoch:20 Batch:15 Loss:0.01934\n",
      "Epoch:40 Batch:15 Loss:0.01081\n",
      "Epoch:60 Batch:15 Loss:0.00915\n",
      "Epoch:80 Batch:15 Loss:0.00737\n",
      "Epoch:100 Batch:15 Loss:0.00677\n",
      "Epoch:120 Batch:15 Loss:0.00674\n",
      "Epoch:140 Batch:15 Loss:0.00666\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.209\n",
      "Epoch:10 Batch:10 Loss:0.203\n",
      "Epoch:20 Batch:10 Loss:0.200\n",
      "Epoch:30 Batch:10 Loss:0.198\n",
      "Epoch:40 Batch:10 Loss:0.196\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 344.13494439766816 setps: 800 count: 800\n",
      "avg rewards: 344.13494439766816\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.33509\n",
      "Epoch:20 Batch:16 Loss:0.01890\n",
      "Epoch:40 Batch:16 Loss:0.01194\n",
      "Epoch:60 Batch:16 Loss:0.00825\n",
      "Epoch:80 Batch:16 Loss:0.00659\n",
      "Epoch:100 Batch:16 Loss:0.00721\n",
      "Epoch:120 Batch:16 Loss:0.00669\n",
      "Epoch:140 Batch:16 Loss:0.00636\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.267\n",
      "Epoch:10 Batch:10 Loss:0.238\n",
      "Epoch:20 Batch:10 Loss:0.230\n",
      "Epoch:30 Batch:10 Loss:0.232\n",
      "Epoch:40 Batch:10 Loss:0.229\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 69.38460769829402 setps: 800 count: 800\n",
      "avg rewards: 69.38460769829402\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.40695\n",
      "Epoch:20 Batch:17 Loss:0.01727\n",
      "Epoch:40 Batch:17 Loss:0.01060\n",
      "Epoch:60 Batch:17 Loss:0.00817\n",
      "Epoch:80 Batch:17 Loss:0.00749\n",
      "Epoch:100 Batch:17 Loss:0.00669\n",
      "Epoch:120 Batch:17 Loss:0.00647\n",
      "Epoch:140 Batch:17 Loss:0.00555\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.227\n",
      "Epoch:10 Batch:10 Loss:0.211\n",
      "Epoch:20 Batch:10 Loss:0.208\n",
      "Epoch:30 Batch:10 Loss:0.205\n",
      "Epoch:40 Batch:10 Loss:0.205\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 168.6386505623133 setps: 800 count: 800\n",
      "avg rewards: 168.6386505623133\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.29671\n",
      "Epoch:20 Batch:18 Loss:0.01484\n",
      "Epoch:40 Batch:18 Loss:0.01057\n",
      "Epoch:60 Batch:18 Loss:0.00946\n",
      "Epoch:80 Batch:18 Loss:0.00702\n",
      "Epoch:100 Batch:18 Loss:0.00682\n",
      "Epoch:120 Batch:18 Loss:0.00607\n",
      "Epoch:140 Batch:18 Loss:0.00542\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.214\n",
      "Epoch:10 Batch:10 Loss:0.204\n",
      "Epoch:20 Batch:10 Loss:0.202\n",
      "Epoch:30 Batch:10 Loss:0.202\n",
      "Epoch:40 Batch:10 Loss:0.202\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 114.25198853289606 setps: 800 count: 800\n",
      "avg rewards: 114.25198853289606\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.27073\n",
      "Epoch:20 Batch:19 Loss:0.01314\n",
      "Epoch:40 Batch:19 Loss:0.00842\n",
      "Epoch:60 Batch:19 Loss:0.00721\n",
      "Epoch:80 Batch:19 Loss:0.00701\n",
      "Epoch:100 Batch:19 Loss:0.00555\n",
      "Epoch:120 Batch:19 Loss:0.00623\n",
      "Epoch:140 Batch:19 Loss:0.00570\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.211\n",
      "Epoch:10 Batch:10 Loss:0.198\n",
      "Epoch:20 Batch:10 Loss:0.197\n",
      "Epoch:30 Batch:10 Loss:0.198\n",
      "Epoch:40 Batch:10 Loss:0.196\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 16.583603012778593 setps: 354 count: 354\n",
      "reward: 19.149859009997446 setps: 251 count: 605\n",
      "reward: 3.8608714977424814 setps: 22 count: 627\n",
      "reward: 5.798953880876068 setps: 75 count: 702\n",
      "reward: 16.043267149734316 setps: 249 count: 951\n",
      "avg rewards: 12.28731091022578\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.25032\n",
      "Epoch:20 Batch:20 Loss:0.01520\n",
      "Epoch:40 Batch:20 Loss:0.00886\n",
      "Epoch:60 Batch:20 Loss:0.00688\n",
      "Epoch:80 Batch:20 Loss:0.00674\n",
      "Epoch:100 Batch:20 Loss:0.00620\n",
      "Epoch:120 Batch:20 Loss:0.00579\n",
      "Epoch:140 Batch:20 Loss:0.00533\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.196\n",
      "Epoch:10 Batch:10 Loss:0.182\n",
      "Epoch:20 Batch:10 Loss:0.180\n",
      "Epoch:30 Batch:10 Loss:0.181\n",
      "Epoch:40 Batch:10 Loss:0.182\n",
      "Done!\n",
      "############# start HumanoidBulletEnv-v0 training ###################\n",
      "(49,)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -21.27815852837957 setps: 18 count: 18\n",
      "reward: -22.47076977281249 setps: 19 count: 37\n",
      "reward: -30.321776842277902 setps: 19 count: 56\n",
      "reward: -25.21742826192494 setps: 19 count: 75\n",
      "reward: -24.207292249455357 setps: 20 count: 95\n",
      "reward: -29.7664181799104 setps: 24 count: 119\n",
      "reward: -23.74311013439292 setps: 19 count: 138\n",
      "reward: -23.36006111258321 setps: 20 count: 158\n",
      "reward: -34.318993776124266 setps: 18 count: 176\n",
      "reward: -26.146251443079382 setps: 25 count: 201\n",
      "reward: -26.807408529175156 setps: 18 count: 219\n",
      "reward: -21.07751676966436 setps: 18 count: 237\n",
      "reward: -42.99177106470597 setps: 17 count: 254\n",
      "reward: -21.11809686110646 setps: 19 count: 273\n",
      "reward: -31.734575641065025 setps: 21 count: 294\n",
      "reward: -32.155941073739086 setps: 24 count: 318\n",
      "reward: -33.46096414426138 setps: 18 count: 336\n",
      "reward: -38.50465430379845 setps: 23 count: 359\n",
      "reward: -39.774160651871355 setps: 18 count: 377\n",
      "reward: -31.689619016315557 setps: 19 count: 396\n",
      "reward: -33.106630958568715 setps: 20 count: 416\n",
      "reward: -32.276966070497295 setps: 19 count: 435\n",
      "reward: -36.04288299209438 setps: 19 count: 454\n",
      "reward: -27.627554653768314 setps: 17 count: 471\n",
      "reward: -32.68496657831711 setps: 19 count: 490\n",
      "reward: -35.6927099502398 setps: 20 count: 510\n",
      "reward: -39.889954649562426 setps: 18 count: 528\n",
      "reward: -26.845603258757915 setps: 18 count: 546\n",
      "reward: -27.075866062492423 setps: 19 count: 565\n",
      "reward: -31.09131065516849 setps: 19 count: 584\n",
      "reward: -28.852063810803518 setps: 19 count: 603\n",
      "reward: -32.33076674514451 setps: 20 count: 623\n",
      "reward: -8.938496169158316 setps: 23 count: 646\n",
      "reward: -18.415821023249006 setps: 18 count: 664\n",
      "reward: -18.717161961598322 setps: 20 count: 684\n",
      "reward: -34.75948575479415 setps: 17 count: 701\n",
      "reward: -46.74182823659794 setps: 18 count: 719\n",
      "reward: -37.49865368420868 setps: 18 count: 737\n",
      "reward: -36.59209646939707 setps: 19 count: 756\n",
      "reward: -34.548925978114134 setps: 19 count: 775\n",
      "reward: -20.69142105957435 setps: 20 count: 795\n",
      "reward: -28.884350139297023 setps: 20 count: 815\n",
      "reward: -36.450555329838245 setps: 18 count: 833\n",
      "reward: -29.937173018731116 setps: 17 count: 850\n",
      "reward: -33.678217343676074 setps: 19 count: 869\n",
      "reward: -31.476157857508223 setps: 18 count: 887\n",
      "reward: -29.679194189971895 setps: 19 count: 906\n",
      "reward: -25.80388652241963 setps: 23 count: 929\n",
      "reward: -40.47353425140464 setps: 16 count: 945\n",
      "reward: -33.41697716347262 setps: 25 count: 970\n",
      "reward: -33.2734043930599 setps: 19 count: 989\n",
      "avg rewards: -30.26744284878685\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:0.60503\n",
      "Epoch:20 Batch:1 Loss:0.57585\n",
      "Epoch:40 Batch:1 Loss:0.53850\n",
      "Epoch:60 Batch:1 Loss:0.50980\n",
      "Epoch:80 Batch:1 Loss:0.48832\n",
      "Epoch:100 Batch:1 Loss:0.45857\n",
      "Epoch:120 Batch:1 Loss:0.42115\n",
      "Epoch:140 Batch:1 Loss:0.37920\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.270\n",
      "Epoch:10 Batch:8 Loss:0.263\n",
      "Epoch:20 Batch:8 Loss:0.262\n",
      "Epoch:30 Batch:8 Loss:0.260\n",
      "Epoch:40 Batch:8 Loss:0.256\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 3.7054516599440808 setps: 24 count: 24\n",
      "reward: 4.522045476597849 setps: 36 count: 60\n",
      "reward: 3.165861975311418 setps: 24 count: 84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 1.1643459942511991 setps: 24 count: 108\n",
      "reward: 5.442642020247876 setps: 27 count: 135\n",
      "reward: 1.0234160096733822 setps: 24 count: 159\n",
      "reward: 4.343135302039444 setps: 24 count: 183\n",
      "reward: 3.3608635395052247 setps: 19 count: 202\n",
      "reward: 1.005621523475566 setps: 22 count: 224\n",
      "reward: 46.79276268990069 setps: 54 count: 278\n",
      "reward: 5.139113588699546 setps: 23 count: 301\n",
      "reward: 15.106479008817406 setps: 26 count: 327\n",
      "reward: 8.274696798459626 setps: 24 count: 351\n",
      "reward: 4.171630615057074 setps: 21 count: 372\n",
      "reward: 7.8271575858612765 setps: 25 count: 397\n",
      "reward: 2.923962831507377 setps: 16 count: 413\n",
      "reward: 10.415615647540834 setps: 28 count: 441\n",
      "reward: 14.34028858660895 setps: 35 count: 476\n",
      "reward: 15.578985295888558 setps: 28 count: 504\n",
      "reward: 3.80701621502667 setps: 24 count: 528\n",
      "reward: -1.984276561863953 setps: 16 count: 544\n",
      "reward: 5.065096047623956 setps: 23 count: 567\n",
      "reward: 4.590011650664383 setps: 18 count: 585\n",
      "reward: 5.583451602944112 setps: 30 count: 615\n",
      "reward: -0.41834522703866295 setps: 22 count: 637\n",
      "reward: 4.25751811643422 setps: 29 count: 666\n",
      "reward: 10.935858280684625 setps: 26 count: 692\n",
      "reward: 12.522733503126073 setps: 23 count: 715\n",
      "reward: 0.8537943022369268 setps: 23 count: 738\n",
      "reward: 3.473208319790137 setps: 23 count: 761\n",
      "reward: 1.3600569742731738 setps: 24 count: 785\n",
      "reward: 11.24426412020257 setps: 27 count: 812\n",
      "reward: 3.5974056082559387 setps: 20 count: 832\n",
      "reward: -1.5103408336202853 setps: 17 count: 849\n",
      "reward: 7.05190745299187 setps: 28 count: 877\n",
      "reward: 15.622939578280782 setps: 45 count: 922\n",
      "reward: 5.88291205815767 setps: 26 count: 948\n",
      "reward: 0.4676607747765953 setps: 23 count: 971\n",
      "reward: 0.5952596976116178 setps: 16 count: 987\n",
      "avg rewards: 6.443646354613994\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:0.51601\n",
      "Epoch:20 Batch:2 Loss:0.41039\n",
      "Epoch:40 Batch:2 Loss:0.36045\n",
      "Epoch:60 Batch:2 Loss:0.30827\n",
      "Epoch:80 Batch:2 Loss:0.27086\n",
      "Epoch:100 Batch:2 Loss:0.24204\n",
      "Epoch:120 Batch:2 Loss:0.21522\n",
      "Epoch:140 Batch:2 Loss:0.19222\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.184\n",
      "Epoch:10 Batch:8 Loss:0.181\n",
      "Epoch:20 Batch:8 Loss:0.180\n",
      "Epoch:30 Batch:8 Loss:0.177\n",
      "Epoch:40 Batch:8 Loss:0.178\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 11.873104691831397 setps: 26 count: 26\n",
      "reward: 3.0223625257494864 setps: 17 count: 43\n",
      "reward: 14.298543003878146 setps: 22 count: 65\n",
      "reward: 6.318250158717272 setps: 23 count: 88\n",
      "reward: 15.682072107009299 setps: 30 count: 118\n",
      "reward: 17.79288956720702 setps: 31 count: 149\n",
      "reward: 4.767430510013945 setps: 22 count: 171\n",
      "reward: 16.366083017904025 setps: 27 count: 198\n",
      "reward: 20.93861235943332 setps: 26 count: 224\n",
      "reward: 7.726209398811623 setps: 22 count: 246\n",
      "reward: 5.374351497596943 setps: 18 count: 264\n",
      "reward: 13.07162094233936 setps: 24 count: 288\n",
      "reward: 9.094498337141705 setps: 21 count: 309\n",
      "reward: 6.758928017781002 setps: 20 count: 329\n",
      "reward: 14.285790611065748 setps: 23 count: 352\n",
      "reward: 11.3735479021052 setps: 21 count: 373\n",
      "reward: 9.48816872602183 setps: 23 count: 396\n",
      "reward: 18.435034470855314 setps: 28 count: 424\n",
      "reward: 14.856549156224354 setps: 24 count: 448\n",
      "reward: 16.821275426624926 setps: 28 count: 476\n",
      "reward: 6.459483849910613 setps: 18 count: 494\n",
      "reward: 4.5819612338586 setps: 16 count: 510\n",
      "reward: -2.1322296713900877 setps: 18 count: 528\n",
      "reward: 15.23878186522488 setps: 26 count: 554\n",
      "reward: 16.585563568871294 setps: 26 count: 580\n",
      "reward: 6.051661035323923 setps: 23 count: 603\n",
      "reward: 18.167497593666486 setps: 28 count: 631\n",
      "reward: 4.444848361326148 setps: 18 count: 649\n",
      "reward: 16.75967267119267 setps: 26 count: 675\n",
      "reward: 18.451036424499762 setps: 29 count: 704\n",
      "reward: 12.56616385211819 setps: 25 count: 729\n",
      "reward: 15.453783583444599 setps: 25 count: 754\n",
      "reward: 17.491941382779622 setps: 30 count: 784\n",
      "reward: 3.249517147173173 setps: 17 count: 801\n",
      "reward: 13.927398141095178 setps: 25 count: 826\n",
      "reward: 16.176741574214248 setps: 24 count: 850\n",
      "reward: 8.124593583002572 setps: 20 count: 870\n",
      "reward: 18.980640082970783 setps: 25 count: 895\n",
      "reward: 6.558203919509833 setps: 18 count: 913\n",
      "reward: 10.028233696824461 setps: 23 count: 936\n",
      "reward: 18.66768502663326 setps: 26 count: 962\n",
      "reward: 17.589777657623927 setps: 26 count: 988\n",
      "avg rewards: 11.946863785909192\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.47303\n",
      "Epoch:20 Batch:3 Loss:0.32724\n",
      "Epoch:40 Batch:3 Loss:0.25637\n",
      "Epoch:60 Batch:3 Loss:0.21916\n",
      "Epoch:80 Batch:3 Loss:0.18922\n",
      "Epoch:100 Batch:3 Loss:0.16564\n",
      "Epoch:120 Batch:3 Loss:0.14189\n",
      "Epoch:140 Batch:3 Loss:0.13451\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.147\n",
      "Epoch:10 Batch:8 Loss:0.140\n",
      "Epoch:20 Batch:8 Loss:0.138\n",
      "Epoch:30 Batch:8 Loss:0.137\n",
      "Epoch:40 Batch:8 Loss:0.137\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 33.07065620765351 setps: 45 count: 45\n",
      "reward: 22.952406511090526 setps: 41 count: 86\n",
      "reward: 30.476288188371104 setps: 38 count: 124\n",
      "reward: 50.24376305923216 setps: 48 count: 172\n",
      "reward: 9.928053513940538 setps: 19 count: 191\n",
      "reward: 22.146230868395648 setps: 49 count: 240\n",
      "reward: 41.0677042839292 setps: 44 count: 284\n",
      "reward: 36.32689108983031 setps: 47 count: 331\n",
      "reward: 22.531412790899047 setps: 38 count: 369\n",
      "reward: -1.4039161092907315 setps: 15 count: 384\n",
      "reward: 28.686395305686165 setps: 42 count: 426\n",
      "reward: 24.85816136072971 setps: 42 count: 468\n",
      "reward: 24.8980265070757 setps: 46 count: 514\n",
      "reward: 13.469919590454083 setps: 19 count: 533\n",
      "reward: 10.036331403987424 setps: 20 count: 553\n",
      "reward: 28.708910658158125 setps: 46 count: 599\n",
      "reward: 34.180151568703884 setps: 42 count: 641\n",
      "reward: 23.814727211977882 setps: 42 count: 683\n",
      "reward: 23.674240871318027 setps: 42 count: 725\n",
      "reward: 8.844880074704996 setps: 18 count: 743\n",
      "reward: 9.051079012750414 setps: 20 count: 763\n",
      "reward: 52.27066535738266 setps: 48 count: 811\n",
      "reward: 17.638989818772828 setps: 40 count: 851\n",
      "reward: 35.351437590741256 setps: 45 count: 896\n",
      "reward: 9.281098125956486 setps: 19 count: 915\n",
      "reward: 49.46370751005161 setps: 48 count: 963\n",
      "avg rewards: 25.444931245096253\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.45343\n",
      "Epoch:20 Batch:4 Loss:0.29635\n",
      "Epoch:40 Batch:4 Loss:0.21885\n",
      "Epoch:60 Batch:4 Loss:0.17495\n",
      "Epoch:80 Batch:4 Loss:0.13854\n",
      "Epoch:100 Batch:4 Loss:0.12124\n",
      "Epoch:120 Batch:4 Loss:0.10950\n",
      "Epoch:140 Batch:4 Loss:0.10451\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.112\n",
      "Epoch:10 Batch:8 Loss:0.109\n",
      "Epoch:20 Batch:8 Loss:0.107\n",
      "Epoch:30 Batch:8 Loss:0.110\n",
      "Epoch:40 Batch:8 Loss:0.109\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 32.92153844210698 setps: 28 count: 28\n",
      "reward: 21.722739583352812 setps: 22 count: 50\n",
      "reward: 33.37544910829602 setps: 30 count: 80\n",
      "reward: 27.4483662639148 setps: 28 count: 108\n",
      "reward: 42.843042386836906 setps: 34 count: 142\n",
      "reward: 31.939230367947317 setps: 33 count: 175\n",
      "reward: 27.549594343399814 setps: 27 count: 202\n",
      "reward: 17.33292194187816 setps: 20 count: 222\n",
      "reward: 46.68863257600896 setps: 33 count: 255\n",
      "reward: 23.130708865421177 setps: 23 count: 278\n",
      "reward: 32.36114930411422 setps: 31 count: 309\n",
      "reward: 40.49101270965473 setps: 32 count: 341\n",
      "reward: 25.763834328339726 setps: 27 count: 368\n",
      "reward: 18.626429902113166 setps: 20 count: 388\n",
      "reward: 41.00650181326927 setps: 31 count: 419\n",
      "reward: 17.47855560537864 setps: 20 count: 439\n",
      "reward: 30.531413484991933 setps: 27 count: 466\n",
      "reward: 37.73858565737609 setps: 31 count: 497\n",
      "reward: 27.457709214603526 setps: 24 count: 521\n",
      "reward: 37.91769458236377 setps: 30 count: 551\n",
      "reward: 36.85899397910689 setps: 31 count: 582\n",
      "reward: 34.11564587755565 setps: 29 count: 611\n",
      "reward: 29.521983934243327 setps: 25 count: 636\n",
      "reward: 33.88240741179762 setps: 27 count: 663\n",
      "reward: 39.58210453655484 setps: 33 count: 696\n",
      "reward: 31.12490585049818 setps: 27 count: 723\n",
      "reward: 30.379664659000984 setps: 28 count: 751\n",
      "reward: 30.759165193384984 setps: 27 count: 778\n",
      "reward: 33.27237574836326 setps: 28 count: 806\n",
      "reward: 10.654768893233266 setps: 17 count: 823\n",
      "reward: 39.59013952137756 setps: 31 count: 854\n",
      "reward: 29.20253577211988 setps: 27 count: 881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 32.81321395371488 setps: 30 count: 911\n",
      "reward: 32.92024206593924 setps: 27 count: 938\n",
      "reward: 40.45667785596305 setps: 33 count: 971\n",
      "avg rewards: 31.413141020977765\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.42317\n",
      "Epoch:20 Batch:5 Loss:0.24098\n",
      "Epoch:40 Batch:5 Loss:0.17004\n",
      "Epoch:60 Batch:5 Loss:0.13753\n",
      "Epoch:80 Batch:5 Loss:0.11619\n",
      "Epoch:100 Batch:5 Loss:0.09739\n",
      "Epoch:120 Batch:5 Loss:0.08762\n",
      "Epoch:140 Batch:5 Loss:0.08282\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.096\n",
      "Epoch:10 Batch:8 Loss:0.095\n",
      "Epoch:20 Batch:8 Loss:0.094\n",
      "Epoch:30 Batch:8 Loss:0.095\n",
      "Epoch:40 Batch:8 Loss:0.092\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 51.75226681606146 setps: 71 count: 71\n",
      "reward: 67.02652126681495 setps: 55 count: 126\n",
      "reward: 74.80880870662949 setps: 77 count: 203\n",
      "reward: 43.67734308913059 setps: 85 count: 288\n",
      "reward: 64.45626865487426 setps: 70 count: 358\n",
      "reward: 56.29331668358209 setps: 61 count: 419\n",
      "reward: 47.23019377548772 setps: 53 count: 472\n",
      "reward: 58.9853188679961 setps: 53 count: 525\n",
      "reward: 53.41377059992956 setps: 56 count: 581\n",
      "reward: 64.39588096130173 setps: 57 count: 638\n",
      "reward: 64.76690317630711 setps: 65 count: 703\n",
      "reward: 64.65986578208103 setps: 56 count: 759\n",
      "reward: 89.36547537711013 setps: 81 count: 840\n",
      "reward: 22.98135655601218 setps: 63 count: 903\n",
      "reward: 14.423872862067945 setps: 17 count: 920\n",
      "reward: 13.202300860729885 setps: 17 count: 937\n",
      "reward: 57.15600452433284 setps: 56 count: 993\n",
      "avg rewards: 53.44679226826171\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.42727\n",
      "Epoch:20 Batch:6 Loss:0.19665\n",
      "Epoch:40 Batch:6 Loss:0.14651\n",
      "Epoch:60 Batch:6 Loss:0.11670\n",
      "Epoch:80 Batch:6 Loss:0.09735\n",
      "Epoch:100 Batch:6 Loss:0.08148\n",
      "Epoch:120 Batch:6 Loss:0.07951\n",
      "Epoch:140 Batch:6 Loss:0.06997\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.095\n",
      "Epoch:10 Batch:8 Loss:0.091\n",
      "Epoch:20 Batch:8 Loss:0.089\n",
      "Epoch:30 Batch:8 Loss:0.087\n",
      "Epoch:40 Batch:8 Loss:0.087\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 38.42657811548124 setps: 37 count: 37\n",
      "reward: 43.40413320717344 setps: 46 count: 83\n",
      "reward: 16.285679432962205 setps: 18 count: 101\n",
      "reward: 21.724702553411767 setps: 34 count: 135\n",
      "reward: 40.65462156918949 setps: 40 count: 175\n",
      "reward: 31.343344978980884 setps: 38 count: 213\n",
      "reward: 35.42954005160572 setps: 44 count: 257\n",
      "reward: 34.11689152270701 setps: 44 count: 301\n",
      "reward: 25.660225114927734 setps: 37 count: 338\n",
      "reward: 63.41278659481903 setps: 60 count: 398\n",
      "reward: 67.44724148444803 setps: 68 count: 466\n",
      "reward: 34.38764058233502 setps: 39 count: 505\n",
      "reward: 9.113773096512887 setps: 17 count: 522\n",
      "reward: 45.723907727911126 setps: 47 count: 569\n",
      "reward: 12.354162580089179 setps: 20 count: 589\n",
      "reward: 26.23452680043847 setps: 35 count: 624\n",
      "reward: 25.70258294328232 setps: 33 count: 657\n",
      "reward: 24.699963113789273 setps: 35 count: 692\n",
      "reward: 28.674507375295796 setps: 40 count: 732\n",
      "reward: 53.619860790096574 setps: 49 count: 781\n",
      "reward: 31.403152715157194 setps: 36 count: 817\n",
      "reward: 24.388481064148074 setps: 32 count: 849\n",
      "reward: 42.833810480333334 setps: 43 count: 892\n",
      "reward: 34.752856943025826 setps: 40 count: 932\n",
      "reward: 40.98385395447695 setps: 42 count: 974\n",
      "avg rewards: 34.11115299170394\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.40950\n",
      "Epoch:20 Batch:7 Loss:0.20209\n",
      "Epoch:40 Batch:7 Loss:0.13656\n",
      "Epoch:60 Batch:7 Loss:0.10746\n",
      "Epoch:80 Batch:7 Loss:0.08761\n",
      "Epoch:100 Batch:7 Loss:0.07697\n",
      "Epoch:120 Batch:7 Loss:0.06253\n",
      "Epoch:140 Batch:7 Loss:0.06206\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.085\n",
      "Epoch:10 Batch:8 Loss:0.085\n",
      "Epoch:20 Batch:8 Loss:0.084\n",
      "Epoch:30 Batch:8 Loss:0.083\n",
      "Epoch:40 Batch:8 Loss:0.083\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 27.17961699056468 setps: 44 count: 44\n",
      "reward: 15.517859132347802 setps: 17 count: 61\n",
      "reward: 27.898510101574356 setps: 45 count: 106\n",
      "reward: 16.04146883154026 setps: 50 count: 156\n",
      "reward: 37.751680414835576 setps: 50 count: 206\n",
      "reward: 10.125105637905646 setps: 51 count: 257\n",
      "reward: 19.670302146955514 setps: 23 count: 280\n",
      "reward: 23.55297714711051 setps: 35 count: 315\n",
      "reward: 8.970168530425875 setps: 36 count: 351\n",
      "reward: 51.68142755511362 setps: 57 count: 408\n",
      "reward: 18.950971092557296 setps: 32 count: 440\n",
      "reward: 38.03035637558787 setps: 39 count: 479\n",
      "reward: 21.653056518033555 setps: 43 count: 522\n",
      "reward: 49.58719110524689 setps: 51 count: 573\n",
      "reward: 23.101262392179336 setps: 40 count: 613\n",
      "reward: 18.05011010806629 setps: 39 count: 652\n",
      "reward: 9.838008253127919 setps: 29 count: 681\n",
      "reward: 47.21494550680508 setps: 49 count: 730\n",
      "reward: 32.87656996839068 setps: 45 count: 775\n",
      "reward: 34.61340119408559 setps: 44 count: 819\n",
      "reward: 59.49985607328707 setps: 71 count: 890\n",
      "reward: 61.05608541630354 setps: 64 count: 954\n",
      "avg rewards: 29.6754968405475\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.42009\n",
      "Epoch:20 Batch:8 Loss:0.17760\n",
      "Epoch:40 Batch:8 Loss:0.11314\n",
      "Epoch:60 Batch:8 Loss:0.09633\n",
      "Epoch:80 Batch:8 Loss:0.08443\n",
      "Epoch:100 Batch:8 Loss:0.07468\n",
      "Epoch:120 Batch:8 Loss:0.06576\n",
      "Epoch:140 Batch:8 Loss:0.06209\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.089\n",
      "Epoch:10 Batch:8 Loss:0.086\n",
      "Epoch:20 Batch:8 Loss:0.086\n",
      "Epoch:30 Batch:8 Loss:0.085\n",
      "Epoch:40 Batch:8 Loss:0.084\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 56.08337172983738 setps: 72 count: 72\n",
      "reward: 19.720982805253875 setps: 20 count: 92\n",
      "reward: 34.71474231673637 setps: 37 count: 129\n",
      "reward: 35.295835530511965 setps: 47 count: 176\n",
      "reward: 60.558859051059706 setps: 59 count: 235\n",
      "reward: 68.28911795796301 setps: 48 count: 283\n",
      "reward: 20.13885630984005 setps: 19 count: 302\n",
      "reward: 21.287493351726145 setps: 21 count: 323\n",
      "reward: 49.82644835219107 setps: 42 count: 365\n",
      "reward: 19.18163094406773 setps: 19 count: 384\n",
      "reward: 63.848908236087304 setps: 46 count: 430\n",
      "reward: 55.71757510485187 setps: 47 count: 477\n",
      "reward: 70.87310472465906 setps: 52 count: 529\n",
      "reward: 66.41885852374398 setps: 50 count: 579\n",
      "reward: 13.16290832063823 setps: 19 count: 598\n",
      "reward: 18.339263660396686 setps: 21 count: 619\n",
      "reward: 88.15341880938651 setps: 74 count: 693\n",
      "reward: 47.05952845192078 setps: 36 count: 729\n",
      "reward: 16.49556352751533 setps: 17 count: 746\n",
      "reward: 71.41957575409033 setps: 51 count: 797\n",
      "reward: 56.50682544088923 setps: 51 count: 848\n",
      "reward: 45.612807966611584 setps: 65 count: 913\n",
      "reward: 14.625568345350619 setps: 20 count: 933\n",
      "reward: 51.11588020352356 setps: 49 count: 982\n",
      "avg rewards: 44.35196355911885\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.39931\n",
      "Epoch:20 Batch:9 Loss:0.14585\n",
      "Epoch:40 Batch:9 Loss:0.10595\n",
      "Epoch:60 Batch:9 Loss:0.08470\n",
      "Epoch:80 Batch:9 Loss:0.07306\n",
      "Epoch:100 Batch:9 Loss:0.06127\n",
      "Epoch:120 Batch:9 Loss:0.06140\n",
      "Epoch:140 Batch:9 Loss:0.05493\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.064\n",
      "Epoch:10 Batch:8 Loss:0.064\n",
      "Epoch:20 Batch:8 Loss:0.062\n",
      "Epoch:30 Batch:8 Loss:0.060\n",
      "Epoch:40 Batch:8 Loss:0.061\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 60.40254510258819 setps: 43 count: 43\n",
      "reward: 16.990718642288996 setps: 20 count: 63\n",
      "reward: 25.911510274131427 setps: 24 count: 87\n",
      "reward: 26.647418409575764 setps: 25 count: 112\n",
      "reward: 68.98016048688005 setps: 46 count: 158\n",
      "reward: 57.74130383433803 setps: 45 count: 203\n",
      "reward: 56.101978725865784 setps: 49 count: 252\n",
      "reward: 30.91692636122316 setps: 27 count: 279\n",
      "reward: 51.70433441459609 setps: 46 count: 325\n",
      "reward: 56.89267878267419 setps: 44 count: 369\n",
      "reward: 53.146209408984575 setps: 38 count: 407\n",
      "reward: 36.63635189976776 setps: 28 count: 435\n",
      "reward: 64.12445669039445 setps: 51 count: 486\n",
      "reward: 21.022997915174344 setps: 19 count: 505\n",
      "reward: 39.95862575031206 setps: 31 count: 536\n",
      "reward: 68.70721526182459 setps: 48 count: 584\n",
      "reward: 45.42506908352227 setps: 45 count: 629\n",
      "reward: 38.44383512741188 setps: 28 count: 657\n",
      "reward: 38.897376030327 setps: 29 count: 686\n",
      "reward: 25.516737355107033 setps: 22 count: 708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 66.20981944226949 setps: 42 count: 750\n",
      "reward: 57.54946182393763 setps: 41 count: 791\n",
      "reward: 17.689424401197176 setps: 18 count: 809\n",
      "reward: 49.7569740575782 setps: 34 count: 843\n",
      "reward: 54.72230876977556 setps: 39 count: 882\n",
      "reward: 61.62041380715674 setps: 41 count: 923\n",
      "reward: 55.66566923990904 setps: 39 count: 962\n",
      "reward: 56.46142187474905 setps: 36 count: 998\n",
      "avg rewards: 46.565855106198576\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.39260\n",
      "Epoch:20 Batch:10 Loss:0.14271\n",
      "Epoch:40 Batch:10 Loss:0.10553\n",
      "Epoch:60 Batch:10 Loss:0.07655\n",
      "Epoch:80 Batch:10 Loss:0.07181\n",
      "Epoch:100 Batch:10 Loss:0.05926\n",
      "Epoch:120 Batch:10 Loss:0.05574\n",
      "Epoch:140 Batch:10 Loss:0.05765\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.051\n",
      "Epoch:10 Batch:8 Loss:0.051\n",
      "Epoch:20 Batch:8 Loss:0.048\n",
      "Epoch:30 Batch:8 Loss:0.048\n",
      "Epoch:40 Batch:8 Loss:0.049\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 47.1158922903749 setps: 38 count: 38\n",
      "reward: 47.22145069327962 setps: 36 count: 74\n",
      "reward: 43.734716037745244 setps: 36 count: 110\n",
      "reward: 41.28351382833062 setps: 37 count: 147\n",
      "reward: 44.4148726962827 setps: 38 count: 185\n",
      "reward: 47.452880456447026 setps: 34 count: 219\n",
      "reward: 20.317746925207032 setps: 20 count: 239\n",
      "reward: 46.891179866701705 setps: 38 count: 277\n",
      "reward: 45.624518547506895 setps: 33 count: 310\n",
      "reward: 55.09879448151334 setps: 48 count: 358\n",
      "reward: 52.045324017827795 setps: 44 count: 402\n",
      "reward: 39.27784741940122 setps: 31 count: 433\n",
      "reward: 62.42569704368652 setps: 43 count: 476\n",
      "reward: 23.072281204015592 setps: 21 count: 497\n",
      "reward: 41.891313579716375 setps: 31 count: 528\n",
      "reward: 43.30090693677193 setps: 34 count: 562\n",
      "reward: 39.57829785752401 setps: 34 count: 596\n",
      "reward: 58.69494769887533 setps: 56 count: 652\n",
      "reward: 35.58542667787115 setps: 30 count: 682\n",
      "reward: 61.4441785941337 setps: 47 count: 729\n",
      "reward: 49.95738330174936 setps: 35 count: 764\n",
      "reward: 52.591241280881505 setps: 41 count: 805\n",
      "reward: 40.196650698003936 setps: 32 count: 837\n",
      "reward: 45.746293031389364 setps: 34 count: 871\n",
      "reward: 54.105583667154136 setps: 41 count: 912\n",
      "reward: 16.143414082104574 setps: 17 count: 929\n",
      "reward: 38.26739143817976 setps: 29 count: 958\n",
      "avg rewards: 44.202953494543536\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.36557\n",
      "Epoch:20 Batch:11 Loss:0.13797\n",
      "Epoch:40 Batch:11 Loss:0.09499\n",
      "Epoch:60 Batch:11 Loss:0.07125\n",
      "Epoch:80 Batch:11 Loss:0.06530\n",
      "Epoch:100 Batch:11 Loss:0.05422\n",
      "Epoch:120 Batch:11 Loss:0.04805\n",
      "Epoch:140 Batch:11 Loss:0.05320\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.048\n",
      "Epoch:10 Batch:8 Loss:0.048\n",
      "Epoch:20 Batch:8 Loss:0.046\n",
      "Epoch:30 Batch:8 Loss:0.046\n",
      "Epoch:40 Batch:8 Loss:0.045\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 48.22955065249408 setps: 37 count: 37\n",
      "reward: 45.778323456346705 setps: 33 count: 70\n",
      "reward: 13.825940647465178 setps: 18 count: 88\n",
      "reward: 42.914954394221425 setps: 32 count: 120\n",
      "reward: 44.2418347794679 setps: 31 count: 151\n",
      "reward: 33.6130375940309 setps: 28 count: 179\n",
      "reward: 18.885090841854982 setps: 19 count: 198\n",
      "reward: 48.87701901621621 setps: 34 count: 232\n",
      "reward: 61.809711301466464 setps: 50 count: 282\n",
      "reward: 40.64165454519533 setps: 31 count: 313\n",
      "reward: 43.06094405996554 setps: 36 count: 349\n",
      "reward: 38.48424630406807 setps: 30 count: 379\n",
      "reward: 38.35255556602642 setps: 31 count: 410\n",
      "reward: 15.954544890813125 setps: 17 count: 427\n",
      "reward: 39.840924066376466 setps: 32 count: 459\n",
      "reward: 42.87470041598862 setps: 32 count: 491\n",
      "reward: 27.219206211922575 setps: 23 count: 514\n",
      "reward: 49.27062891358948 setps: 34 count: 548\n",
      "reward: 43.42620210857567 setps: 31 count: 579\n",
      "reward: 55.31126201835723 setps: 43 count: 622\n",
      "reward: 79.2642670780231 setps: 49 count: 671\n",
      "reward: 35.76973430451617 setps: 27 count: 698\n",
      "reward: 44.343485041608794 setps: 34 count: 732\n",
      "reward: 35.655105438391914 setps: 27 count: 759\n",
      "reward: 61.16696884099801 setps: 44 count: 803\n",
      "reward: 57.17685505321423 setps: 40 count: 843\n",
      "reward: 47.14581473235885 setps: 35 count: 878\n",
      "reward: 28.356697350634203 setps: 24 count: 902\n",
      "reward: 42.25631113575219 setps: 32 count: 934\n",
      "reward: 37.55659033410629 setps: 29 count: 963\n",
      "avg rewards: 42.043472036468195\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.38032\n",
      "Epoch:20 Batch:12 Loss:0.13314\n",
      "Epoch:40 Batch:12 Loss:0.08336\n",
      "Epoch:60 Batch:12 Loss:0.06939\n",
      "Epoch:80 Batch:12 Loss:0.06091\n",
      "Epoch:100 Batch:12 Loss:0.05600\n",
      "Epoch:120 Batch:12 Loss:0.05281\n",
      "Epoch:140 Batch:12 Loss:0.04463\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.046\n",
      "Epoch:10 Batch:8 Loss:0.045\n",
      "Epoch:20 Batch:8 Loss:0.047\n",
      "Epoch:30 Batch:8 Loss:0.044\n",
      "Epoch:40 Batch:8 Loss:0.046\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 56.29454912740184 setps: 41 count: 41\n",
      "reward: 58.26721048845502 setps: 40 count: 81\n",
      "reward: 47.20281178320437 setps: 48 count: 129\n",
      "reward: 59.423632145412554 setps: 38 count: 167\n",
      "reward: 73.19252678397461 setps: 44 count: 211\n",
      "reward: 68.2130093295462 setps: 43 count: 254\n",
      "reward: 40.891585075802865 setps: 34 count: 288\n",
      "reward: 78.35704785317067 setps: 59 count: 347\n",
      "reward: 91.8142909157992 setps: 55 count: 402\n",
      "reward: 49.834004322954584 setps: 36 count: 438\n",
      "reward: 47.067385419904895 setps: 36 count: 474\n",
      "reward: 86.28786344818072 setps: 66 count: 540\n",
      "reward: 48.860391229421644 setps: 48 count: 588\n",
      "reward: 66.67922836657965 setps: 49 count: 637\n",
      "reward: 57.213967547501674 setps: 46 count: 683\n",
      "reward: 30.789044488032236 setps: 25 count: 708\n",
      "reward: 66.99638132013067 setps: 66 count: 774\n",
      "reward: 66.94008964584644 setps: 50 count: 824\n",
      "reward: 81.5361304614198 setps: 64 count: 888\n",
      "reward: 40.29147403070528 setps: 34 count: 922\n",
      "reward: 74.96173192460846 setps: 51 count: 973\n",
      "avg rewards: 61.48163598609779\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.35924\n",
      "Epoch:20 Batch:13 Loss:0.11586\n",
      "Epoch:40 Batch:13 Loss:0.07052\n",
      "Epoch:60 Batch:13 Loss:0.06062\n",
      "Epoch:80 Batch:13 Loss:0.05808\n",
      "Epoch:100 Batch:13 Loss:0.05195\n",
      "Epoch:120 Batch:13 Loss:0.04476\n",
      "Epoch:140 Batch:13 Loss:0.04363\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.042\n",
      "Epoch:10 Batch:8 Loss:0.040\n",
      "Epoch:20 Batch:8 Loss:0.042\n",
      "Epoch:30 Batch:8 Loss:0.041\n",
      "Epoch:40 Batch:8 Loss:0.041\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 29.814590228025917 setps: 26 count: 26\n",
      "reward: 59.689361252180234 setps: 40 count: 66\n",
      "reward: 62.6809343023153 setps: 36 count: 102\n",
      "reward: 58.837161792792905 setps: 41 count: 143\n",
      "reward: 68.64805057506163 setps: 42 count: 185\n",
      "reward: 50.71576259009016 setps: 35 count: 220\n",
      "reward: 76.76418938636051 setps: 53 count: 273\n",
      "reward: 41.93247735099866 setps: 29 count: 302\n",
      "reward: 47.55182162675774 setps: 30 count: 332\n",
      "reward: 57.83154176734388 setps: 44 count: 376\n",
      "reward: 45.18858554154721 setps: 32 count: 408\n",
      "reward: 50.61146989612317 setps: 33 count: 441\n",
      "reward: 94.8327460933273 setps: 53 count: 494\n",
      "reward: 45.36135357382445 setps: 29 count: 523\n",
      "reward: 53.58328717622354 setps: 35 count: 558\n",
      "reward: 28.7428736414222 setps: 23 count: 581\n",
      "reward: 105.06544448422355 setps: 56 count: 637\n",
      "reward: 54.15825669276964 setps: 37 count: 674\n",
      "reward: 59.11958544559895 setps: 38 count: 712\n",
      "reward: 55.390699405623295 setps: 40 count: 752\n",
      "reward: 70.28744679248632 setps: 39 count: 791\n",
      "reward: 84.6492561419276 setps: 48 count: 839\n",
      "reward: 65.53649142888752 setps: 37 count: 876\n",
      "reward: 49.74842699681467 setps: 44 count: 920\n",
      "reward: 48.66198021233286 setps: 37 count: 957\n",
      "reward: 51.116406232323804 setps: 33 count: 990\n",
      "avg rewards: 58.32770002413011\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.35430\n",
      "Epoch:20 Batch:14 Loss:0.11361\n",
      "Epoch:40 Batch:14 Loss:0.06786\n",
      "Epoch:60 Batch:14 Loss:0.05989\n",
      "Epoch:80 Batch:14 Loss:0.04890\n",
      "Epoch:100 Batch:14 Loss:0.04910\n",
      "Epoch:120 Batch:14 Loss:0.04583\n",
      "Epoch:140 Batch:14 Loss:0.04367\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.039\n",
      "Epoch:10 Batch:8 Loss:0.039\n",
      "Epoch:20 Batch:8 Loss:0.039\n",
      "Epoch:30 Batch:8 Loss:0.039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:40 Batch:8 Loss:0.040\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 114.99105402563728 setps: 91 count: 91\n",
      "reward: 88.5577449748307 setps: 48 count: 139\n",
      "reward: 50.74294125922752 setps: 47 count: 186\n",
      "reward: 71.92692831348104 setps: 50 count: 236\n",
      "reward: 86.55886706798336 setps: 48 count: 284\n",
      "reward: 68.05077967246325 setps: 45 count: 329\n",
      "reward: 98.00130128354505 setps: 56 count: 385\n",
      "reward: 86.66800504508282 setps: 57 count: 442\n",
      "reward: 79.78659430548286 setps: 47 count: 489\n",
      "reward: 91.01711374900188 setps: 57 count: 546\n",
      "reward: 85.44053495633997 setps: 46 count: 592\n",
      "reward: 59.5013666248109 setps: 50 count: 642\n",
      "reward: 72.11470399590908 setps: 42 count: 684\n",
      "reward: 66.39285109527555 setps: 44 count: 728\n",
      "reward: 76.51730345746765 setps: 52 count: 780\n",
      "reward: 18.192923766125748 setps: 19 count: 799\n",
      "reward: 63.10411342484148 setps: 51 count: 850\n",
      "reward: 84.00839791324832 setps: 56 count: 906\n",
      "reward: 82.59187430068559 setps: 49 count: 955\n",
      "reward: 67.25062770347286 setps: 41 count: 996\n",
      "avg rewards: 75.57080134674564\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.33916\n",
      "Epoch:20 Batch:15 Loss:0.10043\n",
      "Epoch:40 Batch:15 Loss:0.07134\n",
      "Epoch:60 Batch:15 Loss:0.05415\n",
      "Epoch:80 Batch:15 Loss:0.05299\n",
      "Epoch:100 Batch:15 Loss:0.04548\n",
      "Epoch:120 Batch:15 Loss:0.03986\n",
      "Epoch:140 Batch:15 Loss:0.04113\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.039\n",
      "Epoch:10 Batch:8 Loss:0.038\n",
      "Epoch:20 Batch:8 Loss:0.037\n",
      "Epoch:30 Batch:8 Loss:0.037\n",
      "Epoch:40 Batch:8 Loss:0.038\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 80.76156064010283 setps: 50 count: 50\n",
      "reward: 17.884681657358303 setps: 20 count: 70\n",
      "reward: 93.08852149316806 setps: 57 count: 127\n",
      "reward: 51.16524914848123 setps: 63 count: 190\n",
      "reward: 127.51414222708262 setps: 88 count: 278\n",
      "reward: 87.29143105130204 setps: 61 count: 339\n",
      "reward: 67.02385781449847 setps: 43 count: 382\n",
      "reward: 23.284412454187983 setps: 21 count: 403\n",
      "reward: 77.11211006509839 setps: 47 count: 450\n",
      "reward: 45.58507877743104 setps: 56 count: 506\n",
      "reward: 53.87861649622646 setps: 43 count: 549\n",
      "reward: 29.308357311539297 setps: 24 count: 573\n",
      "reward: 79.87333801883214 setps: 50 count: 623\n",
      "reward: 94.96213803961149 setps: 61 count: 684\n",
      "reward: 71.38580079567035 setps: 48 count: 732\n",
      "reward: 38.85471789451694 setps: 33 count: 765\n",
      "reward: 56.911090307259286 setps: 44 count: 809\n",
      "reward: 86.50757276985097 setps: 53 count: 862\n",
      "reward: 89.64953567692429 setps: 58 count: 920\n",
      "reward: 87.98443374576603 setps: 67 count: 987\n",
      "avg rewards: 68.00133231924539\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.34670\n",
      "Epoch:20 Batch:16 Loss:0.09808\n",
      "Epoch:40 Batch:16 Loss:0.06489\n",
      "Epoch:60 Batch:16 Loss:0.05643\n",
      "Epoch:80 Batch:16 Loss:0.05541\n",
      "Epoch:100 Batch:16 Loss:0.04391\n",
      "Epoch:120 Batch:16 Loss:0.03982\n",
      "Epoch:140 Batch:16 Loss:0.03814\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.039\n",
      "Epoch:10 Batch:8 Loss:0.037\n",
      "Epoch:20 Batch:8 Loss:0.037\n",
      "Epoch:30 Batch:8 Loss:0.037\n",
      "Epoch:40 Batch:8 Loss:0.038\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 59.44827756919695 setps: 38 count: 38\n",
      "reward: 27.063539666087305 setps: 29 count: 67\n",
      "reward: 36.07985126259446 setps: 29 count: 96\n",
      "reward: 59.384691039875904 setps: 43 count: 139\n",
      "reward: 49.30700154456863 setps: 41 count: 180\n",
      "reward: 56.08932258542045 setps: 41 count: 221\n",
      "reward: 73.54372966088852 setps: 48 count: 269\n",
      "reward: 24.3644668849025 setps: 22 count: 291\n",
      "reward: 77.15378914127217 setps: 48 count: 339\n",
      "reward: 33.10526111813523 setps: 26 count: 365\n",
      "reward: 66.76156406338706 setps: 41 count: 406\n",
      "reward: 55.4382911036926 setps: 51 count: 457\n",
      "reward: 63.488669171935186 setps: 43 count: 500\n",
      "reward: 85.13148764060173 setps: 53 count: 553\n",
      "reward: 12.233294897415908 setps: 18 count: 571\n",
      "reward: 43.440797739609835 setps: 33 count: 604\n",
      "reward: 42.54694439129235 setps: 36 count: 640\n",
      "reward: 33.945697346882646 setps: 27 count: 667\n",
      "reward: 46.76720832712162 setps: 34 count: 701\n",
      "reward: 30.97826927072019 setps: 25 count: 726\n",
      "reward: 59.49674971994537 setps: 45 count: 771\n",
      "reward: 60.98907521092625 setps: 41 count: 812\n",
      "reward: 50.435395520419114 setps: 41 count: 853\n",
      "reward: 51.53256718041376 setps: 42 count: 895\n",
      "reward: 55.83761077111557 setps: 45 count: 940\n",
      "reward: 69.72190184950016 setps: 57 count: 997\n",
      "avg rewards: 50.934055949150824\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.34379\n",
      "Epoch:20 Batch:17 Loss:0.09860\n",
      "Epoch:40 Batch:17 Loss:0.06251\n",
      "Epoch:60 Batch:17 Loss:0.05217\n",
      "Epoch:80 Batch:17 Loss:0.04213\n",
      "Epoch:100 Batch:17 Loss:0.04495\n",
      "Epoch:120 Batch:17 Loss:0.04412\n",
      "Epoch:140 Batch:17 Loss:0.03313\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.034\n",
      "Epoch:10 Batch:8 Loss:0.034\n",
      "Epoch:20 Batch:8 Loss:0.035\n",
      "Epoch:30 Batch:8 Loss:0.035\n",
      "Epoch:40 Batch:8 Loss:0.036\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 53.85751733599173 setps: 41 count: 41\n",
      "reward: 125.35165308881551 setps: 101 count: 142\n",
      "reward: 50.743244738901566 setps: 37 count: 179\n",
      "reward: 45.81310431293096 setps: 41 count: 220\n",
      "reward: 78.59177834681759 setps: 49 count: 269\n",
      "reward: 93.5984193229684 setps: 67 count: 336\n",
      "reward: 37.847723028981996 setps: 31 count: 367\n",
      "reward: 71.19778056586654 setps: 50 count: 417\n",
      "reward: 63.49318750633828 setps: 47 count: 464\n",
      "reward: 87.98589880885118 setps: 60 count: 524\n",
      "reward: 61.37688468785635 setps: 44 count: 568\n",
      "reward: 51.283355948718985 setps: 44 count: 612\n",
      "reward: 63.67315973464865 setps: 46 count: 658\n",
      "reward: 66.44214612959331 setps: 83 count: 741\n",
      "reward: 74.21712313495374 setps: 71 count: 812\n",
      "reward: 84.9256490736676 setps: 51 count: 863\n",
      "reward: 45.84493361397762 setps: 45 count: 908\n",
      "avg rewards: 68.01432702234588\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.32285\n",
      "Epoch:20 Batch:18 Loss:0.08573\n",
      "Epoch:40 Batch:18 Loss:0.06177\n",
      "Epoch:60 Batch:18 Loss:0.05586\n",
      "Epoch:80 Batch:18 Loss:0.04562\n",
      "Epoch:100 Batch:18 Loss:0.03857\n",
      "Epoch:120 Batch:18 Loss:0.03904\n",
      "Epoch:140 Batch:18 Loss:0.03195\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.036\n",
      "Epoch:10 Batch:8 Loss:0.036\n",
      "Epoch:20 Batch:8 Loss:0.034\n",
      "Epoch:30 Batch:8 Loss:0.038\n",
      "Epoch:40 Batch:8 Loss:0.037\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 12.720257520965243 setps: 17 count: 17\n",
      "reward: 23.534115550360003 setps: 22 count: 39\n",
      "reward: 49.20363322532211 setps: 36 count: 75\n",
      "reward: 91.88311499947015 setps: 76 count: 151\n",
      "reward: 62.328666593796505 setps: 50 count: 201\n",
      "reward: 62.27109914757894 setps: 44 count: 245\n",
      "reward: 51.913266239376384 setps: 49 count: 294\n",
      "reward: 42.72987882089803 setps: 39 count: 333\n",
      "reward: 15.34336556906055 setps: 17 count: 350\n",
      "reward: 55.192333320849855 setps: 39 count: 389\n",
      "reward: 21.548005881912836 setps: 20 count: 409\n",
      "reward: 63.490194465844255 setps: 63 count: 472\n",
      "reward: 22.12952437244239 setps: 20 count: 492\n",
      "reward: 129.03154845444223 setps: 86 count: 578\n",
      "reward: 18.809419641649576 setps: 20 count: 598\n",
      "reward: 70.82888819342915 setps: 45 count: 643\n",
      "reward: 53.186154696809545 setps: 68 count: 711\n",
      "reward: 50.808365658340335 setps: 55 count: 766\n",
      "reward: 74.44080748799752 setps: 53 count: 819\n",
      "reward: 62.793114034946505 setps: 65 count: 884\n",
      "reward: 29.2335625157837 setps: 24 count: 908\n",
      "reward: 84.44015900250379 setps: 53 count: 961\n",
      "avg rewards: 52.17543069971725\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.32064\n",
      "Epoch:20 Batch:19 Loss:0.08338\n",
      "Epoch:40 Batch:19 Loss:0.06008\n",
      "Epoch:60 Batch:19 Loss:0.04176\n",
      "Epoch:80 Batch:19 Loss:0.04282\n",
      "Epoch:100 Batch:19 Loss:0.03855\n",
      "Epoch:120 Batch:19 Loss:0.03613\n",
      "Epoch:140 Batch:19 Loss:0.03499\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.037\n",
      "Epoch:10 Batch:8 Loss:0.036\n",
      "Epoch:20 Batch:8 Loss:0.035\n",
      "Epoch:30 Batch:8 Loss:0.034\n",
      "Epoch:40 Batch:8 Loss:0.036\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 101.7690159121572 setps: 66 count: 66\n",
      "reward: 68.95617200117121 setps: 47 count: 113\n",
      "reward: 84.68130879507227 setps: 53 count: 166\n",
      "reward: 4.713374753098471 setps: 18 count: 184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 80.08779462982496 setps: 64 count: 248\n",
      "reward: 63.58035893828636 setps: 53 count: 301\n",
      "reward: 62.53825345086952 setps: 55 count: 356\n",
      "reward: 62.42999998444718 setps: 46 count: 402\n",
      "reward: 60.77966167816484 setps: 43 count: 445\n",
      "reward: 117.4570415008915 setps: 84 count: 529\n",
      "reward: 84.27199383995615 setps: 97 count: 626\n",
      "reward: 84.8087738237853 setps: 48 count: 674\n",
      "reward: 75.40014350212584 setps: 58 count: 732\n",
      "reward: 49.9051630846967 setps: 49 count: 781\n",
      "reward: 68.49081105295917 setps: 50 count: 831\n",
      "reward: 70.08954963486467 setps: 52 count: 883\n",
      "reward: 87.76399530485214 setps: 70 count: 953\n",
      "avg rewards: 72.2190242286602\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.31770\n",
      "Epoch:20 Batch:20 Loss:0.08426\n",
      "Epoch:40 Batch:20 Loss:0.05216\n",
      "Epoch:60 Batch:20 Loss:0.04396\n",
      "Epoch:80 Batch:20 Loss:0.03967\n",
      "Epoch:100 Batch:20 Loss:0.03826\n",
      "Epoch:120 Batch:20 Loss:0.03790\n",
      "Epoch:140 Batch:20 Loss:0.03540\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.032\n",
      "Epoch:10 Batch:8 Loss:0.034\n",
      "Epoch:20 Batch:8 Loss:0.033\n",
      "Epoch:30 Batch:8 Loss:0.031\n",
      "Epoch:40 Batch:8 Loss:0.032\n",
      "Done!\n",
      "############# start BipedalWalker-v3 training ###################\n",
      "(49,)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -106.51782285279114 setps: 98 count: 98\n",
      "reward: -97.12831929902724 setps: 82 count: 180\n",
      "reward: -98.1580530881829 setps: 84 count: 264\n",
      "reward: -102.67251352002222 setps: 134 count: 398\n",
      "reward: -124.83895313414128 setps: 128 count: 526\n",
      "reward: -114.26748292708645 setps: 58 count: 584\n",
      "reward: -115.70578029473126 setps: 76 count: 660\n",
      "reward: -98.63186475820342 setps: 162 count: 822\n",
      "reward: -99.85810176943491 setps: 100 count: 922\n",
      "avg rewards: -106.4198768492912\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:0.34777\n",
      "Epoch:20 Batch:1 Loss:0.20386\n",
      "Epoch:40 Batch:1 Loss:0.17298\n",
      "Epoch:60 Batch:1 Loss:0.16253\n",
      "Epoch:80 Batch:1 Loss:0.14384\n",
      "Epoch:100 Batch:1 Loss:0.12319\n",
      "Epoch:120 Batch:1 Loss:0.11085\n",
      "Epoch:140 Batch:1 Loss:0.09827\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.138\n",
      "Epoch:10 Batch:9 Loss:0.138\n",
      "Epoch:20 Batch:9 Loss:0.133\n",
      "Epoch:30 Batch:9 Loss:0.130\n",
      "Epoch:40 Batch:9 Loss:0.129\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -116.77789977066593 setps: 79 count: 79\n",
      "reward: -118.46620597351468 setps: 72 count: 151\n",
      "reward: -112.88926823204444 setps: 69 count: 220\n",
      "reward: -107.4188494358932 setps: 84 count: 304\n",
      "reward: -115.89521478248574 setps: 96 count: 400\n",
      "reward: -135.14188610762483 setps: 166 count: 566\n",
      "reward: -97.6674406340681 setps: 143 count: 709\n",
      "reward: -113.10999827067864 setps: 89 count: 798\n",
      "reward: -115.82772122253415 setps: 87 count: 885\n",
      "reward: -116.75522777082213 setps: 75 count: 960\n",
      "avg rewards: -114.99497122003318\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:0.38015\n",
      "Epoch:20 Batch:2 Loss:0.16516\n",
      "Epoch:40 Batch:2 Loss:0.11430\n",
      "Epoch:60 Batch:2 Loss:0.08995\n",
      "Epoch:80 Batch:2 Loss:0.07667\n",
      "Epoch:100 Batch:2 Loss:0.06674\n",
      "Epoch:120 Batch:2 Loss:0.05675\n",
      "Epoch:140 Batch:2 Loss:0.05073\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.111\n",
      "Epoch:10 Batch:9 Loss:0.102\n",
      "Epoch:20 Batch:9 Loss:0.104\n",
      "Epoch:30 Batch:9 Loss:0.098\n",
      "Epoch:40 Batch:9 Loss:0.102\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -30.360282991286997 setps: 800 count: 800\n",
      "avg rewards: -30.360282991286997\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.34465\n",
      "Epoch:20 Batch:3 Loss:0.13311\n",
      "Epoch:40 Batch:3 Loss:0.06760\n",
      "Epoch:60 Batch:3 Loss:0.06043\n",
      "Epoch:80 Batch:3 Loss:0.05013\n",
      "Epoch:100 Batch:3 Loss:0.04179\n",
      "Epoch:120 Batch:3 Loss:0.03663\n",
      "Epoch:140 Batch:3 Loss:0.03306\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.081\n",
      "Epoch:10 Batch:9 Loss:0.082\n",
      "Epoch:20 Batch:9 Loss:0.077\n",
      "Epoch:30 Batch:9 Loss:0.083\n",
      "Epoch:40 Batch:9 Loss:0.079\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -114.81506227728228 setps: 61 count: 61\n",
      "reward: -112.17433453719752 setps: 58 count: 119\n",
      "reward: -105.7019433081057 setps: 49 count: 168\n",
      "reward: -104.72882260093155 setps: 47 count: 215\n",
      "reward: -112.1410045322394 setps: 54 count: 269\n",
      "reward: -113.4020490388299 setps: 56 count: 325\n",
      "reward: -104.2915910071209 setps: 48 count: 373\n",
      "reward: -104.7436284535167 setps: 47 count: 420\n",
      "reward: -107.85977245884824 setps: 49 count: 469\n",
      "reward: -112.54864394740201 setps: 61 count: 530\n",
      "reward: -104.57807295833031 setps: 47 count: 577\n",
      "reward: -113.82748238174183 setps: 60 count: 637\n",
      "reward: -105.01252881973981 setps: 47 count: 684\n",
      "reward: -112.97223045723295 setps: 55 count: 739\n",
      "reward: -105.78812142803024 setps: 49 count: 788\n",
      "reward: -106.53892723797075 setps: 47 count: 835\n",
      "reward: -113.01910882000625 setps: 62 count: 897\n",
      "reward: -113.55321538669305 setps: 59 count: 956\n",
      "avg rewards: -109.31647442506774\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.35260\n",
      "Epoch:20 Batch:4 Loss:0.10725\n",
      "Epoch:40 Batch:4 Loss:0.06474\n",
      "Epoch:60 Batch:4 Loss:0.04882\n",
      "Epoch:80 Batch:4 Loss:0.03850\n",
      "Epoch:100 Batch:4 Loss:0.03360\n",
      "Epoch:120 Batch:4 Loss:0.03063\n",
      "Epoch:140 Batch:4 Loss:0.02740\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.065\n",
      "Epoch:10 Batch:9 Loss:0.062\n",
      "Epoch:20 Batch:9 Loss:0.063\n",
      "Epoch:30 Batch:9 Loss:0.062\n",
      "Epoch:40 Batch:9 Loss:0.061\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -13.528497795837767 setps: 800 count: 800\n",
      "reward: -111.14249179306564 setps: 72 count: 872\n",
      "avg rewards: -62.335494794451705\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.31643\n",
      "Epoch:20 Batch:5 Loss:0.09170\n",
      "Epoch:40 Batch:5 Loss:0.05680\n",
      "Epoch:60 Batch:5 Loss:0.04183\n",
      "Epoch:80 Batch:5 Loss:0.03550\n",
      "Epoch:100 Batch:5 Loss:0.03027\n",
      "Epoch:120 Batch:5 Loss:0.02652\n",
      "Epoch:140 Batch:5 Loss:0.02725\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.059\n",
      "Epoch:10 Batch:9 Loss:0.060\n",
      "Epoch:20 Batch:9 Loss:0.057\n",
      "Epoch:30 Batch:9 Loss:0.053\n",
      "Epoch:40 Batch:9 Loss:0.052\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -111.7947500663766 setps: 52 count: 52\n",
      "reward: -114.1307255477899 setps: 135 count: 187\n",
      "reward: -11.992636642901841 setps: 800 count: 987\n",
      "avg rewards: -79.30603741902277\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.32289\n",
      "Epoch:20 Batch:6 Loss:0.07483\n",
      "Epoch:40 Batch:6 Loss:0.04293\n",
      "Epoch:60 Batch:6 Loss:0.03459\n",
      "Epoch:80 Batch:6 Loss:0.02960\n",
      "Epoch:100 Batch:6 Loss:0.02525\n",
      "Epoch:120 Batch:6 Loss:0.02624\n",
      "Epoch:140 Batch:6 Loss:0.02611\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.055\n",
      "Epoch:10 Batch:9 Loss:0.050\n",
      "Epoch:20 Batch:9 Loss:0.055\n",
      "Epoch:30 Batch:9 Loss:0.053\n",
      "Epoch:40 Batch:9 Loss:0.049\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -103.71471414851707 setps: 85 count: 85\n",
      "reward: -98.75633896894753 setps: 119 count: 204\n",
      "reward: -108.10489871346113 setps: 55 count: 259\n",
      "reward: -112.8141834965727 setps: 63 count: 322\n",
      "reward: -104.780165047807 setps: 61 count: 383\n",
      "reward: -96.86568361327363 setps: 139 count: 522\n",
      "reward: -115.97752631233632 setps: 65 count: 587\n",
      "reward: -117.25144437321214 setps: 64 count: 651\n",
      "reward: -111.98774370010942 setps: 79 count: 730\n",
      "reward: -114.54427193851046 setps: 57 count: 787\n",
      "reward: -106.03891081630594 setps: 103 count: 890\n",
      "reward: -118.11358733933481 setps: 82 count: 972\n",
      "avg rewards: -109.07912237236569\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.32705\n",
      "Epoch:20 Batch:7 Loss:0.06593\n",
      "Epoch:40 Batch:7 Loss:0.04397\n",
      "Epoch:60 Batch:7 Loss:0.03350\n",
      "Epoch:80 Batch:7 Loss:0.02886\n",
      "Epoch:100 Batch:7 Loss:0.02666\n",
      "Epoch:120 Batch:7 Loss:0.02632\n",
      "Epoch:140 Batch:7 Loss:0.02363\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.057\n",
      "Epoch:10 Batch:9 Loss:0.051\n",
      "Epoch:20 Batch:9 Loss:0.048\n",
      "Epoch:30 Batch:9 Loss:0.050\n",
      "Epoch:40 Batch:9 Loss:0.052\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -93.14800813240558 setps: 84 count: 84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -15.207173017452774 setps: 800 count: 884\n",
      "reward: -115.04758978911923 setps: 96 count: 980\n",
      "avg rewards: -74.46759031299253\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.31539\n",
      "Epoch:20 Batch:8 Loss:0.06022\n",
      "Epoch:40 Batch:8 Loss:0.03925\n",
      "Epoch:60 Batch:8 Loss:0.03145\n",
      "Epoch:80 Batch:8 Loss:0.02828\n",
      "Epoch:100 Batch:8 Loss:0.02675\n",
      "Epoch:120 Batch:8 Loss:0.02641\n",
      "Epoch:140 Batch:8 Loss:0.02571\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.051\n",
      "Epoch:10 Batch:9 Loss:0.050\n",
      "Epoch:20 Batch:9 Loss:0.049\n",
      "Epoch:30 Batch:9 Loss:0.047\n",
      "Epoch:40 Batch:9 Loss:0.048\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -115.31394871042545 setps: 86 count: 86\n",
      "reward: -94.9532883296578 setps: 88 count: 174\n",
      "reward: -94.0644095277867 setps: 96 count: 270\n",
      "avg rewards: -101.44388218928998\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.29678\n",
      "Epoch:20 Batch:9 Loss:0.05456\n",
      "Epoch:40 Batch:9 Loss:0.03588\n",
      "Epoch:60 Batch:9 Loss:0.02936\n",
      "Epoch:80 Batch:9 Loss:0.02688\n",
      "Epoch:100 Batch:9 Loss:0.02773\n",
      "Epoch:120 Batch:9 Loss:0.02493\n",
      "Epoch:140 Batch:9 Loss:0.02530\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.047\n",
      "Epoch:10 Batch:9 Loss:0.046\n",
      "Epoch:20 Batch:9 Loss:0.047\n",
      "Epoch:30 Batch:9 Loss:0.049\n",
      "Epoch:40 Batch:9 Loss:0.042\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -94.60459173233062 setps: 113 count: 113\n",
      "reward: -26.509785305084034 setps: 800 count: 913\n",
      "avg rewards: -60.55718851870733\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.30670\n",
      "Epoch:20 Batch:10 Loss:0.05683\n",
      "Epoch:40 Batch:10 Loss:0.03369\n",
      "Epoch:60 Batch:10 Loss:0.02895\n",
      "Epoch:80 Batch:10 Loss:0.02669\n",
      "Epoch:100 Batch:10 Loss:0.02552\n",
      "Epoch:120 Batch:10 Loss:0.02623\n",
      "Epoch:140 Batch:10 Loss:0.02476\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.047\n",
      "Epoch:10 Batch:9 Loss:0.046\n",
      "Epoch:20 Batch:9 Loss:0.046\n",
      "Epoch:30 Batch:9 Loss:0.042\n",
      "Epoch:40 Batch:9 Loss:0.046\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -23.198831003613765 setps: 800 count: 800\n",
      "avg rewards: -23.198831003613765\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.25227\n",
      "Epoch:20 Batch:11 Loss:0.05222\n",
      "Epoch:40 Batch:11 Loss:0.03479\n",
      "Epoch:60 Batch:11 Loss:0.02917\n",
      "Epoch:80 Batch:11 Loss:0.02791\n",
      "Epoch:100 Batch:11 Loss:0.02755\n",
      "Epoch:120 Batch:11 Loss:0.02598\n",
      "Epoch:140 Batch:11 Loss:0.02357\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.047\n",
      "Epoch:10 Batch:9 Loss:0.041\n",
      "Epoch:20 Batch:9 Loss:0.046\n",
      "Epoch:30 Batch:9 Loss:0.042\n",
      "Epoch:40 Batch:9 Loss:0.042\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -55.796782668755085 setps: 213 count: 213\n",
      "avg rewards: -55.796782668755085\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.30901\n",
      "Epoch:20 Batch:12 Loss:0.05024\n",
      "Epoch:40 Batch:12 Loss:0.03349\n",
      "Epoch:60 Batch:12 Loss:0.03235\n",
      "Epoch:80 Batch:12 Loss:0.02791\n",
      "Epoch:100 Batch:12 Loss:0.02784\n",
      "Epoch:120 Batch:12 Loss:0.02649\n",
      "Epoch:140 Batch:12 Loss:0.02649\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.036\n",
      "Epoch:10 Batch:9 Loss:0.038\n",
      "Epoch:20 Batch:9 Loss:0.036\n",
      "Epoch:30 Batch:9 Loss:0.036\n",
      "Epoch:40 Batch:9 Loss:0.037\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -107.11710577121998 setps: 87 count: 87\n",
      "reward: -11.491430370605535 setps: 800 count: 887\n",
      "avg rewards: -59.304268070912755\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.27788\n",
      "Epoch:20 Batch:13 Loss:0.04790\n",
      "Epoch:40 Batch:13 Loss:0.03337\n",
      "Epoch:60 Batch:13 Loss:0.02790\n",
      "Epoch:80 Batch:13 Loss:0.02743\n",
      "Epoch:100 Batch:13 Loss:0.02792\n",
      "Epoch:120 Batch:13 Loss:0.02729\n",
      "Epoch:140 Batch:13 Loss:0.02612\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.035\n",
      "Epoch:10 Batch:9 Loss:0.037\n",
      "Epoch:20 Batch:9 Loss:0.033\n",
      "Epoch:30 Batch:9 Loss:0.037\n",
      "Epoch:40 Batch:9 Loss:0.040\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 81.64684405141082 setps: 800 count: 800\n",
      "avg rewards: 81.64684405141082\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.25346\n",
      "Epoch:20 Batch:14 Loss:0.04669\n",
      "Epoch:40 Batch:14 Loss:0.03213\n",
      "Epoch:60 Batch:14 Loss:0.03081\n",
      "Epoch:80 Batch:14 Loss:0.02677\n",
      "Epoch:100 Batch:14 Loss:0.02699\n",
      "Epoch:120 Batch:14 Loss:0.02613\n",
      "Epoch:140 Batch:14 Loss:0.02714\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.035\n",
      "Epoch:10 Batch:9 Loss:0.035\n",
      "Epoch:20 Batch:9 Loss:0.037\n",
      "Epoch:30 Batch:9 Loss:0.037\n",
      "Epoch:40 Batch:9 Loss:0.035\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -118.44594524367153 setps: 64 count: 64\n",
      "reward: 37.97438231096353 setps: 800 count: 864\n",
      "avg rewards: -40.235781466354\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.24070\n",
      "Epoch:20 Batch:15 Loss:0.04500\n",
      "Epoch:40 Batch:15 Loss:0.03317\n",
      "Epoch:60 Batch:15 Loss:0.02903\n",
      "Epoch:80 Batch:15 Loss:0.02646\n",
      "Epoch:100 Batch:15 Loss:0.02839\n",
      "Epoch:120 Batch:15 Loss:0.02569\n",
      "Epoch:140 Batch:15 Loss:0.02539\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.032\n",
      "Epoch:10 Batch:9 Loss:0.036\n",
      "Epoch:20 Batch:9 Loss:0.033\n",
      "Epoch:30 Batch:9 Loss:0.034\n",
      "Epoch:40 Batch:9 Loss:0.032\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 23.638016678477985 setps: 800 count: 800\n",
      "avg rewards: 23.638016678477985\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.25907\n",
      "Epoch:20 Batch:16 Loss:0.04270\n",
      "Epoch:40 Batch:16 Loss:0.03305\n",
      "Epoch:60 Batch:16 Loss:0.03027\n",
      "Epoch:80 Batch:16 Loss:0.02950\n",
      "Epoch:100 Batch:16 Loss:0.02874\n",
      "Epoch:120 Batch:16 Loss:0.02577\n",
      "Epoch:140 Batch:16 Loss:0.02679\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.033\n",
      "Epoch:10 Batch:9 Loss:0.035\n",
      "Epoch:20 Batch:9 Loss:0.032\n",
      "Epoch:30 Batch:9 Loss:0.031\n",
      "Epoch:40 Batch:9 Loss:0.034\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 136.6294255278107 setps: 800 count: 800\n",
      "avg rewards: 136.6294255278107\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.23920\n",
      "Epoch:20 Batch:17 Loss:0.04319\n",
      "Epoch:40 Batch:17 Loss:0.03242\n",
      "Epoch:60 Batch:17 Loss:0.02960\n",
      "Epoch:80 Batch:17 Loss:0.02990\n",
      "Epoch:100 Batch:17 Loss:0.02851\n",
      "Epoch:120 Batch:17 Loss:0.02893\n",
      "Epoch:140 Batch:17 Loss:0.02712\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.033\n",
      "Epoch:10 Batch:9 Loss:0.030\n",
      "Epoch:20 Batch:9 Loss:0.031\n",
      "Epoch:30 Batch:9 Loss:0.028\n",
      "Epoch:40 Batch:9 Loss:0.032\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 36.967984024962895 setps: 800 count: 800\n",
      "avg rewards: 36.967984024962895\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.19976\n",
      "Epoch:20 Batch:18 Loss:0.04411\n",
      "Epoch:40 Batch:18 Loss:0.03236\n",
      "Epoch:60 Batch:18 Loss:0.03159\n",
      "Epoch:80 Batch:18 Loss:0.03026\n",
      "Epoch:100 Batch:18 Loss:0.02930\n",
      "Epoch:120 Batch:18 Loss:0.02701\n",
      "Epoch:140 Batch:18 Loss:0.02547\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.033\n",
      "Epoch:10 Batch:9 Loss:0.031\n",
      "Epoch:20 Batch:9 Loss:0.030\n",
      "Epoch:30 Batch:9 Loss:0.031\n",
      "Epoch:40 Batch:9 Loss:0.029\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 146.8639444818335 setps: 800 count: 800\n",
      "avg rewards: 146.8639444818335\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.21063\n",
      "Epoch:20 Batch:19 Loss:0.04370\n",
      "Epoch:40 Batch:19 Loss:0.03227\n",
      "Epoch:60 Batch:19 Loss:0.02916\n",
      "Epoch:80 Batch:19 Loss:0.03079\n",
      "Epoch:100 Batch:19 Loss:0.02806\n",
      "Epoch:120 Batch:19 Loss:0.02579\n",
      "Epoch:140 Batch:19 Loss:0.02716\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.031\n",
      "Epoch:10 Batch:9 Loss:0.032\n",
      "Epoch:20 Batch:9 Loss:0.031\n",
      "Epoch:30 Batch:9 Loss:0.030\n",
      "Epoch:40 Batch:9 Loss:0.032\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 142.32816335408012 setps: 800 count: 800\n",
      "avg rewards: 142.32816335408012\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:20 Loss:0.19771\n",
      "Epoch:20 Batch:20 Loss:0.04106\n",
      "Epoch:40 Batch:20 Loss:0.03219\n",
      "Epoch:60 Batch:20 Loss:0.03138\n",
      "Epoch:80 Batch:20 Loss:0.02673\n",
      "Epoch:100 Batch:20 Loss:0.02656\n",
      "Epoch:120 Batch:20 Loss:0.02750\n",
      "Epoch:140 Batch:20 Loss:0.02535\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.028\n",
      "Epoch:10 Batch:9 Loss:0.028\n",
      "Epoch:20 Batch:9 Loss:0.028\n",
      "Epoch:30 Batch:9 Loss:0.028\n",
      "Epoch:40 Batch:9 Loss:0.030\n",
      "Done!\n",
      "############# start Walker2DBulletEnv-v0 training ###################\n",
      "(49, 1000, 22)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 23.3598238965118 setps: 27 count: 27\n",
      "reward: 13.427080848017068 setps: 10 count: 37\n",
      "reward: 20.447937644658666 setps: 21 count: 58\n",
      "reward: 15.89743476329022 setps: 11 count: 69\n",
      "reward: 12.8848741660433 setps: 8 count: 77\n",
      "reward: 15.188405656484246 setps: 14 count: 91\n",
      "reward: 14.644253449182726 setps: 10 count: 101\n",
      "reward: 12.268839567886607 setps: 7 count: 108\n",
      "reward: 22.879957070975788 setps: 38 count: 146\n",
      "reward: 20.522875732048124 setps: 20 count: 166\n",
      "reward: 17.80420306020387 setps: 18 count: 184\n",
      "reward: 23.57552472402458 setps: 23 count: 207\n",
      "reward: 18.91639764422871 setps: 15 count: 222\n",
      "reward: 19.579976184986297 setps: 19 count: 241\n",
      "reward: 18.567585506930477 setps: 19 count: 260\n",
      "reward: 13.182927738827132 setps: 9 count: 269\n",
      "reward: 15.426981963751313 setps: 12 count: 281\n",
      "reward: 23.036540000706733 setps: 19 count: 300\n",
      "reward: 20.059228059115412 setps: 18 count: 318\n",
      "reward: 17.218703174342224 setps: 13 count: 331\n",
      "reward: 15.450910336208473 setps: 13 count: 344\n",
      "reward: 17.335596105142027 setps: 14 count: 358\n",
      "reward: 15.936367014636929 setps: 16 count: 374\n",
      "reward: 18.878027819246928 setps: 15 count: 389\n",
      "reward: 14.904123069619526 setps: 11 count: 400\n",
      "reward: 16.414455984444068 setps: 11 count: 411\n",
      "reward: 14.114786853561235 setps: 11 count: 422\n",
      "reward: 22.543155703875524 setps: 22 count: 444\n",
      "reward: 19.082036440697266 setps: 18 count: 462\n",
      "reward: 17.201050974809913 setps: 16 count: 478\n",
      "reward: 20.33845903098845 setps: 15 count: 493\n",
      "reward: 12.997955675171397 setps: 9 count: 502\n",
      "reward: 20.00628185101959 setps: 19 count: 521\n",
      "reward: 13.724182081763864 setps: 10 count: 531\n",
      "reward: 18.90627508321922 setps: 15 count: 546\n",
      "reward: 21.32550403205387 setps: 21 count: 567\n",
      "reward: 24.04043122374715 setps: 24 count: 591\n",
      "reward: 17.07126523296756 setps: 23 count: 614\n",
      "reward: 15.972613652025757 setps: 10 count: 624\n",
      "reward: 16.80172079876647 setps: 16 count: 640\n",
      "reward: 15.286474653735057 setps: 10 count: 650\n",
      "reward: 29.780375466399708 setps: 33 count: 683\n",
      "reward: 16.540354495529026 setps: 17 count: 700\n",
      "reward: 22.04590901549091 setps: 22 count: 722\n",
      "reward: 15.46703833794745 setps: 18 count: 740\n",
      "reward: 18.677262847745443 setps: 14 count: 754\n",
      "reward: 19.75043375710666 setps: 19 count: 773\n",
      "reward: 17.56491007366567 setps: 15 count: 788\n",
      "reward: 17.304181547183546 setps: 13 count: 801\n",
      "reward: 24.459950622131874 setps: 25 count: 826\n",
      "reward: 15.08075326136459 setps: 12 count: 838\n",
      "reward: 14.631682121122138 setps: 15 count: 853\n",
      "reward: 15.342971074323579 setps: 10 count: 863\n",
      "reward: 19.34638860751002 setps: 19 count: 882\n",
      "reward: 18.63123921203369 setps: 17 count: 899\n",
      "reward: 20.79012041430105 setps: 22 count: 921\n",
      "reward: 17.032250068732537 setps: 11 count: 932\n",
      "reward: 19.22291105356126 setps: 17 count: 949\n",
      "reward: 15.178495415404905 setps: 13 count: 962\n",
      "reward: 22.368147764010065 setps: 21 count: 983\n",
      "reward: 17.395201157190606 setps: 14 count: 997\n",
      "avg rewards: 18.095603291519186\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:0.34015\n",
      "Epoch:20 Batch:1 Loss:0.15406\n",
      "Epoch:40 Batch:1 Loss:0.11126\n",
      "Epoch:60 Batch:1 Loss:0.10353\n",
      "Epoch:80 Batch:1 Loss:0.08970\n",
      "Epoch:100 Batch:1 Loss:0.06997\n",
      "Epoch:120 Batch:1 Loss:0.05623\n",
      "Epoch:140 Batch:1 Loss:0.05131\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.126\n",
      "Epoch:10 Batch:10 Loss:0.126\n",
      "Epoch:20 Batch:10 Loss:0.122\n",
      "Epoch:30 Batch:10 Loss:0.122\n",
      "Epoch:40 Batch:10 Loss:0.121\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 17.432699606608363 setps: 66 count: 66\n",
      "reward: 16.85462527953203 setps: 67 count: 133\n",
      "reward: 13.754379233137296 setps: 64 count: 197\n",
      "reward: 18.801117509315358 setps: 76 count: 273\n",
      "reward: 19.777515700830556 setps: 66 count: 339\n",
      "reward: 14.857125038038067 setps: 68 count: 407\n",
      "reward: 19.190630669024532 setps: 74 count: 481\n",
      "reward: 23.308101307596374 setps: 85 count: 566\n",
      "reward: 31.23663936835946 setps: 88 count: 654\n",
      "reward: 20.9063486563551 setps: 66 count: 720\n",
      "reward: 17.044370319796148 setps: 62 count: 782\n",
      "reward: 16.763723940259656 setps: 67 count: 849\n",
      "reward: 10.387203952112767 setps: 57 count: 906\n",
      "reward: 13.880427935424091 setps: 65 count: 971\n",
      "avg rewards: 18.156779179742127\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:0.33156\n",
      "Epoch:20 Batch:2 Loss:0.08764\n",
      "Epoch:40 Batch:2 Loss:0.06985\n",
      "Epoch:60 Batch:2 Loss:0.05765\n",
      "Epoch:80 Batch:2 Loss:0.04471\n",
      "Epoch:100 Batch:2 Loss:0.03454\n",
      "Epoch:120 Batch:2 Loss:0.02667\n",
      "Epoch:140 Batch:2 Loss:0.02492\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.097\n",
      "Epoch:10 Batch:10 Loss:0.096\n",
      "Epoch:20 Batch:10 Loss:0.093\n",
      "Epoch:30 Batch:10 Loss:0.094\n",
      "Epoch:40 Batch:10 Loss:0.094\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 45.9792301879905 setps: 144 count: 144\n",
      "reward: 33.227412093423474 setps: 89 count: 233\n",
      "reward: 19.631106946090576 setps: 81 count: 314\n",
      "reward: 27.570123987011897 setps: 77 count: 391\n",
      "reward: 14.985054492075873 setps: 77 count: 468\n",
      "reward: 11.539718639868084 setps: 56 count: 524\n",
      "reward: 56.35491214980284 setps: 88 count: 612\n",
      "reward: 9.399099716982166 setps: 51 count: 663\n",
      "reward: 10.665880155692864 setps: 71 count: 734\n",
      "reward: 25.70804777802696 setps: 103 count: 837\n",
      "reward: 25.319476784733705 setps: 74 count: 911\n",
      "reward: 27.057593202080163 setps: 78 count: 989\n",
      "avg rewards: 25.619804677814926\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.33752\n",
      "Epoch:20 Batch:3 Loss:0.07131\n",
      "Epoch:40 Batch:3 Loss:0.05079\n",
      "Epoch:60 Batch:3 Loss:0.03606\n",
      "Epoch:80 Batch:3 Loss:0.02465\n",
      "Epoch:100 Batch:3 Loss:0.02081\n",
      "Epoch:120 Batch:3 Loss:0.01904\n",
      "Epoch:140 Batch:3 Loss:0.01764\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.086\n",
      "Epoch:10 Batch:10 Loss:0.086\n",
      "Epoch:20 Batch:10 Loss:0.084\n",
      "Epoch:30 Batch:10 Loss:0.083\n",
      "Epoch:40 Batch:10 Loss:0.082\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 8.441748448813449 setps: 48 count: 48\n",
      "reward: 16.81280426862795 setps: 68 count: 116\n",
      "reward: 22.46929187957139 setps: 87 count: 203\n",
      "reward: 49.61251956493797 setps: 78 count: 281\n",
      "reward: 46.11935844185209 setps: 70 count: 351\n",
      "reward: 51.6667785901285 setps: 133 count: 484\n",
      "reward: 47.652560537100385 setps: 141 count: 625\n",
      "reward: 68.38242508547262 setps: 142 count: 767\n",
      "reward: 18.10224100549822 setps: 71 count: 838\n",
      "reward: 22.39196496866643 setps: 77 count: 915\n",
      "reward: 20.524012713255072 setps: 73 count: 988\n",
      "avg rewards: 33.83415504581128\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.31838\n",
      "Epoch:20 Batch:4 Loss:0.06124\n",
      "Epoch:40 Batch:4 Loss:0.03679\n",
      "Epoch:60 Batch:4 Loss:0.02782\n",
      "Epoch:80 Batch:4 Loss:0.02066\n",
      "Epoch:100 Batch:4 Loss:0.01703\n",
      "Epoch:120 Batch:4 Loss:0.01391\n",
      "Epoch:140 Batch:4 Loss:0.01325\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.084\n",
      "Epoch:10 Batch:10 Loss:0.082\n",
      "Epoch:20 Batch:10 Loss:0.080\n",
      "Epoch:30 Batch:10 Loss:0.082\n",
      "Epoch:40 Batch:10 Loss:0.079\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 32.333739194710496 setps: 50 count: 50\n",
      "reward: 369.2068009886802 setps: 800 count: 850\n",
      "reward: 63.26415021651046 setps: 114 count: 964\n",
      "avg rewards: 154.93489679996705\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.31557\n",
      "Epoch:20 Batch:5 Loss:0.04726\n",
      "Epoch:40 Batch:5 Loss:0.02698\n",
      "Epoch:60 Batch:5 Loss:0.01894\n",
      "Epoch:80 Batch:5 Loss:0.01597\n",
      "Epoch:100 Batch:5 Loss:0.01367\n",
      "Epoch:120 Batch:5 Loss:0.01193\n",
      "Epoch:140 Batch:5 Loss:0.01176\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:10 Loss:0.077\n",
      "Epoch:10 Batch:10 Loss:0.076\n",
      "Epoch:20 Batch:10 Loss:0.074\n",
      "Epoch:30 Batch:10 Loss:0.073\n",
      "Epoch:40 Batch:10 Loss:0.074\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 15.622867768460129 setps: 55 count: 55\n",
      "reward: 16.710956578781765 setps: 72 count: 127\n",
      "reward: 11.07971383073018 setps: 61 count: 188\n",
      "reward: 11.502785160462373 setps: 57 count: 245\n",
      "reward: 8.562407927242752 setps: 49 count: 294\n",
      "reward: 18.965361550213135 setps: 79 count: 373\n",
      "reward: 25.28375394612376 setps: 77 count: 450\n",
      "reward: 28.656272665744492 setps: 86 count: 536\n",
      "reward: 6.838807321671625 setps: 45 count: 581\n",
      "reward: 17.213511119436586 setps: 60 count: 641\n",
      "reward: 8.820281345713017 setps: 53 count: 694\n",
      "reward: 8.633741044822091 setps: 50 count: 744\n",
      "reward: 12.863924046086318 setps: 60 count: 804\n",
      "reward: 19.535263364861024 setps: 70 count: 874\n",
      "reward: 22.987873681173387 setps: 91 count: 965\n",
      "avg rewards: 15.551834756768178\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.27649\n",
      "Epoch:20 Batch:6 Loss:0.04004\n",
      "Epoch:40 Batch:6 Loss:0.02292\n",
      "Epoch:60 Batch:6 Loss:0.01600\n",
      "Epoch:80 Batch:6 Loss:0.01317\n",
      "Epoch:100 Batch:6 Loss:0.01138\n",
      "Epoch:120 Batch:6 Loss:0.01110\n",
      "Epoch:140 Batch:6 Loss:0.00975\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.072\n",
      "Epoch:10 Batch:10 Loss:0.072\n",
      "Epoch:20 Batch:10 Loss:0.071\n",
      "Epoch:30 Batch:10 Loss:0.072\n",
      "Epoch:40 Batch:10 Loss:0.069\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 33.10046390544884 setps: 102 count: 102\n",
      "reward: 15.253120488296553 setps: 71 count: 173\n",
      "reward: 44.16768241251989 setps: 114 count: 287\n",
      "reward: 55.02702106266954 setps: 145 count: 432\n",
      "reward: 23.72456414706975 setps: 81 count: 513\n",
      "avg rewards: 34.254570403200916\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.29936\n",
      "Epoch:20 Batch:7 Loss:0.03689\n",
      "Epoch:40 Batch:7 Loss:0.01792\n",
      "Epoch:60 Batch:7 Loss:0.01357\n",
      "Epoch:80 Batch:7 Loss:0.01197\n",
      "Epoch:100 Batch:7 Loss:0.01110\n",
      "Epoch:120 Batch:7 Loss:0.01068\n",
      "Epoch:140 Batch:7 Loss:0.01020\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.072\n",
      "Epoch:10 Batch:10 Loss:0.071\n",
      "Epoch:20 Batch:10 Loss:0.072\n",
      "Epoch:30 Batch:10 Loss:0.070\n",
      "Epoch:40 Batch:10 Loss:0.070\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 57.870321400047395 setps: 90 count: 90\n",
      "reward: 276.91180783284824 setps: 800 count: 890\n",
      "reward: 26.425781847932384 setps: 88 count: 978\n",
      "avg rewards: 120.40263702694267\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.31211\n",
      "Epoch:20 Batch:8 Loss:0.03231\n",
      "Epoch:40 Batch:8 Loss:0.01760\n",
      "Epoch:60 Batch:8 Loss:0.01430\n",
      "Epoch:80 Batch:8 Loss:0.01019\n",
      "Epoch:100 Batch:8 Loss:0.00929\n",
      "Epoch:120 Batch:8 Loss:0.00849\n",
      "Epoch:140 Batch:8 Loss:0.00836\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.077\n",
      "Epoch:10 Batch:10 Loss:0.075\n",
      "Epoch:20 Batch:10 Loss:0.074\n",
      "Epoch:30 Batch:10 Loss:0.073\n",
      "Epoch:40 Batch:10 Loss:0.072\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 475.24301885422733 setps: 800 count: 800\n",
      "reward: 80.50458716970604 setps: 100 count: 900\n",
      "avg rewards: 277.8738030119667\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.24421\n",
      "Epoch:20 Batch:9 Loss:0.02841\n",
      "Epoch:40 Batch:9 Loss:0.01844\n",
      "Epoch:60 Batch:9 Loss:0.01134\n",
      "Epoch:80 Batch:9 Loss:0.00869\n",
      "Epoch:100 Batch:9 Loss:0.00770\n",
      "Epoch:120 Batch:9 Loss:0.00880\n",
      "Epoch:140 Batch:9 Loss:0.00789\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.075\n",
      "Epoch:10 Batch:10 Loss:0.075\n",
      "Epoch:20 Batch:10 Loss:0.074\n",
      "Epoch:30 Batch:10 Loss:0.074\n",
      "Epoch:40 Batch:10 Loss:0.073\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 50.40369163082506 setps: 67 count: 67\n",
      "reward: 33.169575077288044 setps: 85 count: 152\n",
      "reward: 475.0367795272407 setps: 800 count: 952\n",
      "avg rewards: 186.20334874511795\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.22615\n",
      "Epoch:20 Batch:10 Loss:0.02233\n",
      "Epoch:40 Batch:10 Loss:0.01385\n",
      "Epoch:60 Batch:10 Loss:0.01193\n",
      "Epoch:80 Batch:10 Loss:0.00836\n",
      "Epoch:100 Batch:10 Loss:0.00843\n",
      "Epoch:120 Batch:10 Loss:0.00687\n",
      "Epoch:140 Batch:10 Loss:0.00630\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.074\n",
      "Epoch:10 Batch:10 Loss:0.071\n",
      "Epoch:20 Batch:10 Loss:0.070\n",
      "Epoch:30 Batch:10 Loss:0.069\n",
      "Epoch:40 Batch:10 Loss:0.068\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 24.47686483326396 setps: 88 count: 88\n",
      "reward: 67.81398959603973 setps: 167 count: 255\n",
      "reward: 26.45782642444247 setps: 88 count: 343\n",
      "avg rewards: 39.58289361791539\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.19645\n",
      "Epoch:20 Batch:11 Loss:0.01780\n",
      "Epoch:40 Batch:11 Loss:0.01243\n",
      "Epoch:60 Batch:11 Loss:0.01047\n",
      "Epoch:80 Batch:11 Loss:0.00852\n",
      "Epoch:100 Batch:11 Loss:0.00682\n",
      "Epoch:120 Batch:11 Loss:0.00720\n",
      "Epoch:140 Batch:11 Loss:0.00578\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.072\n",
      "Epoch:10 Batch:10 Loss:0.071\n",
      "Epoch:20 Batch:10 Loss:0.070\n",
      "Epoch:30 Batch:10 Loss:0.068\n",
      "Epoch:40 Batch:10 Loss:0.070\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 398.50010351318804 setps: 800 count: 800\n",
      "reward: 48.48335432588355 setps: 57 count: 857\n",
      "reward: 100.39116254976254 setps: 136 count: 993\n",
      "avg rewards: 182.45820679627806\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.24612\n",
      "Epoch:20 Batch:12 Loss:0.01976\n",
      "Epoch:40 Batch:12 Loss:0.00993\n",
      "Epoch:60 Batch:12 Loss:0.00855\n",
      "Epoch:80 Batch:12 Loss:0.00822\n",
      "Epoch:100 Batch:12 Loss:0.00551\n",
      "Epoch:120 Batch:12 Loss:0.00611\n",
      "Epoch:140 Batch:12 Loss:0.00550\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.067\n",
      "Epoch:10 Batch:10 Loss:0.068\n",
      "Epoch:20 Batch:10 Loss:0.067\n",
      "Epoch:30 Batch:10 Loss:0.067\n",
      "Epoch:40 Batch:10 Loss:0.068\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 34.61998807129276 setps: 74 count: 74\n",
      "reward: 74.62653059357547 setps: 113 count: 187\n",
      "reward: 446.2375851078395 setps: 800 count: 987\n",
      "avg rewards: 185.16136792423592\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.19751\n",
      "Epoch:20 Batch:13 Loss:0.01764\n",
      "Epoch:40 Batch:13 Loss:0.01120\n",
      "Epoch:60 Batch:13 Loss:0.00727\n",
      "Epoch:80 Batch:13 Loss:0.00782\n",
      "Epoch:100 Batch:13 Loss:0.00706\n",
      "Epoch:120 Batch:13 Loss:0.00619\n",
      "Epoch:140 Batch:13 Loss:0.00540\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.070\n",
      "Epoch:10 Batch:10 Loss:0.068\n",
      "Epoch:20 Batch:10 Loss:0.068\n",
      "Epoch:30 Batch:10 Loss:0.068\n",
      "Epoch:40 Batch:10 Loss:0.068\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 83.51271423243892 setps: 82 count: 82\n",
      "reward: 434.40286942429395 setps: 800 count: 882\n",
      "reward: 46.91698684486475 setps: 76 count: 958\n",
      "avg rewards: 188.27752350053254\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.24218\n",
      "Epoch:20 Batch:14 Loss:0.01671\n",
      "Epoch:40 Batch:14 Loss:0.01110\n",
      "Epoch:60 Batch:14 Loss:0.00817\n",
      "Epoch:80 Batch:14 Loss:0.00682\n",
      "Epoch:100 Batch:14 Loss:0.00595\n",
      "Epoch:120 Batch:14 Loss:0.00610\n",
      "Epoch:140 Batch:14 Loss:0.00553\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.057\n",
      "Epoch:10 Batch:10 Loss:0.055\n",
      "Epoch:20 Batch:10 Loss:0.054\n",
      "Epoch:30 Batch:10 Loss:0.054\n",
      "Epoch:40 Batch:10 Loss:0.053\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 56.93888416341944 setps: 85 count: 85\n",
      "reward: 41.54981569468363 setps: 59 count: 144\n",
      "reward: 245.53505177157345 setps: 572 count: 716\n",
      "reward: 43.119636097276825 setps: 64 count: 780\n",
      "avg rewards: 96.78584693173835\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.11134\n",
      "Epoch:20 Batch:15 Loss:0.01699\n",
      "Epoch:40 Batch:15 Loss:0.01159\n",
      "Epoch:60 Batch:15 Loss:0.00811\n",
      "Epoch:80 Batch:15 Loss:0.00626\n",
      "Epoch:100 Batch:15 Loss:0.00731\n",
      "Epoch:120 Batch:15 Loss:0.00528\n",
      "Epoch:140 Batch:15 Loss:0.00505\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:10 Loss:0.061\n",
      "Epoch:10 Batch:10 Loss:0.060\n",
      "Epoch:20 Batch:10 Loss:0.056\n",
      "Epoch:30 Batch:10 Loss:0.058\n",
      "Epoch:40 Batch:10 Loss:0.058\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 47.92280343588499 setps: 95 count: 95\n",
      "reward: 10.646975373418545 setps: 49 count: 144\n",
      "reward: 59.68891981944036 setps: 53 count: 197\n",
      "reward: 86.39410942395477 setps: 74 count: 271\n",
      "reward: 60.8904034730469 setps: 57 count: 328\n",
      "reward: 19.299273791979076 setps: 68 count: 396\n",
      "reward: 57.7903813331315 setps: 112 count: 508\n",
      "reward: 80.099692250193 setps: 76 count: 584\n",
      "reward: 67.27295624441322 setps: 62 count: 646\n",
      "reward: 27.190281639090973 setps: 74 count: 720\n",
      "reward: 18.97681534530857 setps: 63 count: 783\n",
      "reward: 41.563527028315015 setps: 87 count: 870\n",
      "reward: 32.99109945599194 setps: 81 count: 951\n",
      "reward: 6.617671662860091 setps: 46 count: 997\n",
      "avg rewards: 44.09606501978777\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.09131\n",
      "Epoch:20 Batch:16 Loss:0.01719\n",
      "Epoch:40 Batch:16 Loss:0.01006\n",
      "Epoch:60 Batch:16 Loss:0.00808\n",
      "Epoch:80 Batch:16 Loss:0.00682\n",
      "Epoch:100 Batch:16 Loss:0.00536\n",
      "Epoch:120 Batch:16 Loss:0.00586\n",
      "Epoch:140 Batch:16 Loss:0.00468\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.054\n",
      "Epoch:10 Batch:10 Loss:0.052\n",
      "Epoch:20 Batch:10 Loss:0.052\n",
      "Epoch:30 Batch:10 Loss:0.051\n",
      "Epoch:40 Batch:10 Loss:0.050\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 59.73873539032645 setps: 51 count: 51\n",
      "reward: 85.13321253147005 setps: 65 count: 116\n",
      "reward: 58.65466601563238 setps: 46 count: 162\n",
      "reward: 34.26142053522634 setps: 43 count: 205\n",
      "reward: 30.470861971681003 setps: 30 count: 235\n",
      "reward: 62.140481368990756 setps: 50 count: 285\n",
      "reward: 65.09092673527483 setps: 66 count: 351\n",
      "reward: 32.31308696851775 setps: 33 count: 384\n",
      "reward: 47.780817118276886 setps: 40 count: 424\n",
      "reward: 57.554723396427285 setps: 51 count: 475\n",
      "reward: 30.769584720408606 setps: 31 count: 506\n",
      "reward: 72.89829502023206 setps: 67 count: 573\n",
      "reward: 37.43328338335996 setps: 46 count: 619\n",
      "reward: 28.21748609658244 setps: 40 count: 659\n",
      "reward: 41.857735492667416 setps: 47 count: 706\n",
      "reward: 33.73786909288465 setps: 41 count: 747\n",
      "reward: 46.62221819526894 setps: 56 count: 803\n",
      "reward: 53.63287352844493 setps: 50 count: 853\n",
      "reward: 47.349898999602004 setps: 48 count: 901\n",
      "reward: 47.233987769662065 setps: 66 count: 967\n",
      "avg rewards: 48.64460821654684\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.20577\n",
      "Epoch:20 Batch:17 Loss:0.01421\n",
      "Epoch:40 Batch:17 Loss:0.00941\n",
      "Epoch:60 Batch:17 Loss:0.00697\n",
      "Epoch:80 Batch:17 Loss:0.00675\n",
      "Epoch:100 Batch:17 Loss:0.00643\n",
      "Epoch:120 Batch:17 Loss:0.00607\n",
      "Epoch:140 Batch:17 Loss:0.00427\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.046\n",
      "Epoch:10 Batch:10 Loss:0.046\n",
      "Epoch:20 Batch:10 Loss:0.043\n",
      "Epoch:30 Batch:10 Loss:0.044\n",
      "Epoch:40 Batch:10 Loss:0.043\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 68.11671600207919 setps: 111 count: 111\n",
      "reward: 15.186877156380788 setps: 58 count: 169\n",
      "reward: 16.65566658761963 setps: 71 count: 240\n",
      "reward: 61.539111999698804 setps: 112 count: 352\n",
      "reward: 25.916738424450163 setps: 83 count: 435\n",
      "reward: 18.389676168700678 setps: 61 count: 496\n",
      "reward: 22.376089202312862 setps: 72 count: 568\n",
      "reward: 21.194063373534302 setps: 56 count: 624\n",
      "reward: 11.508148102081028 setps: 61 count: 685\n",
      "reward: 9.390909115885734 setps: 62 count: 747\n",
      "reward: 12.483546130228206 setps: 62 count: 809\n",
      "reward: 20.096085884563205 setps: 67 count: 876\n",
      "reward: 32.7017355025906 setps: 84 count: 960\n",
      "avg rewards: 25.81195105000963\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.15825\n",
      "Epoch:20 Batch:18 Loss:0.01624\n",
      "Epoch:40 Batch:18 Loss:0.00970\n",
      "Epoch:60 Batch:18 Loss:0.00737\n",
      "Epoch:80 Batch:18 Loss:0.00711\n",
      "Epoch:100 Batch:18 Loss:0.00546\n",
      "Epoch:120 Batch:18 Loss:0.00540\n",
      "Epoch:140 Batch:18 Loss:0.00517\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.037\n",
      "Epoch:10 Batch:10 Loss:0.038\n",
      "Epoch:20 Batch:10 Loss:0.037\n",
      "Epoch:30 Batch:10 Loss:0.037\n",
      "Epoch:40 Batch:10 Loss:0.037\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 34.85962560395127 setps: 74 count: 74\n",
      "reward: 64.20456201368508 setps: 98 count: 172\n",
      "reward: 34.84870236530551 setps: 75 count: 247\n",
      "reward: 40.93779371347774 setps: 86 count: 333\n",
      "reward: 103.53700739856873 setps: 120 count: 453\n",
      "reward: 24.586541334800142 setps: 58 count: 511\n",
      "reward: 42.56997563591721 setps: 76 count: 587\n",
      "reward: 15.114103270467606 setps: 52 count: 639\n",
      "reward: 69.30527963038041 setps: 112 count: 751\n",
      "reward: 106.00867413460074 setps: 122 count: 873\n",
      "reward: 59.7679497305813 setps: 103 count: 976\n",
      "avg rewards: 54.15820134833962\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.07855\n",
      "Epoch:20 Batch:19 Loss:0.01529\n",
      "Epoch:40 Batch:19 Loss:0.00954\n",
      "Epoch:60 Batch:19 Loss:0.00836\n",
      "Epoch:80 Batch:19 Loss:0.00643\n",
      "Epoch:100 Batch:19 Loss:0.00661\n",
      "Epoch:120 Batch:19 Loss:0.00596\n",
      "Epoch:140 Batch:19 Loss:0.00461\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.040\n",
      "Epoch:10 Batch:10 Loss:0.041\n",
      "Epoch:20 Batch:10 Loss:0.039\n",
      "Epoch:30 Batch:10 Loss:0.038\n",
      "Epoch:40 Batch:10 Loss:0.039\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 77.33672467988944 setps: 66 count: 66\n",
      "reward: 79.61067339936564 setps: 80 count: 146\n",
      "reward: 25.005518441610914 setps: 40 count: 186\n",
      "reward: 47.37615635668917 setps: 46 count: 232\n",
      "reward: 146.22248406531574 setps: 181 count: 413\n",
      "reward: 66.85773544121331 setps: 57 count: 470\n",
      "reward: 93.74085182681596 setps: 87 count: 557\n",
      "reward: 44.8213829020955 setps: 61 count: 618\n",
      "reward: 56.60297277916251 setps: 90 count: 708\n",
      "reward: 34.053446842684934 setps: 29 count: 737\n",
      "reward: 41.298215320374595 setps: 52 count: 789\n",
      "reward: 29.01809048604046 setps: 36 count: 825\n",
      "reward: 48.81707566113764 setps: 40 count: 865\n",
      "reward: 68.84476826505852 setps: 62 count: 927\n",
      "reward: 56.08805782818526 setps: 48 count: 975\n",
      "avg rewards: 61.046276953042636\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.08475\n",
      "Epoch:20 Batch:20 Loss:0.01370\n",
      "Epoch:40 Batch:20 Loss:0.00894\n",
      "Epoch:60 Batch:20 Loss:0.00679\n",
      "Epoch:80 Batch:20 Loss:0.00618\n",
      "Epoch:100 Batch:20 Loss:0.00637\n",
      "Epoch:120 Batch:20 Loss:0.00542\n",
      "Epoch:140 Batch:20 Loss:0.00450\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.030\n",
      "Epoch:10 Batch:10 Loss:0.029\n",
      "Epoch:20 Batch:10 Loss:0.029\n",
      "Epoch:30 Batch:10 Loss:0.030\n",
      "Epoch:40 Batch:10 Loss:0.029\n",
      "Done!\n",
      "############# start HopperBulletEnv-v0 training ###################\n",
      "(49,)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 23.882970571222536 setps: 16 count: 16\n",
      "reward: 25.77333283731713 setps: 20 count: 36\n",
      "reward: 19.492831864685282 setps: 7 count: 43\n",
      "reward: 18.192747276357842 setps: 13 count: 56\n",
      "reward: 21.15314723471965 setps: 13 count: 69\n",
      "reward: 19.518454981704416 setps: 7 count: 76\n",
      "reward: 19.379697091480192 setps: 12 count: 88\n",
      "reward: 22.289910380149376 setps: 15 count: 103\n",
      "reward: 17.559274790919154 setps: 12 count: 115\n",
      "reward: 15.310067922304734 setps: 6 count: 121\n",
      "reward: 22.853220536868317 setps: 13 count: 134\n",
      "reward: 16.960894888463375 setps: 6 count: 140\n",
      "reward: 21.985216878214853 setps: 10 count: 150\n",
      "reward: 17.521467491572547 setps: 14 count: 164\n",
      "reward: 20.93758308546676 setps: 14 count: 178\n",
      "reward: 19.983398391485387 setps: 12 count: 190\n",
      "reward: 24.77373765563534 setps: 12 count: 202\n",
      "reward: 17.361751148557232 setps: 6 count: 208\n",
      "reward: 18.251198862388264 setps: 7 count: 215\n",
      "reward: 17.153876800343276 setps: 11 count: 226\n",
      "reward: 20.050667008553866 setps: 17 count: 243\n",
      "reward: 16.894707907368137 setps: 28 count: 271\n",
      "reward: 23.927094945563294 setps: 20 count: 291\n",
      "reward: 22.17488849833171 setps: 19 count: 310\n",
      "reward: 23.476012997746878 setps: 16 count: 326\n",
      "reward: 20.016304672979462 setps: 6 count: 332\n",
      "reward: 21.02450613266701 setps: 8 count: 340\n",
      "reward: 23.183337195735657 setps: 17 count: 357\n",
      "reward: 15.96957965316542 setps: 10 count: 367\n",
      "reward: 26.05255952417938 setps: 21 count: 388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 25.67388937028591 setps: 17 count: 405\n",
      "reward: 22.823568344558588 setps: 18 count: 423\n",
      "reward: 20.473823551423262 setps: 16 count: 439\n",
      "reward: 21.21325226861955 setps: 15 count: 454\n",
      "reward: 25.912644732126502 setps: 17 count: 471\n",
      "reward: 20.70656607906276 setps: 9 count: 480\n",
      "reward: 20.141161935057603 setps: 10 count: 490\n",
      "reward: 18.182721619021322 setps: 12 count: 502\n",
      "reward: 24.605956768852774 setps: 22 count: 524\n",
      "reward: 19.106595502038545 setps: 8 count: 532\n",
      "reward: 18.74057168627187 setps: 10 count: 542\n",
      "reward: 19.184327946242412 setps: 9 count: 551\n",
      "reward: 17.925327985519836 setps: 8 count: 559\n",
      "reward: 23.510943467456674 setps: 16 count: 575\n",
      "reward: 19.868844129686476 setps: 18 count: 593\n",
      "reward: 21.382361138664418 setps: 9 count: 602\n",
      "reward: 18.283211395413673 setps: 10 count: 612\n",
      "reward: 22.052670708434015 setps: 13 count: 625\n",
      "reward: 20.38328785512131 setps: 8 count: 633\n",
      "reward: 17.687608318503774 setps: 13 count: 646\n",
      "reward: 18.69347983791522 setps: 9 count: 655\n",
      "reward: 16.30461863457167 setps: 20 count: 675\n",
      "reward: 18.11971466857067 setps: 8 count: 683\n",
      "reward: 23.164248626033075 setps: 12 count: 695\n",
      "reward: 16.5037636300287 setps: 9 count: 704\n",
      "reward: 28.06955174484901 setps: 18 count: 722\n",
      "reward: 22.20975439484464 setps: 10 count: 732\n",
      "reward: 18.364814836285948 setps: 13 count: 745\n",
      "reward: 20.037957856152204 setps: 9 count: 754\n",
      "reward: 15.416177902868366 setps: 10 count: 764\n",
      "reward: 17.56774065005011 setps: 8 count: 772\n",
      "reward: 28.234505356529553 setps: 19 count: 791\n",
      "reward: 19.503324217768387 setps: 12 count: 803\n",
      "reward: 21.13053579438565 setps: 7 count: 810\n",
      "reward: 22.01644679704623 setps: 13 count: 823\n",
      "reward: 16.852793517665123 setps: 6 count: 829\n",
      "reward: 22.979768488180706 setps: 9 count: 838\n",
      "reward: 23.113229132644484 setps: 16 count: 854\n",
      "reward: 26.738378327057585 setps: 31 count: 885\n",
      "reward: 20.912390428026264 setps: 15 count: 900\n",
      "reward: 22.58735235220374 setps: 15 count: 915\n",
      "reward: 22.480922398895196 setps: 18 count: 933\n",
      "reward: 19.13930281615176 setps: 11 count: 944\n",
      "reward: 20.446870451090216 setps: 8 count: 952\n",
      "reward: 19.797664687117503 setps: 9 count: 961\n",
      "reward: 21.05995721584768 setps: 12 count: 973\n",
      "reward: 20.26123328831018 setps: 12 count: 985\n",
      "reward: 19.535015288081198 setps: 10 count: 995\n",
      "avg rewards: 20.669298555995884\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:0.39177\n",
      "Epoch:20 Batch:1 Loss:0.16976\n",
      "Epoch:40 Batch:1 Loss:0.14810\n",
      "Epoch:60 Batch:1 Loss:0.11835\n",
      "Epoch:80 Batch:1 Loss:0.08325\n",
      "Epoch:100 Batch:1 Loss:0.06498\n",
      "Epoch:120 Batch:1 Loss:0.05747\n",
      "Epoch:140 Batch:1 Loss:0.05279\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.138\n",
      "Epoch:10 Batch:10 Loss:0.137\n",
      "Epoch:20 Batch:10 Loss:0.137\n",
      "Epoch:30 Batch:10 Loss:0.138\n",
      "Epoch:40 Batch:10 Loss:0.131\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 12.015480186218337 setps: 36 count: 36\n",
      "reward: 19.143279207983856 setps: 51 count: 87\n",
      "reward: 14.585657048913708 setps: 40 count: 127\n",
      "reward: 16.19618047765398 setps: 42 count: 169\n",
      "reward: 16.791777582814397 setps: 46 count: 215\n",
      "reward: 11.103819073129854 setps: 42 count: 257\n",
      "reward: 17.80876783209824 setps: 46 count: 303\n",
      "reward: 18.446080844018436 setps: 48 count: 351\n",
      "reward: 13.455333546966719 setps: 37 count: 388\n",
      "reward: 14.14963001028518 setps: 42 count: 430\n",
      "reward: 12.453755305412052 setps: 41 count: 471\n",
      "reward: 17.42111934561253 setps: 47 count: 518\n",
      "reward: 20.29455438202131 setps: 51 count: 569\n",
      "reward: 14.195876629573467 setps: 39 count: 608\n",
      "reward: 17.65654120359395 setps: 46 count: 654\n",
      "reward: 16.367047716566603 setps: 43 count: 697\n",
      "reward: 17.11800502156985 setps: 37 count: 734\n",
      "reward: 17.621421628422095 setps: 45 count: 779\n",
      "reward: 17.69179334152287 setps: 46 count: 825\n",
      "reward: 15.038114216395478 setps: 42 count: 867\n",
      "reward: 13.65430534457846 setps: 41 count: 908\n",
      "reward: 21.353956804712652 setps: 40 count: 948\n",
      "reward: 18.399770311248723 setps: 49 count: 997\n",
      "avg rewards: 16.215750741796207\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:0.35793\n",
      "Epoch:20 Batch:2 Loss:0.11283\n",
      "Epoch:40 Batch:2 Loss:0.09262\n",
      "Epoch:60 Batch:2 Loss:0.06169\n",
      "Epoch:80 Batch:2 Loss:0.04551\n",
      "Epoch:100 Batch:2 Loss:0.03489\n",
      "Epoch:120 Batch:2 Loss:0.03067\n",
      "Epoch:140 Batch:2 Loss:0.02975\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.085\n",
      "Epoch:10 Batch:10 Loss:0.079\n",
      "Epoch:20 Batch:10 Loss:0.078\n",
      "Epoch:30 Batch:10 Loss:0.075\n",
      "Epoch:40 Batch:10 Loss:0.078\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 10.214918169928206 setps: 30 count: 30\n",
      "reward: 15.08307053437893 setps: 34 count: 64\n",
      "reward: 9.925652697590705 setps: 32 count: 96\n",
      "reward: 1.1511279321202876 setps: 29 count: 125\n",
      "reward: 11.788338057264628 setps: 29 count: 154\n",
      "reward: 10.409618664147455 setps: 31 count: 185\n",
      "reward: 8.282777519812226 setps: 31 count: 216\n",
      "reward: 7.064664606066071 setps: 30 count: 246\n",
      "reward: 7.877817607701579 setps: 31 count: 277\n",
      "reward: 7.658992238965583 setps: 31 count: 308\n",
      "reward: 8.53292418294222 setps: 30 count: 338\n",
      "reward: 9.76590784764703 setps: 31 count: 369\n",
      "reward: 5.677691570878958 setps: 31 count: 400\n",
      "reward: 8.079337546884199 setps: 30 count: 430\n",
      "reward: 3.2484676866370124 setps: 30 count: 460\n",
      "reward: 8.156129275307466 setps: 30 count: 490\n",
      "reward: 8.000611713802206 setps: 30 count: 520\n",
      "reward: 8.651820093330024 setps: 30 count: 550\n",
      "reward: 11.986816499719678 setps: 31 count: 581\n",
      "reward: 9.832448983372156 setps: 33 count: 614\n",
      "reward: 8.641359075421132 setps: 31 count: 645\n",
      "reward: 9.45981705905433 setps: 30 count: 675\n",
      "reward: 7.400556216045514 setps: 31 count: 706\n",
      "reward: 11.489482744719133 setps: 31 count: 737\n",
      "reward: 3.3729164224088892 setps: 30 count: 767\n",
      "reward: 7.130944851333334 setps: 32 count: 799\n",
      "reward: 14.569344287544665 setps: 31 count: 830\n",
      "reward: 9.313850186498891 setps: 31 count: 861\n",
      "reward: 12.848804653073602 setps: 31 count: 892\n",
      "reward: 10.353505209448484 setps: 30 count: 922\n",
      "reward: 9.647919475522214 setps: 31 count: 953\n",
      "reward: 10.59057397675424 setps: 30 count: 983\n",
      "avg rewards: 8.944006487072532\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.34617\n",
      "Epoch:20 Batch:3 Loss:0.09318\n",
      "Epoch:40 Batch:3 Loss:0.05026\n",
      "Epoch:60 Batch:3 Loss:0.03591\n",
      "Epoch:80 Batch:3 Loss:0.02842\n",
      "Epoch:100 Batch:3 Loss:0.02750\n",
      "Epoch:120 Batch:3 Loss:0.02380\n",
      "Epoch:140 Batch:3 Loss:0.02236\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.081\n",
      "Epoch:10 Batch:10 Loss:0.073\n",
      "Epoch:20 Batch:10 Loss:0.072\n",
      "Epoch:30 Batch:10 Loss:0.071\n",
      "Epoch:40 Batch:10 Loss:0.070\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 35.54144363490923 setps: 38 count: 38\n",
      "reward: 35.09034613156983 setps: 38 count: 76\n",
      "reward: -6.482067974845993 setps: 35 count: 111\n",
      "reward: 26.002906586077003 setps: 35 count: 146\n",
      "reward: 28.62710570592899 setps: 37 count: 183\n",
      "reward: 38.34058248024668 setps: 47 count: 230\n",
      "reward: 33.87707628962671 setps: 38 count: 268\n",
      "reward: 36.81774356835813 setps: 40 count: 308\n",
      "reward: 28.997723208655948 setps: 37 count: 345\n",
      "reward: 39.2661843256501 setps: 41 count: 386\n",
      "reward: 41.02526005732799 setps: 46 count: 432\n",
      "reward: 28.88976582124742 setps: 35 count: 467\n",
      "reward: 36.160856841191716 setps: 41 count: 508\n",
      "reward: 39.516506500059045 setps: 43 count: 551\n",
      "reward: 49.571501031925436 setps: 57 count: 608\n",
      "reward: 38.11636566912639 setps: 42 count: 650\n",
      "reward: 41.25182826790114 setps: 46 count: 696\n",
      "reward: 30.876555753426505 setps: 36 count: 732\n",
      "reward: 31.235077674212516 setps: 37 count: 769\n",
      "reward: 33.37667397490586 setps: 37 count: 806\n",
      "reward: 38.205357870939764 setps: 48 count: 854\n",
      "reward: 38.12744916773918 setps: 40 count: 894\n",
      "reward: 30.350387607039014 setps: 39 count: 933\n",
      "reward: 33.00094118781709 setps: 43 count: 976\n",
      "avg rewards: 33.57431547420982\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.32019\n",
      "Epoch:20 Batch:4 Loss:0.07705\n",
      "Epoch:40 Batch:4 Loss:0.04034\n",
      "Epoch:60 Batch:4 Loss:0.02857\n",
      "Epoch:80 Batch:4 Loss:0.02636\n",
      "Epoch:100 Batch:4 Loss:0.02323\n",
      "Epoch:120 Batch:4 Loss:0.02212\n",
      "Epoch:140 Batch:4 Loss:0.02023\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:10 Loss:0.073\n",
      "Epoch:10 Batch:10 Loss:0.068\n",
      "Epoch:20 Batch:10 Loss:0.067\n",
      "Epoch:30 Batch:10 Loss:0.072\n",
      "Epoch:40 Batch:10 Loss:0.064\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 42.21335199613823 setps: 28 count: 28\n",
      "reward: 38.5774317968986 setps: 25 count: 53\n",
      "reward: 42.58756663138774 setps: 33 count: 86\n",
      "reward: 41.843418472063775 setps: 28 count: 114\n",
      "reward: 44.273610183909476 setps: 29 count: 143\n",
      "reward: 45.5182965417378 setps: 31 count: 174\n",
      "reward: 46.06607668018988 setps: 32 count: 206\n",
      "reward: 40.525972597477086 setps: 28 count: 234\n",
      "reward: 38.37367503727728 setps: 25 count: 259\n",
      "reward: 39.77331249524723 setps: 26 count: 285\n",
      "reward: 42.6655211506557 setps: 29 count: 314\n",
      "reward: 40.64860400871256 setps: 28 count: 342\n",
      "reward: 40.6486895064314 setps: 27 count: 369\n",
      "reward: 47.04958774221887 setps: 32 count: 401\n",
      "reward: 43.550158059908426 setps: 28 count: 429\n",
      "reward: 43.162343129509836 setps: 30 count: 459\n",
      "reward: 39.845987987593986 setps: 27 count: 486\n",
      "reward: 37.909036141964314 setps: 27 count: 513\n",
      "reward: 44.809378085575005 setps: 30 count: 543\n",
      "reward: 47.47965221269841 setps: 33 count: 576\n",
      "reward: 42.0312655047921 setps: 28 count: 604\n",
      "reward: 39.7783087307398 setps: 26 count: 630\n",
      "reward: 40.11455417608086 setps: 30 count: 660\n",
      "reward: 36.44077858060628 setps: 26 count: 686\n",
      "reward: 38.33009089497064 setps: 25 count: 711\n",
      "reward: 38.60414045782819 setps: 27 count: 738\n",
      "reward: 44.60625546187803 setps: 30 count: 768\n",
      "reward: 44.10445937889453 setps: 31 count: 799\n",
      "reward: 41.91130970278317 setps: 37 count: 836\n",
      "reward: 43.299239275070434 setps: 29 count: 865\n",
      "reward: 41.48076077154838 setps: 28 count: 893\n",
      "reward: 47.50195416360949 setps: 33 count: 926\n",
      "reward: 43.08246335465082 setps: 33 count: 959\n",
      "reward: 41.29248795152671 setps: 27 count: 986\n",
      "avg rewards: 42.06175702536986\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.30683\n",
      "Epoch:20 Batch:5 Loss:0.05644\n",
      "Epoch:40 Batch:5 Loss:0.02866\n",
      "Epoch:60 Batch:5 Loss:0.02428\n",
      "Epoch:80 Batch:5 Loss:0.02127\n",
      "Epoch:100 Batch:5 Loss:0.01921\n",
      "Epoch:120 Batch:5 Loss:0.01682\n",
      "Epoch:140 Batch:5 Loss:0.01690\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.062\n",
      "Epoch:10 Batch:10 Loss:0.057\n",
      "Epoch:20 Batch:10 Loss:0.067\n",
      "Epoch:30 Batch:10 Loss:0.061\n",
      "Epoch:40 Batch:10 Loss:0.062\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 41.86626904636069 setps: 46 count: 46\n",
      "reward: 29.539363082326595 setps: 37 count: 83\n",
      "reward: 30.91263479012414 setps: 38 count: 121\n",
      "reward: 23.823225513710344 setps: 34 count: 155\n",
      "reward: 37.07909508323938 setps: 42 count: 197\n",
      "reward: 27.24542648332572 setps: 36 count: 233\n",
      "reward: 31.30070478801353 setps: 38 count: 271\n",
      "reward: 21.15927670164092 setps: 34 count: 305\n",
      "reward: 26.168557690132005 setps: 40 count: 345\n",
      "reward: 21.08696324698194 setps: 34 count: 379\n",
      "reward: 27.010140341645453 setps: 34 count: 413\n",
      "reward: 39.43348504657115 setps: 44 count: 457\n",
      "reward: 23.113427567506733 setps: 36 count: 493\n",
      "reward: 25.85768993577076 setps: 36 count: 529\n",
      "reward: 25.54015742023184 setps: 34 count: 563\n",
      "reward: 15.421038268714073 setps: 33 count: 596\n",
      "reward: 29.842924557790685 setps: 38 count: 634\n",
      "reward: 36.185311983248546 setps: 41 count: 675\n",
      "reward: 29.167363457845934 setps: 35 count: 710\n",
      "reward: 28.418572321640383 setps: 36 count: 746\n",
      "reward: 22.303590944778996 setps: 33 count: 779\n",
      "reward: 42.63666945883015 setps: 46 count: 825\n",
      "reward: 26.27844708190678 setps: 37 count: 862\n",
      "reward: 22.11644170686268 setps: 35 count: 897\n",
      "reward: 39.397605871781714 setps: 44 count: 941\n",
      "reward: 31.383018959099722 setps: 40 count: 981\n",
      "avg rewards: 29.011053898080032\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.27663\n",
      "Epoch:20 Batch:6 Loss:0.05227\n",
      "Epoch:40 Batch:6 Loss:0.02704\n",
      "Epoch:60 Batch:6 Loss:0.02124\n",
      "Epoch:80 Batch:6 Loss:0.01935\n",
      "Epoch:100 Batch:6 Loss:0.01741\n",
      "Epoch:120 Batch:6 Loss:0.01809\n",
      "Epoch:140 Batch:6 Loss:0.01467\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.057\n",
      "Epoch:10 Batch:10 Loss:0.057\n",
      "Epoch:20 Batch:10 Loss:0.056\n",
      "Epoch:30 Batch:10 Loss:0.055\n",
      "Epoch:40 Batch:10 Loss:0.052\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 36.32416769580305 setps: 40 count: 40\n",
      "reward: 36.17323144444381 setps: 39 count: 79\n",
      "reward: 37.24264277093608 setps: 39 count: 118\n",
      "reward: 31.366732068550483 setps: 36 count: 154\n",
      "reward: 49.23151675212719 setps: 51 count: 205\n",
      "reward: 53.887456466474404 setps: 54 count: 259\n",
      "reward: 48.22663573057944 setps: 49 count: 308\n",
      "reward: 36.82289231937757 setps: 40 count: 348\n",
      "reward: 45.85629462825163 setps: 47 count: 395\n",
      "reward: 50.58401339133852 setps: 51 count: 446\n",
      "reward: 42.86607351714483 setps: 44 count: 490\n",
      "reward: 45.01564513642661 setps: 45 count: 535\n",
      "reward: 45.828264966663845 setps: 46 count: 581\n",
      "reward: 38.15303346466826 setps: 41 count: 622\n",
      "reward: 41.31128246767039 setps: 42 count: 664\n",
      "reward: 51.29846512139193 setps: 51 count: 715\n",
      "reward: 37.07820516587526 setps: 40 count: 755\n",
      "reward: 49.829067081562236 setps: 52 count: 807\n",
      "reward: 29.199469496244276 setps: 38 count: 845\n",
      "reward: 30.23712614390243 setps: 38 count: 883\n",
      "reward: 36.047334240643245 setps: 39 count: 922\n",
      "reward: 40.575490980899474 setps: 42 count: 964\n",
      "avg rewards: 41.50704732049887\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.27919\n",
      "Epoch:20 Batch:7 Loss:0.04229\n",
      "Epoch:40 Batch:7 Loss:0.02284\n",
      "Epoch:60 Batch:7 Loss:0.01875\n",
      "Epoch:80 Batch:7 Loss:0.01671\n",
      "Epoch:100 Batch:7 Loss:0.01794\n",
      "Epoch:120 Batch:7 Loss:0.01643\n",
      "Epoch:140 Batch:7 Loss:0.01547\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.054\n",
      "Epoch:10 Batch:10 Loss:0.053\n",
      "Epoch:20 Batch:10 Loss:0.057\n",
      "Epoch:30 Batch:10 Loss:0.060\n",
      "Epoch:40 Batch:10 Loss:0.055\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 48.41950534256757 setps: 47 count: 47\n",
      "reward: 49.009954984791705 setps: 47 count: 94\n",
      "reward: 46.153713032657095 setps: 46 count: 140\n",
      "reward: 46.978858154403866 setps: 48 count: 188\n",
      "reward: 41.55738847381145 setps: 42 count: 230\n",
      "reward: 48.18563048427458 setps: 47 count: 277\n",
      "reward: 47.69375777942623 setps: 47 count: 324\n",
      "reward: 51.08366363100357 setps: 49 count: 373\n",
      "reward: 41.263014488475164 setps: 40 count: 413\n",
      "reward: 52.57098772326136 setps: 51 count: 464\n",
      "reward: 38.374972804116254 setps: 39 count: 503\n",
      "reward: 45.333001386122476 setps: 45 count: 548\n",
      "reward: 36.8194175282828 setps: 39 count: 587\n",
      "reward: 51.71820464539778 setps: 51 count: 638\n",
      "reward: 45.58986417346023 setps: 45 count: 683\n",
      "reward: 45.74409865176714 setps: 47 count: 730\n",
      "reward: 41.26466298968734 setps: 41 count: 771\n",
      "reward: 45.37938129400136 setps: 44 count: 815\n",
      "reward: 40.801744423818306 setps: 39 count: 854\n",
      "reward: 44.20453301480157 setps: 44 count: 898\n",
      "reward: 37.57591215797437 setps: 39 count: 937\n",
      "reward: 38.757361189027144 setps: 40 count: 977\n",
      "avg rewards: 44.74907401605134\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.25692\n",
      "Epoch:20 Batch:8 Loss:0.03236\n",
      "Epoch:40 Batch:8 Loss:0.02073\n",
      "Epoch:60 Batch:8 Loss:0.01518\n",
      "Epoch:80 Batch:8 Loss:0.01394\n",
      "Epoch:100 Batch:8 Loss:0.01525\n",
      "Epoch:120 Batch:8 Loss:0.01417\n",
      "Epoch:140 Batch:8 Loss:0.01273\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.053\n",
      "Epoch:10 Batch:10 Loss:0.056\n",
      "Epoch:20 Batch:10 Loss:0.048\n",
      "Epoch:30 Batch:10 Loss:0.053\n",
      "Epoch:40 Batch:10 Loss:0.048\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 49.36655518711924 setps: 51 count: 51\n",
      "reward: 35.84013862699212 setps: 39 count: 90\n",
      "reward: 36.80067615545703 setps: 39 count: 129\n",
      "reward: 46.02034556007711 setps: 45 count: 174\n",
      "reward: 44.29916262323531 setps: 43 count: 217\n",
      "reward: 42.94425350096135 setps: 42 count: 259\n",
      "reward: 45.47771886660048 setps: 46 count: 305\n",
      "reward: 50.209653709888514 setps: 51 count: 356\n",
      "reward: 48.32471420073127 setps: 47 count: 403\n",
      "reward: 29.405950734489302 setps: 35 count: 438\n",
      "reward: 47.46068837079365 setps: 47 count: 485\n",
      "reward: 47.39213554449524 setps: 48 count: 533\n",
      "reward: 43.05402172126924 setps: 44 count: 577\n",
      "reward: 42.26218164043968 setps: 43 count: 620\n",
      "reward: 49.11576532644539 setps: 48 count: 668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 52.38091731944297 setps: 58 count: 726\n",
      "reward: 33.40380229494913 setps: 37 count: 763\n",
      "reward: 32.04424197259359 setps: 37 count: 800\n",
      "reward: 46.6838640757429 setps: 47 count: 847\n",
      "reward: 32.31717248738133 setps: 37 count: 884\n",
      "reward: 30.61747661055706 setps: 35 count: 919\n",
      "reward: 31.17871423278557 setps: 36 count: 955\n",
      "reward: 40.029093542663034 setps: 42 count: 997\n",
      "avg rewards: 41.592575839352634\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.22716\n",
      "Epoch:20 Batch:9 Loss:0.02889\n",
      "Epoch:40 Batch:9 Loss:0.01772\n",
      "Epoch:60 Batch:9 Loss:0.01793\n",
      "Epoch:80 Batch:9 Loss:0.01317\n",
      "Epoch:100 Batch:9 Loss:0.01388\n",
      "Epoch:120 Batch:9 Loss:0.01329\n",
      "Epoch:140 Batch:9 Loss:0.01171\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.051\n",
      "Epoch:10 Batch:10 Loss:0.050\n",
      "Epoch:20 Batch:10 Loss:0.055\n",
      "Epoch:30 Batch:10 Loss:0.049\n",
      "Epoch:40 Batch:10 Loss:0.049\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 68.51379189987055 setps: 57 count: 57\n",
      "reward: 68.34373304876792 setps: 57 count: 114\n",
      "reward: 67.34909046613028 setps: 54 count: 168\n",
      "reward: 72.46622417359322 setps: 59 count: 227\n",
      "reward: 77.52110659657482 setps: 67 count: 294\n",
      "reward: 56.905447257441125 setps: 97 count: 391\n",
      "reward: 40.91051423562312 setps: 38 count: 429\n",
      "reward: 75.96556037694825 setps: 64 count: 493\n",
      "reward: 123.199461957645 setps: 88 count: 581\n",
      "reward: 48.67752259025874 setps: 33 count: 614\n",
      "reward: 77.11634697009576 setps: 63 count: 677\n",
      "reward: 78.19096740780662 setps: 65 count: 742\n",
      "reward: 75.0826526772857 setps: 61 count: 803\n",
      "reward: 64.156425283484 setps: 49 count: 852\n",
      "reward: 61.677298230479934 setps: 52 count: 904\n",
      "reward: 74.25452185586764 setps: 63 count: 967\n",
      "avg rewards: 70.64566656424205\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.21004\n",
      "Epoch:20 Batch:10 Loss:0.02641\n",
      "Epoch:40 Batch:10 Loss:0.01750\n",
      "Epoch:60 Batch:10 Loss:0.01445\n",
      "Epoch:80 Batch:10 Loss:0.01168\n",
      "Epoch:100 Batch:10 Loss:0.01139\n",
      "Epoch:120 Batch:10 Loss:0.01153\n",
      "Epoch:140 Batch:10 Loss:0.01123\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.040\n",
      "Epoch:10 Batch:10 Loss:0.039\n",
      "Epoch:20 Batch:10 Loss:0.043\n",
      "Epoch:30 Batch:10 Loss:0.041\n",
      "Epoch:40 Batch:10 Loss:0.039\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 37.17068470914383 setps: 25 count: 25\n",
      "reward: 37.616161566812686 setps: 66 count: 91\n",
      "reward: 33.08582305298477 setps: 24 count: 115\n",
      "reward: 36.33578049528151 setps: 26 count: 141\n",
      "reward: 56.303480160319296 setps: 49 count: 190\n",
      "reward: 60.12092070985819 setps: 44 count: 234\n",
      "reward: 37.56997353826301 setps: 24 count: 258\n",
      "reward: 34.219215603734476 setps: 23 count: 281\n",
      "reward: 44.28264616324305 setps: 33 count: 314\n",
      "reward: 38.73574110918125 setps: 26 count: 340\n",
      "reward: 36.09005701319257 setps: 25 count: 365\n",
      "reward: 41.92485423826147 setps: 27 count: 392\n",
      "reward: 43.77670453295142 setps: 31 count: 423\n",
      "reward: 33.061925599236574 setps: 23 count: 446\n",
      "reward: 31.358104956978053 setps: 22 count: 468\n",
      "reward: 43.17155235975953 setps: 28 count: 496\n",
      "reward: 40.64500009381591 setps: 26 count: 522\n",
      "reward: 34.756436983194725 setps: 23 count: 545\n",
      "reward: 30.96502207474405 setps: 23 count: 568\n",
      "reward: 37.46375716064795 setps: 26 count: 594\n",
      "reward: 36.73937478318693 setps: 24 count: 618\n",
      "reward: 42.2481128314932 setps: 27 count: 645\n",
      "reward: 37.126038548268845 setps: 25 count: 670\n",
      "reward: 36.741032724404064 setps: 25 count: 695\n",
      "reward: 36.263758068633614 setps: 25 count: 720\n",
      "reward: 35.19694858112779 setps: 23 count: 743\n",
      "reward: 36.311109754763315 setps: 25 count: 768\n",
      "reward: 34.537909313302954 setps: 23 count: 791\n",
      "reward: 41.34189148091973 setps: 27 count: 818\n",
      "reward: 35.19631921815308 setps: 24 count: 842\n",
      "reward: 38.60920232449425 setps: 25 count: 867\n",
      "reward: 48.14951335332442 setps: 82 count: 949\n",
      "reward: 49.53464935646624 setps: 33 count: 982\n",
      "avg rewards: 39.292415226064925\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.23618\n",
      "Epoch:20 Batch:11 Loss:0.02481\n",
      "Epoch:40 Batch:11 Loss:0.01521\n",
      "Epoch:60 Batch:11 Loss:0.01327\n",
      "Epoch:80 Batch:11 Loss:0.01234\n",
      "Epoch:100 Batch:11 Loss:0.01175\n",
      "Epoch:120 Batch:11 Loss:0.01144\n",
      "Epoch:140 Batch:11 Loss:0.00993\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.044\n",
      "Epoch:10 Batch:10 Loss:0.048\n",
      "Epoch:20 Batch:10 Loss:0.042\n",
      "Epoch:30 Batch:10 Loss:0.038\n",
      "Epoch:40 Batch:10 Loss:0.045\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 61.02726423209971 setps: 53 count: 53\n",
      "reward: 56.29478351366706 setps: 49 count: 102\n",
      "reward: 59.26879517825582 setps: 95 count: 197\n",
      "reward: 68.59973452700507 setps: 63 count: 260\n",
      "reward: 60.52778449246689 setps: 52 count: 312\n",
      "reward: 88.26396068669862 setps: 132 count: 444\n",
      "reward: 69.81613361176859 setps: 52 count: 496\n",
      "reward: 62.057818474930535 setps: 56 count: 552\n",
      "reward: 53.63690457167105 setps: 47 count: 599\n",
      "reward: 67.94024719477456 setps: 54 count: 653\n",
      "reward: 65.66103781803395 setps: 49 count: 702\n",
      "reward: 58.61032695153263 setps: 53 count: 755\n",
      "reward: 112.94342315060197 setps: 92 count: 847\n",
      "reward: 117.68480882742003 setps: 124 count: 971\n",
      "avg rewards: 71.59521594506619\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.19003\n",
      "Epoch:20 Batch:12 Loss:0.02104\n",
      "Epoch:40 Batch:12 Loss:0.01561\n",
      "Epoch:60 Batch:12 Loss:0.01449\n",
      "Epoch:80 Batch:12 Loss:0.01156\n",
      "Epoch:100 Batch:12 Loss:0.01102\n",
      "Epoch:120 Batch:12 Loss:0.01024\n",
      "Epoch:140 Batch:12 Loss:0.01166\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.036\n",
      "Epoch:10 Batch:10 Loss:0.037\n",
      "Epoch:20 Batch:10 Loss:0.034\n",
      "Epoch:30 Batch:10 Loss:0.032\n",
      "Epoch:40 Batch:10 Loss:0.029\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 38.37260835295748 setps: 70 count: 70\n",
      "reward: 58.005826377660554 setps: 52 count: 122\n",
      "reward: 42.746884763646804 setps: 74 count: 196\n",
      "reward: 65.52664367350809 setps: 63 count: 259\n",
      "reward: 46.079958556408386 setps: 79 count: 338\n",
      "reward: 117.22890339883308 setps: 129 count: 467\n",
      "reward: 51.176123296532026 setps: 52 count: 519\n",
      "reward: 52.940420534051384 setps: 48 count: 567\n",
      "reward: 30.640340799913993 setps: 58 count: 625\n",
      "reward: 63.68723925238156 setps: 61 count: 686\n",
      "reward: 74.29327981559473 setps: 73 count: 759\n",
      "reward: 30.1227098954696 setps: 58 count: 817\n",
      "reward: 35.527450889047756 setps: 65 count: 882\n",
      "reward: 59.76621319392434 setps: 57 count: 939\n",
      "avg rewards: 54.7224716285664\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.18816\n",
      "Epoch:20 Batch:13 Loss:0.02170\n",
      "Epoch:40 Batch:13 Loss:0.01537\n",
      "Epoch:60 Batch:13 Loss:0.01101\n",
      "Epoch:80 Batch:13 Loss:0.01144\n",
      "Epoch:100 Batch:13 Loss:0.01100\n",
      "Epoch:120 Batch:13 Loss:0.00954\n",
      "Epoch:140 Batch:13 Loss:0.01013\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.030\n",
      "Epoch:10 Batch:10 Loss:0.024\n",
      "Epoch:20 Batch:10 Loss:0.028\n",
      "Epoch:30 Batch:10 Loss:0.026\n",
      "Epoch:40 Batch:10 Loss:0.028\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 63.147535554446215 setps: 58 count: 58\n",
      "reward: 67.16461328347216 setps: 64 count: 122\n",
      "reward: 46.20547035192575 setps: 78 count: 200\n",
      "reward: 44.9850086327133 setps: 73 count: 273\n",
      "reward: 65.01034200037829 setps: 57 count: 330\n",
      "reward: 84.87532154860963 setps: 73 count: 403\n",
      "reward: 50.91358396682189 setps: 78 count: 481\n",
      "reward: 102.81593646544931 setps: 106 count: 587\n",
      "reward: 57.07134043450786 setps: 54 count: 641\n",
      "reward: 59.311308768080195 setps: 88 count: 729\n",
      "reward: 45.20405907150853 setps: 33 count: 762\n",
      "reward: 97.74672508740473 setps: 89 count: 851\n",
      "reward: 76.86728555894295 setps: 64 count: 915\n",
      "reward: 50.44804032743702 setps: 77 count: 992\n",
      "avg rewards: 65.12618364654983\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.16157\n",
      "Epoch:20 Batch:14 Loss:0.01851\n",
      "Epoch:40 Batch:14 Loss:0.01248\n",
      "Epoch:60 Batch:14 Loss:0.01171\n",
      "Epoch:80 Batch:14 Loss:0.01049\n",
      "Epoch:100 Batch:14 Loss:0.00954\n",
      "Epoch:120 Batch:14 Loss:0.00786\n",
      "Epoch:140 Batch:14 Loss:0.00918\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.024\n",
      "Epoch:10 Batch:10 Loss:0.023\n",
      "Epoch:20 Batch:10 Loss:0.020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:30 Batch:10 Loss:0.021\n",
      "Epoch:40 Batch:10 Loss:0.023\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 24.315587067707387 setps: 47 count: 47\n",
      "reward: 69.40455807613615 setps: 79 count: 126\n",
      "reward: 31.240826153641684 setps: 53 count: 179\n",
      "reward: 29.590738207806137 setps: 52 count: 231\n",
      "reward: 33.35321740928194 setps: 54 count: 285\n",
      "reward: 35.600725924166916 setps: 60 count: 345\n",
      "reward: 11.686544699224761 setps: 33 count: 378\n",
      "reward: 45.018650356911536 setps: 75 count: 453\n",
      "reward: 28.39755121572379 setps: 52 count: 505\n",
      "reward: 30.00941814454419 setps: 53 count: 558\n",
      "reward: 43.10611706256897 setps: 73 count: 631\n",
      "reward: 56.8055903341097 setps: 90 count: 721\n",
      "reward: 37.42687356273381 setps: 61 count: 782\n",
      "reward: 43.865668649289 setps: 72 count: 854\n",
      "reward: 39.174217565688004 setps: 46 count: 900\n",
      "reward: 86.99751514087912 setps: 83 count: 983\n",
      "avg rewards: 40.37461247315082\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.13286\n",
      "Epoch:20 Batch:15 Loss:0.02191\n",
      "Epoch:40 Batch:15 Loss:0.01366\n",
      "Epoch:60 Batch:15 Loss:0.01202\n",
      "Epoch:80 Batch:15 Loss:0.00849\n",
      "Epoch:100 Batch:15 Loss:0.01010\n",
      "Epoch:120 Batch:15 Loss:0.01079\n",
      "Epoch:140 Batch:15 Loss:0.00858\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.021\n",
      "Epoch:10 Batch:10 Loss:0.023\n",
      "Epoch:20 Batch:10 Loss:0.022\n",
      "Epoch:30 Batch:10 Loss:0.022\n",
      "Epoch:40 Batch:10 Loss:0.021\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 73.61572987433027 setps: 73 count: 73\n",
      "reward: 76.81967187010743 setps: 75 count: 148\n",
      "reward: 56.4115850430564 setps: 53 count: 201\n",
      "reward: 54.96749984863128 setps: 53 count: 254\n",
      "reward: 54.35486674306156 setps: 49 count: 303\n",
      "reward: 69.03797473789746 setps: 67 count: 370\n",
      "reward: 65.08913695962109 setps: 65 count: 435\n",
      "reward: 68.54688066901001 setps: 67 count: 502\n",
      "reward: 53.01897244476423 setps: 49 count: 551\n",
      "reward: 50.482491722423575 setps: 47 count: 598\n",
      "reward: 49.90797180988303 setps: 46 count: 644\n",
      "reward: 66.6861346419071 setps: 65 count: 709\n",
      "reward: 58.44863215339137 setps: 56 count: 765\n",
      "reward: 87.38877168464823 setps: 89 count: 854\n",
      "reward: 50.460434362541136 setps: 48 count: 902\n",
      "reward: 45.42904337819054 setps: 41 count: 943\n",
      "avg rewards: 61.29161237146654\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.09821\n",
      "Epoch:20 Batch:16 Loss:0.01589\n",
      "Epoch:40 Batch:16 Loss:0.01250\n",
      "Epoch:60 Batch:16 Loss:0.01040\n",
      "Epoch:80 Batch:16 Loss:0.01131\n",
      "Epoch:100 Batch:16 Loss:0.00986\n",
      "Epoch:120 Batch:16 Loss:0.00862\n",
      "Epoch:140 Batch:16 Loss:0.00774\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.023\n",
      "Epoch:10 Batch:10 Loss:0.023\n",
      "Epoch:20 Batch:10 Loss:0.021\n",
      "Epoch:30 Batch:10 Loss:0.021\n",
      "Epoch:40 Batch:10 Loss:0.022\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 33.66742531451601 setps: 63 count: 63\n",
      "reward: 41.62546320303665 setps: 74 count: 137\n",
      "reward: 33.243622147844874 setps: 62 count: 199\n",
      "reward: 28.990417671747867 setps: 58 count: 257\n",
      "reward: 26.028649398199924 setps: 52 count: 309\n",
      "reward: 43.64822271881566 setps: 77 count: 386\n",
      "reward: 27.22458603395935 setps: 54 count: 440\n",
      "reward: 35.59162412576407 setps: 66 count: 506\n",
      "reward: 55.129981597062596 setps: 92 count: 598\n",
      "reward: 39.47331337422221 setps: 71 count: 669\n",
      "reward: 37.553045574892906 setps: 69 count: 738\n",
      "reward: 31.23873314238299 setps: 59 count: 797\n",
      "reward: 44.364403728811894 setps: 79 count: 876\n",
      "reward: 31.373406839730148 setps: 59 count: 935\n",
      "avg rewards: 36.36806391935623\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.14655\n",
      "Epoch:20 Batch:17 Loss:0.01650\n",
      "Epoch:40 Batch:17 Loss:0.01172\n",
      "Epoch:60 Batch:17 Loss:0.00870\n",
      "Epoch:80 Batch:17 Loss:0.00821\n",
      "Epoch:100 Batch:17 Loss:0.00927\n",
      "Epoch:120 Batch:17 Loss:0.00712\n",
      "Epoch:140 Batch:17 Loss:0.00807\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.019\n",
      "Epoch:10 Batch:10 Loss:0.019\n",
      "Epoch:20 Batch:10 Loss:0.018\n",
      "Epoch:30 Batch:10 Loss:0.019\n",
      "Epoch:40 Batch:10 Loss:0.018\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 44.53473625334884 setps: 71 count: 71\n",
      "reward: 32.623278644804664 setps: 51 count: 122\n",
      "reward: 58.43773663359169 setps: 89 count: 211\n",
      "reward: 37.994852408549924 setps: 64 count: 275\n",
      "reward: 39.80997866705729 setps: 68 count: 343\n",
      "reward: 33.850038252178635 setps: 56 count: 399\n",
      "reward: 45.67352457355009 setps: 72 count: 471\n",
      "reward: 31.75492176128173 setps: 50 count: 521\n",
      "reward: 45.32403847623207 setps: 72 count: 593\n",
      "reward: 39.02906484640844 setps: 59 count: 652\n",
      "reward: 31.18625165721896 setps: 52 count: 704\n",
      "reward: 32.19831886684405 setps: 45 count: 749\n",
      "reward: 36.990472657595824 setps: 58 count: 807\n",
      "reward: 38.655162965344786 setps: 59 count: 866\n",
      "reward: 52.06475818409381 setps: 79 count: 945\n",
      "avg rewards: 40.00847565654006\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.11064\n",
      "Epoch:20 Batch:18 Loss:0.01565\n",
      "Epoch:40 Batch:18 Loss:0.01070\n",
      "Epoch:60 Batch:18 Loss:0.01072\n",
      "Epoch:80 Batch:18 Loss:0.00865\n",
      "Epoch:100 Batch:18 Loss:0.00775\n",
      "Epoch:120 Batch:18 Loss:0.00746\n",
      "Epoch:140 Batch:18 Loss:0.00771\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.024\n",
      "Epoch:10 Batch:10 Loss:0.022\n",
      "Epoch:20 Batch:10 Loss:0.028\n",
      "Epoch:30 Batch:10 Loss:0.022\n",
      "Epoch:40 Batch:10 Loss:0.018\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 72.5689177133114 setps: 61 count: 61\n",
      "reward: 58.8880755332779 setps: 46 count: 107\n",
      "reward: 73.41750456126753 setps: 60 count: 167\n",
      "reward: 85.46625629997575 setps: 81 count: 248\n",
      "reward: 42.8863803985747 setps: 63 count: 311\n",
      "reward: 49.520253949478494 setps: 74 count: 385\n",
      "reward: 102.04744939614756 setps: 89 count: 474\n",
      "reward: 51.45313737011565 setps: 71 count: 545\n",
      "reward: 82.52872071696332 setps: 78 count: 623\n",
      "reward: 58.907377475552494 setps: 51 count: 674\n",
      "reward: 40.05949599912854 setps: 58 count: 732\n",
      "reward: 119.6038749515632 setps: 111 count: 843\n",
      "reward: 48.528290126845256 setps: 66 count: 909\n",
      "reward: 94.75092740732653 setps: 79 count: 988\n",
      "avg rewards: 70.04476156425203\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.08748\n",
      "Epoch:20 Batch:19 Loss:0.01354\n",
      "Epoch:40 Batch:19 Loss:0.01032\n",
      "Epoch:60 Batch:19 Loss:0.01016\n",
      "Epoch:80 Batch:19 Loss:0.00818\n",
      "Epoch:100 Batch:19 Loss:0.00919\n",
      "Epoch:120 Batch:19 Loss:0.00678\n",
      "Epoch:140 Batch:19 Loss:0.00733\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.025\n",
      "Epoch:10 Batch:10 Loss:0.024\n",
      "Epoch:20 Batch:10 Loss:0.023\n",
      "Epoch:30 Batch:10 Loss:0.021\n",
      "Epoch:40 Batch:10 Loss:0.024\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 39.142430526638044 setps: 66 count: 66\n",
      "reward: 91.13107274008287 setps: 81 count: 147\n",
      "reward: 96.30937469304406 setps: 91 count: 238\n",
      "reward: 86.93876763522927 setps: 125 count: 363\n",
      "reward: 57.61382470264653 setps: 45 count: 408\n",
      "reward: 84.87564902603478 setps: 76 count: 484\n",
      "reward: 35.45478461884776 setps: 64 count: 548\n",
      "reward: 65.39432383855167 setps: 50 count: 598\n",
      "reward: 41.05740130185731 setps: 68 count: 666\n",
      "reward: 42.5253585550803 setps: 67 count: 733\n",
      "reward: 33.79196735490113 setps: 57 count: 790\n",
      "reward: 36.94952212835922 setps: 70 count: 860\n",
      "reward: 77.13400735963369 setps: 66 count: 926\n",
      "reward: 34.025719919669804 setps: 58 count: 984\n",
      "avg rewards: 58.73887174289831\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.08864\n",
      "Epoch:20 Batch:20 Loss:0.01625\n",
      "Epoch:40 Batch:20 Loss:0.00969\n",
      "Epoch:60 Batch:20 Loss:0.00777\n",
      "Epoch:80 Batch:20 Loss:0.00972\n",
      "Epoch:100 Batch:20 Loss:0.00653\n",
      "Epoch:120 Batch:20 Loss:0.00755\n",
      "Epoch:140 Batch:20 Loss:0.00707\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.024\n",
      "Epoch:10 Batch:10 Loss:0.019\n",
      "Epoch:20 Batch:10 Loss:0.020\n",
      "Epoch:30 Batch:10 Loss:0.019\n",
      "Epoch:40 Batch:10 Loss:0.022\n",
      "Done!\n",
      "############# start HalfCheetahBulletEnv-v0 training ###################\n",
      "(50, 1000, 26)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -930.2400049518964 setps: 800 count: 800\n",
      "avg rewards: -930.2400049518964\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:1 Loss:0.37056\n",
      "Epoch:20 Batch:1 Loss:0.28308\n",
      "Epoch:40 Batch:1 Loss:0.21838\n",
      "Epoch:60 Batch:1 Loss:0.20214\n",
      "Epoch:80 Batch:1 Loss:0.18479\n",
      "Epoch:100 Batch:1 Loss:0.16475\n",
      "Epoch:120 Batch:1 Loss:0.14704\n",
      "Epoch:140 Batch:1 Loss:0.12909\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.279\n",
      "Epoch:10 Batch:10 Loss:0.277\n",
      "Epoch:20 Batch:10 Loss:0.277\n",
      "Epoch:30 Batch:10 Loss:0.281\n",
      "Epoch:40 Batch:10 Loss:0.277\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1093.3955074543755 setps: 800 count: 800\n",
      "avg rewards: -1093.3955074543755\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:0.48872\n",
      "Epoch:20 Batch:2 Loss:0.17355\n",
      "Epoch:40 Batch:2 Loss:0.12281\n",
      "Epoch:60 Batch:2 Loss:0.10832\n",
      "Epoch:80 Batch:2 Loss:0.08808\n",
      "Epoch:100 Batch:2 Loss:0.06968\n",
      "Epoch:120 Batch:2 Loss:0.06314\n",
      "Epoch:140 Batch:2 Loss:0.06061\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.210\n",
      "Epoch:10 Batch:10 Loss:0.212\n",
      "Epoch:20 Batch:10 Loss:0.209\n",
      "Epoch:30 Batch:10 Loss:0.207\n",
      "Epoch:40 Batch:10 Loss:0.204\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1199.6665828632838 setps: 800 count: 800\n",
      "avg rewards: -1199.6665828632838\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.43128\n",
      "Epoch:20 Batch:3 Loss:0.13098\n",
      "Epoch:40 Batch:3 Loss:0.08052\n",
      "Epoch:60 Batch:3 Loss:0.06730\n",
      "Epoch:80 Batch:3 Loss:0.05640\n",
      "Epoch:100 Batch:3 Loss:0.04681\n",
      "Epoch:120 Batch:3 Loss:0.04165\n",
      "Epoch:140 Batch:3 Loss:0.03805\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.164\n",
      "Epoch:10 Batch:10 Loss:0.149\n",
      "Epoch:20 Batch:10 Loss:0.145\n",
      "Epoch:30 Batch:10 Loss:0.147\n",
      "Epoch:40 Batch:10 Loss:0.146\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1211.0498832171238 setps: 800 count: 800\n",
      "avg rewards: -1211.0498832171238\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.41911\n",
      "Epoch:20 Batch:4 Loss:0.09536\n",
      "Epoch:40 Batch:4 Loss:0.06011\n",
      "Epoch:60 Batch:4 Loss:0.04554\n",
      "Epoch:80 Batch:4 Loss:0.04247\n",
      "Epoch:100 Batch:4 Loss:0.02923\n",
      "Epoch:120 Batch:4 Loss:0.02706\n",
      "Epoch:140 Batch:4 Loss:0.02880\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.127\n",
      "Epoch:10 Batch:10 Loss:0.123\n",
      "Epoch:20 Batch:10 Loss:0.122\n",
      "Epoch:30 Batch:10 Loss:0.123\n",
      "Epoch:40 Batch:10 Loss:0.121\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1053.9258979352671 setps: 800 count: 800\n",
      "avg rewards: -1053.9258979352671\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.40085\n",
      "Epoch:20 Batch:5 Loss:0.07416\n",
      "Epoch:40 Batch:5 Loss:0.05514\n",
      "Epoch:60 Batch:5 Loss:0.03874\n",
      "Epoch:80 Batch:5 Loss:0.03037\n",
      "Epoch:100 Batch:5 Loss:0.02963\n",
      "Epoch:120 Batch:5 Loss:0.02473\n",
      "Epoch:140 Batch:5 Loss:0.02308\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.117\n",
      "Epoch:10 Batch:10 Loss:0.117\n",
      "Epoch:20 Batch:10 Loss:0.112\n",
      "Epoch:30 Batch:10 Loss:0.114\n",
      "Epoch:40 Batch:10 Loss:0.112\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -987.8676050519382 setps: 800 count: 800\n",
      "avg rewards: -987.8676050519382\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.38832\n",
      "Epoch:20 Batch:6 Loss:0.06142\n",
      "Epoch:40 Batch:6 Loss:0.04301\n",
      "Epoch:60 Batch:6 Loss:0.02942\n",
      "Epoch:80 Batch:6 Loss:0.02477\n",
      "Epoch:100 Batch:6 Loss:0.02376\n",
      "Epoch:120 Batch:6 Loss:0.02411\n",
      "Epoch:140 Batch:6 Loss:0.02529\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.098\n",
      "Epoch:10 Batch:10 Loss:0.096\n",
      "Epoch:20 Batch:10 Loss:0.093\n",
      "Epoch:30 Batch:10 Loss:0.092\n",
      "Epoch:40 Batch:10 Loss:0.096\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1075.64034402821 setps: 800 count: 800\n",
      "avg rewards: -1075.64034402821\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.39785\n",
      "Epoch:20 Batch:7 Loss:0.05784\n",
      "Epoch:40 Batch:7 Loss:0.03787\n",
      "Epoch:60 Batch:7 Loss:0.03058\n",
      "Epoch:80 Batch:7 Loss:0.02631\n",
      "Epoch:100 Batch:7 Loss:0.02076\n",
      "Epoch:120 Batch:7 Loss:0.02082\n",
      "Epoch:140 Batch:7 Loss:0.02118\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.080\n",
      "Epoch:10 Batch:10 Loss:0.074\n",
      "Epoch:20 Batch:10 Loss:0.073\n",
      "Epoch:30 Batch:10 Loss:0.073\n",
      "Epoch:40 Batch:10 Loss:0.075\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -768.0933375526367 setps: 800 count: 800\n",
      "avg rewards: -768.0933375526367\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.37783\n",
      "Epoch:20 Batch:8 Loss:0.05412\n",
      "Epoch:40 Batch:8 Loss:0.03693\n",
      "Epoch:60 Batch:8 Loss:0.02709\n",
      "Epoch:80 Batch:8 Loss:0.02432\n",
      "Epoch:100 Batch:8 Loss:0.02153\n",
      "Epoch:120 Batch:8 Loss:0.02189\n",
      "Epoch:140 Batch:8 Loss:0.01840\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.068\n",
      "Epoch:10 Batch:10 Loss:0.067\n",
      "Epoch:20 Batch:10 Loss:0.068\n",
      "Epoch:30 Batch:10 Loss:0.065\n",
      "Epoch:40 Batch:10 Loss:0.065\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1242.4583031522216 setps: 800 count: 800\n",
      "avg rewards: -1242.4583031522216\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.38677\n",
      "Epoch:20 Batch:9 Loss:0.05174\n",
      "Epoch:40 Batch:9 Loss:0.03372\n",
      "Epoch:60 Batch:9 Loss:0.02535\n",
      "Epoch:80 Batch:9 Loss:0.02073\n",
      "Epoch:100 Batch:9 Loss:0.01973\n",
      "Epoch:120 Batch:9 Loss:0.02008\n",
      "Epoch:140 Batch:9 Loss:0.01679\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.064\n",
      "Epoch:10 Batch:10 Loss:0.063\n",
      "Epoch:20 Batch:10 Loss:0.061\n",
      "Epoch:30 Batch:10 Loss:0.064\n",
      "Epoch:40 Batch:10 Loss:0.061\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1008.4996886277218 setps: 800 count: 800\n",
      "avg rewards: -1008.4996886277218\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.38218\n",
      "Epoch:20 Batch:10 Loss:0.04567\n",
      "Epoch:40 Batch:10 Loss:0.02584\n",
      "Epoch:60 Batch:10 Loss:0.02270\n",
      "Epoch:80 Batch:10 Loss:0.02068\n",
      "Epoch:100 Batch:10 Loss:0.02013\n",
      "Epoch:120 Batch:10 Loss:0.01834\n",
      "Epoch:140 Batch:10 Loss:0.01695\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.057\n",
      "Epoch:10 Batch:10 Loss:0.057\n",
      "Epoch:20 Batch:10 Loss:0.056\n",
      "Epoch:30 Batch:10 Loss:0.059\n",
      "Epoch:40 Batch:10 Loss:0.055\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1179.288221597165 setps: 800 count: 800\n",
      "avg rewards: -1179.288221597165\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.39118\n",
      "Epoch:20 Batch:11 Loss:0.05035\n",
      "Epoch:40 Batch:11 Loss:0.02726\n",
      "Epoch:60 Batch:11 Loss:0.02355\n",
      "Epoch:80 Batch:11 Loss:0.01842\n",
      "Epoch:100 Batch:11 Loss:0.01799\n",
      "Epoch:120 Batch:11 Loss:0.01661\n",
      "Epoch:140 Batch:11 Loss:0.01742\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.052\n",
      "Epoch:10 Batch:10 Loss:0.051\n",
      "Epoch:20 Batch:10 Loss:0.053\n",
      "Epoch:30 Batch:10 Loss:0.051\n",
      "Epoch:40 Batch:10 Loss:0.052\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1255.0810144116413 setps: 800 count: 800\n",
      "avg rewards: -1255.0810144116413\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.38849\n",
      "Epoch:20 Batch:12 Loss:0.04328\n",
      "Epoch:40 Batch:12 Loss:0.02558\n",
      "Epoch:60 Batch:12 Loss:0.02191\n",
      "Epoch:80 Batch:12 Loss:0.02038\n",
      "Epoch:100 Batch:12 Loss:0.01732\n",
      "Epoch:120 Batch:12 Loss:0.01791\n",
      "Epoch:140 Batch:12 Loss:0.01417\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.051\n",
      "Epoch:10 Batch:10 Loss:0.048\n",
      "Epoch:20 Batch:10 Loss:0.051\n",
      "Epoch:30 Batch:10 Loss:0.049\n",
      "Epoch:40 Batch:10 Loss:0.049\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1055.333109789788 setps: 800 count: 800\n",
      "avg rewards: -1055.333109789788\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.35350\n",
      "Epoch:20 Batch:13 Loss:0.04361\n",
      "Epoch:40 Batch:13 Loss:0.02344\n",
      "Epoch:60 Batch:13 Loss:0.01896\n",
      "Epoch:80 Batch:13 Loss:0.01686\n",
      "Epoch:100 Batch:13 Loss:0.01800\n",
      "Epoch:120 Batch:13 Loss:0.01449\n",
      "Epoch:140 Batch:13 Loss:0.01607\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:10 Loss:0.051\n",
      "Epoch:10 Batch:10 Loss:0.048\n",
      "Epoch:20 Batch:10 Loss:0.048\n",
      "Epoch:30 Batch:10 Loss:0.047\n",
      "Epoch:40 Batch:10 Loss:0.047\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1199.9375191515762 setps: 800 count: 800\n",
      "avg rewards: -1199.9375191515762\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.31418\n",
      "Epoch:20 Batch:14 Loss:0.03456\n",
      "Epoch:40 Batch:14 Loss:0.02390\n",
      "Epoch:60 Batch:14 Loss:0.01576\n",
      "Epoch:80 Batch:14 Loss:0.01689\n",
      "Epoch:100 Batch:14 Loss:0.01439\n",
      "Epoch:120 Batch:14 Loss:0.01660\n",
      "Epoch:140 Batch:14 Loss:0.01230\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.053\n",
      "Epoch:10 Batch:10 Loss:0.050\n",
      "Epoch:20 Batch:10 Loss:0.049\n",
      "Epoch:30 Batch:10 Loss:0.050\n",
      "Epoch:40 Batch:10 Loss:0.050\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1208.2280343868504 setps: 800 count: 800\n",
      "avg rewards: -1208.2280343868504\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.29493\n",
      "Epoch:20 Batch:15 Loss:0.03466\n",
      "Epoch:40 Batch:15 Loss:0.01988\n",
      "Epoch:60 Batch:15 Loss:0.01852\n",
      "Epoch:80 Batch:15 Loss:0.01502\n",
      "Epoch:100 Batch:15 Loss:0.01714\n",
      "Epoch:120 Batch:15 Loss:0.01432\n",
      "Epoch:140 Batch:15 Loss:0.01512\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.051\n",
      "Epoch:10 Batch:10 Loss:0.053\n",
      "Epoch:20 Batch:10 Loss:0.054\n",
      "Epoch:30 Batch:10 Loss:0.050\n",
      "Epoch:40 Batch:10 Loss:0.050\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1105.211682853638 setps: 800 count: 800\n",
      "avg rewards: -1105.211682853638\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.28861\n",
      "Epoch:20 Batch:16 Loss:0.03517\n",
      "Epoch:40 Batch:16 Loss:0.02238\n",
      "Epoch:60 Batch:16 Loss:0.01601\n",
      "Epoch:80 Batch:16 Loss:0.01578\n",
      "Epoch:100 Batch:16 Loss:0.01433\n",
      "Epoch:120 Batch:16 Loss:0.01262\n",
      "Epoch:140 Batch:16 Loss:0.01046\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.047\n",
      "Epoch:10 Batch:10 Loss:0.044\n",
      "Epoch:20 Batch:10 Loss:0.047\n",
      "Epoch:30 Batch:10 Loss:0.045\n",
      "Epoch:40 Batch:10 Loss:0.047\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -937.8170388583917 setps: 800 count: 800\n",
      "avg rewards: -937.8170388583917\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.30918\n",
      "Epoch:20 Batch:17 Loss:0.02837\n",
      "Epoch:40 Batch:17 Loss:0.01909\n",
      "Epoch:60 Batch:17 Loss:0.01614\n",
      "Epoch:80 Batch:17 Loss:0.01661\n",
      "Epoch:100 Batch:17 Loss:0.01061\n",
      "Epoch:120 Batch:17 Loss:0.01392\n",
      "Epoch:140 Batch:17 Loss:0.01497\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.049\n",
      "Epoch:10 Batch:10 Loss:0.045\n",
      "Epoch:20 Batch:10 Loss:0.047\n",
      "Epoch:30 Batch:10 Loss:0.048\n",
      "Epoch:40 Batch:10 Loss:0.045\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1016.8178513706496 setps: 800 count: 800\n",
      "avg rewards: -1016.8178513706496\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.31800\n",
      "Epoch:20 Batch:18 Loss:0.02806\n",
      "Epoch:40 Batch:18 Loss:0.01744\n",
      "Epoch:60 Batch:18 Loss:0.01792\n",
      "Epoch:80 Batch:18 Loss:0.01324\n",
      "Epoch:100 Batch:18 Loss:0.01038\n",
      "Epoch:120 Batch:18 Loss:0.01305\n",
      "Epoch:140 Batch:18 Loss:0.00955\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.050\n",
      "Epoch:10 Batch:10 Loss:0.046\n",
      "Epoch:20 Batch:10 Loss:0.049\n",
      "Epoch:30 Batch:10 Loss:0.047\n",
      "Epoch:40 Batch:10 Loss:0.047\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1115.363427739037 setps: 800 count: 800\n",
      "avg rewards: -1115.363427739037\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.23545\n",
      "Epoch:20 Batch:19 Loss:0.02866\n",
      "Epoch:40 Batch:19 Loss:0.02059\n",
      "Epoch:60 Batch:19 Loss:0.01460\n",
      "Epoch:80 Batch:19 Loss:0.01506\n",
      "Epoch:100 Batch:19 Loss:0.01208\n",
      "Epoch:120 Batch:19 Loss:0.01340\n",
      "Epoch:140 Batch:19 Loss:0.01163\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.049\n",
      "Epoch:10 Batch:10 Loss:0.046\n",
      "Epoch:20 Batch:10 Loss:0.045\n",
      "Epoch:30 Batch:10 Loss:0.045\n",
      "Epoch:40 Batch:10 Loss:0.048\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -930.4695218928806 setps: 800 count: 800\n",
      "avg rewards: -930.4695218928806\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.27948\n",
      "Epoch:20 Batch:20 Loss:0.02751\n",
      "Epoch:40 Batch:20 Loss:0.01899\n",
      "Epoch:60 Batch:20 Loss:0.01465\n",
      "Epoch:80 Batch:20 Loss:0.01214\n",
      "Epoch:100 Batch:20 Loss:0.01170\n",
      "Epoch:120 Batch:20 Loss:0.00902\n",
      "Epoch:140 Batch:20 Loss:0.00962\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.047\n",
      "Epoch:10 Batch:10 Loss:0.047\n",
      "Epoch:20 Batch:10 Loss:0.045\n",
      "Epoch:30 Batch:10 Loss:0.044\n",
      "Epoch:40 Batch:10 Loss:0.045\n",
      "Done!\n",
      "############# start AntBulletEnv-v0 training ###################\n",
      "(50, 1000, 28)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 141.98239282669812 setps: 302 count: 302\n",
      "avg rewards: 141.98239282669812\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:0.43117\n",
      "Epoch:20 Batch:1 Loss:0.10729\n",
      "Epoch:40 Batch:1 Loss:0.06500\n",
      "Epoch:60 Batch:1 Loss:0.06037\n",
      "Epoch:80 Batch:1 Loss:0.05060\n",
      "Epoch:100 Batch:1 Loss:0.03494\n",
      "Epoch:120 Batch:1 Loss:0.02756\n",
      "Epoch:140 Batch:1 Loss:0.02578\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.333\n",
      "Epoch:10 Batch:10 Loss:0.332\n",
      "Epoch:20 Batch:10 Loss:0.334\n",
      "Epoch:30 Batch:10 Loss:0.331\n",
      "Epoch:40 Batch:10 Loss:0.329\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 324.7512308667779 setps: 800 count: 800\n",
      "avg rewards: 324.7512308667779\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:0.49949\n",
      "Epoch:20 Batch:2 Loss:0.09272\n",
      "Epoch:40 Batch:2 Loss:0.04275\n",
      "Epoch:60 Batch:2 Loss:0.03833\n",
      "Epoch:80 Batch:2 Loss:0.03165\n",
      "Epoch:100 Batch:2 Loss:0.02408\n",
      "Epoch:120 Batch:2 Loss:0.02219\n",
      "Epoch:140 Batch:2 Loss:0.01994\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.257\n",
      "Epoch:10 Batch:10 Loss:0.256\n",
      "Epoch:20 Batch:10 Loss:0.252\n",
      "Epoch:30 Batch:10 Loss:0.251\n",
      "Epoch:40 Batch:10 Loss:0.251\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 279.98405093698244 setps: 800 count: 800\n",
      "avg rewards: 279.98405093698244\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.51864\n",
      "Epoch:20 Batch:3 Loss:0.05864\n",
      "Epoch:40 Batch:3 Loss:0.03331\n",
      "Epoch:60 Batch:3 Loss:0.02706\n",
      "Epoch:80 Batch:3 Loss:0.02047\n",
      "Epoch:100 Batch:3 Loss:0.01589\n",
      "Epoch:120 Batch:3 Loss:0.01199\n",
      "Epoch:140 Batch:3 Loss:0.01079\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.229\n",
      "Epoch:10 Batch:10 Loss:0.227\n",
      "Epoch:20 Batch:10 Loss:0.226\n",
      "Epoch:30 Batch:10 Loss:0.225\n",
      "Epoch:40 Batch:10 Loss:0.226\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 73.5995916435218 setps: 229 count: 229\n",
      "avg rewards: 73.5995916435218\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.52000\n",
      "Epoch:20 Batch:4 Loss:0.04431\n",
      "Epoch:40 Batch:4 Loss:0.02472\n",
      "Epoch:60 Batch:4 Loss:0.01628\n",
      "Epoch:80 Batch:4 Loss:0.01207\n",
      "Epoch:100 Batch:4 Loss:0.01073\n",
      "Epoch:120 Batch:4 Loss:0.00978\n",
      "Epoch:140 Batch:4 Loss:0.00857\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.240\n",
      "Epoch:10 Batch:10 Loss:0.233\n",
      "Epoch:20 Batch:10 Loss:0.232\n",
      "Epoch:30 Batch:10 Loss:0.227\n",
      "Epoch:40 Batch:10 Loss:0.230\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 464.7464591418942 setps: 800 count: 800\n",
      "avg rewards: 464.7464591418942\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.50894\n",
      "Epoch:20 Batch:5 Loss:0.03377\n",
      "Epoch:40 Batch:5 Loss:0.01816\n",
      "Epoch:60 Batch:5 Loss:0.01235\n",
      "Epoch:80 Batch:5 Loss:0.00974\n",
      "Epoch:100 Batch:5 Loss:0.00849\n",
      "Epoch:120 Batch:5 Loss:0.00805\n",
      "Epoch:140 Batch:5 Loss:0.00689\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.210\n",
      "Epoch:10 Batch:10 Loss:0.207\n",
      "Epoch:20 Batch:10 Loss:0.207\n",
      "Epoch:30 Batch:10 Loss:0.205\n",
      "Epoch:40 Batch:10 Loss:0.207\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 387.8300491166813 setps: 800 count: 800\n",
      "avg rewards: 387.8300491166813\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.50204\n",
      "Epoch:20 Batch:6 Loss:0.03769\n",
      "Epoch:40 Batch:6 Loss:0.01508\n",
      "Epoch:60 Batch:6 Loss:0.00936\n",
      "Epoch:80 Batch:6 Loss:0.00866\n",
      "Epoch:100 Batch:6 Loss:0.00723\n",
      "Epoch:120 Batch:6 Loss:0.00660\n",
      "Epoch:140 Batch:6 Loss:0.00603\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.223\n",
      "Epoch:10 Batch:10 Loss:0.218\n",
      "Epoch:20 Batch:10 Loss:0.216\n",
      "Epoch:30 Batch:10 Loss:0.215\n",
      "Epoch:40 Batch:10 Loss:0.216\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 475.8937214688921 setps: 800 count: 800\n",
      "avg rewards: 475.8937214688921\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.53075\n",
      "Epoch:20 Batch:7 Loss:0.03011\n",
      "Epoch:40 Batch:7 Loss:0.01456\n",
      "Epoch:60 Batch:7 Loss:0.00889\n",
      "Epoch:80 Batch:7 Loss:0.00757\n",
      "Epoch:100 Batch:7 Loss:0.00684\n",
      "Epoch:120 Batch:7 Loss:0.00601\n",
      "Epoch:140 Batch:7 Loss:0.00529\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.219\n",
      "Epoch:10 Batch:10 Loss:0.212\n",
      "Epoch:20 Batch:10 Loss:0.211\n",
      "Epoch:30 Batch:10 Loss:0.211\n",
      "Epoch:40 Batch:10 Loss:0.211\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 253.99634532262118 setps: 800 count: 800\n",
      "avg rewards: 253.99634532262118\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.52026\n",
      "Epoch:20 Batch:8 Loss:0.02614\n",
      "Epoch:40 Batch:8 Loss:0.01087\n",
      "Epoch:60 Batch:8 Loss:0.00753\n",
      "Epoch:80 Batch:8 Loss:0.00619\n",
      "Epoch:100 Batch:8 Loss:0.00595\n",
      "Epoch:120 Batch:8 Loss:0.00628\n",
      "Epoch:140 Batch:8 Loss:0.00571\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.232\n",
      "Epoch:10 Batch:10 Loss:0.231\n",
      "Epoch:20 Batch:10 Loss:0.228\n",
      "Epoch:30 Batch:10 Loss:0.226\n",
      "Epoch:40 Batch:10 Loss:0.228\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 333.85890100940037 setps: 800 count: 800\n",
      "avg rewards: 333.85890100940037\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.44263\n",
      "Epoch:20 Batch:9 Loss:0.01859\n",
      "Epoch:40 Batch:9 Loss:0.00877\n",
      "Epoch:60 Batch:9 Loss:0.00673\n",
      "Epoch:80 Batch:9 Loss:0.00588\n",
      "Epoch:100 Batch:9 Loss:0.00543\n",
      "Epoch:120 Batch:9 Loss:0.00526\n",
      "Epoch:140 Batch:9 Loss:0.00494\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.215\n",
      "Epoch:10 Batch:10 Loss:0.207\n",
      "Epoch:20 Batch:10 Loss:0.204\n",
      "Epoch:30 Batch:10 Loss:0.206\n",
      "Epoch:40 Batch:10 Loss:0.205\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 487.38200425360066 setps: 800 count: 800\n",
      "avg rewards: 487.38200425360066\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.48757\n",
      "Epoch:20 Batch:10 Loss:0.01844\n",
      "Epoch:40 Batch:10 Loss:0.00822\n",
      "Epoch:60 Batch:10 Loss:0.00694\n",
      "Epoch:80 Batch:10 Loss:0.00662\n",
      "Epoch:100 Batch:10 Loss:0.00542\n",
      "Epoch:120 Batch:10 Loss:0.00529\n",
      "Epoch:140 Batch:10 Loss:0.00444\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.221\n",
      "Epoch:10 Batch:10 Loss:0.216\n",
      "Epoch:20 Batch:10 Loss:0.216\n",
      "Epoch:30 Batch:10 Loss:0.214\n",
      "Epoch:40 Batch:10 Loss:0.214\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 466.9628666734818 setps: 800 count: 800\n",
      "avg rewards: 466.9628666734818\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.45573\n",
      "Epoch:20 Batch:11 Loss:0.01811\n",
      "Epoch:40 Batch:11 Loss:0.00870\n",
      "Epoch:60 Batch:11 Loss:0.00780\n",
      "Epoch:80 Batch:11 Loss:0.00565\n",
      "Epoch:100 Batch:11 Loss:0.00481\n",
      "Epoch:120 Batch:11 Loss:0.00444\n",
      "Epoch:140 Batch:11 Loss:0.00391\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.210\n",
      "Epoch:10 Batch:10 Loss:0.202\n",
      "Epoch:20 Batch:10 Loss:0.204\n",
      "Epoch:30 Batch:10 Loss:0.200\n",
      "Epoch:40 Batch:10 Loss:0.201\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 385.9054421317166 setps: 800 count: 800\n",
      "avg rewards: 385.9054421317166\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.49831\n",
      "Epoch:20 Batch:12 Loss:0.01367\n",
      "Epoch:40 Batch:12 Loss:0.00871\n",
      "Epoch:60 Batch:12 Loss:0.00659\n",
      "Epoch:80 Batch:12 Loss:0.00513\n",
      "Epoch:100 Batch:12 Loss:0.00484\n",
      "Epoch:120 Batch:12 Loss:0.00476\n",
      "Epoch:140 Batch:12 Loss:0.00442\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.197\n",
      "Epoch:10 Batch:10 Loss:0.191\n",
      "Epoch:20 Batch:10 Loss:0.190\n",
      "Epoch:30 Batch:10 Loss:0.188\n",
      "Epoch:40 Batch:10 Loss:0.188\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 457.8932150963853 setps: 800 count: 800\n",
      "avg rewards: 457.8932150963853\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.42862\n",
      "Epoch:20 Batch:13 Loss:0.01181\n",
      "Epoch:40 Batch:13 Loss:0.00761\n",
      "Epoch:60 Batch:13 Loss:0.00624\n",
      "Epoch:80 Batch:13 Loss:0.00519\n",
      "Epoch:100 Batch:13 Loss:0.00528\n",
      "Epoch:120 Batch:13 Loss:0.00471\n",
      "Epoch:140 Batch:13 Loss:0.00418\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.183\n",
      "Epoch:10 Batch:10 Loss:0.171\n",
      "Epoch:20 Batch:10 Loss:0.171\n",
      "Epoch:30 Batch:10 Loss:0.171\n",
      "Epoch:40 Batch:10 Loss:0.170\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 180.6599969231641 setps: 800 count: 800\n",
      "avg rewards: 180.6599969231641\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.39803\n",
      "Epoch:20 Batch:14 Loss:0.01296\n",
      "Epoch:40 Batch:14 Loss:0.00837\n",
      "Epoch:60 Batch:14 Loss:0.00635\n",
      "Epoch:80 Batch:14 Loss:0.00556\n",
      "Epoch:100 Batch:14 Loss:0.00472\n",
      "Epoch:120 Batch:14 Loss:0.00470\n",
      "Epoch:140 Batch:14 Loss:0.00412\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.186\n",
      "Epoch:10 Batch:10 Loss:0.173\n",
      "Epoch:20 Batch:10 Loss:0.168\n",
      "Epoch:30 Batch:10 Loss:0.167\n",
      "Epoch:40 Batch:10 Loss:0.166\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 448.9858555312778 setps: 800 count: 800\n",
      "avg rewards: 448.9858555312778\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.42287\n",
      "Epoch:20 Batch:15 Loss:0.01225\n",
      "Epoch:40 Batch:15 Loss:0.00662\n",
      "Epoch:60 Batch:15 Loss:0.00609\n",
      "Epoch:80 Batch:15 Loss:0.00564\n",
      "Epoch:100 Batch:15 Loss:0.00468\n",
      "Epoch:120 Batch:15 Loss:0.00453\n",
      "Epoch:140 Batch:15 Loss:0.00431\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.153\n",
      "Epoch:10 Batch:10 Loss:0.140\n",
      "Epoch:20 Batch:10 Loss:0.137\n",
      "Epoch:30 Batch:10 Loss:0.133\n",
      "Epoch:40 Batch:10 Loss:0.137\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 377.0531210046244 setps: 800 count: 800\n",
      "avg rewards: 377.0531210046244\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.35288\n",
      "Epoch:20 Batch:16 Loss:0.01112\n",
      "Epoch:40 Batch:16 Loss:0.00750\n",
      "Epoch:60 Batch:16 Loss:0.00665\n",
      "Epoch:80 Batch:16 Loss:0.00459\n",
      "Epoch:100 Batch:16 Loss:0.00464\n",
      "Epoch:120 Batch:16 Loss:0.00459\n",
      "Epoch:140 Batch:16 Loss:0.00392\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.140\n",
      "Epoch:10 Batch:10 Loss:0.124\n",
      "Epoch:20 Batch:10 Loss:0.121\n",
      "Epoch:30 Batch:10 Loss:0.121\n",
      "Epoch:40 Batch:10 Loss:0.123\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 126.55164749034316 setps: 800 count: 800\n",
      "avg rewards: 126.55164749034316\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.39797\n",
      "Epoch:20 Batch:17 Loss:0.01203\n",
      "Epoch:40 Batch:17 Loss:0.00717\n",
      "Epoch:60 Batch:17 Loss:0.00553\n",
      "Epoch:80 Batch:17 Loss:0.00502\n",
      "Epoch:100 Batch:17 Loss:0.00482\n",
      "Epoch:120 Batch:17 Loss:0.00451\n",
      "Epoch:140 Batch:17 Loss:0.00417\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.093\n",
      "Epoch:10 Batch:10 Loss:0.089\n",
      "Epoch:20 Batch:10 Loss:0.088\n",
      "Epoch:30 Batch:10 Loss:0.086\n",
      "Epoch:40 Batch:10 Loss:0.087\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -182.99108977895932 setps: 800 count: 800\n",
      "avg rewards: -182.99108977895932\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.36978\n",
      "Epoch:20 Batch:18 Loss:0.01142\n",
      "Epoch:40 Batch:18 Loss:0.00771\n",
      "Epoch:60 Batch:18 Loss:0.00555\n",
      "Epoch:80 Batch:18 Loss:0.00530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:100 Batch:18 Loss:0.00426\n",
      "Epoch:120 Batch:18 Loss:0.00422\n",
      "Epoch:140 Batch:18 Loss:0.00428\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.097\n",
      "Epoch:10 Batch:10 Loss:0.092\n",
      "Epoch:20 Batch:10 Loss:0.091\n",
      "Epoch:30 Batch:10 Loss:0.092\n",
      "Epoch:40 Batch:10 Loss:0.090\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 242.0735243830509 setps: 800 count: 800\n",
      "avg rewards: 242.0735243830509\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.38957\n",
      "Epoch:20 Batch:19 Loss:0.01159\n",
      "Epoch:40 Batch:19 Loss:0.00670\n",
      "Epoch:60 Batch:19 Loss:0.00544\n",
      "Epoch:80 Batch:19 Loss:0.00510\n",
      "Epoch:100 Batch:19 Loss:0.00485\n",
      "Epoch:120 Batch:19 Loss:0.00337\n",
      "Epoch:140 Batch:19 Loss:0.00426\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.098\n",
      "Epoch:10 Batch:10 Loss:0.091\n",
      "Epoch:20 Batch:10 Loss:0.090\n",
      "Epoch:30 Batch:10 Loss:0.091\n",
      "Epoch:40 Batch:10 Loss:0.089\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -62.52792428836113 setps: 800 count: 800\n",
      "avg rewards: -62.52792428836113\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.30260\n",
      "Epoch:20 Batch:20 Loss:0.01054\n",
      "Epoch:40 Batch:20 Loss:0.00753\n",
      "Epoch:60 Batch:20 Loss:0.00602\n",
      "Epoch:80 Batch:20 Loss:0.00497\n",
      "Epoch:100 Batch:20 Loss:0.00461\n",
      "Epoch:120 Batch:20 Loss:0.00427\n",
      "Epoch:140 Batch:20 Loss:0.00454\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.093\n",
      "Epoch:10 Batch:10 Loss:0.085\n",
      "Epoch:20 Batch:10 Loss:0.084\n",
      "Epoch:30 Batch:10 Loss:0.084\n",
      "Epoch:40 Batch:10 Loss:0.083\n",
      "Done!\n",
      "############# start HumanoidBulletEnv-v0 training ###################\n",
      "(49,)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -23.930504222029416 setps: 21 count: 21\n",
      "reward: -44.832111978673495 setps: 19 count: 40\n",
      "reward: -33.58876023627963 setps: 19 count: 59\n",
      "reward: -25.92361880076642 setps: 19 count: 78\n",
      "reward: -26.585914267043705 setps: 20 count: 98\n",
      "reward: -28.786084618244786 setps: 19 count: 117\n",
      "reward: -34.1914647980986 setps: 17 count: 134\n",
      "reward: -30.358921716589247 setps: 20 count: 154\n",
      "reward: -18.08054182232445 setps: 19 count: 173\n",
      "reward: -57.65301880087355 setps: 19 count: 192\n",
      "reward: -27.16646399433666 setps: 20 count: 212\n",
      "reward: -28.022576801269317 setps: 19 count: 231\n",
      "reward: -36.95166069329862 setps: 19 count: 250\n",
      "reward: -27.409624021807392 setps: 22 count: 272\n",
      "reward: -23.648219335862084 setps: 17 count: 289\n",
      "reward: -22.391593500814636 setps: 18 count: 307\n",
      "reward: -32.326349868203394 setps: 19 count: 326\n",
      "reward: -31.04949865229865 setps: 19 count: 345\n",
      "reward: -30.520214039421994 setps: 19 count: 364\n",
      "reward: -30.10761921199737 setps: 18 count: 382\n",
      "reward: -17.6856600570376 setps: 17 count: 399\n",
      "reward: -38.02910445531306 setps: 18 count: 417\n",
      "reward: -41.96597049820265 setps: 19 count: 436\n",
      "reward: -22.744429027616572 setps: 20 count: 456\n",
      "reward: -27.466838021275187 setps: 21 count: 477\n",
      "reward: -25.418721662537425 setps: 22 count: 499\n",
      "reward: -13.383837923497778 setps: 20 count: 519\n",
      "reward: -29.334350272355362 setps: 19 count: 538\n",
      "reward: -32.68848228619609 setps: 19 count: 557\n",
      "reward: -29.951686735973638 setps: 21 count: 578\n",
      "reward: -19.160243210608318 setps: 18 count: 596\n",
      "reward: -16.999182235320042 setps: 18 count: 614\n",
      "reward: -26.010695673814915 setps: 19 count: 633\n",
      "reward: -22.91498172552674 setps: 18 count: 651\n",
      "reward: -34.79813765357831 setps: 19 count: 670\n",
      "reward: -44.06062952949869 setps: 17 count: 687\n",
      "reward: -30.75969831473048 setps: 19 count: 706\n",
      "reward: -19.832492106322025 setps: 19 count: 725\n",
      "reward: -31.070794982687215 setps: 19 count: 744\n",
      "reward: -47.582544886093814 setps: 18 count: 762\n",
      "reward: -66.7593067625392 setps: 33 count: 795\n",
      "reward: -41.14785905380995 setps: 22 count: 817\n",
      "reward: -40.77110538025736 setps: 18 count: 835\n",
      "reward: -31.554773128077795 setps: 19 count: 854\n",
      "reward: -27.048210133392423 setps: 19 count: 873\n",
      "reward: -42.03738607068517 setps: 18 count: 891\n",
      "reward: -17.79626596370508 setps: 21 count: 912\n",
      "reward: -28.548658798259563 setps: 19 count: 931\n",
      "reward: -30.842491852634698 setps: 18 count: 949\n",
      "reward: -23.387407220486782 setps: 19 count: 968\n",
      "reward: -30.513364205423564 setps: 19 count: 987\n",
      "avg rewards: -30.701766102111588\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:0.59871\n",
      "Epoch:20 Batch:1 Loss:0.56483\n",
      "Epoch:40 Batch:1 Loss:0.51988\n",
      "Epoch:60 Batch:1 Loss:0.49933\n",
      "Epoch:80 Batch:1 Loss:0.48107\n",
      "Epoch:100 Batch:1 Loss:0.45425\n",
      "Epoch:120 Batch:1 Loss:0.41606\n",
      "Epoch:140 Batch:1 Loss:0.37445\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.256\n",
      "Epoch:10 Batch:8 Loss:0.251\n",
      "Epoch:20 Batch:8 Loss:0.253\n",
      "Epoch:30 Batch:8 Loss:0.251\n",
      "Epoch:40 Batch:8 Loss:0.248\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 19.295228570931062 setps: 19 count: 19\n",
      "reward: 14.719059360635583 setps: 20 count: 39\n",
      "reward: 16.345902257105628 setps: 19 count: 58\n",
      "reward: 18.064093865090403 setps: 19 count: 77\n",
      "reward: 14.955479377879238 setps: 19 count: 96\n",
      "reward: 16.850058841423014 setps: 20 count: 116\n",
      "reward: 16.331715693454317 setps: 19 count: 135\n",
      "reward: 17.689049318399334 setps: 19 count: 154\n",
      "reward: 15.113459989352847 setps: 19 count: 173\n",
      "reward: 17.53520522281761 setps: 20 count: 193\n",
      "reward: 18.35994428022823 setps: 20 count: 213\n",
      "reward: 18.89045133437903 setps: 19 count: 232\n",
      "reward: 19.145648790855198 setps: 20 count: 252\n",
      "reward: 17.401976534280404 setps: 19 count: 271\n",
      "reward: 20.218875619649772 setps: 20 count: 291\n",
      "reward: 16.102180543533173 setps: 20 count: 311\n",
      "reward: 18.047998708359955 setps: 19 count: 330\n",
      "reward: 14.509685822381288 setps: 19 count: 349\n",
      "reward: 16.2078164604216 setps: 19 count: 368\n",
      "reward: 18.561989924788943 setps: 20 count: 388\n",
      "reward: 16.735773155403148 setps: 20 count: 408\n",
      "reward: 19.66074813219602 setps: 20 count: 428\n",
      "reward: 17.777888842090036 setps: 20 count: 448\n",
      "reward: 15.55541434105107 setps: 20 count: 468\n",
      "reward: 16.600201269393434 setps: 19 count: 487\n",
      "reward: 17.441154599716533 setps: 20 count: 507\n",
      "reward: 16.510714401303268 setps: 19 count: 526\n",
      "reward: 16.408521432429556 setps: 19 count: 545\n",
      "reward: 15.153018112169228 setps: 19 count: 564\n",
      "reward: 19.10786043764965 setps: 19 count: 583\n",
      "reward: 19.999664682282308 setps: 20 count: 603\n",
      "reward: 17.492751527696964 setps: 19 count: 622\n",
      "reward: 16.964327695782416 setps: 19 count: 641\n",
      "reward: 17.391768112989666 setps: 19 count: 660\n",
      "reward: 16.211153494950846 setps: 19 count: 679\n",
      "reward: 12.7153321037913 setps: 19 count: 698\n",
      "reward: 12.715347678576654 setps: 16 count: 714\n",
      "reward: 19.755807361578626 setps: 19 count: 733\n",
      "reward: 17.500830123625928 setps: 19 count: 752\n",
      "reward: 18.159907844441477 setps: 20 count: 772\n",
      "reward: 19.1531086780713 setps: 20 count: 792\n",
      "reward: 15.336743684878456 setps: 19 count: 811\n",
      "reward: 18.137559656357915 setps: 20 count: 831\n",
      "reward: 21.102958570975172 setps: 20 count: 851\n",
      "reward: 17.33728482000879 setps: 19 count: 870\n",
      "reward: 17.345458462112582 setps: 19 count: 889\n",
      "reward: 15.036120507692974 setps: 19 count: 908\n",
      "reward: 17.90617812734272 setps: 20 count: 928\n",
      "reward: 18.114242679321617 setps: 19 count: 947\n",
      "reward: 14.589668383936802 setps: 20 count: 967\n",
      "reward: 17.945806800779252 setps: 20 count: 987\n",
      "avg rewards: 17.18057129875612\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:0.46601\n",
      "Epoch:20 Batch:2 Loss:0.36747\n",
      "Epoch:40 Batch:2 Loss:0.32359\n",
      "Epoch:60 Batch:2 Loss:0.28588\n",
      "Epoch:80 Batch:2 Loss:0.25443\n",
      "Epoch:100 Batch:2 Loss:0.22228\n",
      "Epoch:120 Batch:2 Loss:0.19241\n",
      "Epoch:140 Batch:2 Loss:0.16749\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.210\n",
      "Epoch:10 Batch:8 Loss:0.204\n",
      "Epoch:20 Batch:8 Loss:0.203\n",
      "Epoch:30 Batch:8 Loss:0.198\n",
      "Epoch:40 Batch:8 Loss:0.201\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -0.1328939594008265 setps: 21 count: 21\n",
      "reward: -0.7170145230935319 setps: 18 count: 39\n",
      "reward: -2.1093217163812366 setps: 14 count: 53\n",
      "reward: -0.16076691056368997 setps: 21 count: 74\n",
      "reward: -4.819398589154298 setps: 21 count: 95\n",
      "reward: -4.372316715272611 setps: 20 count: 115\n",
      "reward: -4.932549045549241 setps: 22 count: 137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -3.1297987669517173 setps: 20 count: 157\n",
      "reward: -5.169513343089784 setps: 19 count: 176\n",
      "reward: -1.0973618111442196 setps: 22 count: 198\n",
      "reward: -1.832125915624784 setps: 20 count: 218\n",
      "reward: 0.15498966001177816 setps: 21 count: 239\n",
      "reward: -2.60916873751121 setps: 21 count: 260\n",
      "reward: 6.220851982857857 setps: 19 count: 279\n",
      "reward: -2.2809724079808813 setps: 20 count: 299\n",
      "reward: -2.740424232708757 setps: 21 count: 320\n",
      "reward: -4.450960769387894 setps: 20 count: 340\n",
      "reward: -0.4110828985503776 setps: 20 count: 360\n",
      "reward: -3.4951423710212115 setps: 21 count: 381\n",
      "reward: 0.25178513689752346 setps: 23 count: 404\n",
      "reward: -4.728132330924564 setps: 19 count: 423\n",
      "reward: 6.191367337452538 setps: 17 count: 440\n",
      "reward: -6.6060395912165415 setps: 26 count: 466\n",
      "reward: -3.127683445904405 setps: 22 count: 488\n",
      "reward: -1.0494895786672722 setps: 21 count: 509\n",
      "reward: -2.032596889069828 setps: 21 count: 530\n",
      "reward: -4.39477815538121 setps: 20 count: 550\n",
      "reward: 2.4170705865341002 setps: 17 count: 567\n",
      "reward: -3.4521926772795273 setps: 20 count: 587\n",
      "reward: -2.730047379738245 setps: 27 count: 614\n",
      "reward: -2.2736134231148766 setps: 21 count: 635\n",
      "reward: -2.7892040825434385 setps: 23 count: 658\n",
      "reward: -4.987387968345139 setps: 21 count: 679\n",
      "reward: 1.470081823761575 setps: 18 count: 697\n",
      "reward: -1.6348086659330892 setps: 21 count: 718\n",
      "reward: 2.758991768636042 setps: 19 count: 737\n",
      "reward: -1.4864436578252938 setps: 20 count: 757\n",
      "reward: -7.5680231191450735 setps: 20 count: 777\n",
      "reward: -0.9513105995472876 setps: 22 count: 799\n",
      "reward: 7.514268264037672 setps: 19 count: 818\n",
      "reward: -4.396429449300921 setps: 20 count: 838\n",
      "reward: 0.041362237113934164 setps: 21 count: 859\n",
      "reward: -2.0387284610653302 setps: 19 count: 878\n",
      "reward: -1.3043610400767653 setps: 21 count: 899\n",
      "reward: -0.7319201347912894 setps: 23 count: 922\n",
      "reward: -1.2554953042024868 setps: 23 count: 945\n",
      "reward: 7.173292488562584 setps: 18 count: 963\n",
      "reward: 15.623138753816605 setps: 36 count: 999\n",
      "avg rewards: -1.1287978880786802\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.45361\n",
      "Epoch:20 Batch:3 Loss:0.32439\n",
      "Epoch:40 Batch:3 Loss:0.23881\n",
      "Epoch:60 Batch:3 Loss:0.20311\n",
      "Epoch:80 Batch:3 Loss:0.17975\n",
      "Epoch:100 Batch:3 Loss:0.15887\n",
      "Epoch:120 Batch:3 Loss:0.13306\n",
      "Epoch:140 Batch:3 Loss:0.11686\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.135\n",
      "Epoch:10 Batch:8 Loss:0.130\n",
      "Epoch:20 Batch:8 Loss:0.129\n",
      "Epoch:30 Batch:8 Loss:0.127\n",
      "Epoch:40 Batch:8 Loss:0.129\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 16.104808847472302 setps: 25 count: 25\n",
      "reward: 15.417760165262735 setps: 25 count: 50\n",
      "reward: 12.370515987314866 setps: 19 count: 69\n",
      "reward: 20.911364505387603 setps: 23 count: 92\n",
      "reward: 16.369531944020125 setps: 25 count: 117\n",
      "reward: 15.218430637296112 setps: 26 count: 143\n",
      "reward: 14.589809360848449 setps: 25 count: 168\n",
      "reward: 16.22960482959024 setps: 25 count: 193\n",
      "reward: 16.858228833241444 setps: 21 count: 214\n",
      "reward: 10.80623337920988 setps: 19 count: 233\n",
      "reward: 12.38667620817141 setps: 25 count: 258\n",
      "reward: 11.885757467085204 setps: 21 count: 279\n",
      "reward: 22.070235239724568 setps: 28 count: 307\n",
      "reward: 17.405744987432264 setps: 26 count: 333\n",
      "reward: 18.351273336460874 setps: 30 count: 363\n",
      "reward: 13.91879768041399 setps: 25 count: 388\n",
      "reward: 10.474553984338126 setps: 25 count: 413\n",
      "reward: 12.9194189807924 setps: 24 count: 437\n",
      "reward: 17.602146674079993 setps: 25 count: 462\n",
      "reward: 10.892647603082878 setps: 26 count: 488\n",
      "reward: 17.03992359768599 setps: 26 count: 514\n",
      "reward: 14.316133474504749 setps: 25 count: 539\n",
      "reward: 22.952348108206937 setps: 30 count: 569\n",
      "reward: 20.390274588088506 setps: 28 count: 597\n",
      "reward: 21.17866187544132 setps: 25 count: 622\n",
      "reward: 10.87059794521192 setps: 18 count: 640\n",
      "reward: 15.185862899810308 setps: 24 count: 664\n",
      "reward: 19.65542553897976 setps: 29 count: 693\n",
      "reward: 12.251269565241818 setps: 24 count: 717\n",
      "reward: 16.308072839082158 setps: 25 count: 742\n",
      "reward: 14.45421570913604 setps: 25 count: 767\n",
      "reward: 13.585842209773546 setps: 20 count: 787\n",
      "reward: 15.585355641476049 setps: 26 count: 813\n",
      "reward: 12.119161323002483 setps: 24 count: 837\n",
      "reward: 13.560185009990526 setps: 26 count: 863\n",
      "reward: 14.512313022458695 setps: 26 count: 889\n",
      "reward: 12.337597878689119 setps: 25 count: 914\n",
      "reward: 15.440292678122933 setps: 24 count: 938\n",
      "reward: 17.228685323192618 setps: 25 count: 963\n",
      "reward: 18.355663956028003 setps: 32 count: 995\n",
      "avg rewards: 15.502785595883726\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.42975\n",
      "Epoch:20 Batch:4 Loss:0.27535\n",
      "Epoch:40 Batch:4 Loss:0.20664\n",
      "Epoch:60 Batch:4 Loss:0.14681\n",
      "Epoch:80 Batch:4 Loss:0.13200\n",
      "Epoch:100 Batch:4 Loss:0.11335\n",
      "Epoch:120 Batch:4 Loss:0.09705\n",
      "Epoch:140 Batch:4 Loss:0.09068\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.103\n",
      "Epoch:10 Batch:8 Loss:0.101\n",
      "Epoch:20 Batch:8 Loss:0.099\n",
      "Epoch:30 Batch:8 Loss:0.100\n",
      "Epoch:40 Batch:8 Loss:0.099\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 29.633391322166425 setps: 43 count: 43\n",
      "reward: 4.1771826886149945 setps: 18 count: 61\n",
      "reward: 3.200296226544014 setps: 32 count: 93\n",
      "reward: 5.014903869685073 setps: 25 count: 118\n",
      "reward: 2.3614642404514585 setps: 43 count: 161\n",
      "reward: 7.2746428739206745 setps: 24 count: 185\n",
      "reward: 5.285803189102443 setps: 36 count: 221\n",
      "reward: 10.029466855106877 setps: 34 count: 255\n",
      "reward: 4.555768480343977 setps: 26 count: 281\n",
      "reward: -2.6554724974630535 setps: 33 count: 314\n",
      "reward: 5.471034939236416 setps: 19 count: 333\n",
      "reward: 3.953818857733857 setps: 17 count: 350\n",
      "reward: 5.6791483672001055 setps: 29 count: 379\n",
      "reward: -1.9690590995989634 setps: 24 count: 403\n",
      "reward: 9.101256398514671 setps: 18 count: 421\n",
      "reward: 1.2378256235126162 setps: 26 count: 447\n",
      "reward: 4.425730443465001 setps: 35 count: 482\n",
      "reward: 6.932444760530778 setps: 36 count: 518\n",
      "reward: 0.6511583474377409 setps: 17 count: 535\n",
      "reward: 4.854107335093429 setps: 26 count: 561\n",
      "reward: 7.117469760240057 setps: 17 count: 578\n",
      "reward: -2.97947628015827 setps: 25 count: 603\n",
      "reward: 15.800205362224366 setps: 35 count: 638\n",
      "reward: -0.4191556908466727 setps: 25 count: 663\n",
      "reward: 3.2672448338533275 setps: 39 count: 702\n",
      "reward: -4.282207960952656 setps: 37 count: 739\n",
      "reward: 4.534783809001965 setps: 22 count: 761\n",
      "reward: 30.28871906350978 setps: 44 count: 805\n",
      "reward: 8.3422519438318 setps: 29 count: 834\n",
      "reward: 8.673060419903774 setps: 19 count: 853\n",
      "reward: 6.066015505760152 setps: 17 count: 870\n",
      "reward: 23.99031934477825 setps: 50 count: 920\n",
      "reward: -3.891464440400891 setps: 41 count: 961\n",
      "reward: 6.633118265462686 setps: 18 count: 979\n",
      "avg rewards: 6.245758739935476\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.42592\n",
      "Epoch:20 Batch:5 Loss:0.24928\n",
      "Epoch:40 Batch:5 Loss:0.16756\n",
      "Epoch:60 Batch:5 Loss:0.13074\n",
      "Epoch:80 Batch:5 Loss:0.10879\n",
      "Epoch:100 Batch:5 Loss:0.09637\n",
      "Epoch:120 Batch:5 Loss:0.08934\n",
      "Epoch:140 Batch:5 Loss:0.08513\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.089\n",
      "Epoch:10 Batch:8 Loss:0.080\n",
      "Epoch:20 Batch:8 Loss:0.078\n",
      "Epoch:30 Batch:8 Loss:0.078\n",
      "Epoch:40 Batch:8 Loss:0.078\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 17.992870608755034 setps: 27 count: 27\n",
      "reward: 20.615694930372413 setps: 37 count: 64\n",
      "reward: 27.02553048533882 setps: 30 count: 94\n",
      "reward: 11.39566394007415 setps: 16 count: 110\n",
      "reward: 10.286911839310775 setps: 19 count: 129\n",
      "reward: 23.88059209060885 setps: 31 count: 160\n",
      "reward: 24.3715119680288 setps: 36 count: 196\n",
      "reward: 18.83742483956593 setps: 30 count: 226\n",
      "reward: 22.348501901612323 setps: 29 count: 255\n",
      "reward: 16.550595894761507 setps: 28 count: 283\n",
      "reward: 25.408388699934584 setps: 29 count: 312\n",
      "reward: 20.309675514725676 setps: 29 count: 341\n",
      "reward: 12.685060442834219 setps: 20 count: 361\n",
      "reward: 23.41986737514671 setps: 31 count: 392\n",
      "reward: 13.09333866691886 setps: 17 count: 409\n",
      "reward: 8.463868488058505 setps: 18 count: 427\n",
      "reward: 36.054711335127635 setps: 57 count: 484\n",
      "reward: 20.56335998315771 setps: 29 count: 513\n",
      "reward: 32.90386178763293 setps: 34 count: 547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 28.931252892826155 setps: 37 count: 584\n",
      "reward: 46.728690080359236 setps: 56 count: 640\n",
      "reward: 24.67142155315087 setps: 37 count: 677\n",
      "reward: 16.335512013225525 setps: 25 count: 702\n",
      "reward: 17.20215156794002 setps: 20 count: 722\n",
      "reward: 19.76297554571211 setps: 28 count: 750\n",
      "reward: 20.67688871511201 setps: 30 count: 780\n",
      "reward: 20.593665361258896 setps: 29 count: 809\n",
      "reward: 33.693876699463004 setps: 46 count: 855\n",
      "reward: 19.960533190175195 setps: 29 count: 884\n",
      "reward: 15.606424757276544 setps: 28 count: 912\n",
      "reward: 28.91766429117415 setps: 35 count: 947\n",
      "reward: 20.11847374063509 setps: 33 count: 980\n",
      "avg rewards: 21.85646753750857\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.41974\n",
      "Epoch:20 Batch:6 Loss:0.20984\n",
      "Epoch:40 Batch:6 Loss:0.14790\n",
      "Epoch:60 Batch:6 Loss:0.11234\n",
      "Epoch:80 Batch:6 Loss:0.09817\n",
      "Epoch:100 Batch:6 Loss:0.08388\n",
      "Epoch:120 Batch:6 Loss:0.07410\n",
      "Epoch:140 Batch:6 Loss:0.06858\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.074\n",
      "Epoch:10 Batch:8 Loss:0.073\n",
      "Epoch:20 Batch:8 Loss:0.073\n",
      "Epoch:30 Batch:8 Loss:0.072\n",
      "Epoch:40 Batch:8 Loss:0.071\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 9.616426702123135 setps: 19 count: 19\n",
      "reward: 41.35631770729088 setps: 44 count: 63\n",
      "reward: 28.392214907826567 setps: 30 count: 93\n",
      "reward: 23.597201995366778 setps: 32 count: 125\n",
      "reward: 12.043128560914193 setps: 19 count: 144\n",
      "reward: 29.892668866242452 setps: 36 count: 180\n",
      "reward: 18.434511363615453 setps: 37 count: 217\n",
      "reward: 17.02472114971752 setps: 35 count: 252\n",
      "reward: 24.445709425104738 setps: 33 count: 285\n",
      "reward: 36.583744233324246 setps: 36 count: 321\n",
      "reward: 20.212810439988967 setps: 22 count: 343\n",
      "reward: 25.093913205547143 setps: 31 count: 374\n",
      "reward: 33.084118523600054 setps: 40 count: 414\n",
      "reward: 11.664765737173727 setps: 17 count: 431\n",
      "reward: 28.285720582952493 setps: 30 count: 461\n",
      "reward: 27.865144405997118 setps: 37 count: 498\n",
      "reward: 24.529009066267466 setps: 33 count: 531\n",
      "reward: 30.677458274812672 setps: 37 count: 568\n",
      "reward: 34.804416706739 setps: 38 count: 606\n",
      "reward: 18.39110552585043 setps: 34 count: 640\n",
      "reward: 47.24469289832923 setps: 44 count: 684\n",
      "reward: 32.95603477497062 setps: 46 count: 730\n",
      "reward: 22.391559892377703 setps: 24 count: 754\n",
      "reward: -4.25961704645888 setps: 12 count: 766\n",
      "reward: 20.185442799204615 setps: 38 count: 804\n",
      "reward: 41.94431437525345 setps: 39 count: 843\n",
      "reward: 24.017489628485052 setps: 32 count: 875\n",
      "reward: 37.202907132073605 setps: 38 count: 913\n",
      "reward: 12.90794279671536 setps: 17 count: 930\n",
      "reward: 28.42950257770427 setps: 43 count: 973\n",
      "avg rewards: 25.300512573637\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.41740\n",
      "Epoch:20 Batch:7 Loss:0.19757\n",
      "Epoch:40 Batch:7 Loss:0.13660\n",
      "Epoch:60 Batch:7 Loss:0.10276\n",
      "Epoch:80 Batch:7 Loss:0.09347\n",
      "Epoch:100 Batch:7 Loss:0.07847\n",
      "Epoch:120 Batch:7 Loss:0.06468\n",
      "Epoch:140 Batch:7 Loss:0.06518\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.069\n",
      "Epoch:10 Batch:8 Loss:0.067\n",
      "Epoch:20 Batch:8 Loss:0.068\n",
      "Epoch:30 Batch:8 Loss:0.067\n",
      "Epoch:40 Batch:8 Loss:0.066\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 22.22883523202036 setps: 26 count: 26\n",
      "reward: 19.384045204732686 setps: 26 count: 52\n",
      "reward: 23.40804607104365 setps: 28 count: 80\n",
      "reward: 22.736818796109574 setps: 27 count: 107\n",
      "reward: 17.35006868461059 setps: 22 count: 129\n",
      "reward: 23.673280465131384 setps: 27 count: 156\n",
      "reward: 20.499359606810323 setps: 26 count: 182\n",
      "reward: 24.94339807776123 setps: 27 count: 209\n",
      "reward: 22.417042330831464 setps: 25 count: 234\n",
      "reward: 19.00680808959005 setps: 25 count: 259\n",
      "reward: 24.234832961826758 setps: 27 count: 286\n",
      "reward: 19.652622744937247 setps: 25 count: 311\n",
      "reward: 22.429306979317342 setps: 27 count: 338\n",
      "reward: 20.89246361440746 setps: 25 count: 363\n",
      "reward: 16.221875802552677 setps: 27 count: 390\n",
      "reward: 26.62460051059315 setps: 28 count: 418\n",
      "reward: 4.004191048229404 setps: 16 count: 434\n",
      "reward: 6.424362557500719 setps: 19 count: 453\n",
      "reward: 21.779151434953388 setps: 26 count: 479\n",
      "reward: 32.6351885510623 setps: 33 count: 512\n",
      "reward: 21.462343548281932 setps: 29 count: 541\n",
      "reward: 24.338481596067144 setps: 27 count: 568\n",
      "reward: 20.045048295491142 setps: 24 count: 592\n",
      "reward: 15.217080448925843 setps: 18 count: 610\n",
      "reward: 21.49401099124371 setps: 29 count: 639\n",
      "reward: 21.278642080682033 setps: 26 count: 665\n",
      "reward: 26.02477546781302 setps: 29 count: 694\n",
      "reward: 28.083929321401225 setps: 30 count: 724\n",
      "reward: 26.4979866965412 setps: 29 count: 753\n",
      "reward: 22.030805796060303 setps: 27 count: 780\n",
      "reward: 15.949738567406893 setps: 19 count: 799\n",
      "reward: 25.488273526872224 setps: 28 count: 827\n",
      "reward: 22.647261191727015 setps: 28 count: 855\n",
      "reward: 21.462514314826688 setps: 27 count: 882\n",
      "reward: 23.405000640529032 setps: 26 count: 908\n",
      "reward: 22.28903558622987 setps: 26 count: 934\n",
      "reward: 25.32870383574773 setps: 26 count: 960\n",
      "reward: 22.330703657708362 setps: 25 count: 985\n",
      "avg rewards: 21.471595640199396\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.40101\n",
      "Epoch:20 Batch:8 Loss:0.17812\n",
      "Epoch:40 Batch:8 Loss:0.12542\n",
      "Epoch:60 Batch:8 Loss:0.09366\n",
      "Epoch:80 Batch:8 Loss:0.08230\n",
      "Epoch:100 Batch:8 Loss:0.07260\n",
      "Epoch:120 Batch:8 Loss:0.06485\n",
      "Epoch:140 Batch:8 Loss:0.06039\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.068\n",
      "Epoch:10 Batch:8 Loss:0.065\n",
      "Epoch:20 Batch:8 Loss:0.065\n",
      "Epoch:30 Batch:8 Loss:0.064\n",
      "Epoch:40 Batch:8 Loss:0.066\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 32.83797500700167 setps: 30 count: 30\n",
      "reward: 30.464938198178423 setps: 29 count: 59\n",
      "reward: 27.985095903601902 setps: 28 count: 87\n",
      "reward: 31.24028085989412 setps: 31 count: 118\n",
      "reward: 26.333527744532333 setps: 28 count: 146\n",
      "reward: 31.113181160922977 setps: 28 count: 174\n",
      "reward: 28.017095576768046 setps: 28 count: 202\n",
      "reward: 24.253089518658815 setps: 27 count: 229\n",
      "reward: 18.79898152069509 setps: 20 count: 249\n",
      "reward: 29.15138090717228 setps: 27 count: 276\n",
      "reward: 21.81912699249806 setps: 21 count: 297\n",
      "reward: 34.73549950352026 setps: 30 count: 327\n",
      "reward: 27.225781982239276 setps: 26 count: 353\n",
      "reward: 27.84650711251452 setps: 27 count: 380\n",
      "reward: 29.63575253392192 setps: 27 count: 407\n",
      "reward: 34.66626149084914 setps: 32 count: 439\n",
      "reward: 34.36015159465751 setps: 32 count: 471\n",
      "reward: 27.214530950871996 setps: 28 count: 499\n",
      "reward: 28.590678337313875 setps: 28 count: 527\n",
      "reward: 28.365187533896822 setps: 27 count: 554\n",
      "reward: 35.35437339914934 setps: 30 count: 584\n",
      "reward: 31.961213342778507 setps: 28 count: 612\n",
      "reward: 26.515499671950234 setps: 26 count: 638\n",
      "reward: 31.22995805770333 setps: 30 count: 668\n",
      "reward: 28.6418197324936 setps: 28 count: 696\n",
      "reward: 28.99836701985041 setps: 27 count: 723\n",
      "reward: 27.147137804981316 setps: 27 count: 750\n",
      "reward: 29.918157628022897 setps: 27 count: 777\n",
      "reward: 26.6874962291724 setps: 26 count: 803\n",
      "reward: 33.96688341372792 setps: 30 count: 833\n",
      "reward: 29.778112440941914 setps: 30 count: 863\n",
      "reward: 32.84100448128011 setps: 27 count: 890\n",
      "reward: 27.825475591837318 setps: 27 count: 917\n",
      "reward: 29.381541610106066 setps: 28 count: 945\n",
      "reward: 32.097979517285424 setps: 37 count: 982\n",
      "avg rewards: 29.342858410599703\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.39828\n",
      "Epoch:20 Batch:9 Loss:0.16205\n",
      "Epoch:40 Batch:9 Loss:0.10157\n",
      "Epoch:60 Batch:9 Loss:0.08026\n",
      "Epoch:80 Batch:9 Loss:0.06658\n",
      "Epoch:100 Batch:9 Loss:0.06676\n",
      "Epoch:120 Batch:9 Loss:0.05915\n",
      "Epoch:140 Batch:9 Loss:0.06184\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.064\n",
      "Epoch:10 Batch:8 Loss:0.059\n",
      "Epoch:20 Batch:8 Loss:0.061\n",
      "Epoch:30 Batch:8 Loss:0.060\n",
      "Epoch:40 Batch:8 Loss:0.060\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 31.575171017952382 setps: 29 count: 29\n",
      "reward: 31.348850532261718 setps: 27 count: 56\n",
      "reward: 31.311781044643435 setps: 29 count: 85\n",
      "reward: 28.431015001358166 setps: 28 count: 113\n",
      "reward: 37.54783592485182 setps: 32 count: 145\n",
      "reward: 7.806219123238408 setps: 17 count: 162\n",
      "reward: 34.23270992104954 setps: 29 count: 191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 26.193055598463978 setps: 26 count: 217\n",
      "reward: 41.99776807556482 setps: 37 count: 254\n",
      "reward: 29.038870842053438 setps: 26 count: 280\n",
      "reward: 26.79602828144562 setps: 27 count: 307\n",
      "reward: 23.078521765186455 setps: 23 count: 330\n",
      "reward: 36.84604410501197 setps: 34 count: 364\n",
      "reward: 28.36499288809136 setps: 27 count: 391\n",
      "reward: 30.552716219262216 setps: 26 count: 417\n",
      "reward: 36.96875865286566 setps: 38 count: 455\n",
      "reward: 29.412829955140477 setps: 27 count: 482\n",
      "reward: 37.112800207884 setps: 34 count: 516\n",
      "reward: 27.037200356618264 setps: 27 count: 543\n",
      "reward: 31.386892298763264 setps: 31 count: 574\n",
      "reward: 29.945439877895122 setps: 28 count: 602\n",
      "reward: 30.98080915710016 setps: 28 count: 630\n",
      "reward: 30.768015499222383 setps: 35 count: 665\n",
      "reward: 44.76832263731776 setps: 35 count: 700\n",
      "reward: 36.02113052909554 setps: 31 count: 731\n",
      "reward: 29.976864052379096 setps: 27 count: 758\n",
      "reward: 30.715403089211026 setps: 27 count: 785\n",
      "reward: 40.21427932457882 setps: 33 count: 818\n",
      "reward: 33.09947647009248 setps: 30 count: 848\n",
      "reward: 17.176164297248764 setps: 20 count: 868\n",
      "reward: 28.921665981200935 setps: 29 count: 897\n",
      "reward: 39.22147444602597 setps: 34 count: 931\n",
      "reward: 61.90484268623403 setps: 45 count: 976\n",
      "avg rewards: 32.14405908664573\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.37600\n",
      "Epoch:20 Batch:10 Loss:0.14986\n",
      "Epoch:40 Batch:10 Loss:0.10210\n",
      "Epoch:60 Batch:10 Loss:0.07531\n",
      "Epoch:80 Batch:10 Loss:0.06764\n",
      "Epoch:100 Batch:10 Loss:0.06075\n",
      "Epoch:120 Batch:10 Loss:0.05319\n",
      "Epoch:140 Batch:10 Loss:0.06084\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.063\n",
      "Epoch:10 Batch:8 Loss:0.062\n",
      "Epoch:20 Batch:8 Loss:0.061\n",
      "Epoch:30 Batch:8 Loss:0.061\n",
      "Epoch:40 Batch:8 Loss:0.061\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 25.60465732117008 setps: 27 count: 27\n",
      "reward: 23.53137650285789 setps: 25 count: 52\n",
      "reward: 20.97908510188427 setps: 27 count: 79\n",
      "reward: 28.794785416944077 setps: 27 count: 106\n",
      "reward: 23.25343327827432 setps: 24 count: 130\n",
      "reward: 28.109351988737764 setps: 27 count: 157\n",
      "reward: 23.867623005904896 setps: 27 count: 184\n",
      "reward: 22.785295361705366 setps: 26 count: 210\n",
      "reward: 25.497896430258688 setps: 27 count: 237\n",
      "reward: 27.19613302532379 setps: 30 count: 267\n",
      "reward: 18.770063982081773 setps: 20 count: 287\n",
      "reward: 24.55336433193443 setps: 25 count: 312\n",
      "reward: 22.357765275318524 setps: 30 count: 342\n",
      "reward: 23.588779507824807 setps: 25 count: 367\n",
      "reward: 26.34761389025953 setps: 28 count: 395\n",
      "reward: 15.937313914303376 setps: 18 count: 413\n",
      "reward: 27.146295604098125 setps: 25 count: 438\n",
      "reward: 25.306542000589246 setps: 25 count: 463\n",
      "reward: 31.509291281813066 setps: 33 count: 496\n",
      "reward: 27.59989658498089 setps: 27 count: 523\n",
      "reward: 25.336654478791754 setps: 25 count: 548\n",
      "reward: 21.406850817920347 setps: 20 count: 568\n",
      "reward: 21.969110851200824 setps: 22 count: 590\n",
      "reward: 22.386336654536716 setps: 22 count: 612\n",
      "reward: 26.301297090924347 setps: 27 count: 639\n",
      "reward: 22.70214735140035 setps: 25 count: 664\n",
      "reward: 22.7246512760481 setps: 26 count: 690\n",
      "reward: 23.81843455728813 setps: 26 count: 716\n",
      "reward: 18.60073562651523 setps: 19 count: 735\n",
      "reward: 24.912454323798016 setps: 28 count: 763\n",
      "reward: 22.944746082890195 setps: 28 count: 791\n",
      "reward: 24.81861496610218 setps: 25 count: 816\n",
      "reward: 27.637465558726397 setps: 29 count: 845\n",
      "reward: 23.542051848662965 setps: 25 count: 870\n",
      "reward: 27.094746089114054 setps: 26 count: 896\n",
      "reward: 30.851088246153083 setps: 27 count: 923\n",
      "reward: 25.88196415048588 setps: 26 count: 949\n",
      "reward: 26.493035195260013 setps: 25 count: 974\n",
      "avg rewards: 24.53049865716009\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.36817\n",
      "Epoch:20 Batch:11 Loss:0.13661\n",
      "Epoch:40 Batch:11 Loss:0.09013\n",
      "Epoch:60 Batch:11 Loss:0.07773\n",
      "Epoch:80 Batch:11 Loss:0.06145\n",
      "Epoch:100 Batch:11 Loss:0.05724\n",
      "Epoch:120 Batch:11 Loss:0.05141\n",
      "Epoch:140 Batch:11 Loss:0.04817\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.062\n",
      "Epoch:10 Batch:8 Loss:0.059\n",
      "Epoch:20 Batch:8 Loss:0.059\n",
      "Epoch:30 Batch:8 Loss:0.059\n",
      "Epoch:40 Batch:8 Loss:0.060\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 29.89777163117978 setps: 28 count: 28\n",
      "reward: 28.916908896474343 setps: 27 count: 55\n",
      "reward: 26.129474121132805 setps: 26 count: 81\n",
      "reward: 27.98637482257618 setps: 27 count: 108\n",
      "reward: 27.106992259313113 setps: 27 count: 135\n",
      "reward: 26.03626838561759 setps: 26 count: 161\n",
      "reward: 29.98130007387809 setps: 29 count: 190\n",
      "reward: 26.395929250847256 setps: 27 count: 217\n",
      "reward: 32.485530975714205 setps: 32 count: 249\n",
      "reward: 32.34935798714578 setps: 29 count: 278\n",
      "reward: 32.09291279373719 setps: 31 count: 309\n",
      "reward: 28.58813178374549 setps: 26 count: 335\n",
      "reward: 29.97046780098317 setps: 26 count: 361\n",
      "reward: 36.08729818105639 setps: 35 count: 396\n",
      "reward: 26.89321622505959 setps: 27 count: 423\n",
      "reward: 26.12208453005151 setps: 27 count: 450\n",
      "reward: 19.537631180038442 setps: 20 count: 470\n",
      "reward: 19.252792685681197 setps: 20 count: 490\n",
      "reward: 25.589505291395465 setps: 26 count: 516\n",
      "reward: 19.03223112165142 setps: 20 count: 536\n",
      "reward: 30.47215450230287 setps: 27 count: 563\n",
      "reward: 28.031996815334423 setps: 26 count: 589\n",
      "reward: 28.552303444412246 setps: 27 count: 616\n",
      "reward: 21.932536106689078 setps: 22 count: 638\n",
      "reward: 30.194078642550448 setps: 28 count: 666\n",
      "reward: 27.481040397519244 setps: 29 count: 695\n",
      "reward: 17.43070516498119 setps: 19 count: 714\n",
      "reward: 10.589847862361056 setps: 19 count: 733\n",
      "reward: 30.87012239799806 setps: 31 count: 764\n",
      "reward: 26.012076573660305 setps: 25 count: 789\n",
      "reward: 37.044388114294264 setps: 31 count: 820\n",
      "reward: 25.39249974224949 setps: 26 count: 846\n",
      "reward: 36.79800927251199 setps: 30 count: 876\n",
      "reward: 37.10904385520699 setps: 35 count: 911\n",
      "reward: 20.088545117208554 setps: 20 count: 931\n",
      "reward: 15.47143481628882 setps: 17 count: 948\n",
      "reward: 13.332461234850047 setps: 18 count: 966\n",
      "reward: 29.279349222857853 setps: 26 count: 992\n",
      "avg rewards: 26.750915086330423\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.37707\n",
      "Epoch:20 Batch:12 Loss:0.13179\n",
      "Epoch:40 Batch:12 Loss:0.07632\n",
      "Epoch:60 Batch:12 Loss:0.06978\n",
      "Epoch:80 Batch:12 Loss:0.05903\n",
      "Epoch:100 Batch:12 Loss:0.05472\n",
      "Epoch:120 Batch:12 Loss:0.05058\n",
      "Epoch:140 Batch:12 Loss:0.04625\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.059\n",
      "Epoch:10 Batch:8 Loss:0.056\n",
      "Epoch:20 Batch:8 Loss:0.058\n",
      "Epoch:30 Batch:8 Loss:0.058\n",
      "Epoch:40 Batch:8 Loss:0.056\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 25.99117224280635 setps: 26 count: 26\n",
      "reward: 26.987200104883232 setps: 26 count: 52\n",
      "reward: 16.637693718392985 setps: 19 count: 71\n",
      "reward: 25.213684907737477 setps: 28 count: 99\n",
      "reward: 29.205501572156212 setps: 27 count: 126\n",
      "reward: 27.839454733803 setps: 26 count: 152\n",
      "reward: 28.370840278455578 setps: 27 count: 179\n",
      "reward: 25.564058731566192 setps: 25 count: 204\n",
      "reward: 30.81109380449198 setps: 30 count: 234\n",
      "reward: 30.448068110040904 setps: 27 count: 261\n",
      "reward: 27.58788255696563 setps: 27 count: 288\n",
      "reward: 27.077471919174425 setps: 26 count: 314\n",
      "reward: 25.52229217843124 setps: 26 count: 340\n",
      "reward: 22.17998710476531 setps: 25 count: 365\n",
      "reward: 27.153911656871788 setps: 25 count: 390\n",
      "reward: 27.263863991142724 setps: 27 count: 417\n",
      "reward: 26.961991280278017 setps: 25 count: 442\n",
      "reward: 26.28387869338039 setps: 27 count: 469\n",
      "reward: 18.357876787809072 setps: 20 count: 489\n",
      "reward: 28.452548283799846 setps: 28 count: 517\n",
      "reward: 27.175360644684407 setps: 28 count: 545\n",
      "reward: 29.236765024121265 setps: 26 count: 571\n",
      "reward: 27.55024857762037 setps: 26 count: 597\n",
      "reward: 25.866417458814972 setps: 27 count: 624\n",
      "reward: 24.097090065834347 setps: 26 count: 650\n",
      "reward: 27.005661871604385 setps: 28 count: 678\n",
      "reward: 26.4570349697955 setps: 27 count: 705\n",
      "reward: 26.191612839426675 setps: 26 count: 731\n",
      "reward: 27.402125294475994 setps: 26 count: 757\n",
      "reward: 28.487297374987975 setps: 26 count: 783\n",
      "reward: 26.53682099652506 setps: 25 count: 808\n",
      "reward: 23.173219309988788 setps: 22 count: 830\n",
      "reward: 26.226211825615607 setps: 25 count: 855\n",
      "reward: 27.299751071551867 setps: 26 count: 881\n",
      "reward: 25.55885307793942 setps: 28 count: 909\n",
      "reward: 24.126516007081953 setps: 26 count: 935\n",
      "reward: 28.15676389366854 setps: 27 count: 962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 26.563471923478932 setps: 26 count: 988\n",
      "avg rewards: 26.342676181162325\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.34854\n",
      "Epoch:20 Batch:13 Loss:0.11892\n",
      "Epoch:40 Batch:13 Loss:0.07903\n",
      "Epoch:60 Batch:13 Loss:0.06326\n",
      "Epoch:80 Batch:13 Loss:0.05852\n",
      "Epoch:100 Batch:13 Loss:0.04999\n",
      "Epoch:120 Batch:13 Loss:0.04773\n",
      "Epoch:140 Batch:13 Loss:0.04810\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.061\n",
      "Epoch:10 Batch:8 Loss:0.058\n",
      "Epoch:20 Batch:8 Loss:0.058\n",
      "Epoch:30 Batch:8 Loss:0.058\n",
      "Epoch:40 Batch:8 Loss:0.057\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 25.537121398680025 setps: 27 count: 27\n",
      "reward: 28.78701032858662 setps: 26 count: 53\n",
      "reward: 25.677698359616617 setps: 26 count: 79\n",
      "reward: 20.458376804244473 setps: 22 count: 101\n",
      "reward: 26.594291933423662 setps: 26 count: 127\n",
      "reward: 24.44168694700929 setps: 27 count: 154\n",
      "reward: 27.129972199538322 setps: 29 count: 183\n",
      "reward: 25.518700159137367 setps: 25 count: 208\n",
      "reward: 17.778843375746504 setps: 19 count: 227\n",
      "reward: 22.973608180669544 setps: 25 count: 252\n",
      "reward: 23.124121587353873 setps: 24 count: 276\n",
      "reward: 29.972234975709576 setps: 30 count: 306\n",
      "reward: 25.177713197375116 setps: 26 count: 332\n",
      "reward: 29.8112310071112 setps: 26 count: 358\n",
      "reward: 29.983810875058406 setps: 28 count: 386\n",
      "reward: 27.178639900873527 setps: 28 count: 414\n",
      "reward: 24.329002339694128 setps: 27 count: 441\n",
      "reward: 26.011038044844454 setps: 25 count: 466\n",
      "reward: 26.097563365247332 setps: 27 count: 493\n",
      "reward: 22.110180503301667 setps: 24 count: 517\n",
      "reward: 16.42548424115084 setps: 18 count: 535\n",
      "reward: 27.680985898547807 setps: 26 count: 561\n",
      "reward: 26.521955465612695 setps: 26 count: 587\n",
      "reward: 22.589878246399167 setps: 26 count: 613\n",
      "reward: 25.681752101944582 setps: 25 count: 638\n",
      "reward: 24.432763797398362 setps: 25 count: 663\n",
      "reward: 23.39627875457082 setps: 25 count: 688\n",
      "reward: 19.12060711810628 setps: 22 count: 710\n",
      "reward: 24.512653373621287 setps: 25 count: 735\n",
      "reward: 24.254750861064533 setps: 26 count: 761\n",
      "reward: 27.9275387901318 setps: 26 count: 787\n",
      "reward: 26.569423040309637 setps: 26 count: 813\n",
      "reward: 25.1712605900655 setps: 26 count: 839\n",
      "reward: 23.26189862128522 setps: 24 count: 863\n",
      "reward: 23.840265987766905 setps: 25 count: 888\n",
      "reward: 27.46886746693635 setps: 27 count: 915\n",
      "reward: 28.060080724807634 setps: 26 count: 941\n",
      "reward: 24.4631468232561 setps: 26 count: 967\n",
      "reward: 23.072525294720247 setps: 24 count: 991\n",
      "avg rewards: 24.952434940536342\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.35172\n",
      "Epoch:20 Batch:14 Loss:0.10978\n",
      "Epoch:40 Batch:14 Loss:0.07659\n",
      "Epoch:60 Batch:14 Loss:0.06915\n",
      "Epoch:80 Batch:14 Loss:0.04997\n",
      "Epoch:100 Batch:14 Loss:0.05132\n",
      "Epoch:120 Batch:14 Loss:0.04365\n",
      "Epoch:140 Batch:14 Loss:0.04091\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.062\n",
      "Epoch:10 Batch:8 Loss:0.058\n",
      "Epoch:20 Batch:8 Loss:0.059\n",
      "Epoch:30 Batch:8 Loss:0.055\n",
      "Epoch:40 Batch:8 Loss:0.055\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 26.49916597120755 setps: 25 count: 25\n",
      "reward: 25.09506991193339 setps: 24 count: 49\n",
      "reward: 22.981864895782202 setps: 24 count: 73\n",
      "reward: 21.726910455597682 setps: 24 count: 97\n",
      "reward: 21.59015891358285 setps: 25 count: 122\n",
      "reward: 24.55862472332665 setps: 24 count: 146\n",
      "reward: 23.892681230860763 setps: 25 count: 171\n",
      "reward: 25.10070066232729 setps: 24 count: 195\n",
      "reward: 17.571695614764753 setps: 19 count: 214\n",
      "reward: 22.464276254427382 setps: 24 count: 238\n",
      "reward: 26.031665998921376 setps: 25 count: 263\n",
      "reward: 26.30024122810719 setps: 24 count: 287\n",
      "reward: 18.750533901341264 setps: 20 count: 307\n",
      "reward: 18.038165116732124 setps: 20 count: 327\n",
      "reward: 25.44290650150943 setps: 23 count: 350\n",
      "reward: 26.280408447219816 setps: 24 count: 374\n",
      "reward: 25.58985248906974 setps: 25 count: 399\n",
      "reward: 24.73589799969923 setps: 24 count: 423\n",
      "reward: 24.756865672560526 setps: 23 count: 446\n",
      "reward: 23.693476706143702 setps: 24 count: 470\n",
      "reward: 26.419361412743456 setps: 26 count: 496\n",
      "reward: 25.23205015405547 setps: 25 count: 521\n",
      "reward: 24.12737426909152 setps: 25 count: 546\n",
      "reward: 25.229635508873614 setps: 25 count: 571\n",
      "reward: 24.645701017386454 setps: 24 count: 595\n",
      "reward: 16.028687574533976 setps: 19 count: 614\n",
      "reward: 24.128939712543794 setps: 25 count: 639\n",
      "reward: 25.304868257667113 setps: 23 count: 662\n",
      "reward: 24.227943982079157 setps: 23 count: 685\n",
      "reward: 23.24193187356286 setps: 23 count: 708\n",
      "reward: 21.899074158401348 setps: 24 count: 732\n",
      "reward: 25.041124196043526 setps: 25 count: 757\n",
      "reward: 20.788671019808678 setps: 22 count: 779\n",
      "reward: 24.343452630302636 setps: 24 count: 803\n",
      "reward: 24.179886656021694 setps: 24 count: 827\n",
      "reward: 23.044468470441647 setps: 25 count: 852\n",
      "reward: 25.351462608580192 setps: 25 count: 877\n",
      "reward: 22.78240482795809 setps: 24 count: 901\n",
      "reward: 26.861046652612277 setps: 24 count: 925\n",
      "reward: 22.49909774707194 setps: 25 count: 950\n",
      "reward: 25.324406946421366 setps: 24 count: 974\n",
      "reward: 22.20847196227406 setps: 25 count: 999\n",
      "avg rewards: 23.66693391270452\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.35222\n",
      "Epoch:20 Batch:15 Loss:0.10017\n",
      "Epoch:40 Batch:15 Loss:0.07599\n",
      "Epoch:60 Batch:15 Loss:0.05691\n",
      "Epoch:80 Batch:15 Loss:0.05751\n",
      "Epoch:100 Batch:15 Loss:0.04582\n",
      "Epoch:120 Batch:15 Loss:0.04906\n",
      "Epoch:140 Batch:15 Loss:0.04197\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.060\n",
      "Epoch:10 Batch:8 Loss:0.057\n",
      "Epoch:20 Batch:8 Loss:0.058\n",
      "Epoch:30 Batch:8 Loss:0.058\n",
      "Epoch:40 Batch:8 Loss:0.057\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 30.87996561375476 setps: 26 count: 26\n",
      "reward: 27.34955822420015 setps: 24 count: 50\n",
      "reward: 29.25652087206399 setps: 26 count: 76\n",
      "reward: 28.61557874114951 setps: 26 count: 102\n",
      "reward: 31.279651539285265 setps: 27 count: 129\n",
      "reward: 27.973546225231253 setps: 29 count: 158\n",
      "reward: 29.820163135687476 setps: 24 count: 182\n",
      "reward: 29.84388431070984 setps: 25 count: 207\n",
      "reward: 31.656260756807754 setps: 29 count: 236\n",
      "reward: 31.46482295429159 setps: 27 count: 263\n",
      "reward: 28.843145947485752 setps: 27 count: 290\n",
      "reward: 30.435352554854767 setps: 26 count: 316\n",
      "reward: 29.82436935661681 setps: 26 count: 342\n",
      "reward: 28.624996800573722 setps: 26 count: 368\n",
      "reward: 28.736062804279207 setps: 26 count: 394\n",
      "reward: 21.344633499367045 setps: 21 count: 415\n",
      "reward: 24.371737861984002 setps: 22 count: 437\n",
      "reward: 32.457117016942355 setps: 29 count: 466\n",
      "reward: 29.351782870762687 setps: 26 count: 492\n",
      "reward: 27.698136942769633 setps: 26 count: 518\n",
      "reward: 33.00446611978842 setps: 28 count: 546\n",
      "reward: 21.06104168569727 setps: 20 count: 566\n",
      "reward: 26.424965955650258 setps: 24 count: 590\n",
      "reward: 30.32108468652586 setps: 28 count: 618\n",
      "reward: 31.50253640263254 setps: 25 count: 643\n",
      "reward: 31.837063344434135 setps: 28 count: 671\n",
      "reward: 31.13651634751441 setps: 25 count: 696\n",
      "reward: 30.391041044938895 setps: 26 count: 722\n",
      "reward: 29.682699112415136 setps: 26 count: 748\n",
      "reward: 28.042921776873122 setps: 28 count: 776\n",
      "reward: 31.663560627039995 setps: 29 count: 805\n",
      "reward: 36.71515273798286 setps: 30 count: 835\n",
      "reward: 19.865194324153713 setps: 20 count: 855\n",
      "reward: 29.092633054155158 setps: 29 count: 884\n",
      "reward: 28.396320779656527 setps: 26 count: 910\n",
      "reward: 28.770783487986776 setps: 27 count: 937\n",
      "reward: 23.924989993964843 setps: 22 count: 959\n",
      "reward: 28.50557783545374 setps: 27 count: 986\n",
      "avg rewards: 28.951732561728452\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.34245\n",
      "Epoch:20 Batch:16 Loss:0.08742\n",
      "Epoch:40 Batch:16 Loss:0.07263\n",
      "Epoch:60 Batch:16 Loss:0.05396\n",
      "Epoch:80 Batch:16 Loss:0.05303\n",
      "Epoch:100 Batch:16 Loss:0.04625\n",
      "Epoch:120 Batch:16 Loss:0.04286\n",
      "Epoch:140 Batch:16 Loss:0.04741\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.059\n",
      "Epoch:10 Batch:8 Loss:0.057\n",
      "Epoch:20 Batch:8 Loss:0.058\n",
      "Epoch:30 Batch:8 Loss:0.056\n",
      "Epoch:40 Batch:8 Loss:0.057\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 20.273782596230735 setps: 23 count: 23\n",
      "reward: 29.316185767568818 setps: 28 count: 51\n",
      "reward: 14.71190604146541 setps: 20 count: 71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 29.73836910034151 setps: 26 count: 97\n",
      "reward: 31.078957188763887 setps: 26 count: 123\n",
      "reward: 15.067704796150792 setps: 17 count: 140\n",
      "reward: 31.04083153184183 setps: 26 count: 166\n",
      "reward: 26.59364055939804 setps: 27 count: 193\n",
      "reward: 29.492662327084687 setps: 26 count: 219\n",
      "reward: 29.131313036811484 setps: 25 count: 244\n",
      "reward: 27.65379889065807 setps: 27 count: 271\n",
      "reward: 30.362355874583592 setps: 26 count: 297\n",
      "reward: 29.23539134405873 setps: 27 count: 324\n",
      "reward: 27.375329970625174 setps: 25 count: 349\n",
      "reward: 27.79669343632559 setps: 25 count: 374\n",
      "reward: 32.577274686939205 setps: 26 count: 400\n",
      "reward: 31.19455162579397 setps: 26 count: 426\n",
      "reward: 34.30643713022874 setps: 28 count: 454\n",
      "reward: 29.71192291749903 setps: 26 count: 480\n",
      "reward: 29.956654203978548 setps: 26 count: 506\n",
      "reward: 24.900596988553296 setps: 26 count: 532\n",
      "reward: 34.58334202683909 setps: 28 count: 560\n",
      "reward: 29.494946240179704 setps: 25 count: 585\n",
      "reward: 29.305506021332985 setps: 26 count: 611\n",
      "reward: 28.12102043165941 setps: 25 count: 636\n",
      "reward: 32.65227666979772 setps: 27 count: 663\n",
      "reward: 31.677924308217182 setps: 28 count: 691\n",
      "reward: 26.07862297152169 setps: 27 count: 718\n",
      "reward: 27.03785571633489 setps: 26 count: 744\n",
      "reward: 30.80009115237481 setps: 27 count: 771\n",
      "reward: 30.999098568961198 setps: 26 count: 797\n",
      "reward: 27.092855157981106 setps: 25 count: 822\n",
      "reward: 4.725802949942589 setps: 18 count: 840\n",
      "reward: 18.23120487773849 setps: 21 count: 861\n",
      "reward: 30.64858917880337 setps: 27 count: 888\n",
      "reward: 28.439529217238306 setps: 26 count: 914\n",
      "reward: 27.431061615419452 setps: 26 count: 940\n",
      "reward: 31.095115790086858 setps: 27 count: 967\n",
      "reward: 31.577880489858217 setps: 27 count: 994\n",
      "avg rewards: 27.731002138440726\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.34382\n",
      "Epoch:20 Batch:17 Loss:0.09847\n",
      "Epoch:40 Batch:17 Loss:0.06157\n",
      "Epoch:60 Batch:17 Loss:0.05280\n",
      "Epoch:80 Batch:17 Loss:0.05153\n",
      "Epoch:100 Batch:17 Loss:0.04423\n",
      "Epoch:120 Batch:17 Loss:0.03989\n",
      "Epoch:140 Batch:17 Loss:0.04047\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.057\n",
      "Epoch:10 Batch:8 Loss:0.056\n",
      "Epoch:20 Batch:8 Loss:0.054\n",
      "Epoch:30 Batch:8 Loss:0.056\n",
      "Epoch:40 Batch:8 Loss:0.055\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 24.416289755718026 setps: 23 count: 23\n",
      "reward: 6.539223100511298 setps: 18 count: 41\n",
      "reward: 26.688044318258463 setps: 24 count: 65\n",
      "reward: 25.198224503287925 setps: 27 count: 92\n",
      "reward: 26.2942169638045 setps: 25 count: 117\n",
      "reward: 27.93667795785295 setps: 26 count: 143\n",
      "reward: 29.74890664659907 setps: 27 count: 170\n",
      "reward: 29.2944060438007 setps: 27 count: 197\n",
      "reward: 25.121210847295874 setps: 24 count: 221\n",
      "reward: 31.891609091983984 setps: 28 count: 249\n",
      "reward: 31.354402900060816 setps: 30 count: 279\n",
      "reward: 32.922872737761644 setps: 27 count: 306\n",
      "reward: 20.410144088440575 setps: 22 count: 328\n",
      "reward: 30.82174131254288 setps: 29 count: 357\n",
      "reward: 29.657048128852324 setps: 27 count: 384\n",
      "reward: 30.65293733102881 setps: 26 count: 410\n",
      "reward: 30.821916850004346 setps: 28 count: 438\n",
      "reward: 15.468265558627895 setps: 19 count: 457\n",
      "reward: 30.622233254837923 setps: 29 count: 486\n",
      "reward: 16.463009777321712 setps: 20 count: 506\n",
      "reward: 35.002793908742014 setps: 32 count: 538\n",
      "reward: 26.678308892835044 setps: 26 count: 564\n",
      "reward: 19.31001368475117 setps: 22 count: 586\n",
      "reward: 28.89002254221269 setps: 24 count: 610\n",
      "reward: 27.101553502264146 setps: 28 count: 638\n",
      "reward: 29.942173116064808 setps: 27 count: 665\n",
      "reward: 30.19350385146536 setps: 27 count: 692\n",
      "reward: 28.935007753688836 setps: 26 count: 718\n",
      "reward: 33.880294944759214 setps: 29 count: 747\n",
      "reward: 26.169911383888397 setps: 25 count: 772\n",
      "reward: 13.661807470950588 setps: 19 count: 791\n",
      "reward: 32.87734410464472 setps: 31 count: 822\n",
      "reward: 24.246983607434956 setps: 23 count: 845\n",
      "reward: 19.284682439723102 setps: 20 count: 865\n",
      "reward: 27.957476271656915 setps: 27 count: 892\n",
      "reward: 29.251717546404684 setps: 28 count: 920\n",
      "reward: 31.35916236619087 setps: 31 count: 951\n",
      "reward: 27.890734509793397 setps: 28 count: 979\n",
      "avg rewards: 26.709391396475336\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.32999\n",
      "Epoch:20 Batch:18 Loss:0.08160\n",
      "Epoch:40 Batch:18 Loss:0.05681\n",
      "Epoch:60 Batch:18 Loss:0.04846\n",
      "Epoch:80 Batch:18 Loss:0.04256\n",
      "Epoch:100 Batch:18 Loss:0.04156\n",
      "Epoch:120 Batch:18 Loss:0.04367\n",
      "Epoch:140 Batch:18 Loss:0.03742\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.056\n",
      "Epoch:10 Batch:8 Loss:0.054\n",
      "Epoch:20 Batch:8 Loss:0.055\n",
      "Epoch:30 Batch:8 Loss:0.055\n",
      "Epoch:40 Batch:8 Loss:0.054\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 29.238385657084287 setps: 26 count: 26\n",
      "reward: 27.94186523016834 setps: 25 count: 51\n",
      "reward: 24.074451502457666 setps: 25 count: 76\n",
      "reward: 28.039606156547958 setps: 25 count: 101\n",
      "reward: 26.885739830014064 setps: 25 count: 126\n",
      "reward: 30.153244150917445 setps: 25 count: 151\n",
      "reward: 22.064127503473717 setps: 23 count: 174\n",
      "reward: 27.190637304660054 setps: 26 count: 200\n",
      "reward: 21.858166647914913 setps: 22 count: 222\n",
      "reward: 31.298269477716527 setps: 27 count: 249\n",
      "reward: 30.90089538527537 setps: 27 count: 276\n",
      "reward: 28.346181529854945 setps: 26 count: 302\n",
      "reward: 29.33282428721577 setps: 25 count: 327\n",
      "reward: 27.436713887768562 setps: 25 count: 352\n",
      "reward: 27.914666169584958 setps: 25 count: 377\n",
      "reward: 26.98011268096743 setps: 25 count: 402\n",
      "reward: 25.18599080102431 setps: 27 count: 429\n",
      "reward: 29.228058273204077 setps: 25 count: 454\n",
      "reward: 28.60545091719832 setps: 26 count: 480\n",
      "reward: 28.158324933996482 setps: 26 count: 506\n",
      "reward: 25.706558109787874 setps: 25 count: 531\n",
      "reward: 28.398812395140705 setps: 28 count: 559\n",
      "reward: 27.65602276337158 setps: 26 count: 585\n",
      "reward: 27.12732076522807 setps: 26 count: 611\n",
      "reward: 27.39957744198619 setps: 25 count: 636\n",
      "reward: 25.497489955059425 setps: 25 count: 661\n",
      "reward: 26.802776834470574 setps: 26 count: 687\n",
      "reward: 19.852335378088178 setps: 19 count: 706\n",
      "reward: 25.22081739143323 setps: 24 count: 730\n",
      "reward: 28.695618552104857 setps: 24 count: 754\n",
      "reward: 16.702917300252007 setps: 19 count: 773\n",
      "reward: 28.222327233232498 setps: 26 count: 799\n",
      "reward: 25.2216522222152 setps: 25 count: 824\n",
      "reward: 26.40266589199017 setps: 26 count: 850\n",
      "reward: 25.28089181205433 setps: 25 count: 875\n",
      "reward: 23.592803219128104 setps: 22 count: 897\n",
      "reward: 27.351157644350312 setps: 26 count: 923\n",
      "reward: 26.100363874800674 setps: 25 count: 948\n",
      "reward: 19.28547842003172 setps: 21 count: 969\n",
      "reward: 11.984605255931095 setps: 20 count: 989\n",
      "avg rewards: 26.08339761969255\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.32375\n",
      "Epoch:20 Batch:19 Loss:0.08894\n",
      "Epoch:40 Batch:19 Loss:0.05866\n",
      "Epoch:60 Batch:19 Loss:0.04577\n",
      "Epoch:80 Batch:19 Loss:0.04127\n",
      "Epoch:100 Batch:19 Loss:0.04113\n",
      "Epoch:120 Batch:19 Loss:0.04090\n",
      "Epoch:140 Batch:19 Loss:0.03765\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.059\n",
      "Epoch:10 Batch:8 Loss:0.055\n",
      "Epoch:20 Batch:8 Loss:0.054\n",
      "Epoch:30 Batch:8 Loss:0.054\n",
      "Epoch:40 Batch:8 Loss:0.055\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 75.27837430336221 setps: 48 count: 48\n",
      "reward: 25.38046209603489 setps: 33 count: 81\n",
      "reward: 41.01979433445958 setps: 39 count: 120\n",
      "reward: 30.447986987010516 setps: 39 count: 159\n",
      "reward: 26.15184401030274 setps: 38 count: 197\n",
      "reward: 61.882709755990085 setps: 45 count: 242\n",
      "reward: 34.60786074625212 setps: 34 count: 276\n",
      "reward: 27.97175711148738 setps: 25 count: 301\n",
      "reward: 54.90180042675784 setps: 43 count: 344\n",
      "reward: 37.046082909952375 setps: 34 count: 378\n",
      "reward: 23.08367277263605 setps: 28 count: 406\n",
      "reward: 39.746319689010846 setps: 48 count: 454\n",
      "reward: 23.215563335653858 setps: 31 count: 485\n",
      "reward: 22.02858483941818 setps: 33 count: 518\n",
      "reward: 24.44933002194011 setps: 23 count: 541\n",
      "reward: 36.911312771074876 setps: 37 count: 578\n",
      "reward: 29.145680909548542 setps: 40 count: 618\n",
      "reward: 31.6542592502141 setps: 40 count: 658\n",
      "reward: 23.56579482480738 setps: 24 count: 682\n",
      "reward: 27.395613863496692 setps: 29 count: 711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 24.74236774956371 setps: 24 count: 735\n",
      "reward: 21.70587762504001 setps: 29 count: 764\n",
      "reward: 28.706627843366 setps: 32 count: 796\n",
      "reward: 31.410659690012107 setps: 44 count: 840\n",
      "reward: 42.43795649726963 setps: 42 count: 882\n",
      "reward: 57.17547978615185 setps: 48 count: 930\n",
      "reward: 47.63866192849236 setps: 41 count: 971\n",
      "avg rewards: 35.17416429923356\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.31978\n",
      "Epoch:20 Batch:20 Loss:0.07835\n",
      "Epoch:40 Batch:20 Loss:0.05174\n",
      "Epoch:60 Batch:20 Loss:0.04645\n",
      "Epoch:80 Batch:20 Loss:0.04012\n",
      "Epoch:100 Batch:20 Loss:0.03968\n",
      "Epoch:120 Batch:20 Loss:0.03787\n",
      "Epoch:140 Batch:20 Loss:0.03561\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.056\n",
      "Epoch:10 Batch:8 Loss:0.054\n",
      "Epoch:20 Batch:8 Loss:0.054\n",
      "Epoch:30 Batch:8 Loss:0.055\n",
      "Epoch:40 Batch:8 Loss:0.054\n",
      "Done!\n",
      "############# start BipedalWalker-v3 training ###################\n",
      "(49,)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -99.0992617930056 setps: 76 count: 76\n",
      "reward: -103.89475929486287 setps: 92 count: 168\n",
      "reward: -99.33922472825057 setps: 72 count: 240\n",
      "avg rewards: -100.77774860537302\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:0.33888\n",
      "Epoch:20 Batch:1 Loss:0.15229\n",
      "Epoch:40 Batch:1 Loss:0.13021\n",
      "Epoch:60 Batch:1 Loss:0.12435\n",
      "Epoch:80 Batch:1 Loss:0.11863\n",
      "Epoch:100 Batch:1 Loss:0.11100\n",
      "Epoch:120 Batch:1 Loss:0.10187\n",
      "Epoch:140 Batch:1 Loss:0.09224\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.177\n",
      "Epoch:10 Batch:9 Loss:0.171\n",
      "Epoch:20 Batch:9 Loss:0.162\n",
      "Epoch:30 Batch:9 Loss:0.160\n",
      "Epoch:40 Batch:9 Loss:0.157\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -116.12113833469029 setps: 70 count: 70\n",
      "reward: -120.11243168659323 setps: 77 count: 147\n",
      "reward: -115.91556641031988 setps: 71 count: 218\n",
      "reward: -117.69006497060943 setps: 69 count: 287\n",
      "reward: -119.1681972461107 setps: 79 count: 366\n",
      "reward: -117.32688777978532 setps: 72 count: 438\n",
      "reward: -116.1505166271745 setps: 70 count: 508\n",
      "reward: -116.41728524675965 setps: 71 count: 579\n",
      "reward: -116.20082164488423 setps: 69 count: 648\n",
      "reward: -118.65864942131999 setps: 78 count: 726\n",
      "reward: -116.17861932208886 setps: 69 count: 795\n",
      "reward: -116.26907203990221 setps: 72 count: 867\n",
      "reward: -121.54297863360185 setps: 79 count: 946\n",
      "avg rewards: -117.51940225875695\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:0.36464\n",
      "Epoch:20 Batch:2 Loss:0.16703\n",
      "Epoch:40 Batch:2 Loss:0.10787\n",
      "Epoch:60 Batch:2 Loss:0.07456\n",
      "Epoch:80 Batch:2 Loss:0.06418\n",
      "Epoch:100 Batch:2 Loss:0.05697\n",
      "Epoch:120 Batch:2 Loss:0.05330\n",
      "Epoch:140 Batch:2 Loss:0.04600\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.126\n",
      "Epoch:10 Batch:9 Loss:0.112\n",
      "Epoch:20 Batch:9 Loss:0.108\n",
      "Epoch:30 Batch:9 Loss:0.113\n",
      "Epoch:40 Batch:9 Loss:0.110\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -110.43948284386191 setps: 166 count: 166\n",
      "reward: -113.73521847034866 setps: 191 count: 357\n",
      "reward: -135.6722210609292 setps: 302 count: 659\n",
      "reward: -117.61173159490029 setps: 151 count: 810\n",
      "reward: -120.47352872259853 setps: 129 count: 939\n",
      "avg rewards: -119.5864365385277\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.35983\n",
      "Epoch:20 Batch:3 Loss:0.12486\n",
      "Epoch:40 Batch:3 Loss:0.08618\n",
      "Epoch:60 Batch:3 Loss:0.07017\n",
      "Epoch:80 Batch:3 Loss:0.05466\n",
      "Epoch:100 Batch:3 Loss:0.04698\n",
      "Epoch:120 Batch:3 Loss:0.04094\n",
      "Epoch:140 Batch:3 Loss:0.03864\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.096\n",
      "Epoch:10 Batch:9 Loss:0.087\n",
      "Epoch:20 Batch:9 Loss:0.088\n",
      "Epoch:30 Batch:9 Loss:0.089\n",
      "Epoch:40 Batch:9 Loss:0.084\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -90.24672037652198 setps: 136 count: 136\n",
      "reward: -97.73753907504896 setps: 132 count: 268\n",
      "reward: -107.08450454984016 setps: 119 count: 387\n",
      "reward: -102.86160006242059 setps: 246 count: 633\n",
      "reward: -102.21884950961359 setps: 156 count: 789\n",
      "avg rewards: -100.02984271468907\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.35652\n",
      "Epoch:20 Batch:4 Loss:0.11286\n",
      "Epoch:40 Batch:4 Loss:0.07045\n",
      "Epoch:60 Batch:4 Loss:0.05442\n",
      "Epoch:80 Batch:4 Loss:0.04608\n",
      "Epoch:100 Batch:4 Loss:0.04403\n",
      "Epoch:120 Batch:4 Loss:0.03706\n",
      "Epoch:140 Batch:4 Loss:0.03613\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.082\n",
      "Epoch:10 Batch:9 Loss:0.081\n",
      "Epoch:20 Batch:9 Loss:0.080\n",
      "Epoch:30 Batch:9 Loss:0.078\n",
      "Epoch:40 Batch:9 Loss:0.078\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -109.55975268959875 setps: 58 count: 58\n",
      "reward: -110.642543114687 setps: 59 count: 117\n",
      "reward: -109.75107433170825 setps: 57 count: 174\n",
      "reward: -109.76742372562984 setps: 59 count: 233\n",
      "avg rewards: -109.93019846540597\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.33783\n",
      "Epoch:20 Batch:5 Loss:0.08318\n",
      "Epoch:40 Batch:5 Loss:0.05588\n",
      "Epoch:60 Batch:5 Loss:0.04437\n",
      "Epoch:80 Batch:5 Loss:0.03641\n",
      "Epoch:100 Batch:5 Loss:0.03301\n",
      "Epoch:120 Batch:5 Loss:0.02960\n",
      "Epoch:140 Batch:5 Loss:0.02900\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.069\n",
      "Epoch:10 Batch:9 Loss:0.065\n",
      "Epoch:20 Batch:9 Loss:0.064\n",
      "Epoch:30 Batch:9 Loss:0.061\n",
      "Epoch:40 Batch:9 Loss:0.063\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -124.98405443702762 setps: 100 count: 100\n",
      "reward: -111.77616203828902 setps: 59 count: 159\n",
      "reward: -120.99691188042269 setps: 82 count: 241\n",
      "reward: -119.80951673323474 setps: 78 count: 319\n",
      "reward: -123.55095679566016 setps: 73 count: 392\n",
      "reward: -111.93820264211111 setps: 63 count: 455\n",
      "reward: -122.17250944779255 setps: 84 count: 539\n",
      "reward: -126.13046467408537 setps: 75 count: 614\n",
      "reward: -112.7462649488002 setps: 57 count: 671\n",
      "reward: -119.05885610817559 setps: 80 count: 751\n",
      "reward: -113.87972668980683 setps: 62 count: 813\n",
      "reward: -114.00523741622517 setps: 54 count: 867\n",
      "reward: -116.00085352642 setps: 90 count: 957\n",
      "avg rewards: -118.23459364138854\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.32520\n",
      "Epoch:20 Batch:6 Loss:0.08326\n",
      "Epoch:40 Batch:6 Loss:0.04999\n",
      "Epoch:60 Batch:6 Loss:0.03921\n",
      "Epoch:80 Batch:6 Loss:0.03608\n",
      "Epoch:100 Batch:6 Loss:0.02994\n",
      "Epoch:120 Batch:6 Loss:0.02890\n",
      "Epoch:140 Batch:6 Loss:0.02755\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.073\n",
      "Epoch:10 Batch:9 Loss:0.067\n",
      "Epoch:20 Batch:9 Loss:0.067\n",
      "Epoch:30 Batch:9 Loss:0.061\n",
      "Epoch:40 Batch:9 Loss:0.064\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -14.058331643603124 setps: 800 count: 800\n",
      "avg rewards: -14.058331643603124\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.33972\n",
      "Epoch:20 Batch:7 Loss:0.07332\n",
      "Epoch:40 Batch:7 Loss:0.04137\n",
      "Epoch:60 Batch:7 Loss:0.03336\n",
      "Epoch:80 Batch:7 Loss:0.03053\n",
      "Epoch:100 Batch:7 Loss:0.02824\n",
      "Epoch:120 Batch:7 Loss:0.02668\n",
      "Epoch:140 Batch:7 Loss:0.02571\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.061\n",
      "Epoch:10 Batch:9 Loss:0.059\n",
      "Epoch:20 Batch:9 Loss:0.059\n",
      "Epoch:30 Batch:9 Loss:0.061\n",
      "Epoch:40 Batch:9 Loss:0.057\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -22.47997625019866 setps: 800 count: 800\n",
      "avg rewards: -22.47997625019866\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.31836\n",
      "Epoch:20 Batch:8 Loss:0.06855\n",
      "Epoch:40 Batch:8 Loss:0.04170\n",
      "Epoch:60 Batch:8 Loss:0.03159\n",
      "Epoch:80 Batch:8 Loss:0.03043\n",
      "Epoch:100 Batch:8 Loss:0.02629\n",
      "Epoch:120 Batch:8 Loss:0.02724\n",
      "Epoch:140 Batch:8 Loss:0.02695\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.060\n",
      "Epoch:10 Batch:9 Loss:0.057\n",
      "Epoch:20 Batch:9 Loss:0.058\n",
      "Epoch:30 Batch:9 Loss:0.056\n",
      "Epoch:40 Batch:9 Loss:0.058\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -26.676124908864498 setps: 800 count: 800\n",
      "avg rewards: -26.676124908864498\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.30239\n",
      "Epoch:20 Batch:9 Loss:0.06193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:40 Batch:9 Loss:0.03988\n",
      "Epoch:60 Batch:9 Loss:0.03375\n",
      "Epoch:80 Batch:9 Loss:0.02756\n",
      "Epoch:100 Batch:9 Loss:0.02559\n",
      "Epoch:120 Batch:9 Loss:0.02690\n",
      "Epoch:140 Batch:9 Loss:0.02314\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.059\n",
      "Epoch:10 Batch:9 Loss:0.056\n",
      "Epoch:20 Batch:9 Loss:0.056\n",
      "Epoch:30 Batch:9 Loss:0.056\n",
      "Epoch:40 Batch:9 Loss:0.057\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -19.907543351842097 setps: 800 count: 800\n",
      "reward: -114.62570342977159 setps: 56 count: 856\n",
      "avg rewards: -67.26662339080684\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.28054\n",
      "Epoch:20 Batch:10 Loss:0.05538\n",
      "Epoch:40 Batch:10 Loss:0.03792\n",
      "Epoch:60 Batch:10 Loss:0.02975\n",
      "Epoch:80 Batch:10 Loss:0.02608\n",
      "Epoch:100 Batch:10 Loss:0.02582\n",
      "Epoch:120 Batch:10 Loss:0.02426\n",
      "Epoch:140 Batch:10 Loss:0.02202\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.054\n",
      "Epoch:10 Batch:9 Loss:0.054\n",
      "Epoch:20 Batch:9 Loss:0.055\n",
      "Epoch:30 Batch:9 Loss:0.053\n",
      "Epoch:40 Batch:9 Loss:0.056\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -31.269034088473724 setps: 800 count: 800\n",
      "avg rewards: -31.269034088473724\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.25501\n",
      "Epoch:20 Batch:11 Loss:0.05378\n",
      "Epoch:40 Batch:11 Loss:0.03429\n",
      "Epoch:60 Batch:11 Loss:0.03069\n",
      "Epoch:80 Batch:11 Loss:0.02747\n",
      "Epoch:100 Batch:11 Loss:0.02365\n",
      "Epoch:120 Batch:11 Loss:0.02532\n",
      "Epoch:140 Batch:11 Loss:0.02229\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.052\n",
      "Epoch:10 Batch:9 Loss:0.054\n",
      "Epoch:20 Batch:9 Loss:0.051\n",
      "Epoch:30 Batch:9 Loss:0.052\n",
      "Epoch:40 Batch:9 Loss:0.051\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -39.0277256302051 setps: 800 count: 800\n",
      "avg rewards: -39.0277256302051\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.29053\n",
      "Epoch:20 Batch:12 Loss:0.05054\n",
      "Epoch:40 Batch:12 Loss:0.03381\n",
      "Epoch:60 Batch:12 Loss:0.02841\n",
      "Epoch:80 Batch:12 Loss:0.02685\n",
      "Epoch:100 Batch:12 Loss:0.02653\n",
      "Epoch:120 Batch:12 Loss:0.02601\n",
      "Epoch:140 Batch:12 Loss:0.02218\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.056\n",
      "Epoch:10 Batch:9 Loss:0.052\n",
      "Epoch:20 Batch:9 Loss:0.056\n",
      "Epoch:30 Batch:9 Loss:0.055\n",
      "Epoch:40 Batch:9 Loss:0.054\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -29.120022530453276 setps: 800 count: 800\n",
      "avg rewards: -29.120022530453276\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.27351\n",
      "Epoch:20 Batch:13 Loss:0.04945\n",
      "Epoch:40 Batch:13 Loss:0.03229\n",
      "Epoch:60 Batch:13 Loss:0.02967\n",
      "Epoch:80 Batch:13 Loss:0.02666\n",
      "Epoch:100 Batch:13 Loss:0.02668\n",
      "Epoch:120 Batch:13 Loss:0.02395\n",
      "Epoch:140 Batch:13 Loss:0.02431\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.050\n",
      "Epoch:10 Batch:9 Loss:0.053\n",
      "Epoch:20 Batch:9 Loss:0.051\n",
      "Epoch:30 Batch:9 Loss:0.054\n",
      "Epoch:40 Batch:9 Loss:0.052\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 8.570779650028179 setps: 800 count: 800\n",
      "avg rewards: 8.570779650028179\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.27585\n",
      "Epoch:20 Batch:14 Loss:0.04962\n",
      "Epoch:40 Batch:14 Loss:0.03457\n",
      "Epoch:60 Batch:14 Loss:0.02851\n",
      "Epoch:80 Batch:14 Loss:0.02730\n",
      "Epoch:100 Batch:14 Loss:0.02705\n",
      "Epoch:120 Batch:14 Loss:0.02628\n",
      "Epoch:140 Batch:14 Loss:0.02445\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.045\n",
      "Epoch:10 Batch:9 Loss:0.047\n",
      "Epoch:20 Batch:9 Loss:0.048\n",
      "Epoch:30 Batch:9 Loss:0.048\n",
      "Epoch:40 Batch:9 Loss:0.048\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -112.51873828590412 setps: 58 count: 58\n",
      "reward: -113.44840044417356 setps: 89 count: 147\n",
      "reward: -123.32719220820441 setps: 132 count: 279\n",
      "reward: -116.41368595153652 setps: 68 count: 347\n",
      "reward: -113.58164060764511 setps: 62 count: 409\n",
      "reward: -117.22283447682298 setps: 81 count: 490\n",
      "reward: -113.8760828796979 setps: 63 count: 553\n",
      "avg rewards: -115.7697964077121\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.22953\n",
      "Epoch:20 Batch:15 Loss:0.04322\n",
      "Epoch:40 Batch:15 Loss:0.03388\n",
      "Epoch:60 Batch:15 Loss:0.02762\n",
      "Epoch:80 Batch:15 Loss:0.02418\n",
      "Epoch:100 Batch:15 Loss:0.02545\n",
      "Epoch:120 Batch:15 Loss:0.02605\n",
      "Epoch:140 Batch:15 Loss:0.02494\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.047\n",
      "Epoch:10 Batch:9 Loss:0.044\n",
      "Epoch:20 Batch:9 Loss:0.039\n",
      "Epoch:30 Batch:9 Loss:0.043\n",
      "Epoch:40 Batch:9 Loss:0.042\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 82.69871169230477 setps: 800 count: 800\n",
      "avg rewards: 82.69871169230477\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.23067\n",
      "Epoch:20 Batch:16 Loss:0.04348\n",
      "Epoch:40 Batch:16 Loss:0.03244\n",
      "Epoch:60 Batch:16 Loss:0.02919\n",
      "Epoch:80 Batch:16 Loss:0.03037\n",
      "Epoch:100 Batch:16 Loss:0.02458\n",
      "Epoch:120 Batch:16 Loss:0.02542\n",
      "Epoch:140 Batch:16 Loss:0.02495\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.040\n",
      "Epoch:10 Batch:9 Loss:0.037\n",
      "Epoch:20 Batch:9 Loss:0.039\n",
      "Epoch:30 Batch:9 Loss:0.039\n",
      "Epoch:40 Batch:9 Loss:0.037\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 24.080695539140436 setps: 800 count: 800\n",
      "avg rewards: 24.080695539140436\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.24246\n",
      "Epoch:20 Batch:17 Loss:0.04308\n",
      "Epoch:40 Batch:17 Loss:0.03380\n",
      "Epoch:60 Batch:17 Loss:0.02810\n",
      "Epoch:80 Batch:17 Loss:0.02806\n",
      "Epoch:100 Batch:17 Loss:0.02555\n",
      "Epoch:120 Batch:17 Loss:0.02684\n",
      "Epoch:140 Batch:17 Loss:0.02426\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.039\n",
      "Epoch:10 Batch:9 Loss:0.037\n",
      "Epoch:20 Batch:9 Loss:0.037\n",
      "Epoch:30 Batch:9 Loss:0.034\n",
      "Epoch:40 Batch:9 Loss:0.036\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 39.770567760427845 setps: 800 count: 800\n",
      "avg rewards: 39.770567760427845\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.21332\n",
      "Epoch:20 Batch:18 Loss:0.04598\n",
      "Epoch:40 Batch:18 Loss:0.03443\n",
      "Epoch:60 Batch:18 Loss:0.02886\n",
      "Epoch:80 Batch:18 Loss:0.02770\n",
      "Epoch:100 Batch:18 Loss:0.02675\n",
      "Epoch:120 Batch:18 Loss:0.02603\n",
      "Epoch:140 Batch:18 Loss:0.02426\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.037\n",
      "Epoch:10 Batch:9 Loss:0.037\n",
      "Epoch:20 Batch:9 Loss:0.033\n",
      "Epoch:30 Batch:9 Loss:0.038\n",
      "Epoch:40 Batch:9 Loss:0.038\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 87.49148081814072 setps: 800 count: 800\n",
      "avg rewards: 87.49148081814072\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.21601\n",
      "Epoch:20 Batch:19 Loss:0.04040\n",
      "Epoch:40 Batch:19 Loss:0.03139\n",
      "Epoch:60 Batch:19 Loss:0.02911\n",
      "Epoch:80 Batch:19 Loss:0.02910\n",
      "Epoch:100 Batch:19 Loss:0.02707\n",
      "Epoch:120 Batch:19 Loss:0.02461\n",
      "Epoch:140 Batch:19 Loss:0.02467\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.032\n",
      "Epoch:10 Batch:9 Loss:0.034\n",
      "Epoch:20 Batch:9 Loss:0.033\n",
      "Epoch:30 Batch:9 Loss:0.031\n",
      "Epoch:40 Batch:9 Loss:0.032\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -31.47709800046748 setps: 645 count: 645\n",
      "reward: -76.57595922231427 setps: 264 count: 909\n",
      "avg rewards: -54.02652861139087\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.17873\n",
      "Epoch:20 Batch:20 Loss:0.04261\n",
      "Epoch:40 Batch:20 Loss:0.03270\n",
      "Epoch:60 Batch:20 Loss:0.03093\n",
      "Epoch:80 Batch:20 Loss:0.02555\n",
      "Epoch:100 Batch:20 Loss:0.02480\n",
      "Epoch:120 Batch:20 Loss:0.02358\n",
      "Epoch:140 Batch:20 Loss:0.02219\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.030\n",
      "Epoch:10 Batch:9 Loss:0.036\n",
      "Epoch:20 Batch:9 Loss:0.031\n",
      "Epoch:30 Batch:9 Loss:0.034\n",
      "Epoch:40 Batch:9 Loss:0.031\n",
      "Done!\n",
      "############# start Walker2DBulletEnv-v0 training ###################\n",
      "(49, 1000, 22)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 13.414238475778255 setps: 9 count: 9\n",
      "reward: 14.186659335390143 setps: 12 count: 21\n",
      "reward: 27.318206631367502 setps: 26 count: 47\n",
      "reward: 16.85843329593772 setps: 15 count: 62\n",
      "reward: 21.397395350715666 setps: 22 count: 84\n",
      "reward: 14.089880464441375 setps: 10 count: 94\n",
      "reward: 15.02987096743018 setps: 10 count: 104\n",
      "reward: 12.356591047155963 setps: 8 count: 112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 15.39555321254302 setps: 12 count: 124\n",
      "reward: 21.90494244393631 setps: 32 count: 156\n",
      "reward: 17.712883703976697 setps: 12 count: 168\n",
      "reward: 15.081593605913802 setps: 12 count: 180\n",
      "reward: 18.173342215211598 setps: 15 count: 195\n",
      "reward: 14.495556036867493 setps: 11 count: 206\n",
      "reward: 20.50079650690896 setps: 18 count: 224\n",
      "reward: 14.092909942976258 setps: 11 count: 235\n",
      "reward: 22.970959771334307 setps: 22 count: 257\n",
      "reward: 19.444699858610694 setps: 17 count: 274\n",
      "reward: 14.374875546406837 setps: 11 count: 285\n",
      "reward: 15.00305928625894 setps: 11 count: 296\n",
      "reward: 15.243818437888697 setps: 11 count: 307\n",
      "reward: 17.307870128586362 setps: 16 count: 323\n",
      "reward: 21.64621891505667 setps: 24 count: 347\n",
      "reward: 18.4285451403106 setps: 23 count: 370\n",
      "reward: 14.281873992230974 setps: 9 count: 379\n",
      "reward: 15.119889230746777 setps: 12 count: 391\n",
      "reward: 18.797276167415834 setps: 17 count: 408\n",
      "reward: 15.763713427065523 setps: 12 count: 420\n",
      "reward: 27.516996524079882 setps: 27 count: 447\n",
      "reward: 14.47504786134523 setps: 14 count: 461\n",
      "reward: 13.766097358506522 setps: 9 count: 470\n",
      "reward: 15.688697406946448 setps: 13 count: 483\n",
      "reward: 16.920485684316375 setps: 12 count: 495\n",
      "reward: 17.479992435606253 setps: 17 count: 512\n",
      "reward: 14.840384492183512 setps: 10 count: 522\n",
      "reward: 14.28074710001092 setps: 12 count: 534\n",
      "reward: 19.182011935595074 setps: 18 count: 552\n",
      "reward: 20.766663108166536 setps: 20 count: 572\n",
      "reward: 18.396893175621518 setps: 14 count: 586\n",
      "reward: 13.09486350460356 setps: 12 count: 598\n",
      "reward: 12.057490507187321 setps: 8 count: 606\n",
      "reward: 13.962488749361365 setps: 11 count: 617\n",
      "reward: 15.674586982601614 setps: 15 count: 632\n",
      "reward: 16.85779056666797 setps: 14 count: 646\n",
      "reward: 21.517029446948435 setps: 18 count: 664\n",
      "reward: 17.094856722462282 setps: 13 count: 677\n",
      "reward: 14.998163725581254 setps: 11 count: 688\n",
      "reward: 16.90535577871633 setps: 14 count: 702\n",
      "reward: 14.277336815766466 setps: 10 count: 712\n",
      "reward: 20.004583934898253 setps: 19 count: 731\n",
      "reward: 17.435491645979347 setps: 16 count: 747\n",
      "reward: 15.532703819955351 setps: 16 count: 763\n",
      "reward: 16.185374740588305 setps: 12 count: 775\n",
      "reward: 15.464810367433529 setps: 11 count: 786\n",
      "reward: 18.658744219029902 setps: 17 count: 803\n",
      "reward: 30.083171446801856 setps: 38 count: 841\n",
      "reward: 15.39770746484719 setps: 14 count: 855\n",
      "reward: 13.569465087012212 setps: 9 count: 864\n",
      "reward: 18.393536165813565 setps: 16 count: 880\n",
      "reward: 13.687444823609257 setps: 11 count: 891\n",
      "reward: 12.510306997306179 setps: 10 count: 901\n",
      "reward: 14.378060417677624 setps: 15 count: 916\n",
      "reward: 34.87067628929944 setps: 34 count: 950\n",
      "reward: 13.436906401602025 setps: 10 count: 960\n",
      "reward: 18.5650234917819 setps: 15 count: 975\n",
      "reward: 16.098111184232405 setps: 12 count: 987\n",
      "reward: 13.769625021524552 setps: 9 count: 996\n",
      "avg rewards: 17.13712502298703\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:0.35054\n",
      "Epoch:20 Batch:1 Loss:0.13681\n",
      "Epoch:40 Batch:1 Loss:0.11405\n",
      "Epoch:60 Batch:1 Loss:0.10518\n",
      "Epoch:80 Batch:1 Loss:0.09061\n",
      "Epoch:100 Batch:1 Loss:0.06913\n",
      "Epoch:120 Batch:1 Loss:0.05475\n",
      "Epoch:140 Batch:1 Loss:0.04917\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.115\n",
      "Epoch:10 Batch:10 Loss:0.113\n",
      "Epoch:20 Batch:10 Loss:0.111\n",
      "Epoch:30 Batch:10 Loss:0.111\n",
      "Epoch:40 Batch:10 Loss:0.111\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 467.92872182696357 setps: 800 count: 800\n",
      "reward: 18.9599417761361 setps: 65 count: 865\n",
      "reward: 27.823673727949775 setps: 84 count: 949\n",
      "avg rewards: 171.5707791103498\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:0.33784\n",
      "Epoch:20 Batch:2 Loss:0.07928\n",
      "Epoch:40 Batch:2 Loss:0.06478\n",
      "Epoch:60 Batch:2 Loss:0.04702\n",
      "Epoch:80 Batch:2 Loss:0.03508\n",
      "Epoch:100 Batch:2 Loss:0.02904\n",
      "Epoch:120 Batch:2 Loss:0.02710\n",
      "Epoch:140 Batch:2 Loss:0.02375\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.105\n",
      "Epoch:10 Batch:10 Loss:0.102\n",
      "Epoch:20 Batch:10 Loss:0.102\n",
      "Epoch:30 Batch:10 Loss:0.100\n",
      "Epoch:40 Batch:10 Loss:0.099\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 8.009424364115697 setps: 49 count: 49\n",
      "reward: 17.471498285389668 setps: 62 count: 111\n",
      "reward: 8.237586728681343 setps: 51 count: 162\n",
      "reward: 9.023177639265489 setps: 53 count: 215\n",
      "reward: 23.815545258953357 setps: 74 count: 289\n",
      "reward: 20.135976189101346 setps: 68 count: 357\n",
      "reward: 21.103518944294777 setps: 66 count: 423\n",
      "reward: 14.985888144158524 setps: 54 count: 477\n",
      "reward: 8.475320977692906 setps: 49 count: 526\n",
      "reward: 20.310978024918587 setps: 64 count: 590\n",
      "reward: 24.96686897290491 setps: 79 count: 669\n",
      "reward: 18.237529086432186 setps: 62 count: 731\n",
      "reward: 17.41716788798076 setps: 72 count: 803\n",
      "reward: 13.177076932947962 setps: 60 count: 863\n",
      "reward: 13.278693131936599 setps: 60 count: 923\n",
      "reward: 8.864252816713996 setps: 50 count: 973\n",
      "avg rewards: 15.469406461593007\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.32341\n",
      "Epoch:20 Batch:3 Loss:0.06565\n",
      "Epoch:40 Batch:3 Loss:0.04772\n",
      "Epoch:60 Batch:3 Loss:0.03658\n",
      "Epoch:80 Batch:3 Loss:0.02314\n",
      "Epoch:100 Batch:3 Loss:0.01819\n",
      "Epoch:120 Batch:3 Loss:0.01782\n",
      "Epoch:140 Batch:3 Loss:0.01832\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.093\n",
      "Epoch:10 Batch:10 Loss:0.092\n",
      "Epoch:20 Batch:10 Loss:0.093\n",
      "Epoch:30 Batch:10 Loss:0.092\n",
      "Epoch:40 Batch:10 Loss:0.090\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 36.515564188463024 setps: 101 count: 101\n",
      "reward: 56.9415220994255 setps: 77 count: 178\n",
      "reward: 444.856006297748 setps: 800 count: 978\n",
      "avg rewards: 179.43769752854553\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.30584\n",
      "Epoch:20 Batch:4 Loss:0.04940\n",
      "Epoch:40 Batch:4 Loss:0.03104\n",
      "Epoch:60 Batch:4 Loss:0.02296\n",
      "Epoch:80 Batch:4 Loss:0.01672\n",
      "Epoch:100 Batch:4 Loss:0.01516\n",
      "Epoch:120 Batch:4 Loss:0.01350\n",
      "Epoch:140 Batch:4 Loss:0.01422\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.092\n",
      "Epoch:10 Batch:10 Loss:0.090\n",
      "Epoch:20 Batch:10 Loss:0.088\n",
      "Epoch:30 Batch:10 Loss:0.089\n",
      "Epoch:40 Batch:10 Loss:0.088\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 17.66619264634792 setps: 65 count: 65\n",
      "reward: 22.540760794289234 setps: 74 count: 139\n",
      "reward: 20.59765126206329 setps: 74 count: 213\n",
      "reward: 22.14012369455595 setps: 68 count: 281\n",
      "reward: 12.511778314212279 setps: 57 count: 338\n",
      "reward: 15.366225769296586 setps: 61 count: 399\n",
      "reward: 43.253450539465106 setps: 115 count: 514\n",
      "reward: 29.70253054401691 setps: 77 count: 591\n",
      "reward: 11.788969322424961 setps: 54 count: 645\n",
      "reward: 12.629030844275254 setps: 59 count: 704\n",
      "reward: 19.683254333799404 setps: 69 count: 773\n",
      "reward: 24.470518530905238 setps: 78 count: 851\n",
      "reward: 26.13025469742133 setps: 78 count: 929\n",
      "reward: 16.712970128803857 setps: 65 count: 994\n",
      "avg rewards: 21.085265101562666\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.30547\n",
      "Epoch:20 Batch:5 Loss:0.04505\n",
      "Epoch:40 Batch:5 Loss:0.02749\n",
      "Epoch:60 Batch:5 Loss:0.02014\n",
      "Epoch:80 Batch:5 Loss:0.01325\n",
      "Epoch:100 Batch:5 Loss:0.01250\n",
      "Epoch:120 Batch:5 Loss:0.01331\n",
      "Epoch:140 Batch:5 Loss:0.01076\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.090\n",
      "Epoch:10 Batch:10 Loss:0.089\n",
      "Epoch:20 Batch:10 Loss:0.088\n",
      "Epoch:30 Batch:10 Loss:0.090\n",
      "Epoch:40 Batch:10 Loss:0.088\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 22.70885946212657 setps: 61 count: 61\n",
      "reward: 18.801809361940826 setps: 50 count: 111\n",
      "reward: 67.83639898751716 setps: 50 count: 161\n",
      "reward: 88.9796774806222 setps: 74 count: 235\n",
      "reward: 98.45759954254899 setps: 87 count: 322\n",
      "reward: 46.68941461031646 setps: 97 count: 419\n",
      "reward: 96.5212633568808 setps: 85 count: 504\n",
      "reward: 107.1076075058328 setps: 97 count: 601\n",
      "reward: 37.40565820707125 setps: 80 count: 681\n",
      "reward: 107.34977387373945 setps: 102 count: 783\n",
      "reward: 25.602641891136592 setps: 35 count: 818\n",
      "reward: 35.509756253326486 setps: 79 count: 897\n",
      "reward: 55.47143527978622 setps: 46 count: 943\n",
      "avg rewards: 62.18783813944968\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:6 Loss:0.28159\n",
      "Epoch:20 Batch:6 Loss:0.04278\n",
      "Epoch:40 Batch:6 Loss:0.02090\n",
      "Epoch:60 Batch:6 Loss:0.01606\n",
      "Epoch:80 Batch:6 Loss:0.01272\n",
      "Epoch:100 Batch:6 Loss:0.01083\n",
      "Epoch:120 Batch:6 Loss:0.00945\n",
      "Epoch:140 Batch:6 Loss:0.01016\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.071\n",
      "Epoch:10 Batch:10 Loss:0.069\n",
      "Epoch:20 Batch:10 Loss:0.070\n",
      "Epoch:30 Batch:10 Loss:0.068\n",
      "Epoch:40 Batch:10 Loss:0.068\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 21.01252200104645 setps: 67 count: 67\n",
      "reward: 54.51885680489649 setps: 72 count: 139\n",
      "reward: 22.799982070067195 setps: 59 count: 198\n",
      "reward: 52.31948766279823 setps: 108 count: 306\n",
      "avg rewards: 37.66271213470209\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.30512\n",
      "Epoch:20 Batch:7 Loss:0.03720\n",
      "Epoch:40 Batch:7 Loss:0.01836\n",
      "Epoch:60 Batch:7 Loss:0.01213\n",
      "Epoch:80 Batch:7 Loss:0.01046\n",
      "Epoch:100 Batch:7 Loss:0.01087\n",
      "Epoch:120 Batch:7 Loss:0.00980\n",
      "Epoch:140 Batch:7 Loss:0.00847\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.074\n",
      "Epoch:10 Batch:10 Loss:0.073\n",
      "Epoch:20 Batch:10 Loss:0.072\n",
      "Epoch:30 Batch:10 Loss:0.074\n",
      "Epoch:40 Batch:10 Loss:0.073\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 28.666016224141647 setps: 82 count: 82\n",
      "reward: 23.051161095440335 setps: 74 count: 156\n",
      "reward: 343.0094490994498 setps: 509 count: 665\n",
      "reward: 40.58410620614013 setps: 105 count: 770\n",
      "reward: 35.82075404475183 setps: 93 count: 863\n",
      "reward: 13.634249423946308 setps: 54 count: 917\n",
      "reward: 26.96791920589748 setps: 81 count: 998\n",
      "avg rewards: 73.10480789996679\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.26977\n",
      "Epoch:20 Batch:8 Loss:0.03297\n",
      "Epoch:40 Batch:8 Loss:0.01533\n",
      "Epoch:60 Batch:8 Loss:0.01229\n",
      "Epoch:80 Batch:8 Loss:0.00980\n",
      "Epoch:100 Batch:8 Loss:0.00920\n",
      "Epoch:120 Batch:8 Loss:0.00812\n",
      "Epoch:140 Batch:8 Loss:0.00853\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.071\n",
      "Epoch:10 Batch:10 Loss:0.067\n",
      "Epoch:20 Batch:10 Loss:0.066\n",
      "Epoch:30 Batch:10 Loss:0.067\n",
      "Epoch:40 Batch:10 Loss:0.067\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 13.435226179021985 setps: 67 count: 67\n",
      "reward: 18.278473981647398 setps: 81 count: 148\n",
      "reward: 13.990323851603893 setps: 86 count: 234\n",
      "reward: 14.201776382121905 setps: 66 count: 300\n",
      "reward: 66.85870235698965 setps: 112 count: 412\n",
      "reward: 9.83344620426651 setps: 64 count: 476\n",
      "reward: 15.536841580380862 setps: 76 count: 552\n",
      "reward: 15.676610745114163 setps: 76 count: 628\n",
      "reward: 15.372001989874105 setps: 71 count: 699\n",
      "reward: 13.115301715258084 setps: 97 count: 796\n",
      "reward: 16.484981249843248 setps: 78 count: 874\n",
      "reward: 19.228075206340872 setps: 79 count: 953\n",
      "avg rewards: 19.334313453538556\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.23384\n",
      "Epoch:20 Batch:9 Loss:0.02882\n",
      "Epoch:40 Batch:9 Loss:0.01393\n",
      "Epoch:60 Batch:9 Loss:0.01128\n",
      "Epoch:80 Batch:9 Loss:0.00912\n",
      "Epoch:100 Batch:9 Loss:0.00918\n",
      "Epoch:120 Batch:9 Loss:0.00771\n",
      "Epoch:140 Batch:9 Loss:0.00752\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.060\n",
      "Epoch:10 Batch:10 Loss:0.059\n",
      "Epoch:20 Batch:10 Loss:0.058\n",
      "Epoch:30 Batch:10 Loss:0.059\n",
      "Epoch:40 Batch:10 Loss:0.055\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 20.749617164225484 setps: 63 count: 63\n",
      "reward: 24.97176943250961 setps: 71 count: 134\n",
      "reward: 25.647696551730036 setps: 75 count: 209\n",
      "reward: 17.612965886510214 setps: 63 count: 272\n",
      "reward: 29.869908007104815 setps: 88 count: 360\n",
      "reward: 27.625443133599756 setps: 72 count: 432\n",
      "reward: 25.7680861138666 setps: 68 count: 500\n",
      "reward: 16.476681753805305 setps: 72 count: 572\n",
      "reward: 21.716641776324824 setps: 71 count: 643\n",
      "reward: 51.79904887902957 setps: 78 count: 721\n",
      "reward: 20.56177549827845 setps: 63 count: 784\n",
      "reward: 30.11627474393462 setps: 87 count: 871\n",
      "reward: 31.42215802322753 setps: 79 count: 950\n",
      "avg rewards: 26.48754361262668\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.22775\n",
      "Epoch:20 Batch:10 Loss:0.02098\n",
      "Epoch:40 Batch:10 Loss:0.01391\n",
      "Epoch:60 Batch:10 Loss:0.01038\n",
      "Epoch:80 Batch:10 Loss:0.00801\n",
      "Epoch:100 Batch:10 Loss:0.00727\n",
      "Epoch:120 Batch:10 Loss:0.00617\n",
      "Epoch:140 Batch:10 Loss:0.00611\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.056\n",
      "Epoch:10 Batch:10 Loss:0.055\n",
      "Epoch:20 Batch:10 Loss:0.055\n",
      "Epoch:30 Batch:10 Loss:0.054\n",
      "Epoch:40 Batch:10 Loss:0.055\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 14.532270583711211 setps: 64 count: 64\n",
      "reward: 25.600714866137427 setps: 70 count: 134\n",
      "reward: 20.436376002967876 setps: 78 count: 212\n",
      "reward: 71.38390415883478 setps: 98 count: 310\n",
      "reward: 16.999639154331817 setps: 70 count: 380\n",
      "reward: 33.74412545686792 setps: 82 count: 462\n",
      "reward: 28.878161960619043 setps: 88 count: 550\n",
      "reward: 14.717243421972666 setps: 61 count: 611\n",
      "reward: 59.68972970329341 setps: 81 count: 692\n",
      "reward: 17.67477794592414 setps: 61 count: 753\n",
      "reward: 31.748614888454913 setps: 75 count: 828\n",
      "reward: 41.62232721748733 setps: 90 count: 918\n",
      "reward: 23.649963403433496 setps: 75 count: 993\n",
      "avg rewards: 30.821372981848928\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.18083\n",
      "Epoch:20 Batch:11 Loss:0.02090\n",
      "Epoch:40 Batch:11 Loss:0.01348\n",
      "Epoch:60 Batch:11 Loss:0.01036\n",
      "Epoch:80 Batch:11 Loss:0.00901\n",
      "Epoch:100 Batch:11 Loss:0.00824\n",
      "Epoch:120 Batch:11 Loss:0.00672\n",
      "Epoch:140 Batch:11 Loss:0.00703\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.060\n",
      "Epoch:10 Batch:10 Loss:0.059\n",
      "Epoch:20 Batch:10 Loss:0.059\n",
      "Epoch:30 Batch:10 Loss:0.060\n",
      "Epoch:40 Batch:10 Loss:0.057\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 25.822754860518035 setps: 31 count: 31\n",
      "reward: 32.15843786233308 setps: 33 count: 64\n",
      "reward: 24.264002339729632 setps: 31 count: 95\n",
      "reward: 25.828217992564895 setps: 28 count: 123\n",
      "reward: 22.072326192657055 setps: 25 count: 148\n",
      "reward: 23.623926951846805 setps: 26 count: 174\n",
      "reward: 25.566496615085637 setps: 29 count: 203\n",
      "reward: 22.150371400163568 setps: 25 count: 228\n",
      "reward: 21.08895736744744 setps: 27 count: 255\n",
      "reward: 25.722806104061604 setps: 30 count: 285\n",
      "reward: 23.453627785861322 setps: 26 count: 311\n",
      "reward: 26.49242356058239 setps: 29 count: 340\n",
      "reward: 25.035837287307366 setps: 28 count: 368\n",
      "reward: 24.6074333130673 setps: 30 count: 398\n",
      "reward: 37.71674249767238 setps: 37 count: 435\n",
      "reward: 25.157310039753796 setps: 27 count: 462\n",
      "reward: 26.268837985537534 setps: 32 count: 494\n",
      "reward: 28.619816167661337 setps: 30 count: 524\n",
      "reward: 28.27689335444593 setps: 40 count: 564\n",
      "reward: 26.514566531003222 setps: 26 count: 590\n",
      "reward: 23.717310463250033 setps: 27 count: 617\n",
      "reward: 30.339201370823144 setps: 38 count: 655\n",
      "reward: 24.242393237276705 setps: 29 count: 684\n",
      "reward: 22.68028000343038 setps: 26 count: 710\n",
      "reward: 29.281517577328486 setps: 32 count: 742\n",
      "reward: 25.145872856363713 setps: 30 count: 772\n",
      "reward: 23.086693498914244 setps: 26 count: 798\n",
      "reward: 26.337473266360757 setps: 29 count: 827\n",
      "reward: 28.582703294725796 setps: 34 count: 861\n",
      "reward: 21.64872171864408 setps: 27 count: 888\n",
      "reward: 22.255676046975708 setps: 24 count: 912\n",
      "reward: 23.275425715606254 setps: 31 count: 943\n",
      "reward: 25.0953975892262 setps: 25 count: 968\n",
      "reward: 24.73300716963859 setps: 25 count: 993\n",
      "avg rewards: 25.61363117699601\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.25721\n",
      "Epoch:20 Batch:12 Loss:0.02035\n",
      "Epoch:40 Batch:12 Loss:0.01174\n",
      "Epoch:60 Batch:12 Loss:0.00981\n",
      "Epoch:80 Batch:12 Loss:0.00887\n",
      "Epoch:100 Batch:12 Loss:0.00764\n",
      "Epoch:120 Batch:12 Loss:0.00670\n",
      "Epoch:140 Batch:12 Loss:0.00687\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.058\n",
      "Epoch:10 Batch:10 Loss:0.054\n",
      "Epoch:20 Batch:10 Loss:0.054\n",
      "Epoch:30 Batch:10 Loss:0.053\n",
      "Epoch:40 Batch:10 Loss:0.052\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 79.283756466223 setps: 73 count: 73\n",
      "reward: 62.16037206020407 setps: 122 count: 195\n",
      "reward: 20.322331159196622 setps: 64 count: 259\n",
      "reward: 72.25877270067285 setps: 81 count: 340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 151.69334852978335 setps: 290 count: 630\n",
      "reward: 78.85644629419689 setps: 165 count: 795\n",
      "avg rewards: 77.42917120171279\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.17545\n",
      "Epoch:20 Batch:13 Loss:0.01669\n",
      "Epoch:40 Batch:13 Loss:0.01104\n",
      "Epoch:60 Batch:13 Loss:0.00969\n",
      "Epoch:80 Batch:13 Loss:0.00767\n",
      "Epoch:100 Batch:13 Loss:0.00723\n",
      "Epoch:120 Batch:13 Loss:0.00721\n",
      "Epoch:140 Batch:13 Loss:0.00650\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.053\n",
      "Epoch:10 Batch:10 Loss:0.050\n",
      "Epoch:20 Batch:10 Loss:0.051\n",
      "Epoch:30 Batch:10 Loss:0.050\n",
      "Epoch:40 Batch:10 Loss:0.051\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 16.33975716509158 setps: 19 count: 19\n",
      "reward: 18.450521964814104 setps: 21 count: 40\n",
      "reward: 17.224866413499697 setps: 19 count: 59\n",
      "reward: 25.273331877386955 setps: 24 count: 83\n",
      "reward: 19.789734195462373 setps: 22 count: 105\n",
      "reward: 21.97963000570598 setps: 23 count: 128\n",
      "reward: 16.651169554937226 setps: 21 count: 149\n",
      "reward: 18.489564490097113 setps: 22 count: 171\n",
      "reward: 16.154169474997612 setps: 19 count: 190\n",
      "reward: 24.533126964436086 setps: 25 count: 215\n",
      "reward: 16.226715064448943 setps: 21 count: 236\n",
      "reward: 21.12315296188899 setps: 23 count: 259\n",
      "reward: 15.036382771577337 setps: 21 count: 280\n",
      "reward: 23.063708293456873 setps: 24 count: 304\n",
      "reward: 23.02732870710315 setps: 24 count: 328\n",
      "reward: 20.799399730539882 setps: 22 count: 350\n",
      "reward: 21.705233445815978 setps: 23 count: 373\n",
      "reward: 16.43092913527798 setps: 21 count: 394\n",
      "reward: 18.865232253049903 setps: 21 count: 415\n",
      "reward: 18.036657904567257 setps: 21 count: 436\n",
      "reward: 23.67119498314278 setps: 24 count: 460\n",
      "reward: 19.40767048684502 setps: 22 count: 482\n",
      "reward: 16.8507806851223 setps: 21 count: 503\n",
      "reward: 24.706229096258177 setps: 24 count: 527\n",
      "reward: 16.971045970138228 setps: 19 count: 546\n",
      "reward: 16.303819004123213 setps: 22 count: 568\n",
      "reward: 17.919010596032603 setps: 20 count: 588\n",
      "reward: 20.360667416396605 setps: 22 count: 610\n",
      "reward: 17.608311053036594 setps: 23 count: 633\n",
      "reward: 19.23024383179 setps: 22 count: 655\n",
      "reward: 17.325830337217486 setps: 19 count: 674\n",
      "reward: 20.58889895523899 setps: 22 count: 696\n",
      "reward: 18.54220373335411 setps: 21 count: 717\n",
      "reward: 21.90203297314874 setps: 24 count: 741\n",
      "reward: 26.575142616816446 setps: 25 count: 766\n",
      "reward: 17.034107116296944 setps: 19 count: 785\n",
      "reward: 22.998331259452964 setps: 23 count: 808\n",
      "reward: 22.14950453432975 setps: 23 count: 831\n",
      "reward: 22.203317097853862 setps: 22 count: 853\n",
      "reward: 17.810558061995838 setps: 23 count: 876\n",
      "reward: 21.00849984855595 setps: 23 count: 899\n",
      "reward: 24.875443176267435 setps: 24 count: 923\n",
      "reward: 21.360144968741228 setps: 23 count: 946\n",
      "reward: 15.380223797113285 setps: 19 count: 965\n",
      "reward: 19.17834260036907 setps: 21 count: 986\n",
      "avg rewards: 19.80360370163984\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.14239\n",
      "Epoch:20 Batch:14 Loss:0.01877\n",
      "Epoch:40 Batch:14 Loss:0.01052\n",
      "Epoch:60 Batch:14 Loss:0.00957\n",
      "Epoch:80 Batch:14 Loss:0.00785\n",
      "Epoch:100 Batch:14 Loss:0.00758\n",
      "Epoch:120 Batch:14 Loss:0.00738\n",
      "Epoch:140 Batch:14 Loss:0.00705\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.053\n",
      "Epoch:10 Batch:10 Loss:0.047\n",
      "Epoch:20 Batch:10 Loss:0.049\n",
      "Epoch:30 Batch:10 Loss:0.048\n",
      "Epoch:40 Batch:10 Loss:0.048\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 469.83001003968684 setps: 800 count: 800\n",
      "reward: 34.54242898097 setps: 87 count: 887\n",
      "reward: 22.983705732536333 setps: 73 count: 960\n",
      "avg rewards: 175.7853815843977\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.13696\n",
      "Epoch:20 Batch:15 Loss:0.01778\n",
      "Epoch:40 Batch:15 Loss:0.01025\n",
      "Epoch:60 Batch:15 Loss:0.00891\n",
      "Epoch:80 Batch:15 Loss:0.00782\n",
      "Epoch:100 Batch:15 Loss:0.00614\n",
      "Epoch:120 Batch:15 Loss:0.00613\n",
      "Epoch:140 Batch:15 Loss:0.00515\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.048\n",
      "Epoch:10 Batch:10 Loss:0.047\n",
      "Epoch:20 Batch:10 Loss:0.047\n",
      "Epoch:30 Batch:10 Loss:0.048\n",
      "Epoch:40 Batch:10 Loss:0.046\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 554.0009371416389 setps: 800 count: 800\n",
      "reward: 69.37019687134888 setps: 73 count: 873\n",
      "reward: 68.50987377639977 setps: 73 count: 946\n",
      "avg rewards: 230.62700259646252\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.11574\n",
      "Epoch:20 Batch:16 Loss:0.01396\n",
      "Epoch:40 Batch:16 Loss:0.01071\n",
      "Epoch:60 Batch:16 Loss:0.00817\n",
      "Epoch:80 Batch:16 Loss:0.00666\n",
      "Epoch:100 Batch:16 Loss:0.00649\n",
      "Epoch:120 Batch:16 Loss:0.00560\n",
      "Epoch:140 Batch:16 Loss:0.00526\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.050\n",
      "Epoch:10 Batch:10 Loss:0.049\n",
      "Epoch:20 Batch:10 Loss:0.048\n",
      "Epoch:30 Batch:10 Loss:0.048\n",
      "Epoch:40 Batch:10 Loss:0.048\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 17.930974747514117 setps: 60 count: 60\n",
      "reward: 49.337894146388884 setps: 118 count: 178\n",
      "reward: 21.71237990222144 setps: 63 count: 241\n",
      "reward: 21.80705574998137 setps: 66 count: 307\n",
      "reward: 17.052136037210587 setps: 58 count: 365\n",
      "reward: 18.763505490585516 setps: 61 count: 426\n",
      "reward: 69.17108698806695 setps: 69 count: 495\n",
      "reward: 35.06381580927991 setps: 92 count: 587\n",
      "reward: 28.533416967216183 setps: 82 count: 669\n",
      "reward: 25.420935605210254 setps: 74 count: 743\n",
      "reward: 18.69971141257557 setps: 50 count: 793\n",
      "avg rewards: 29.40844662329553\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.10828\n",
      "Epoch:20 Batch:17 Loss:0.01255\n",
      "Epoch:40 Batch:17 Loss:0.01075\n",
      "Epoch:60 Batch:17 Loss:0.00736\n",
      "Epoch:80 Batch:17 Loss:0.00708\n",
      "Epoch:100 Batch:17 Loss:0.00714\n",
      "Epoch:120 Batch:17 Loss:0.00566\n",
      "Epoch:140 Batch:17 Loss:0.00561\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.041\n",
      "Epoch:10 Batch:10 Loss:0.043\n",
      "Epoch:20 Batch:10 Loss:0.044\n",
      "Epoch:30 Batch:10 Loss:0.044\n",
      "Epoch:40 Batch:10 Loss:0.042\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 120.94099378556854 setps: 147 count: 147\n",
      "reward: 83.27395021620465 setps: 90 count: 237\n",
      "reward: 101.61231594651326 setps: 116 count: 353\n",
      "reward: 52.03517318728846 setps: 55 count: 408\n",
      "reward: 17.042460646538522 setps: 79 count: 487\n",
      "reward: 70.79160111142704 setps: 74 count: 561\n",
      "reward: 104.85997254425604 setps: 121 count: 682\n",
      "reward: 66.94564145787734 setps: 68 count: 750\n",
      "reward: 86.46755618701863 setps: 95 count: 845\n",
      "reward: 61.93371075709729 setps: 145 count: 990\n",
      "avg rewards: 76.59033758397898\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.13065\n",
      "Epoch:20 Batch:18 Loss:0.01389\n",
      "Epoch:40 Batch:18 Loss:0.00943\n",
      "Epoch:60 Batch:18 Loss:0.00789\n",
      "Epoch:80 Batch:18 Loss:0.00684\n",
      "Epoch:100 Batch:18 Loss:0.00665\n",
      "Epoch:120 Batch:18 Loss:0.00525\n",
      "Epoch:140 Batch:18 Loss:0.00507\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.045\n",
      "Epoch:10 Batch:10 Loss:0.044\n",
      "Epoch:20 Batch:10 Loss:0.043\n",
      "Epoch:30 Batch:10 Loss:0.043\n",
      "Epoch:40 Batch:10 Loss:0.043\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 70.99386068847672 setps: 70 count: 70\n",
      "reward: 67.26259567818053 setps: 66 count: 136\n",
      "reward: 66.553709142335 setps: 66 count: 202\n",
      "reward: 80.36402565734167 setps: 84 count: 286\n",
      "reward: 75.39414577422284 setps: 76 count: 362\n",
      "reward: 23.53385786878788 setps: 54 count: 416\n",
      "reward: 92.59658939018699 setps: 100 count: 516\n",
      "reward: 86.21978303388896 setps: 90 count: 606\n",
      "reward: 21.94754372671596 setps: 45 count: 651\n",
      "reward: 28.96376174542529 setps: 61 count: 712\n",
      "reward: 80.67777707553323 setps: 84 count: 796\n",
      "reward: 66.5558529854403 setps: 65 count: 861\n",
      "reward: 75.33446558573344 setps: 76 count: 937\n",
      "reward: 22.551761993899714 setps: 47 count: 984\n",
      "avg rewards: 61.353552167583466\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.09180\n",
      "Epoch:20 Batch:19 Loss:0.01251\n",
      "Epoch:40 Batch:19 Loss:0.00932\n",
      "Epoch:60 Batch:19 Loss:0.00664\n",
      "Epoch:80 Batch:19 Loss:0.00717\n",
      "Epoch:100 Batch:19 Loss:0.00648\n",
      "Epoch:120 Batch:19 Loss:0.00546\n",
      "Epoch:140 Batch:19 Loss:0.00591\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:10 Loss:0.045\n",
      "Epoch:10 Batch:10 Loss:0.044\n",
      "Epoch:20 Batch:10 Loss:0.044\n",
      "Epoch:30 Batch:10 Loss:0.044\n",
      "Epoch:40 Batch:10 Loss:0.044\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 39.10430530724698 setps: 72 count: 72\n",
      "reward: 74.81866016155837 setps: 72 count: 144\n",
      "reward: 44.40321249123807 setps: 81 count: 225\n",
      "reward: 80.97833334901368 setps: 76 count: 301\n",
      "reward: 35.082146493962476 setps: 65 count: 366\n",
      "reward: 60.65587098096003 setps: 59 count: 425\n",
      "reward: 62.39749000284355 setps: 59 count: 484\n",
      "reward: 46.51398767690698 setps: 88 count: 572\n",
      "reward: 40.17340380546668 setps: 85 count: 657\n",
      "reward: 94.03916180867672 setps: 83 count: 740\n",
      "reward: 52.320435167131656 setps: 94 count: 834\n",
      "reward: 100.7577819987316 setps: 161 count: 995\n",
      "avg rewards: 60.9370657703114\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.06995\n",
      "Epoch:20 Batch:20 Loss:0.01377\n",
      "Epoch:40 Batch:20 Loss:0.00978\n",
      "Epoch:60 Batch:20 Loss:0.00727\n",
      "Epoch:80 Batch:20 Loss:0.00570\n",
      "Epoch:100 Batch:20 Loss:0.00621\n",
      "Epoch:120 Batch:20 Loss:0.00545\n",
      "Epoch:140 Batch:20 Loss:0.00608\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.045\n",
      "Epoch:10 Batch:10 Loss:0.043\n",
      "Epoch:20 Batch:10 Loss:0.043\n",
      "Epoch:30 Batch:10 Loss:0.043\n",
      "Epoch:40 Batch:10 Loss:0.043\n",
      "Done!\n",
      "############# start HopperBulletEnv-v0 training ###################\n",
      "(49,)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 19.940754284516153 setps: 11 count: 11\n",
      "reward: 15.18361010433291 setps: 9 count: 20\n",
      "reward: 20.427877554857695 setps: 13 count: 33\n",
      "reward: 21.88116727068264 setps: 11 count: 44\n",
      "reward: 22.24640270066593 setps: 10 count: 54\n",
      "reward: 24.771825490304035 setps: 17 count: 71\n",
      "reward: 19.916474294567887 setps: 9 count: 80\n",
      "reward: 17.87030452345207 setps: 12 count: 92\n",
      "reward: 14.777987863341693 setps: 12 count: 104\n",
      "reward: 23.02766747134592 setps: 15 count: 119\n",
      "reward: 18.266081375647627 setps: 18 count: 137\n",
      "reward: 24.400351782921643 setps: 12 count: 149\n",
      "reward: 27.543598983506676 setps: 34 count: 183\n",
      "reward: 21.137342184047156 setps: 9 count: 192\n",
      "reward: 23.6751615703819 setps: 11 count: 203\n",
      "reward: 15.675702211250607 setps: 11 count: 214\n",
      "reward: 20.3985522982839 setps: 19 count: 233\n",
      "reward: 19.29716568623553 setps: 9 count: 242\n",
      "reward: 21.29027102702239 setps: 9 count: 251\n",
      "reward: 22.55049308847811 setps: 19 count: 270\n",
      "reward: 24.892823693787793 setps: 24 count: 294\n",
      "reward: 20.740839364171553 setps: 10 count: 304\n",
      "reward: 20.685774354219028 setps: 8 count: 312\n",
      "reward: 13.528804626743659 setps: 13 count: 325\n",
      "reward: 20.673734619058088 setps: 9 count: 334\n",
      "reward: 20.597663600507072 setps: 13 count: 347\n",
      "reward: 18.421369598315504 setps: 8 count: 355\n",
      "reward: 20.34181689176621 setps: 23 count: 378\n",
      "reward: 17.067862376091945 setps: 7 count: 385\n",
      "reward: 17.332808136034874 setps: 7 count: 392\n",
      "reward: 24.818784885395143 setps: 16 count: 408\n",
      "reward: 17.70183660267794 setps: 11 count: 419\n",
      "reward: 16.518992313581112 setps: 18 count: 437\n",
      "reward: 21.415015180989577 setps: 10 count: 447\n",
      "reward: 19.99178239277972 setps: 14 count: 461\n",
      "reward: 24.13352903753257 setps: 36 count: 497\n",
      "reward: 16.29371288717084 setps: 11 count: 508\n",
      "reward: 20.296380166715245 setps: 8 count: 516\n",
      "reward: 22.727720259167835 setps: 11 count: 527\n",
      "reward: 24.936544267747372 setps: 14 count: 541\n",
      "reward: 20.14505569097528 setps: 9 count: 550\n",
      "reward: 18.178493603259266 setps: 12 count: 562\n",
      "reward: 21.499036535067717 setps: 11 count: 573\n",
      "reward: 17.506575718449312 setps: 18 count: 591\n",
      "reward: 16.781879988196305 setps: 5 count: 596\n",
      "reward: 23.855096632603093 setps: 18 count: 614\n",
      "reward: 18.34478324249503 setps: 13 count: 627\n",
      "reward: 20.569078911659016 setps: 10 count: 637\n",
      "reward: 21.610378441622018 setps: 9 count: 646\n",
      "reward: 26.756845662239353 setps: 23 count: 669\n",
      "reward: 19.578665946373075 setps: 8 count: 677\n",
      "reward: 19.765895238333908 setps: 13 count: 690\n",
      "reward: 22.462712116261535 setps: 16 count: 706\n",
      "reward: 19.1706065392209 setps: 14 count: 720\n",
      "reward: 25.21183347274491 setps: 14 count: 734\n",
      "reward: 18.747514857458007 setps: 15 count: 749\n",
      "reward: 20.415830333587653 setps: 8 count: 757\n",
      "reward: 20.178940688955485 setps: 13 count: 770\n",
      "reward: 21.004408671590497 setps: 15 count: 785\n",
      "reward: 21.317462889206944 setps: 11 count: 796\n",
      "reward: 18.98967876029492 setps: 23 count: 819\n",
      "reward: 22.10876309793384 setps: 10 count: 829\n",
      "reward: 17.835255807251087 setps: 8 count: 837\n",
      "reward: 20.434889454625957 setps: 8 count: 845\n",
      "reward: 20.096150887824475 setps: 14 count: 859\n",
      "reward: 28.422177244794145 setps: 23 count: 882\n",
      "reward: 21.92841461336066 setps: 11 count: 893\n",
      "reward: 19.863832522036677 setps: 16 count: 909\n",
      "reward: 20.244243758390073 setps: 9 count: 918\n",
      "reward: 20.740309191960836 setps: 8 count: 926\n",
      "reward: 21.235046094466817 setps: 10 count: 936\n",
      "reward: 20.61790115200274 setps: 15 count: 951\n",
      "reward: 21.763729312180658 setps: 10 count: 961\n",
      "reward: 25.240172596130286 setps: 14 count: 975\n",
      "reward: 22.081607468234143 setps: 12 count: 987\n",
      "avg rewards: 20.694664722161097\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:0.40067\n",
      "Epoch:20 Batch:1 Loss:0.17261\n",
      "Epoch:40 Batch:1 Loss:0.14617\n",
      "Epoch:60 Batch:1 Loss:0.12202\n",
      "Epoch:80 Batch:1 Loss:0.08822\n",
      "Epoch:100 Batch:1 Loss:0.06459\n",
      "Epoch:120 Batch:1 Loss:0.05706\n",
      "Epoch:140 Batch:1 Loss:0.05195\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.128\n",
      "Epoch:10 Batch:10 Loss:0.139\n",
      "Epoch:20 Batch:10 Loss:0.125\n",
      "Epoch:30 Batch:10 Loss:0.125\n",
      "Epoch:40 Batch:10 Loss:0.122\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 46.2516000765405 setps: 47 count: 47\n",
      "reward: 46.62015372758905 setps: 48 count: 95\n",
      "reward: 71.793672974121 setps: 76 count: 171\n",
      "reward: 44.28302566443454 setps: 77 count: 248\n",
      "reward: 54.71778465009994 setps: 57 count: 305\n",
      "reward: 57.86340204456501 setps: 61 count: 366\n",
      "reward: 45.089897151004685 setps: 48 count: 414\n",
      "reward: 42.593539641147096 setps: 76 count: 490\n",
      "reward: 51.381954990472856 setps: 52 count: 542\n",
      "reward: 55.79643264170153 setps: 58 count: 600\n",
      "reward: 51.45778936619754 setps: 52 count: 652\n",
      "reward: 37.78915625519149 setps: 42 count: 694\n",
      "reward: 45.05590936218504 setps: 45 count: 739\n",
      "reward: 51.24425829239772 setps: 52 count: 791\n",
      "reward: 45.84965290713444 setps: 47 count: 838\n",
      "reward: 48.30130690764927 setps: 49 count: 887\n",
      "reward: 50.788221470256396 setps: 53 count: 940\n",
      "reward: 45.19665392952303 setps: 46 count: 986\n",
      "avg rewards: 49.55968955845617\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:0.34540\n",
      "Epoch:20 Batch:2 Loss:0.11758\n",
      "Epoch:40 Batch:2 Loss:0.08958\n",
      "Epoch:60 Batch:2 Loss:0.05945\n",
      "Epoch:80 Batch:2 Loss:0.03990\n",
      "Epoch:100 Batch:2 Loss:0.03271\n",
      "Epoch:120 Batch:2 Loss:0.02934\n",
      "Epoch:140 Batch:2 Loss:0.02719\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.078\n",
      "Epoch:10 Batch:10 Loss:0.080\n",
      "Epoch:20 Batch:10 Loss:0.080\n",
      "Epoch:30 Batch:10 Loss:0.074\n",
      "Epoch:40 Batch:10 Loss:0.084\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 39.00742270022601 setps: 40 count: 40\n",
      "reward: 39.462713366811656 setps: 40 count: 80\n",
      "reward: 23.563131133814746 setps: 36 count: 116\n",
      "reward: 33.63264451345967 setps: 37 count: 153\n",
      "reward: 38.56251236041062 setps: 40 count: 193\n",
      "reward: 46.08005962056922 setps: 48 count: 241\n",
      "reward: 45.61967298769887 setps: 47 count: 288\n",
      "reward: 44.9254124022118 setps: 44 count: 332\n",
      "reward: 35.52654310191865 setps: 38 count: 370\n",
      "reward: 36.92007566243991 setps: 41 count: 411\n",
      "reward: 40.947199940291476 setps: 42 count: 453\n",
      "reward: 41.19977490212478 setps: 44 count: 497\n",
      "reward: 46.93207785254925 setps: 48 count: 545\n",
      "reward: 29.35245826829632 setps: 34 count: 579\n",
      "reward: 44.77866800541522 setps: 47 count: 626\n",
      "reward: 30.147686075010284 setps: 35 count: 661\n",
      "reward: 41.415916021751755 setps: 44 count: 705\n",
      "reward: 41.814351428837114 setps: 44 count: 749\n",
      "reward: 44.61800723507621 setps: 44 count: 793\n",
      "reward: 36.203663359586784 setps: 39 count: 832\n",
      "reward: 43.01908691940915 setps: 43 count: 875\n",
      "reward: 41.1672628077882 setps: 41 count: 916\n",
      "reward: 35.17925517012518 setps: 38 count: 954\n",
      "reward: 34.07135139456004 setps: 37 count: 991\n",
      "avg rewards: 38.922789467932624\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:3 Loss:0.31566\n",
      "Epoch:20 Batch:3 Loss:0.09044\n",
      "Epoch:40 Batch:3 Loss:0.05580\n",
      "Epoch:60 Batch:3 Loss:0.03736\n",
      "Epoch:80 Batch:3 Loss:0.02939\n",
      "Epoch:100 Batch:3 Loss:0.02252\n",
      "Epoch:120 Batch:3 Loss:0.02061\n",
      "Epoch:140 Batch:3 Loss:0.02103\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.083\n",
      "Epoch:10 Batch:10 Loss:0.076\n",
      "Epoch:20 Batch:10 Loss:0.071\n",
      "Epoch:30 Batch:10 Loss:0.073\n",
      "Epoch:40 Batch:10 Loss:0.077\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 3.4809973708030153 setps: 28 count: 28\n",
      "reward: 11.107525319993872 setps: 30 count: 58\n",
      "reward: 10.725459773655164 setps: 34 count: 92\n",
      "reward: 6.230503145199327 setps: 31 count: 123\n",
      "reward: 10.244574635780008 setps: 31 count: 154\n",
      "reward: 10.84323888282888 setps: 30 count: 184\n",
      "reward: 10.365732594076688 setps: 30 count: 214\n",
      "reward: 12.092185292704015 setps: 31 count: 245\n",
      "reward: 15.310723925323689 setps: 34 count: 279\n",
      "reward: 8.379851853226864 setps: 31 count: 310\n",
      "reward: 7.926655410129749 setps: 32 count: 342\n",
      "reward: 3.1224681168328967 setps: 29 count: 371\n",
      "reward: 8.961723591602457 setps: 30 count: 401\n",
      "reward: 6.728342107596107 setps: 31 count: 432\n",
      "reward: 31.422808405492106 setps: 39 count: 471\n",
      "reward: 6.882585377298526 setps: 31 count: 502\n",
      "reward: 8.031999402849031 setps: 32 count: 534\n",
      "reward: 10.090572731129031 setps: 30 count: 564\n",
      "reward: 15.923694703528593 setps: 35 count: 599\n",
      "reward: 6.901076309293784 setps: 31 count: 630\n",
      "reward: 5.5486772367614305 setps: 30 count: 660\n",
      "reward: 9.058363061504496 setps: 32 count: 692\n",
      "reward: 12.77455897614709 setps: 33 count: 725\n",
      "reward: 10.988460200317602 setps: 30 count: 755\n",
      "reward: 7.122888065663573 setps: 31 count: 786\n",
      "reward: 13.04363178537751 setps: 35 count: 821\n",
      "reward: 12.057519159965157 setps: 31 count: 852\n",
      "reward: 14.107722765174548 setps: 32 count: 884\n",
      "reward: 14.280106852058088 setps: 34 count: 918\n",
      "reward: 14.016960080286658 setps: 33 count: 951\n",
      "reward: 21.311159563981345 setps: 38 count: 989\n",
      "avg rewards: 10.93815376440585\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.31525\n",
      "Epoch:20 Batch:4 Loss:0.06761\n",
      "Epoch:40 Batch:4 Loss:0.03775\n",
      "Epoch:60 Batch:4 Loss:0.02521\n",
      "Epoch:80 Batch:4 Loss:0.01990\n",
      "Epoch:100 Batch:4 Loss:0.01953\n",
      "Epoch:120 Batch:4 Loss:0.01761\n",
      "Epoch:140 Batch:4 Loss:0.01687\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.074\n",
      "Epoch:10 Batch:10 Loss:0.070\n",
      "Epoch:20 Batch:10 Loss:0.075\n",
      "Epoch:30 Batch:10 Loss:0.068\n",
      "Epoch:40 Batch:10 Loss:0.070\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 26.402740913977325 setps: 38 count: 38\n",
      "reward: 12.472504557017237 setps: 31 count: 69\n",
      "reward: 26.46560281930287 setps: 37 count: 106\n",
      "reward: 29.10778390560154 setps: 36 count: 142\n",
      "reward: 24.350815278085065 setps: 34 count: 176\n",
      "reward: 25.089268372883094 setps: 37 count: 213\n",
      "reward: 21.434290961023358 setps: 34 count: 247\n",
      "reward: 30.021542441770727 setps: 39 count: 286\n",
      "reward: 29.9360613445242 setps: 40 count: 326\n",
      "reward: 25.362612350229753 setps: 38 count: 364\n",
      "reward: 26.061477494231077 setps: 35 count: 399\n",
      "reward: 37.15570415270778 setps: 42 count: 441\n",
      "reward: 27.71346638546238 setps: 41 count: 482\n",
      "reward: 11.504003907648439 setps: 31 count: 513\n",
      "reward: 28.099462598796524 setps: 37 count: 550\n",
      "reward: 20.108428359244865 setps: 33 count: 583\n",
      "reward: 36.77622001363924 setps: 41 count: 624\n",
      "reward: 25.126227659182035 setps: 33 count: 657\n",
      "reward: 27.287851390698055 setps: 36 count: 693\n",
      "reward: 34.960862932658344 setps: 44 count: 737\n",
      "reward: 30.26239313851984 setps: 41 count: 778\n",
      "reward: 33.83269171050924 setps: 40 count: 818\n",
      "reward: 27.25441989271785 setps: 38 count: 856\n",
      "reward: 25.85905306388595 setps: 38 count: 894\n",
      "reward: 23.844004179914187 setps: 33 count: 927\n",
      "reward: 25.450934495449477 setps: 34 count: 961\n",
      "reward: 27.45293527185131 setps: 35 count: 996\n",
      "avg rewards: 26.644198503390065\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.30053\n",
      "Epoch:20 Batch:5 Loss:0.05316\n",
      "Epoch:40 Batch:5 Loss:0.02679\n",
      "Epoch:60 Batch:5 Loss:0.02115\n",
      "Epoch:80 Batch:5 Loss:0.01762\n",
      "Epoch:100 Batch:5 Loss:0.01688\n",
      "Epoch:120 Batch:5 Loss:0.01452\n",
      "Epoch:140 Batch:5 Loss:0.01498\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.065\n",
      "Epoch:10 Batch:10 Loss:0.070\n",
      "Epoch:20 Batch:10 Loss:0.068\n",
      "Epoch:30 Batch:10 Loss:0.074\n",
      "Epoch:40 Batch:10 Loss:0.063\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 46.9971942328848 setps: 34 count: 34\n",
      "reward: 44.7226561483607 setps: 32 count: 66\n",
      "reward: 48.18885575882014 setps: 33 count: 99\n",
      "reward: 46.53011792673787 setps: 33 count: 132\n",
      "reward: 48.009111030791246 setps: 35 count: 167\n",
      "reward: 46.617718325159515 setps: 34 count: 201\n",
      "reward: 51.87048878628119 setps: 36 count: 237\n",
      "reward: 45.62176499894995 setps: 32 count: 269\n",
      "reward: 47.16068312501011 setps: 34 count: 303\n",
      "reward: 46.12982395867437 setps: 33 count: 336\n",
      "reward: 46.56903815944534 setps: 33 count: 369\n",
      "reward: 45.78987147294101 setps: 33 count: 402\n",
      "reward: 47.51604844718821 setps: 35 count: 437\n",
      "reward: 46.938483126006034 setps: 32 count: 469\n",
      "reward: 43.420990435428386 setps: 30 count: 499\n",
      "reward: 47.55636595975374 setps: 32 count: 531\n",
      "reward: 51.01199930820731 setps: 36 count: 567\n",
      "reward: 42.22003507986811 setps: 29 count: 596\n",
      "reward: 48.231129537445675 setps: 34 count: 630\n",
      "reward: 42.30736865824146 setps: 30 count: 660\n",
      "reward: 49.91679388051998 setps: 36 count: 696\n",
      "reward: 43.36476529091597 setps: 29 count: 725\n",
      "reward: 48.756915862299515 setps: 33 count: 758\n",
      "reward: 50.84052479132079 setps: 36 count: 794\n",
      "reward: 45.13312524975917 setps: 31 count: 825\n",
      "reward: 50.02660392104153 setps: 35 count: 860\n",
      "reward: 51.82658198824793 setps: 37 count: 897\n",
      "reward: 46.273784838843866 setps: 32 count: 929\n",
      "reward: 47.92514995100064 setps: 37 count: 966\n",
      "reward: 45.88487976941688 setps: 32 count: 998\n",
      "avg rewards: 47.11196233398537\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.25709\n",
      "Epoch:20 Batch:6 Loss:0.04929\n",
      "Epoch:40 Batch:6 Loss:0.02339\n",
      "Epoch:60 Batch:6 Loss:0.01696\n",
      "Epoch:80 Batch:6 Loss:0.01667\n",
      "Epoch:100 Batch:6 Loss:0.01464\n",
      "Epoch:120 Batch:6 Loss:0.01190\n",
      "Epoch:140 Batch:6 Loss:0.01306\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.069\n",
      "Epoch:10 Batch:10 Loss:0.071\n",
      "Epoch:20 Batch:10 Loss:0.061\n",
      "Epoch:30 Batch:10 Loss:0.060\n",
      "Epoch:40 Batch:10 Loss:0.065\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 27.004593013253185 setps: 36 count: 36\n",
      "reward: 10.286393286993551 setps: 32 count: 68\n",
      "reward: 19.962457123943018 setps: 34 count: 102\n",
      "reward: 13.846880769429845 setps: 31 count: 133\n",
      "reward: 27.57872288434446 setps: 36 count: 169\n",
      "reward: 8.393090844927064 setps: 31 count: 200\n",
      "reward: 23.378707248165306 setps: 36 count: 236\n",
      "reward: 24.875933302582414 setps: 39 count: 275\n",
      "reward: 27.914545835675376 setps: 40 count: 315\n",
      "reward: 28.14541520937637 setps: 41 count: 356\n",
      "reward: 29.43605032020714 setps: 36 count: 392\n",
      "reward: 15.77848926659499 setps: 32 count: 424\n",
      "reward: 19.739878515808957 setps: 33 count: 457\n",
      "reward: 18.138456521912307 setps: 35 count: 492\n",
      "reward: 23.349651251622706 setps: 34 count: 526\n",
      "reward: 20.937337106595805 setps: 36 count: 562\n",
      "reward: 11.228768993832999 setps: 31 count: 593\n",
      "reward: 19.12737736766139 setps: 35 count: 628\n",
      "reward: 29.646732997900475 setps: 36 count: 664\n",
      "reward: 19.722564807880556 setps: 35 count: 699\n",
      "reward: 25.438633580600435 setps: 35 count: 734\n",
      "reward: 16.757675153397575 setps: 32 count: 766\n",
      "reward: 20.4879022957015 setps: 34 count: 800\n",
      "reward: 13.553248041593177 setps: 32 count: 832\n",
      "reward: 21.109381745762946 setps: 33 count: 865\n",
      "reward: 25.025739902239724 setps: 34 count: 899\n",
      "reward: 21.69940456362237 setps: 37 count: 936\n",
      "reward: 28.43339584715431 setps: 39 count: 975\n",
      "avg rewards: 21.107050992813566\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.27636\n",
      "Epoch:20 Batch:7 Loss:0.03493\n",
      "Epoch:40 Batch:7 Loss:0.02154\n",
      "Epoch:60 Batch:7 Loss:0.01463\n",
      "Epoch:80 Batch:7 Loss:0.01407\n",
      "Epoch:100 Batch:7 Loss:0.01259\n",
      "Epoch:120 Batch:7 Loss:0.01181\n",
      "Epoch:140 Batch:7 Loss:0.01242\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:10 Loss:0.065\n",
      "Epoch:10 Batch:10 Loss:0.064\n",
      "Epoch:20 Batch:10 Loss:0.062\n",
      "Epoch:30 Batch:10 Loss:0.059\n",
      "Epoch:40 Batch:10 Loss:0.064\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 51.47245947885678 setps: 37 count: 37\n",
      "reward: 47.815681426812084 setps: 36 count: 73\n",
      "reward: 48.242156753344176 setps: 35 count: 108\n",
      "reward: 53.12047395345872 setps: 37 count: 145\n",
      "reward: 44.59028033375653 setps: 48 count: 193\n",
      "reward: 49.19861661695178 setps: 34 count: 227\n",
      "reward: 45.28827040563338 setps: 35 count: 262\n",
      "reward: 45.38244987802173 setps: 31 count: 293\n",
      "reward: 49.128194084763514 setps: 33 count: 326\n",
      "reward: 55.97251896763482 setps: 37 count: 363\n",
      "reward: 54.14427132889397 setps: 37 count: 400\n",
      "reward: 52.48166267535416 setps: 36 count: 436\n",
      "reward: 52.84661656886019 setps: 36 count: 472\n",
      "reward: 50.86230367036478 setps: 35 count: 507\n",
      "reward: 49.15221247746812 setps: 35 count: 542\n",
      "reward: 46.351886036062204 setps: 32 count: 574\n",
      "reward: 55.23099059526429 setps: 40 count: 614\n",
      "reward: 52.49399676981265 setps: 35 count: 649\n",
      "reward: 49.768757513660255 setps: 40 count: 689\n",
      "reward: 53.32339006573747 setps: 37 count: 726\n",
      "reward: 46.440369978106176 setps: 31 count: 757\n",
      "reward: 49.2370034734442 setps: 36 count: 793\n",
      "reward: 50.450819658199904 setps: 33 count: 826\n",
      "reward: 50.851296758129315 setps: 36 count: 862\n",
      "reward: 48.26455819794354 setps: 32 count: 894\n",
      "reward: 51.1246002628337 setps: 34 count: 928\n",
      "reward: 47.80288220825315 setps: 32 count: 960\n",
      "reward: 52.034968821122305 setps: 37 count: 997\n",
      "avg rewards: 50.10977460566942\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.25523\n",
      "Epoch:20 Batch:8 Loss:0.03333\n",
      "Epoch:40 Batch:8 Loss:0.01970\n",
      "Epoch:60 Batch:8 Loss:0.01498\n",
      "Epoch:80 Batch:8 Loss:0.01430\n",
      "Epoch:100 Batch:8 Loss:0.01324\n",
      "Epoch:120 Batch:8 Loss:0.01249\n",
      "Epoch:140 Batch:8 Loss:0.01130\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.056\n",
      "Epoch:10 Batch:10 Loss:0.054\n",
      "Epoch:20 Batch:10 Loss:0.054\n",
      "Epoch:30 Batch:10 Loss:0.054\n",
      "Epoch:40 Batch:10 Loss:0.054\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 63.84304616952139 setps: 56 count: 56\n",
      "reward: 65.36187045896223 setps: 58 count: 114\n",
      "reward: 55.754771285486655 setps: 51 count: 165\n",
      "reward: 46.42078487695251 setps: 47 count: 212\n",
      "reward: 56.990151337621505 setps: 49 count: 261\n",
      "reward: 62.77372566225529 setps: 54 count: 315\n",
      "reward: 50.937202545480986 setps: 48 count: 363\n",
      "reward: 53.33112135768898 setps: 49 count: 412\n",
      "reward: 56.657759306984374 setps: 50 count: 462\n",
      "reward: 44.5542805971985 setps: 44 count: 506\n",
      "reward: 40.81293285861612 setps: 45 count: 551\n",
      "reward: 60.878696293261605 setps: 54 count: 605\n",
      "reward: 49.8712513967941 setps: 46 count: 651\n",
      "reward: 53.151836731158255 setps: 49 count: 700\n",
      "reward: 63.070735771335606 setps: 56 count: 756\n",
      "reward: 51.5595390582559 setps: 48 count: 804\n",
      "reward: 61.43487182506505 setps: 55 count: 859\n",
      "reward: 52.88038817026683 setps: 47 count: 906\n",
      "reward: 53.28866165293148 setps: 48 count: 954\n",
      "avg rewards: 54.924927755570394\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.21118\n",
      "Epoch:20 Batch:9 Loss:0.02849\n",
      "Epoch:40 Batch:9 Loss:0.01552\n",
      "Epoch:60 Batch:9 Loss:0.01311\n",
      "Epoch:80 Batch:9 Loss:0.01154\n",
      "Epoch:100 Batch:9 Loss:0.01101\n",
      "Epoch:120 Batch:9 Loss:0.01070\n",
      "Epoch:140 Batch:9 Loss:0.01064\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.057\n",
      "Epoch:10 Batch:10 Loss:0.050\n",
      "Epoch:20 Batch:10 Loss:0.055\n",
      "Epoch:30 Batch:10 Loss:0.054\n",
      "Epoch:40 Batch:10 Loss:0.049\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 54.81665213000816 setps: 68 count: 68\n",
      "reward: 64.21185803932894 setps: 51 count: 119\n",
      "reward: 58.12753980235138 setps: 46 count: 165\n",
      "reward: 104.45420222392424 setps: 93 count: 258\n",
      "reward: 129.97131366370013 setps: 111 count: 369\n",
      "reward: 54.6874108267788 setps: 40 count: 409\n",
      "reward: 48.09477662014107 setps: 34 count: 443\n",
      "reward: 61.63269659505021 setps: 45 count: 488\n",
      "reward: 55.96525052295327 setps: 40 count: 528\n",
      "reward: 99.6844683916527 setps: 92 count: 620\n",
      "reward: 43.86430631197 setps: 32 count: 652\n",
      "reward: 70.9641162048545 setps: 68 count: 720\n",
      "reward: 158.23881162369364 setps: 119 count: 839\n",
      "reward: 143.45834649790626 setps: 107 count: 946\n",
      "reward: 58.55222472926252 setps: 43 count: 989\n",
      "avg rewards: 80.44826494557171\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.25169\n",
      "Epoch:20 Batch:10 Loss:0.02791\n",
      "Epoch:40 Batch:10 Loss:0.01515\n",
      "Epoch:60 Batch:10 Loss:0.01351\n",
      "Epoch:80 Batch:10 Loss:0.01293\n",
      "Epoch:100 Batch:10 Loss:0.01004\n",
      "Epoch:120 Batch:10 Loss:0.01129\n",
      "Epoch:140 Batch:10 Loss:0.00976\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.025\n",
      "Epoch:10 Batch:10 Loss:0.023\n",
      "Epoch:20 Batch:10 Loss:0.024\n",
      "Epoch:30 Batch:10 Loss:0.024\n",
      "Epoch:40 Batch:10 Loss:0.026\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 122.97229174476084 setps: 105 count: 105\n",
      "reward: 148.75391047943378 setps: 171 count: 276\n",
      "reward: 86.95154384443157 setps: 82 count: 358\n",
      "reward: 72.0967413877457 setps: 67 count: 425\n",
      "reward: 63.33493523707877 setps: 78 count: 503\n",
      "reward: 55.97285482249428 setps: 46 count: 549\n",
      "reward: 96.96071107646273 setps: 83 count: 632\n",
      "reward: 60.296937164354325 setps: 57 count: 689\n",
      "reward: 53.27439664534176 setps: 41 count: 730\n",
      "reward: 90.97771318304733 setps: 99 count: 829\n",
      "reward: 68.93068884971001 setps: 71 count: 900\n",
      "reward: 77.7512545020523 setps: 74 count: 974\n",
      "avg rewards: 83.18949824474278\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.18893\n",
      "Epoch:20 Batch:11 Loss:0.02670\n",
      "Epoch:40 Batch:11 Loss:0.01375\n",
      "Epoch:60 Batch:11 Loss:0.01242\n",
      "Epoch:80 Batch:11 Loss:0.01141\n",
      "Epoch:100 Batch:11 Loss:0.01047\n",
      "Epoch:120 Batch:11 Loss:0.01069\n",
      "Epoch:140 Batch:11 Loss:0.00926\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.025\n",
      "Epoch:10 Batch:10 Loss:0.023\n",
      "Epoch:20 Batch:10 Loss:0.025\n",
      "Epoch:30 Batch:10 Loss:0.023\n",
      "Epoch:40 Batch:10 Loss:0.022\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 61.80892409802618 setps: 57 count: 57\n",
      "reward: 53.639456846046954 setps: 49 count: 106\n",
      "reward: 50.40456334867921 setps: 48 count: 154\n",
      "reward: 56.82863445108231 setps: 53 count: 207\n",
      "reward: 50.554984311129374 setps: 47 count: 254\n",
      "reward: 52.801953248091735 setps: 49 count: 303\n",
      "reward: 53.24170005764173 setps: 50 count: 353\n",
      "reward: 49.71320756887581 setps: 47 count: 400\n",
      "reward: 49.74576860352682 setps: 47 count: 447\n",
      "reward: 55.07547729952928 setps: 50 count: 497\n",
      "reward: 54.341393007566516 setps: 50 count: 547\n",
      "reward: 50.72294443995926 setps: 48 count: 595\n",
      "reward: 47.13421316332825 setps: 46 count: 641\n",
      "reward: 49.63236700251727 setps: 48 count: 689\n",
      "reward: 55.5257673053362 setps: 50 count: 739\n",
      "reward: 54.52959270360299 setps: 50 count: 789\n",
      "reward: 51.97770052160923 setps: 49 count: 838\n",
      "reward: 60.341889808377886 setps: 55 count: 893\n",
      "reward: 52.24371318415361 setps: 51 count: 944\n",
      "reward: 46.81475268004579 setps: 45 count: 989\n",
      "avg rewards: 52.85395018245632\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.22861\n",
      "Epoch:20 Batch:12 Loss:0.02251\n",
      "Epoch:40 Batch:12 Loss:0.01417\n",
      "Epoch:60 Batch:12 Loss:0.01185\n",
      "Epoch:80 Batch:12 Loss:0.01244\n",
      "Epoch:100 Batch:12 Loss:0.00962\n",
      "Epoch:120 Batch:12 Loss:0.01045\n",
      "Epoch:140 Batch:12 Loss:0.00963\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.023\n",
      "Epoch:10 Batch:10 Loss:0.023\n",
      "Epoch:20 Batch:10 Loss:0.022\n",
      "Epoch:30 Batch:10 Loss:0.022\n",
      "Epoch:40 Batch:10 Loss:0.021\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 74.57874972844148 setps: 61 count: 61\n",
      "reward: 58.729227292457665 setps: 52 count: 113\n",
      "reward: 60.163718826604594 setps: 52 count: 165\n",
      "reward: 67.71988985156491 setps: 54 count: 219\n",
      "reward: 69.03142164357558 setps: 59 count: 278\n",
      "reward: 60.38611138693669 setps: 53 count: 331\n",
      "reward: 63.59967858563032 setps: 52 count: 383\n",
      "reward: 60.401803485612625 setps: 51 count: 434\n",
      "reward: 78.5556443956375 setps: 64 count: 498\n",
      "reward: 69.04318974076884 setps: 57 count: 555\n",
      "reward: 64.87701438994846 setps: 54 count: 609\n",
      "reward: 64.28654027670562 setps: 55 count: 664\n",
      "reward: 61.38450448549048 setps: 59 count: 723\n",
      "reward: 63.563464061173725 setps: 52 count: 775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 64.18664048486826 setps: 60 count: 835\n",
      "reward: 59.946705369937995 setps: 54 count: 889\n",
      "reward: 60.29838844390179 setps: 50 count: 939\n",
      "reward: 74.57760126796201 setps: 61 count: 1000\n",
      "avg rewards: 65.29612742873437\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.16622\n",
      "Epoch:20 Batch:13 Loss:0.02200\n",
      "Epoch:40 Batch:13 Loss:0.01479\n",
      "Epoch:60 Batch:13 Loss:0.01095\n",
      "Epoch:80 Batch:13 Loss:0.01184\n",
      "Epoch:100 Batch:13 Loss:0.00990\n",
      "Epoch:120 Batch:13 Loss:0.01013\n",
      "Epoch:140 Batch:13 Loss:0.00819\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.022\n",
      "Epoch:10 Batch:10 Loss:0.020\n",
      "Epoch:20 Batch:10 Loss:0.022\n",
      "Epoch:30 Batch:10 Loss:0.020\n",
      "Epoch:40 Batch:10 Loss:0.022\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 52.91584811277571 setps: 42 count: 42\n",
      "reward: 51.32458778730652 setps: 44 count: 86\n",
      "reward: 46.66405638204744 setps: 39 count: 125\n",
      "reward: 54.43209145747678 setps: 44 count: 169\n",
      "reward: 46.74946021627838 setps: 38 count: 207\n",
      "reward: 51.10297714124026 setps: 45 count: 252\n",
      "reward: 57.65775076390711 setps: 51 count: 303\n",
      "reward: 45.72778443004937 setps: 35 count: 338\n",
      "reward: 50.85658161622559 setps: 43 count: 381\n",
      "reward: 62.33178192067425 setps: 58 count: 439\n",
      "reward: 49.22642092806054 setps: 40 count: 479\n",
      "reward: 49.34743886234793 setps: 41 count: 520\n",
      "reward: 49.47660992333113 setps: 42 count: 562\n",
      "reward: 48.16666357457725 setps: 39 count: 601\n",
      "reward: 53.08080200772381 setps: 48 count: 649\n",
      "reward: 58.2156131756783 setps: 51 count: 700\n",
      "reward: 53.438834645248434 setps: 44 count: 744\n",
      "reward: 48.57537503739732 setps: 40 count: 784\n",
      "reward: 57.15793796994403 setps: 49 count: 833\n",
      "reward: 52.661091492805284 setps: 45 count: 878\n",
      "reward: 45.433709334475864 setps: 38 count: 916\n",
      "reward: 53.13179957704269 setps: 44 count: 960\n",
      "reward: 44.803019784657224 setps: 35 count: 995\n",
      "avg rewards: 51.412097223533536\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.11453\n",
      "Epoch:20 Batch:14 Loss:0.01872\n",
      "Epoch:40 Batch:14 Loss:0.01373\n",
      "Epoch:60 Batch:14 Loss:0.00987\n",
      "Epoch:80 Batch:14 Loss:0.01016\n",
      "Epoch:100 Batch:14 Loss:0.00847\n",
      "Epoch:120 Batch:14 Loss:0.00867\n",
      "Epoch:140 Batch:14 Loss:0.00800\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.022\n",
      "Epoch:10 Batch:10 Loss:0.020\n",
      "Epoch:20 Batch:10 Loss:0.018\n",
      "Epoch:30 Batch:10 Loss:0.018\n",
      "Epoch:40 Batch:10 Loss:0.019\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 53.31118914521503 setps: 47 count: 47\n",
      "reward: 53.56620863975841 setps: 48 count: 95\n",
      "reward: 70.72969892978291 setps: 69 count: 164\n",
      "reward: 69.33822791255517 setps: 60 count: 224\n",
      "reward: 57.823693332668334 setps: 49 count: 273\n",
      "reward: 50.32860362085121 setps: 46 count: 319\n",
      "reward: 52.891591927013366 setps: 48 count: 367\n",
      "reward: 60.86439114758687 setps: 54 count: 421\n",
      "reward: 60.02351139964447 setps: 54 count: 475\n",
      "reward: 57.88264167374582 setps: 49 count: 524\n",
      "reward: 61.309261991204416 setps: 51 count: 575\n",
      "reward: 58.91341587007627 setps: 51 count: 626\n",
      "reward: 56.43994078567629 setps: 51 count: 677\n",
      "reward: 71.6943604215194 setps: 64 count: 741\n",
      "reward: 60.143180751899486 setps: 53 count: 794\n",
      "reward: 57.37311299551367 setps: 51 count: 845\n",
      "reward: 64.58323735756132 setps: 54 count: 899\n",
      "reward: 59.93498667297972 setps: 53 count: 952\n",
      "avg rewards: 59.84173636529178\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.12097\n",
      "Epoch:20 Batch:15 Loss:0.01844\n",
      "Epoch:40 Batch:15 Loss:0.01078\n",
      "Epoch:60 Batch:15 Loss:0.01008\n",
      "Epoch:80 Batch:15 Loss:0.00933\n",
      "Epoch:100 Batch:15 Loss:0.00858\n",
      "Epoch:120 Batch:15 Loss:0.00861\n",
      "Epoch:140 Batch:15 Loss:0.00783\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.017\n",
      "Epoch:10 Batch:10 Loss:0.019\n",
      "Epoch:20 Batch:10 Loss:0.018\n",
      "Epoch:30 Batch:10 Loss:0.018\n",
      "Epoch:40 Batch:10 Loss:0.016\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 88.42696080649182 setps: 84 count: 84\n",
      "reward: 211.12484533761014 setps: 143 count: 227\n",
      "reward: 75.12736080323958 setps: 73 count: 300\n",
      "reward: 146.51981454274355 setps: 154 count: 454\n",
      "reward: 80.04358739535091 setps: 72 count: 526\n",
      "reward: 81.72048911597595 setps: 74 count: 600\n",
      "reward: 76.80836207540561 setps: 71 count: 671\n",
      "reward: 90.71194449842585 setps: 86 count: 757\n",
      "reward: 69.01324270268813 setps: 62 count: 819\n",
      "reward: 103.31147213378246 setps: 98 count: 917\n",
      "reward: 74.31191289946581 setps: 76 count: 993\n",
      "avg rewards: 99.73818111919815\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.15626\n",
      "Epoch:20 Batch:16 Loss:0.01756\n",
      "Epoch:40 Batch:16 Loss:0.01102\n",
      "Epoch:60 Batch:16 Loss:0.01066\n",
      "Epoch:80 Batch:16 Loss:0.00817\n",
      "Epoch:100 Batch:16 Loss:0.00937\n",
      "Epoch:120 Batch:16 Loss:0.00743\n",
      "Epoch:140 Batch:16 Loss:0.00721\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.017\n",
      "Epoch:10 Batch:10 Loss:0.017\n",
      "Epoch:20 Batch:10 Loss:0.018\n",
      "Epoch:30 Batch:10 Loss:0.018\n",
      "Epoch:40 Batch:10 Loss:0.016\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 83.74196541563144 setps: 86 count: 86\n",
      "reward: 91.68363400973831 setps: 82 count: 168\n",
      "reward: 71.66858686401251 setps: 67 count: 235\n",
      "reward: 81.33409029368485 setps: 86 count: 321\n",
      "reward: 73.59296450020773 setps: 75 count: 396\n",
      "reward: 89.94943021842191 setps: 95 count: 491\n",
      "reward: 77.33895370202006 setps: 82 count: 573\n",
      "reward: 58.26131295654922 setps: 55 count: 628\n",
      "reward: 60.91749660622592 setps: 61 count: 689\n",
      "reward: 112.50327235863006 setps: 107 count: 796\n",
      "reward: 72.09407725849451 setps: 79 count: 875\n",
      "reward: 76.71532356322278 setps: 70 count: 945\n",
      "avg rewards: 79.15009231223661\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.12535\n",
      "Epoch:20 Batch:17 Loss:0.01636\n",
      "Epoch:40 Batch:17 Loss:0.01223\n",
      "Epoch:60 Batch:17 Loss:0.00822\n",
      "Epoch:80 Batch:17 Loss:0.00868\n",
      "Epoch:100 Batch:17 Loss:0.00739\n",
      "Epoch:120 Batch:17 Loss:0.00765\n",
      "Epoch:140 Batch:17 Loss:0.00746\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.017\n",
      "Epoch:10 Batch:10 Loss:0.013\n",
      "Epoch:20 Batch:10 Loss:0.014\n",
      "Epoch:30 Batch:10 Loss:0.014\n",
      "Epoch:40 Batch:10 Loss:0.015\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 41.83584655482264 setps: 31 count: 31\n",
      "reward: 41.376202919657224 setps: 30 count: 61\n",
      "reward: 48.490085319208454 setps: 40 count: 101\n",
      "reward: 39.583699114937914 setps: 28 count: 129\n",
      "reward: 43.743924933564266 setps: 32 count: 161\n",
      "reward: 47.3144594094876 setps: 36 count: 197\n",
      "reward: 42.251334601029505 setps: 31 count: 228\n",
      "reward: 43.06378268271072 setps: 31 count: 259\n",
      "reward: 36.58462526991644 setps: 25 count: 284\n",
      "reward: 40.867472535686105 setps: 30 count: 314\n",
      "reward: 48.866917561565074 setps: 36 count: 350\n",
      "reward: 40.71531276024326 setps: 30 count: 380\n",
      "reward: 24.82069785315398 setps: 29 count: 409\n",
      "reward: 40.31699348998226 setps: 30 count: 439\n",
      "reward: 47.17288252787694 setps: 36 count: 475\n",
      "reward: 40.54431045192467 setps: 29 count: 504\n",
      "reward: 43.693747721843835 setps: 31 count: 535\n",
      "reward: 45.64196146114264 setps: 32 count: 567\n",
      "reward: 44.51363734063054 setps: 33 count: 600\n",
      "reward: 48.56569435118727 setps: 38 count: 638\n",
      "reward: 44.94180575441133 setps: 33 count: 671\n",
      "reward: 42.45714479891176 setps: 33 count: 704\n",
      "reward: 42.95387404245122 setps: 31 count: 735\n",
      "reward: 41.15051434401685 setps: 30 count: 765\n",
      "reward: 41.79662167322968 setps: 30 count: 795\n",
      "reward: 41.749855945566374 setps: 30 count: 825\n",
      "reward: 49.68911255524727 setps: 39 count: 864\n",
      "reward: 40.819090504475746 setps: 29 count: 893\n",
      "reward: 41.40187703066912 setps: 30 count: 923\n",
      "reward: 39.351938379179046 setps: 29 count: 952\n",
      "reward: 45.09633744554593 setps: 32 count: 984\n",
      "avg rewards: 42.62489552691211\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.09555\n",
      "Epoch:20 Batch:18 Loss:0.01635\n",
      "Epoch:40 Batch:18 Loss:0.01032\n",
      "Epoch:60 Batch:18 Loss:0.00897\n",
      "Epoch:80 Batch:18 Loss:0.00811\n",
      "Epoch:100 Batch:18 Loss:0.00738\n",
      "Epoch:120 Batch:18 Loss:0.00654\n",
      "Epoch:140 Batch:18 Loss:0.00723\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.016\n",
      "Epoch:10 Batch:10 Loss:0.014\n",
      "Epoch:20 Batch:10 Loss:0.015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:30 Batch:10 Loss:0.015\n",
      "Epoch:40 Batch:10 Loss:0.015\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 82.81838706417697 setps: 82 count: 82\n",
      "reward: 98.61619178540859 setps: 98 count: 180\n",
      "reward: 121.82817610809582 setps: 130 count: 310\n",
      "reward: 59.76526148924459 setps: 57 count: 367\n",
      "reward: 112.94221877298548 setps: 119 count: 486\n",
      "reward: 99.88956166645987 setps: 99 count: 585\n",
      "reward: 57.88265200759341 setps: 54 count: 639\n",
      "reward: 86.08039172509089 setps: 83 count: 722\n",
      "reward: 52.15169400964223 setps: 47 count: 769\n",
      "reward: 135.63957188489007 setps: 149 count: 918\n",
      "avg rewards: 90.76141065135879\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.06857\n",
      "Epoch:20 Batch:19 Loss:0.01612\n",
      "Epoch:40 Batch:19 Loss:0.01075\n",
      "Epoch:60 Batch:19 Loss:0.00841\n",
      "Epoch:80 Batch:19 Loss:0.00714\n",
      "Epoch:100 Batch:19 Loss:0.00653\n",
      "Epoch:120 Batch:19 Loss:0.00710\n",
      "Epoch:140 Batch:19 Loss:0.00566\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.014\n",
      "Epoch:10 Batch:10 Loss:0.014\n",
      "Epoch:20 Batch:10 Loss:0.013\n",
      "Epoch:30 Batch:10 Loss:0.013\n",
      "Epoch:40 Batch:10 Loss:0.014\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 66.80378743873298 setps: 57 count: 57\n",
      "reward: 175.98231944913564 setps: 193 count: 250\n",
      "reward: 72.60368894399 setps: 59 count: 309\n",
      "reward: 64.66453819065063 setps: 54 count: 363\n",
      "reward: 75.90760448449107 setps: 67 count: 430\n",
      "reward: 145.83354534343383 setps: 153 count: 583\n",
      "reward: 69.1029080192311 setps: 57 count: 640\n",
      "reward: 64.5423599351561 setps: 54 count: 694\n",
      "reward: 58.26112284648552 setps: 49 count: 743\n",
      "reward: 102.50466224437548 setps: 98 count: 841\n",
      "reward: 80.45695395875227 setps: 73 count: 914\n",
      "reward: 63.47690952151314 setps: 54 count: 968\n",
      "avg rewards: 86.67836669799563\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.06999\n",
      "Epoch:20 Batch:20 Loss:0.01435\n",
      "Epoch:40 Batch:20 Loss:0.00966\n",
      "Epoch:60 Batch:20 Loss:0.00851\n",
      "Epoch:80 Batch:20 Loss:0.00726\n",
      "Epoch:100 Batch:20 Loss:0.00694\n",
      "Epoch:120 Batch:20 Loss:0.00815\n",
      "Epoch:140 Batch:20 Loss:0.00553\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.011\n",
      "Epoch:10 Batch:10 Loss:0.014\n",
      "Epoch:20 Batch:10 Loss:0.012\n",
      "Epoch:30 Batch:10 Loss:0.014\n",
      "Epoch:40 Batch:10 Loss:0.015\n",
      "Done!\n",
      "############# start HalfCheetahBulletEnv-v0 training ###################\n",
      "(50, 1000, 26)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1037.5825255152004 setps: 800 count: 800\n",
      "avg rewards: -1037.5825255152004\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:0.37372\n",
      "Epoch:20 Batch:1 Loss:0.24689\n",
      "Epoch:40 Batch:1 Loss:0.20823\n",
      "Epoch:60 Batch:1 Loss:0.20151\n",
      "Epoch:80 Batch:1 Loss:0.19287\n",
      "Epoch:100 Batch:1 Loss:0.17694\n",
      "Epoch:120 Batch:1 Loss:0.15619\n",
      "Epoch:140 Batch:1 Loss:0.14179\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.270\n",
      "Epoch:10 Batch:10 Loss:0.272\n",
      "Epoch:20 Batch:10 Loss:0.274\n",
      "Epoch:30 Batch:10 Loss:0.277\n",
      "Epoch:40 Batch:10 Loss:0.274\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1160.640817266238 setps: 800 count: 800\n",
      "avg rewards: -1160.640817266238\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:0.39380\n",
      "Epoch:20 Batch:2 Loss:0.13107\n",
      "Epoch:40 Batch:2 Loss:0.10317\n",
      "Epoch:60 Batch:2 Loss:0.08899\n",
      "Epoch:80 Batch:2 Loss:0.07772\n",
      "Epoch:100 Batch:2 Loss:0.07335\n",
      "Epoch:120 Batch:2 Loss:0.06312\n",
      "Epoch:140 Batch:2 Loss:0.05597\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.216\n",
      "Epoch:10 Batch:10 Loss:0.204\n",
      "Epoch:20 Batch:10 Loss:0.205\n",
      "Epoch:30 Batch:10 Loss:0.208\n",
      "Epoch:40 Batch:10 Loss:0.203\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1134.7796150111103 setps: 800 count: 800\n",
      "avg rewards: -1134.7796150111103\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.37932\n",
      "Epoch:20 Batch:3 Loss:0.10628\n",
      "Epoch:40 Batch:3 Loss:0.07373\n",
      "Epoch:60 Batch:3 Loss:0.06298\n",
      "Epoch:80 Batch:3 Loss:0.05140\n",
      "Epoch:100 Batch:3 Loss:0.04685\n",
      "Epoch:120 Batch:3 Loss:0.04239\n",
      "Epoch:140 Batch:3 Loss:0.03954\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.176\n",
      "Epoch:10 Batch:10 Loss:0.169\n",
      "Epoch:20 Batch:10 Loss:0.163\n",
      "Epoch:30 Batch:10 Loss:0.167\n",
      "Epoch:40 Batch:10 Loss:0.168\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1268.3832021958956 setps: 800 count: 800\n",
      "avg rewards: -1268.3832021958956\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.37750\n",
      "Epoch:20 Batch:4 Loss:0.09085\n",
      "Epoch:40 Batch:4 Loss:0.05926\n",
      "Epoch:60 Batch:4 Loss:0.05307\n",
      "Epoch:80 Batch:4 Loss:0.03878\n",
      "Epoch:100 Batch:4 Loss:0.03897\n",
      "Epoch:120 Batch:4 Loss:0.03434\n",
      "Epoch:140 Batch:4 Loss:0.03233\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.112\n",
      "Epoch:10 Batch:10 Loss:0.111\n",
      "Epoch:20 Batch:10 Loss:0.108\n",
      "Epoch:30 Batch:10 Loss:0.109\n",
      "Epoch:40 Batch:10 Loss:0.104\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1432.3903031657223 setps: 800 count: 800\n",
      "avg rewards: -1432.3903031657223\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.39769\n",
      "Epoch:20 Batch:5 Loss:0.08007\n",
      "Epoch:40 Batch:5 Loss:0.05678\n",
      "Epoch:60 Batch:5 Loss:0.04172\n",
      "Epoch:80 Batch:5 Loss:0.03355\n",
      "Epoch:100 Batch:5 Loss:0.03523\n",
      "Epoch:120 Batch:5 Loss:0.03355\n",
      "Epoch:140 Batch:5 Loss:0.02982\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.070\n",
      "Epoch:10 Batch:10 Loss:0.069\n",
      "Epoch:20 Batch:10 Loss:0.069\n",
      "Epoch:30 Batch:10 Loss:0.066\n",
      "Epoch:40 Batch:10 Loss:0.065\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1118.9155701062841 setps: 800 count: 800\n",
      "avg rewards: -1118.9155701062841\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.39327\n",
      "Epoch:20 Batch:6 Loss:0.07097\n",
      "Epoch:40 Batch:6 Loss:0.04884\n",
      "Epoch:60 Batch:6 Loss:0.03398\n",
      "Epoch:80 Batch:6 Loss:0.03065\n",
      "Epoch:100 Batch:6 Loss:0.03221\n",
      "Epoch:120 Batch:6 Loss:0.02580\n",
      "Epoch:140 Batch:6 Loss:0.02526\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.064\n",
      "Epoch:10 Batch:10 Loss:0.061\n",
      "Epoch:20 Batch:10 Loss:0.061\n",
      "Epoch:30 Batch:10 Loss:0.062\n",
      "Epoch:40 Batch:10 Loss:0.060\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1377.1521022910404 setps: 800 count: 800\n",
      "avg rewards: -1377.1521022910404\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.37964\n",
      "Epoch:20 Batch:7 Loss:0.06670\n",
      "Epoch:40 Batch:7 Loss:0.04172\n",
      "Epoch:60 Batch:7 Loss:0.03139\n",
      "Epoch:80 Batch:7 Loss:0.02686\n",
      "Epoch:100 Batch:7 Loss:0.02587\n",
      "Epoch:120 Batch:7 Loss:0.02251\n",
      "Epoch:140 Batch:7 Loss:0.02280\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.058\n",
      "Epoch:10 Batch:10 Loss:0.058\n",
      "Epoch:20 Batch:10 Loss:0.056\n",
      "Epoch:30 Batch:10 Loss:0.054\n",
      "Epoch:40 Batch:10 Loss:0.055\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1176.4057866930352 setps: 800 count: 800\n",
      "avg rewards: -1176.4057866930352\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.37945\n",
      "Epoch:20 Batch:8 Loss:0.06234\n",
      "Epoch:40 Batch:8 Loss:0.03291\n",
      "Epoch:60 Batch:8 Loss:0.02940\n",
      "Epoch:80 Batch:8 Loss:0.02642\n",
      "Epoch:100 Batch:8 Loss:0.02449\n",
      "Epoch:120 Batch:8 Loss:0.02276\n",
      "Epoch:140 Batch:8 Loss:0.01881\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.058\n",
      "Epoch:10 Batch:10 Loss:0.056\n",
      "Epoch:20 Batch:10 Loss:0.056\n",
      "Epoch:30 Batch:10 Loss:0.054\n",
      "Epoch:40 Batch:10 Loss:0.055\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1177.9926795288038 setps: 800 count: 800\n",
      "avg rewards: -1177.9926795288038\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.36799\n",
      "Epoch:20 Batch:9 Loss:0.05978\n",
      "Epoch:40 Batch:9 Loss:0.03781\n",
      "Epoch:60 Batch:9 Loss:0.02803\n",
      "Epoch:80 Batch:9 Loss:0.02362\n",
      "Epoch:100 Batch:9 Loss:0.02419\n",
      "Epoch:120 Batch:9 Loss:0.01852\n",
      "Epoch:140 Batch:9 Loss:0.02047\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10 Batch:10 Loss:0.053\n",
      "Epoch:20 Batch:10 Loss:0.051\n",
      "Epoch:30 Batch:10 Loss:0.049\n",
      "Epoch:40 Batch:10 Loss:0.050\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1015.824575540504 setps: 800 count: 800\n",
      "avg rewards: -1015.824575540504\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.32962\n",
      "Epoch:20 Batch:10 Loss:0.05603\n",
      "Epoch:40 Batch:10 Loss:0.03229\n",
      "Epoch:60 Batch:10 Loss:0.02915\n",
      "Epoch:80 Batch:10 Loss:0.02232\n",
      "Epoch:100 Batch:10 Loss:0.02160\n",
      "Epoch:120 Batch:10 Loss:0.02002\n",
      "Epoch:140 Batch:10 Loss:0.01645\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.050\n",
      "Epoch:10 Batch:10 Loss:0.049\n",
      "Epoch:20 Batch:10 Loss:0.048\n",
      "Epoch:30 Batch:10 Loss:0.050\n",
      "Epoch:40 Batch:10 Loss:0.049\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1276.0007272450105 setps: 800 count: 800\n",
      "avg rewards: -1276.0007272450105\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.27009\n",
      "Epoch:20 Batch:11 Loss:0.05318\n",
      "Epoch:40 Batch:11 Loss:0.03367\n",
      "Epoch:60 Batch:11 Loss:0.02400\n",
      "Epoch:80 Batch:11 Loss:0.02134\n",
      "Epoch:100 Batch:11 Loss:0.02035\n",
      "Epoch:120 Batch:11 Loss:0.01781\n",
      "Epoch:140 Batch:11 Loss:0.01705\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.048\n",
      "Epoch:10 Batch:10 Loss:0.050\n",
      "Epoch:20 Batch:10 Loss:0.046\n",
      "Epoch:30 Batch:10 Loss:0.045\n",
      "Epoch:40 Batch:10 Loss:0.046\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1355.4919503687017 setps: 800 count: 800\n",
      "avg rewards: -1355.4919503687017\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.34308\n",
      "Epoch:20 Batch:12 Loss:0.04413\n",
      "Epoch:40 Batch:12 Loss:0.03035\n",
      "Epoch:60 Batch:12 Loss:0.02184\n",
      "Epoch:80 Batch:12 Loss:0.01953\n",
      "Epoch:100 Batch:12 Loss:0.01659\n",
      "Epoch:120 Batch:12 Loss:0.01563\n",
      "Epoch:140 Batch:12 Loss:0.01549\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.050\n",
      "Epoch:10 Batch:10 Loss:0.047\n",
      "Epoch:20 Batch:10 Loss:0.048\n",
      "Epoch:30 Batch:10 Loss:0.048\n",
      "Epoch:40 Batch:10 Loss:0.046\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1039.9158765901288 setps: 800 count: 800\n",
      "avg rewards: -1039.9158765901288\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.33274\n",
      "Epoch:20 Batch:13 Loss:0.03373\n",
      "Epoch:40 Batch:13 Loss:0.02726\n",
      "Epoch:60 Batch:13 Loss:0.02245\n",
      "Epoch:80 Batch:13 Loss:0.02315\n",
      "Epoch:100 Batch:13 Loss:0.01882\n",
      "Epoch:120 Batch:13 Loss:0.01497\n",
      "Epoch:140 Batch:13 Loss:0.01461\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.047\n",
      "Epoch:10 Batch:10 Loss:0.046\n",
      "Epoch:20 Batch:10 Loss:0.045\n",
      "Epoch:30 Batch:10 Loss:0.048\n",
      "Epoch:40 Batch:10 Loss:0.046\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -875.4208005803711 setps: 800 count: 800\n",
      "avg rewards: -875.4208005803711\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.27022\n",
      "Epoch:20 Batch:14 Loss:0.03913\n",
      "Epoch:40 Batch:14 Loss:0.02714\n",
      "Epoch:60 Batch:14 Loss:0.02011\n",
      "Epoch:80 Batch:14 Loss:0.02051\n",
      "Epoch:100 Batch:14 Loss:0.01694\n",
      "Epoch:120 Batch:14 Loss:0.01711\n",
      "Epoch:140 Batch:14 Loss:0.01381\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.050\n",
      "Epoch:10 Batch:10 Loss:0.047\n",
      "Epoch:20 Batch:10 Loss:0.047\n",
      "Epoch:30 Batch:10 Loss:0.047\n",
      "Epoch:40 Batch:10 Loss:0.048\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1281.1471046113268 setps: 800 count: 800\n",
      "avg rewards: -1281.1471046113268\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.25684\n",
      "Epoch:20 Batch:15 Loss:0.03367\n",
      "Epoch:40 Batch:15 Loss:0.02304\n",
      "Epoch:60 Batch:15 Loss:0.01548\n",
      "Epoch:80 Batch:15 Loss:0.01802\n",
      "Epoch:100 Batch:15 Loss:0.01827\n",
      "Epoch:120 Batch:15 Loss:0.01356\n",
      "Epoch:140 Batch:15 Loss:0.01340\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.051\n",
      "Epoch:10 Batch:10 Loss:0.049\n",
      "Epoch:20 Batch:10 Loss:0.047\n",
      "Epoch:30 Batch:10 Loss:0.048\n",
      "Epoch:40 Batch:10 Loss:0.047\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1198.6401123210276 setps: 800 count: 800\n",
      "avg rewards: -1198.6401123210276\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.19723\n",
      "Epoch:20 Batch:16 Loss:0.03527\n",
      "Epoch:40 Batch:16 Loss:0.01998\n",
      "Epoch:60 Batch:16 Loss:0.01806\n",
      "Epoch:80 Batch:16 Loss:0.01588\n",
      "Epoch:100 Batch:16 Loss:0.01576\n",
      "Epoch:120 Batch:16 Loss:0.01497\n",
      "Epoch:140 Batch:16 Loss:0.01693\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.049\n",
      "Epoch:10 Batch:10 Loss:0.049\n",
      "Epoch:20 Batch:10 Loss:0.046\n",
      "Epoch:30 Batch:10 Loss:0.044\n",
      "Epoch:40 Batch:10 Loss:0.044\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1353.001756532432 setps: 800 count: 800\n",
      "avg rewards: -1353.001756532432\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.24955\n",
      "Epoch:20 Batch:17 Loss:0.02754\n",
      "Epoch:40 Batch:17 Loss:0.01909\n",
      "Epoch:60 Batch:17 Loss:0.01556\n",
      "Epoch:80 Batch:17 Loss:0.01659\n",
      "Epoch:100 Batch:17 Loss:0.01231\n",
      "Epoch:120 Batch:17 Loss:0.01342\n",
      "Epoch:140 Batch:17 Loss:0.00987\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.048\n",
      "Epoch:10 Batch:10 Loss:0.048\n",
      "Epoch:20 Batch:10 Loss:0.048\n",
      "Epoch:30 Batch:10 Loss:0.045\n",
      "Epoch:40 Batch:10 Loss:0.047\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 84.86603432522956 setps: 800 count: 800\n",
      "avg rewards: 84.86603432522956\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.19816\n",
      "Epoch:20 Batch:18 Loss:0.02999\n",
      "Epoch:40 Batch:18 Loss:0.02262\n",
      "Epoch:60 Batch:18 Loss:0.01794\n",
      "Epoch:80 Batch:18 Loss:0.01612\n",
      "Epoch:100 Batch:18 Loss:0.01484\n",
      "Epoch:120 Batch:18 Loss:0.01319\n",
      "Epoch:140 Batch:18 Loss:0.01277\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.033\n",
      "Epoch:10 Batch:10 Loss:0.034\n",
      "Epoch:20 Batch:10 Loss:0.031\n",
      "Epoch:30 Batch:10 Loss:0.029\n",
      "Epoch:40 Batch:10 Loss:0.031\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1033.9000672082725 setps: 800 count: 800\n",
      "avg rewards: -1033.9000672082725\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.15064\n",
      "Epoch:20 Batch:19 Loss:0.03136\n",
      "Epoch:40 Batch:19 Loss:0.02177\n",
      "Epoch:60 Batch:19 Loss:0.01549\n",
      "Epoch:80 Batch:19 Loss:0.01737\n",
      "Epoch:100 Batch:19 Loss:0.01569\n",
      "Epoch:120 Batch:19 Loss:0.01301\n",
      "Epoch:140 Batch:19 Loss:0.01316\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.034\n",
      "Epoch:10 Batch:10 Loss:0.031\n",
      "Epoch:20 Batch:10 Loss:0.030\n",
      "Epoch:30 Batch:10 Loss:0.030\n",
      "Epoch:40 Batch:10 Loss:0.030\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 41.25229001149274 setps: 800 count: 800\n",
      "avg rewards: 41.25229001149274\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.16261\n",
      "Epoch:20 Batch:20 Loss:0.03330\n",
      "Epoch:40 Batch:20 Loss:0.01851\n",
      "Epoch:60 Batch:20 Loss:0.01881\n",
      "Epoch:80 Batch:20 Loss:0.01627\n",
      "Epoch:100 Batch:20 Loss:0.01633\n",
      "Epoch:120 Batch:20 Loss:0.01335\n",
      "Epoch:140 Batch:20 Loss:0.01420\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.026\n",
      "Epoch:10 Batch:10 Loss:0.025\n",
      "Epoch:20 Batch:10 Loss:0.026\n",
      "Epoch:30 Batch:10 Loss:0.024\n",
      "Epoch:40 Batch:10 Loss:0.027\n",
      "Done!\n",
      "############# start AntBulletEnv-v0 training ###################\n",
      "(50, 1000, 28)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 387.879813451547 setps: 800 count: 800\n",
      "avg rewards: 387.879813451547\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:0.40351\n",
      "Epoch:20 Batch:1 Loss:0.10094\n",
      "Epoch:40 Batch:1 Loss:0.05194\n",
      "Epoch:60 Batch:1 Loss:0.04786\n",
      "Epoch:80 Batch:1 Loss:0.04184\n",
      "Epoch:100 Batch:1 Loss:0.03149\n",
      "Epoch:120 Batch:1 Loss:0.02395\n",
      "Epoch:140 Batch:1 Loss:0.02206\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.325\n",
      "Epoch:10 Batch:10 Loss:0.324\n",
      "Epoch:20 Batch:10 Loss:0.325\n",
      "Epoch:30 Batch:10 Loss:0.323\n",
      "Epoch:40 Batch:10 Loss:0.326\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 67.40305975336229 setps: 140 count: 140\n",
      "reward: 244.6038845321265 setps: 800 count: 940\n",
      "avg rewards: 156.00347214274439\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:0.42480\n",
      "Epoch:20 Batch:2 Loss:0.07744\n",
      "Epoch:40 Batch:2 Loss:0.03450\n",
      "Epoch:60 Batch:2 Loss:0.02578\n",
      "Epoch:80 Batch:2 Loss:0.01907\n",
      "Epoch:100 Batch:2 Loss:0.01248\n",
      "Epoch:120 Batch:2 Loss:0.01050\n",
      "Epoch:140 Batch:2 Loss:0.00942\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.288\n",
      "Epoch:10 Batch:10 Loss:0.289\n",
      "Epoch:20 Batch:10 Loss:0.286\n",
      "Epoch:30 Batch:10 Loss:0.284\n",
      "Epoch:40 Batch:10 Loss:0.286\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 57.50564612998716 setps: 117 count: 117\n",
      "reward: 23.497867381376274 setps: 57 count: 174\n",
      "reward: 81.76180171585877 setps: 159 count: 333\n",
      "reward: 12.228188910675811 setps: 30 count: 363\n",
      "reward: 32.143697843338295 setps: 62 count: 425\n",
      "reward: 86.839755388099 setps: 169 count: 594\n",
      "reward: 81.03855643982384 setps: 183 count: 777\n",
      "reward: 52.82868019237358 setps: 107 count: 884\n",
      "avg rewards: 53.48052425019159\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.43471\n",
      "Epoch:20 Batch:3 Loss:0.08583\n",
      "Epoch:40 Batch:3 Loss:0.03552\n",
      "Epoch:60 Batch:3 Loss:0.01719\n",
      "Epoch:80 Batch:3 Loss:0.01091\n",
      "Epoch:100 Batch:3 Loss:0.00968\n",
      "Epoch:120 Batch:3 Loss:0.00965\n",
      "Epoch:140 Batch:3 Loss:0.01000\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.284\n",
      "Epoch:10 Batch:10 Loss:0.281\n",
      "Epoch:20 Batch:10 Loss:0.282\n",
      "Epoch:30 Batch:10 Loss:0.278\n",
      "Epoch:40 Batch:10 Loss:0.278\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 331.06198740195214 setps: 800 count: 800\n",
      "avg rewards: 331.06198740195214\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.49484\n",
      "Epoch:20 Batch:4 Loss:0.05639\n",
      "Epoch:40 Batch:4 Loss:0.02222\n",
      "Epoch:60 Batch:4 Loss:0.01167\n",
      "Epoch:80 Batch:4 Loss:0.00967\n",
      "Epoch:100 Batch:4 Loss:0.00847\n",
      "Epoch:120 Batch:4 Loss:0.00846\n",
      "Epoch:140 Batch:4 Loss:0.00807\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.278\n",
      "Epoch:10 Batch:10 Loss:0.272\n",
      "Epoch:20 Batch:10 Loss:0.273\n",
      "Epoch:30 Batch:10 Loss:0.272\n",
      "Epoch:40 Batch:10 Loss:0.271\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 15.038752043238489 setps: 29 count: 29\n",
      "reward: 9.848424830312435 setps: 21 count: 50\n",
      "reward: 22.8235807402918 setps: 40 count: 90\n",
      "reward: 13.626981071110638 setps: 26 count: 116\n",
      "reward: 13.66576309387019 setps: 26 count: 142\n",
      "reward: 8.793280421887175 setps: 20 count: 162\n",
      "reward: 11.217985906862303 setps: 24 count: 186\n",
      "reward: 13.984957540511097 setps: 26 count: 212\n",
      "reward: 17.22317112953752 setps: 30 count: 242\n",
      "reward: 12.035923447729145 setps: 23 count: 265\n",
      "reward: 15.02906775620213 setps: 29 count: 294\n",
      "reward: 12.825318269575657 setps: 24 count: 318\n",
      "reward: 19.45810571522452 setps: 34 count: 352\n",
      "reward: 10.72166615278984 setps: 22 count: 374\n",
      "reward: 15.55360799457121 setps: 28 count: 402\n",
      "reward: 13.36758042693109 setps: 28 count: 430\n",
      "reward: 13.401329887230531 setps: 25 count: 455\n",
      "reward: 13.091513117881549 setps: 25 count: 480\n",
      "reward: 11.330230853414106 setps: 22 count: 502\n",
      "reward: 12.119246852006471 setps: 24 count: 526\n",
      "reward: 8.992974925758608 setps: 20 count: 546\n",
      "reward: 12.531789282833053 setps: 26 count: 572\n",
      "reward: 11.680913699061781 setps: 24 count: 596\n",
      "reward: 105.09263236848787 setps: 156 count: 752\n",
      "reward: 22.72789521649247 setps: 39 count: 791\n",
      "reward: 64.06299446793417 setps: 107 count: 898\n",
      "reward: 14.432457598646577 setps: 26 count: 924\n",
      "reward: 11.826406730616872 setps: 23 count: 947\n",
      "reward: 9.452116450262837 setps: 21 count: 968\n",
      "avg rewards: 18.48126441349214\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.49449\n",
      "Epoch:20 Batch:5 Loss:0.04792\n",
      "Epoch:40 Batch:5 Loss:0.01909\n",
      "Epoch:60 Batch:5 Loss:0.01346\n",
      "Epoch:80 Batch:5 Loss:0.01064\n",
      "Epoch:100 Batch:5 Loss:0.00985\n",
      "Epoch:120 Batch:5 Loss:0.00897\n",
      "Epoch:140 Batch:5 Loss:0.00741\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.296\n",
      "Epoch:10 Batch:10 Loss:0.291\n",
      "Epoch:20 Batch:10 Loss:0.289\n",
      "Epoch:30 Batch:10 Loss:0.287\n",
      "Epoch:40 Batch:10 Loss:0.287\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 364.72690960572925 setps: 800 count: 800\n",
      "avg rewards: 364.72690960572925\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.43586\n",
      "Epoch:20 Batch:6 Loss:0.03326\n",
      "Epoch:40 Batch:6 Loss:0.01399\n",
      "Epoch:60 Batch:6 Loss:0.01198\n",
      "Epoch:80 Batch:6 Loss:0.01040\n",
      "Epoch:100 Batch:6 Loss:0.00895\n",
      "Epoch:120 Batch:6 Loss:0.00728\n",
      "Epoch:140 Batch:6 Loss:0.00720\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.245\n",
      "Epoch:10 Batch:10 Loss:0.230\n",
      "Epoch:20 Batch:10 Loss:0.230\n",
      "Epoch:30 Batch:10 Loss:0.229\n",
      "Epoch:40 Batch:10 Loss:0.227\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 8.397992177079137 setps: 20 count: 20\n",
      "reward: 8.521037724083 setps: 20 count: 40\n",
      "reward: 8.725000295547943 setps: 20 count: 60\n",
      "reward: 8.572243491160043 setps: 20 count: 80\n",
      "reward: 8.695497296578832 setps: 20 count: 100\n",
      "reward: 8.637896642698616 setps: 20 count: 120\n",
      "reward: 8.673083677784595 setps: 20 count: 140\n",
      "reward: 8.354752832672967 setps: 20 count: 160\n",
      "reward: 8.703216593909021 setps: 20 count: 180\n",
      "reward: 8.51549804169772 setps: 20 count: 200\n",
      "reward: 8.953971336498217 setps: 20 count: 220\n",
      "reward: 8.553351429245957 setps: 20 count: 240\n",
      "reward: 8.897193279080966 setps: 20 count: 260\n",
      "reward: 8.808350380620684 setps: 20 count: 280\n",
      "reward: 8.678647015488242 setps: 20 count: 300\n",
      "reward: 8.557993102436011 setps: 20 count: 320\n",
      "reward: 8.490363575212541 setps: 20 count: 340\n",
      "reward: 8.414110924288982 setps: 20 count: 360\n",
      "reward: 8.791879349124791 setps: 20 count: 380\n",
      "reward: 8.877288915992542 setps: 20 count: 400\n",
      "reward: 8.853817703736423 setps: 20 count: 420\n",
      "reward: 8.461066424009912 setps: 20 count: 440\n",
      "reward: 8.541358245738955 setps: 20 count: 460\n",
      "reward: 8.832894600978761 setps: 20 count: 480\n",
      "reward: 8.697972271847538 setps: 20 count: 500\n",
      "reward: 8.968841273008728 setps: 20 count: 520\n",
      "reward: 8.612592263247643 setps: 20 count: 540\n",
      "reward: 8.705009520729071 setps: 20 count: 560\n",
      "reward: 8.900747509190115 setps: 20 count: 580\n",
      "reward: 8.935173120313268 setps: 20 count: 600\n",
      "reward: 8.724517016255415 setps: 20 count: 620\n",
      "reward: 8.89836360244226 setps: 20 count: 640\n",
      "reward: 8.968542142186198 setps: 20 count: 660\n",
      "reward: 8.734335361653935 setps: 20 count: 680\n",
      "reward: 8.685943247337129 setps: 20 count: 700\n",
      "reward: 8.966934157100331 setps: 20 count: 720\n",
      "reward: 8.593238217174074 setps: 20 count: 740\n",
      "reward: 8.896383350629185 setps: 20 count: 760\n",
      "reward: 8.693813271613905 setps: 20 count: 780\n",
      "reward: 8.631968812609557 setps: 20 count: 800\n",
      "reward: 8.78333741636452 setps: 20 count: 820\n",
      "reward: 8.565300499809384 setps: 20 count: 840\n",
      "reward: 8.386335766784033 setps: 20 count: 860\n",
      "reward: 8.64234724049602 setps: 20 count: 880\n",
      "reward: 8.855236236212658 setps: 20 count: 900\n",
      "reward: 8.581556163095227 setps: 20 count: 920\n",
      "reward: 8.732715125373215 setps: 20 count: 940\n",
      "reward: 8.488068281901358 setps: 20 count: 960\n",
      "reward: 8.590387949057913 setps: 20 count: 980\n",
      "reward: 8.535700483732217 setps: 20 count: 1000\n",
      "avg rewards: 8.685677307116595\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.45152\n",
      "Epoch:20 Batch:7 Loss:0.02944\n",
      "Epoch:40 Batch:7 Loss:0.01481\n",
      "Epoch:60 Batch:7 Loss:0.01205\n",
      "Epoch:80 Batch:7 Loss:0.00960\n",
      "Epoch:100 Batch:7 Loss:0.00868\n",
      "Epoch:120 Batch:7 Loss:0.00711\n",
      "Epoch:140 Batch:7 Loss:0.00620\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.255\n",
      "Epoch:10 Batch:10 Loss:0.244\n",
      "Epoch:20 Batch:10 Loss:0.237\n",
      "Epoch:30 Batch:10 Loss:0.235\n",
      "Epoch:40 Batch:10 Loss:0.236\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 320.80340455235716 setps: 800 count: 800\n",
      "avg rewards: 320.80340455235716\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.43216\n",
      "Epoch:20 Batch:8 Loss:0.02757\n",
      "Epoch:40 Batch:8 Loss:0.01413\n",
      "Epoch:60 Batch:8 Loss:0.01207\n",
      "Epoch:80 Batch:8 Loss:0.00906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:100 Batch:8 Loss:0.00832\n",
      "Epoch:120 Batch:8 Loss:0.00613\n",
      "Epoch:140 Batch:8 Loss:0.00600\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.259\n",
      "Epoch:10 Batch:10 Loss:0.246\n",
      "Epoch:20 Batch:10 Loss:0.248\n",
      "Epoch:30 Batch:10 Loss:0.244\n",
      "Epoch:40 Batch:10 Loss:0.241\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 7.838647989870518 setps: 20 count: 20\n",
      "reward: 8.065410833198985 setps: 20 count: 40\n",
      "reward: 8.112473962383229 setps: 20 count: 60\n",
      "reward: 8.141255043710407 setps: 20 count: 80\n",
      "reward: 8.16733192217216 setps: 20 count: 100\n",
      "reward: 7.9424133452776 setps: 20 count: 120\n",
      "reward: 7.905361029063352 setps: 20 count: 140\n",
      "reward: 7.4782319198988265 setps: 20 count: 160\n",
      "reward: 7.837472591745607 setps: 20 count: 180\n",
      "reward: 7.66545615408395 setps: 20 count: 200\n",
      "reward: 7.9676528265074 setps: 20 count: 220\n",
      "reward: 7.850258109692367 setps: 20 count: 240\n",
      "reward: 7.595770241483115 setps: 20 count: 260\n",
      "reward: 8.042726635662257 setps: 20 count: 280\n",
      "reward: 7.804742487431212 setps: 20 count: 300\n",
      "reward: 7.846120727191737 setps: 20 count: 320\n",
      "reward: 7.947722183293077 setps: 20 count: 340\n",
      "reward: 7.984323610787396 setps: 20 count: 360\n",
      "reward: 7.652708446601172 setps: 20 count: 380\n",
      "reward: 7.823833576362813 setps: 20 count: 400\n",
      "reward: 7.785708098705799 setps: 20 count: 420\n",
      "reward: 7.76847349090967 setps: 20 count: 440\n",
      "reward: 7.970584837281784 setps: 20 count: 460\n",
      "reward: 7.943511911680982 setps: 20 count: 480\n",
      "reward: 7.8754616047983275 setps: 20 count: 500\n",
      "reward: 7.938290201316705 setps: 20 count: 520\n",
      "reward: 7.881000557921654 setps: 20 count: 540\n",
      "reward: 7.858416789727925 setps: 20 count: 560\n",
      "reward: 7.964788119042351 setps: 20 count: 580\n",
      "reward: 8.009492871162365 setps: 20 count: 600\n",
      "reward: 8.127448433414976 setps: 20 count: 620\n",
      "reward: 7.759698233063681 setps: 20 count: 640\n",
      "reward: 7.977851245286001 setps: 20 count: 660\n",
      "reward: 7.884401833728773 setps: 20 count: 680\n",
      "reward: 7.911551441589836 setps: 20 count: 700\n",
      "reward: 7.739641790135647 setps: 20 count: 720\n",
      "reward: 8.037804842293554 setps: 20 count: 740\n",
      "reward: 8.151991293147146 setps: 20 count: 760\n",
      "reward: 7.913267235852256 setps: 20 count: 780\n",
      "reward: 7.780479326358181 setps: 20 count: 800\n",
      "reward: 7.861080542967829 setps: 20 count: 820\n",
      "reward: 7.73273812694533 setps: 20 count: 840\n",
      "reward: 7.880181526990786 setps: 20 count: 860\n",
      "reward: 8.033447091150444 setps: 20 count: 880\n",
      "reward: 7.821415218072071 setps: 20 count: 900\n",
      "reward: 7.950809855063561 setps: 20 count: 920\n",
      "reward: 7.771066371415507 setps: 20 count: 940\n",
      "reward: 7.686474148198611 setps: 20 count: 960\n",
      "reward: 8.237339196325049 setps: 20 count: 980\n",
      "reward: 7.837764150887958 setps: 20 count: 1000\n",
      "avg rewards: 7.895241880437038\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.45415\n",
      "Epoch:20 Batch:9 Loss:0.02417\n",
      "Epoch:40 Batch:9 Loss:0.01395\n",
      "Epoch:60 Batch:9 Loss:0.01155\n",
      "Epoch:80 Batch:9 Loss:0.00898\n",
      "Epoch:100 Batch:9 Loss:0.00698\n",
      "Epoch:120 Batch:9 Loss:0.00682\n",
      "Epoch:140 Batch:9 Loss:0.00618\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.245\n",
      "Epoch:10 Batch:10 Loss:0.240\n",
      "Epoch:20 Batch:10 Loss:0.236\n",
      "Epoch:30 Batch:10 Loss:0.236\n",
      "Epoch:40 Batch:10 Loss:0.235\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 395.85271188602354 setps: 800 count: 800\n",
      "avg rewards: 395.85271188602354\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.44014\n",
      "Epoch:20 Batch:10 Loss:0.02224\n",
      "Epoch:40 Batch:10 Loss:0.01275\n",
      "Epoch:60 Batch:10 Loss:0.01112\n",
      "Epoch:80 Batch:10 Loss:0.00828\n",
      "Epoch:100 Batch:10 Loss:0.00740\n",
      "Epoch:120 Batch:10 Loss:0.00645\n",
      "Epoch:140 Batch:10 Loss:0.00527\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.241\n",
      "Epoch:10 Batch:10 Loss:0.229\n",
      "Epoch:20 Batch:10 Loss:0.227\n",
      "Epoch:30 Batch:10 Loss:0.226\n",
      "Epoch:40 Batch:10 Loss:0.225\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 400.61141583099516 setps: 800 count: 800\n",
      "reward: 86.23685979919242 setps: 164 count: 964\n",
      "avg rewards: 243.4241378150938\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.38803\n",
      "Epoch:20 Batch:11 Loss:0.01920\n",
      "Epoch:40 Batch:11 Loss:0.00932\n",
      "Epoch:60 Batch:11 Loss:0.00806\n",
      "Epoch:80 Batch:11 Loss:0.00689\n",
      "Epoch:100 Batch:11 Loss:0.00612\n",
      "Epoch:120 Batch:11 Loss:0.00557\n",
      "Epoch:140 Batch:11 Loss:0.00541\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.231\n",
      "Epoch:10 Batch:10 Loss:0.225\n",
      "Epoch:20 Batch:10 Loss:0.224\n",
      "Epoch:30 Batch:10 Loss:0.221\n",
      "Epoch:40 Batch:10 Loss:0.218\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 232.64172712707068 setps: 800 count: 800\n",
      "avg rewards: 232.64172712707068\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.46105\n",
      "Epoch:20 Batch:12 Loss:0.02000\n",
      "Epoch:40 Batch:12 Loss:0.01032\n",
      "Epoch:60 Batch:12 Loss:0.00856\n",
      "Epoch:80 Batch:12 Loss:0.00696\n",
      "Epoch:100 Batch:12 Loss:0.00564\n",
      "Epoch:120 Batch:12 Loss:0.00556\n",
      "Epoch:140 Batch:12 Loss:0.00538\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.244\n",
      "Epoch:10 Batch:10 Loss:0.227\n",
      "Epoch:20 Batch:10 Loss:0.224\n",
      "Epoch:30 Batch:10 Loss:0.222\n",
      "Epoch:40 Batch:10 Loss:0.221\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 7.884898875019277 setps: 20 count: 20\n",
      "reward: 7.838859339975167 setps: 20 count: 40\n",
      "reward: 7.655434789965511 setps: 20 count: 60\n",
      "reward: 8.008960973177452 setps: 20 count: 80\n",
      "reward: 7.809519952484696 setps: 20 count: 100\n",
      "reward: 7.755181755917147 setps: 20 count: 120\n",
      "reward: 7.891886228992375 setps: 20 count: 140\n",
      "reward: 7.768099349271506 setps: 20 count: 160\n",
      "reward: 7.920445582471438 setps: 20 count: 180\n",
      "reward: 7.739085314537805 setps: 20 count: 200\n",
      "reward: 7.830109315236038 setps: 20 count: 220\n",
      "reward: 7.852019269431185 setps: 20 count: 240\n",
      "reward: 7.741959257735289 setps: 20 count: 260\n",
      "reward: 7.892616486658516 setps: 20 count: 280\n",
      "reward: 8.025094090175115 setps: 20 count: 300\n",
      "reward: 7.883914312401612 setps: 20 count: 320\n",
      "reward: 8.084737747520556 setps: 20 count: 340\n",
      "reward: 7.66952716470114 setps: 20 count: 360\n",
      "reward: 7.700374574780289 setps: 20 count: 380\n",
      "reward: 7.881355395732681 setps: 20 count: 400\n",
      "reward: 7.771495329325262 setps: 20 count: 420\n",
      "reward: 7.884055143526347 setps: 20 count: 440\n",
      "reward: 7.971828699776962 setps: 20 count: 460\n",
      "reward: 7.718983304305582 setps: 20 count: 480\n",
      "reward: 7.793097101042802 setps: 20 count: 500\n",
      "reward: 7.732123809521726 setps: 20 count: 520\n",
      "reward: 7.514807203880627 setps: 20 count: 540\n",
      "reward: 7.986082978363266 setps: 20 count: 560\n",
      "reward: 7.740343380106788 setps: 20 count: 580\n",
      "reward: 7.896512534529029 setps: 20 count: 600\n",
      "reward: 8.151498811048805 setps: 20 count: 620\n",
      "reward: 7.846365715537105 setps: 20 count: 640\n",
      "reward: 7.9511223800887825 setps: 20 count: 660\n",
      "reward: 8.054182834064704 setps: 20 count: 680\n",
      "reward: 7.83748762794712 setps: 20 count: 700\n",
      "reward: 8.079250924116058 setps: 20 count: 720\n",
      "reward: 8.300250461287217 setps: 20 count: 740\n",
      "reward: 8.101722004005566 setps: 20 count: 760\n",
      "reward: 7.786678768454293 setps: 20 count: 780\n",
      "reward: 8.008584100463485 setps: 20 count: 800\n",
      "reward: 8.016571381183166 setps: 20 count: 820\n",
      "reward: 7.617477911934837 setps: 20 count: 840\n",
      "reward: 7.884190954521181 setps: 20 count: 860\n",
      "reward: 7.8489077948266655 setps: 20 count: 880\n",
      "reward: 7.79119855844183 setps: 20 count: 900\n",
      "reward: 7.97447013977071 setps: 20 count: 920\n",
      "reward: 7.960003494814738 setps: 20 count: 940\n",
      "reward: 7.944884992699371 setps: 20 count: 960\n",
      "reward: 7.5713111821416526 setps: 20 count: 980\n",
      "reward: 7.7154194035858366 setps: 20 count: 1000\n",
      "avg rewards: 7.865699774029926\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.38416\n",
      "Epoch:20 Batch:13 Loss:0.01661\n",
      "Epoch:40 Batch:13 Loss:0.01023\n",
      "Epoch:60 Batch:13 Loss:0.00791\n",
      "Epoch:80 Batch:13 Loss:0.00679\n",
      "Epoch:100 Batch:13 Loss:0.00626\n",
      "Epoch:120 Batch:13 Loss:0.00547\n",
      "Epoch:140 Batch:13 Loss:0.00464\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.225\n",
      "Epoch:10 Batch:10 Loss:0.216\n",
      "Epoch:20 Batch:10 Loss:0.218\n",
      "Epoch:30 Batch:10 Loss:0.218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:40 Batch:10 Loss:0.215\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 9.260485393002455 setps: 20 count: 20\n",
      "reward: 9.023734744891405 setps: 20 count: 40\n",
      "reward: 9.138339523960896 setps: 20 count: 60\n",
      "reward: 9.378071411054407 setps: 20 count: 80\n",
      "reward: 9.262002287952052 setps: 20 count: 100\n",
      "reward: 9.286747605934213 setps: 20 count: 120\n",
      "reward: 9.292547354004634 setps: 20 count: 140\n",
      "reward: 9.18434051815275 setps: 20 count: 160\n",
      "reward: 9.079863088118145 setps: 20 count: 180\n",
      "reward: 8.877830962077132 setps: 20 count: 200\n",
      "reward: 9.146398533276807 setps: 20 count: 220\n",
      "reward: 9.10160062448704 setps: 20 count: 240\n",
      "reward: 9.178183460119177 setps: 20 count: 260\n",
      "reward: 9.099471833666033 setps: 20 count: 280\n",
      "reward: 8.971451253065602 setps: 20 count: 300\n",
      "reward: 9.306303762491734 setps: 20 count: 320\n",
      "reward: 9.196521180699348 setps: 20 count: 340\n",
      "reward: 8.959798206581036 setps: 20 count: 360\n",
      "reward: 9.271442207253129 setps: 20 count: 380\n",
      "reward: 9.220746466857964 setps: 20 count: 400\n",
      "reward: 9.231077867672138 setps: 20 count: 420\n",
      "reward: 9.208403919842388 setps: 20 count: 440\n",
      "reward: 9.272540248841688 setps: 20 count: 460\n",
      "reward: 9.178742130289903 setps: 20 count: 480\n",
      "reward: 8.984053247184782 setps: 20 count: 500\n",
      "reward: 9.227964678683202 setps: 20 count: 520\n",
      "reward: 9.188246299185264 setps: 20 count: 540\n",
      "reward: 8.81911781561357 setps: 20 count: 560\n",
      "reward: 9.34504872040561 setps: 20 count: 580\n",
      "reward: 8.887411115331632 setps: 20 count: 600\n",
      "reward: 9.192403186364391 setps: 20 count: 620\n",
      "reward: 8.970595376325946 setps: 20 count: 640\n",
      "reward: 8.937199595390119 setps: 20 count: 660\n",
      "reward: 9.213924559894078 setps: 20 count: 680\n",
      "reward: 9.164867616283297 setps: 20 count: 700\n",
      "reward: 9.099127358716212 setps: 20 count: 720\n",
      "reward: 9.210609429363103 setps: 20 count: 740\n",
      "reward: 9.223668966641707 setps: 20 count: 760\n",
      "reward: 8.998977534145522 setps: 20 count: 780\n",
      "reward: 9.142755265452434 setps: 20 count: 800\n",
      "reward: 9.060749906525595 setps: 20 count: 820\n",
      "reward: 9.080392859186395 setps: 20 count: 840\n",
      "reward: 9.227583388824133 setps: 20 count: 860\n",
      "reward: 9.159744180784038 setps: 20 count: 880\n",
      "reward: 9.103400441948907 setps: 20 count: 900\n",
      "reward: 9.045097394124605 setps: 20 count: 920\n",
      "reward: 9.135458268360523 setps: 20 count: 940\n",
      "reward: 9.333180495678972 setps: 20 count: 960\n",
      "reward: 9.112667794928708 setps: 20 count: 980\n",
      "reward: 9.027428521045659 setps: 20 count: 1000\n",
      "avg rewards: 9.14036637201361\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.38619\n",
      "Epoch:20 Batch:14 Loss:0.01644\n",
      "Epoch:40 Batch:14 Loss:0.00958\n",
      "Epoch:60 Batch:14 Loss:0.00825\n",
      "Epoch:80 Batch:14 Loss:0.00643\n",
      "Epoch:100 Batch:14 Loss:0.00660\n",
      "Epoch:120 Batch:14 Loss:0.00628\n",
      "Epoch:140 Batch:14 Loss:0.00469\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.219\n",
      "Epoch:10 Batch:10 Loss:0.215\n",
      "Epoch:20 Batch:10 Loss:0.217\n",
      "Epoch:30 Batch:10 Loss:0.217\n",
      "Epoch:40 Batch:10 Loss:0.214\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 92.64705567551138 setps: 156 count: 156\n",
      "reward: 153.3753904296115 setps: 311 count: 467\n",
      "avg rewards: 123.01122305256145\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.32559\n",
      "Epoch:20 Batch:15 Loss:0.01403\n",
      "Epoch:40 Batch:15 Loss:0.00891\n",
      "Epoch:60 Batch:15 Loss:0.00730\n",
      "Epoch:80 Batch:15 Loss:0.00547\n",
      "Epoch:100 Batch:15 Loss:0.00559\n",
      "Epoch:120 Batch:15 Loss:0.00553\n",
      "Epoch:140 Batch:15 Loss:0.00532\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.240\n",
      "Epoch:10 Batch:10 Loss:0.231\n",
      "Epoch:20 Batch:10 Loss:0.230\n",
      "Epoch:30 Batch:10 Loss:0.225\n",
      "Epoch:40 Batch:10 Loss:0.226\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 31.857389245812364 setps: 78 count: 78\n",
      "reward: 292.37904627964014 setps: 528 count: 606\n",
      "reward: 75.49846749201242 setps: 160 count: 766\n",
      "avg rewards: 133.24496767248831\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.27514\n",
      "Epoch:20 Batch:16 Loss:0.01382\n",
      "Epoch:40 Batch:16 Loss:0.00936\n",
      "Epoch:60 Batch:16 Loss:0.00710\n",
      "Epoch:80 Batch:16 Loss:0.00630\n",
      "Epoch:100 Batch:16 Loss:0.00637\n",
      "Epoch:120 Batch:16 Loss:0.00581\n",
      "Epoch:140 Batch:16 Loss:0.00532\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.254\n",
      "Epoch:10 Batch:10 Loss:0.228\n",
      "Epoch:20 Batch:10 Loss:0.229\n",
      "Epoch:30 Batch:10 Loss:0.228\n",
      "Epoch:40 Batch:10 Loss:0.226\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 366.6774476628723 setps: 800 count: 800\n",
      "avg rewards: 366.6774476628723\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.37710\n",
      "Epoch:20 Batch:17 Loss:0.01368\n",
      "Epoch:40 Batch:17 Loss:0.00835\n",
      "Epoch:60 Batch:17 Loss:0.00745\n",
      "Epoch:80 Batch:17 Loss:0.00573\n",
      "Epoch:100 Batch:17 Loss:0.00599\n",
      "Epoch:120 Batch:17 Loss:0.00511\n",
      "Epoch:140 Batch:17 Loss:0.00465\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.218\n",
      "Epoch:10 Batch:10 Loss:0.206\n",
      "Epoch:20 Batch:10 Loss:0.202\n",
      "Epoch:30 Batch:10 Loss:0.203\n",
      "Epoch:40 Batch:10 Loss:0.201\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -225.72559657267288 setps: 480 count: 480\n",
      "avg rewards: -225.72559657267288\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.29211\n",
      "Epoch:20 Batch:18 Loss:0.01220\n",
      "Epoch:40 Batch:18 Loss:0.00781\n",
      "Epoch:60 Batch:18 Loss:0.00743\n",
      "Epoch:80 Batch:18 Loss:0.00683\n",
      "Epoch:100 Batch:18 Loss:0.00528\n",
      "Epoch:120 Batch:18 Loss:0.00579\n",
      "Epoch:140 Batch:18 Loss:0.00443\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.215\n",
      "Epoch:10 Batch:10 Loss:0.199\n",
      "Epoch:20 Batch:10 Loss:0.198\n",
      "Epoch:30 Batch:10 Loss:0.195\n",
      "Epoch:40 Batch:10 Loss:0.197\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 410.3748676661608 setps: 800 count: 800\n",
      "avg rewards: 410.3748676661608\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.33426\n",
      "Epoch:20 Batch:19 Loss:0.01221\n",
      "Epoch:40 Batch:19 Loss:0.00903\n",
      "Epoch:60 Batch:19 Loss:0.00723\n",
      "Epoch:80 Batch:19 Loss:0.00614\n",
      "Epoch:100 Batch:19 Loss:0.00524\n",
      "Epoch:120 Batch:19 Loss:0.00507\n",
      "Epoch:140 Batch:19 Loss:0.00519\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.217\n",
      "Epoch:10 Batch:10 Loss:0.205\n",
      "Epoch:20 Batch:10 Loss:0.205\n",
      "Epoch:30 Batch:10 Loss:0.200\n",
      "Epoch:40 Batch:10 Loss:0.202\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 57.43469580451927 setps: 71 count: 71\n",
      "reward: 561.8085915674955 setps: 800 count: 871\n",
      "avg rewards: 309.62164368600736\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.26560\n",
      "Epoch:20 Batch:20 Loss:0.01202\n",
      "Epoch:40 Batch:20 Loss:0.00854\n",
      "Epoch:60 Batch:20 Loss:0.00721\n",
      "Epoch:80 Batch:20 Loss:0.00613\n",
      "Epoch:100 Batch:20 Loss:0.00525\n",
      "Epoch:120 Batch:20 Loss:0.00510\n",
      "Epoch:140 Batch:20 Loss:0.00506\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.173\n",
      "Epoch:10 Batch:10 Loss:0.169\n",
      "Epoch:20 Batch:10 Loss:0.166\n",
      "Epoch:30 Batch:10 Loss:0.166\n",
      "Epoch:40 Batch:10 Loss:0.164\n",
      "Done!\n",
      "############# start HumanoidBulletEnv-v0 training ###################\n",
      "(49,)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -32.59327098235517 setps: 22 count: 22\n",
      "reward: -20.025285138403706 setps: 20 count: 42\n",
      "reward: -20.49645852637623 setps: 20 count: 62\n",
      "reward: -25.700962189897837 setps: 19 count: 81\n",
      "reward: -18.788999088015405 setps: 19 count: 100\n",
      "reward: -22.790281978456186 setps: 19 count: 119\n",
      "reward: -31.185122736895575 setps: 18 count: 137\n",
      "reward: -29.665487870843208 setps: 22 count: 159\n",
      "reward: -12.346137957232713 setps: 18 count: 177\n",
      "reward: -36.271735022617214 setps: 18 count: 195\n",
      "reward: -29.163276184146525 setps: 20 count: 215\n",
      "reward: -28.285630425611448 setps: 24 count: 239\n",
      "reward: -28.61326972336537 setps: 19 count: 258\n",
      "reward: -28.023421662834885 setps: 19 count: 277\n",
      "reward: -31.298959614802158 setps: 20 count: 297\n",
      "reward: -39.98181828571687 setps: 24 count: 321\n",
      "reward: -25.35084381789202 setps: 19 count: 340\n",
      "reward: -18.6547190843441 setps: 19 count: 359\n",
      "reward: -29.4562628077474 setps: 22 count: 381\n",
      "reward: -22.268161394484927 setps: 19 count: 400\n",
      "reward: -29.36021647918678 setps: 22 count: 422\n",
      "reward: -33.95250150139473 setps: 18 count: 440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -22.44168341341428 setps: 19 count: 459\n",
      "reward: -40.260897488791656 setps: 25 count: 484\n",
      "reward: -41.470481391105565 setps: 18 count: 502\n",
      "reward: -20.20329166664451 setps: 17 count: 519\n",
      "reward: -34.32584393924917 setps: 19 count: 538\n",
      "reward: -31.343376773374622 setps: 18 count: 556\n",
      "reward: -35.648598603361464 setps: 19 count: 575\n",
      "reward: -23.357746788808438 setps: 18 count: 593\n",
      "reward: -25.497490061561983 setps: 17 count: 610\n",
      "reward: -39.738364452582026 setps: 21 count: 631\n",
      "reward: -37.0995766303502 setps: 14 count: 645\n",
      "reward: -33.31335272940923 setps: 21 count: 666\n",
      "reward: -29.650835852792078 setps: 21 count: 687\n",
      "reward: -26.70514389798627 setps: 18 count: 705\n",
      "reward: -30.882442079905015 setps: 20 count: 725\n",
      "reward: -31.567458384926432 setps: 19 count: 744\n",
      "reward: -39.22092470549688 setps: 18 count: 762\n",
      "reward: -21.634807421127334 setps: 19 count: 781\n",
      "reward: -25.851094098725298 setps: 19 count: 800\n",
      "reward: -40.10702863238257 setps: 20 count: 820\n",
      "reward: -30.30569269703265 setps: 20 count: 840\n",
      "reward: -20.469640096045623 setps: 20 count: 860\n",
      "reward: -14.706564948265438 setps: 18 count: 878\n",
      "reward: -31.243943485678756 setps: 22 count: 900\n",
      "reward: -26.16057961039478 setps: 18 count: 918\n",
      "reward: -20.239103065265223 setps: 19 count: 937\n",
      "reward: -36.009881602830134 setps: 19 count: 956\n",
      "reward: -35.112843998239256 setps: 17 count: 973\n",
      "reward: -35.9117987034042 setps: 19 count: 992\n",
      "avg rewards: -28.916731562583752\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:0.57310\n",
      "Epoch:20 Batch:1 Loss:0.54361\n",
      "Epoch:40 Batch:1 Loss:0.50808\n",
      "Epoch:60 Batch:1 Loss:0.47908\n",
      "Epoch:80 Batch:1 Loss:0.46045\n",
      "Epoch:100 Batch:1 Loss:0.43450\n",
      "Epoch:120 Batch:1 Loss:0.39629\n",
      "Epoch:140 Batch:1 Loss:0.35637\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.256\n",
      "Epoch:10 Batch:8 Loss:0.252\n",
      "Epoch:20 Batch:8 Loss:0.251\n",
      "Epoch:30 Batch:8 Loss:0.244\n",
      "Epoch:40 Batch:8 Loss:0.242\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 37.94050609684491 setps: 38 count: 38\n",
      "reward: 26.296014792483767 setps: 43 count: 81\n",
      "reward: 9.850295396608999 setps: 18 count: 99\n",
      "reward: 14.445090681091825 setps: 42 count: 141\n",
      "reward: 10.867662243588708 setps: 53 count: 194\n",
      "reward: -27.821425087095125 setps: 45 count: 239\n",
      "reward: 41.29957180635245 setps: 64 count: 303\n",
      "reward: 28.07807275879604 setps: 44 count: 347\n",
      "reward: 26.568507587138445 setps: 73 count: 420\n",
      "reward: -25.046065171055677 setps: 54 count: 474\n",
      "reward: 53.09007225024251 setps: 85 count: 559\n",
      "reward: 0.48940703323168755 setps: 39 count: 598\n",
      "reward: 7.516600054226 setps: 35 count: 633\n",
      "reward: 4.54549162775802 setps: 25 count: 658\n",
      "reward: -21.17994132435852 setps: 74 count: 732\n",
      "reward: -10.804051584188716 setps: 88 count: 820\n",
      "reward: -8.419083535240503 setps: 61 count: 881\n",
      "reward: 15.910061165275694 setps: 42 count: 923\n",
      "reward: 39.53636349658044 setps: 63 count: 986\n",
      "avg rewards: 11.745428962541103\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:0.50137\n",
      "Epoch:20 Batch:2 Loss:0.36499\n",
      "Epoch:40 Batch:2 Loss:0.30272\n",
      "Epoch:60 Batch:2 Loss:0.27510\n",
      "Epoch:80 Batch:2 Loss:0.25379\n",
      "Epoch:100 Batch:2 Loss:0.21777\n",
      "Epoch:120 Batch:2 Loss:0.19438\n",
      "Epoch:140 Batch:2 Loss:0.16352\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.130\n",
      "Epoch:10 Batch:8 Loss:0.127\n",
      "Epoch:20 Batch:8 Loss:0.126\n",
      "Epoch:30 Batch:8 Loss:0.123\n",
      "Epoch:40 Batch:8 Loss:0.125\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 7.941158466518392 setps: 21 count: 21\n",
      "reward: 34.66495734058553 setps: 39 count: 60\n",
      "reward: 23.430960954283368 setps: 27 count: 87\n",
      "reward: 10.584724586561789 setps: 20 count: 107\n",
      "reward: 22.71228456550307 setps: 32 count: 139\n",
      "reward: 23.53240538105165 setps: 27 count: 166\n",
      "reward: 31.64238119122456 setps: 33 count: 199\n",
      "reward: 5.4566030738889815 setps: 18 count: 217\n",
      "reward: 20.175410812853077 setps: 27 count: 244\n",
      "reward: 9.92697690290224 setps: 19 count: 263\n",
      "reward: 25.776178073584738 setps: 35 count: 298\n",
      "reward: 11.215414095696177 setps: 17 count: 315\n",
      "reward: 21.93040871044068 setps: 26 count: 341\n",
      "reward: 14.756398570674353 setps: 23 count: 364\n",
      "reward: 31.0649608583888 setps: 31 count: 395\n",
      "reward: 10.90568546491704 setps: 17 count: 412\n",
      "reward: 24.671359630736703 setps: 32 count: 444\n",
      "reward: 29.242571865064388 setps: 36 count: 480\n",
      "reward: 26.351589129929202 setps: 30 count: 510\n",
      "reward: 6.188855907760443 setps: 18 count: 528\n",
      "reward: 3.8646938157849937 setps: 19 count: 547\n",
      "reward: 5.709818687889491 setps: 19 count: 566\n",
      "reward: 18.661446859703574 setps: 28 count: 594\n",
      "reward: 20.077579648331444 setps: 28 count: 622\n",
      "reward: 9.064304827066369 setps: 18 count: 640\n",
      "reward: 12.540139984607233 setps: 23 count: 663\n",
      "reward: 36.37326466844534 setps: 35 count: 698\n",
      "reward: 19.253041741308696 setps: 26 count: 724\n",
      "reward: 15.369818267459046 setps: 25 count: 749\n",
      "reward: 4.139680087142914 setps: 19 count: 768\n",
      "reward: 4.286339753492211 setps: 19 count: 787\n",
      "reward: 0.33052685023139894 setps: 16 count: 803\n",
      "reward: 8.498399099950618 setps: 21 count: 824\n",
      "reward: 21.75023000074725 setps: 26 count: 850\n",
      "reward: 4.795745147098204 setps: 19 count: 869\n",
      "reward: 3.3351926415736646 setps: 19 count: 888\n",
      "reward: 3.254432585577886 setps: 19 count: 907\n",
      "reward: 25.107106737268627 setps: 29 count: 936\n",
      "reward: 3.89116820817726 setps: 19 count: 955\n",
      "reward: 25.792857554805238 setps: 31 count: 986\n",
      "avg rewards: 15.956676818730665\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.47888\n",
      "Epoch:20 Batch:3 Loss:0.30670\n",
      "Epoch:40 Batch:3 Loss:0.24399\n",
      "Epoch:60 Batch:3 Loss:0.20825\n",
      "Epoch:80 Batch:3 Loss:0.17112\n",
      "Epoch:100 Batch:3 Loss:0.14923\n",
      "Epoch:120 Batch:3 Loss:0.12912\n",
      "Epoch:140 Batch:3 Loss:0.11818\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.103\n",
      "Epoch:10 Batch:8 Loss:0.097\n",
      "Epoch:20 Batch:8 Loss:0.097\n",
      "Epoch:30 Batch:8 Loss:0.095\n",
      "Epoch:40 Batch:8 Loss:0.093\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 26.550947783928134 setps: 30 count: 30\n",
      "reward: 50.57588805269624 setps: 45 count: 75\n",
      "reward: 1.6666328004255782 setps: 18 count: 93\n",
      "reward: 24.571770015724177 setps: 29 count: 122\n",
      "reward: 28.044119131198382 setps: 33 count: 155\n",
      "reward: 5.906518106471047 setps: 18 count: 173\n",
      "reward: 35.22954844585474 setps: 35 count: 208\n",
      "reward: 8.603146057139385 setps: 19 count: 227\n",
      "reward: 27.879726461957038 setps: 29 count: 256\n",
      "reward: 26.127036868558204 setps: 28 count: 284\n",
      "reward: 22.582433193115868 setps: 28 count: 312\n",
      "reward: 30.02476035842847 setps: 30 count: 342\n",
      "reward: 25.78646039039159 setps: 30 count: 372\n",
      "reward: 26.213494057858775 setps: 32 count: 404\n",
      "reward: 23.95645694731502 setps: 31 count: 435\n",
      "reward: 23.525666164480207 setps: 34 count: 469\n",
      "reward: 43.10092713009507 setps: 40 count: 509\n",
      "reward: 38.614261770305156 setps: 37 count: 546\n",
      "reward: 29.11441060049255 setps: 32 count: 578\n",
      "reward: 29.121872373994847 setps: 32 count: 610\n",
      "reward: 28.28070177975169 setps: 32 count: 642\n",
      "reward: 12.073684097119257 setps: 21 count: 663\n",
      "reward: 10.606587436831614 setps: 24 count: 687\n",
      "reward: 35.18308127050405 setps: 37 count: 724\n",
      "reward: 22.86080714407435 setps: 28 count: 752\n",
      "reward: 22.140223879097906 setps: 32 count: 784\n",
      "reward: 33.21590954480489 setps: 34 count: 818\n",
      "reward: 49.35327965582692 setps: 39 count: 857\n",
      "reward: 33.58233938451887 setps: 32 count: 889\n",
      "reward: 29.928910082107176 setps: 32 count: 921\n",
      "reward: 35.099618362959866 setps: 35 count: 956\n",
      "reward: 38.23833588221023 setps: 35 count: 991\n",
      "avg rewards: 27.429986100944916\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.44992\n",
      "Epoch:20 Batch:4 Loss:0.25577\n",
      "Epoch:40 Batch:4 Loss:0.20429\n",
      "Epoch:60 Batch:4 Loss:0.15817\n",
      "Epoch:80 Batch:4 Loss:0.13816\n",
      "Epoch:100 Batch:4 Loss:0.11104\n",
      "Epoch:120 Batch:4 Loss:0.10279\n",
      "Epoch:140 Batch:4 Loss:0.09132\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.086\n",
      "Epoch:10 Batch:8 Loss:0.082\n",
      "Epoch:20 Batch:8 Loss:0.080\n",
      "Epoch:30 Batch:8 Loss:0.080\n",
      "Epoch:40 Batch:8 Loss:0.082\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 37.30501359343325 setps: 34 count: 34\n",
      "reward: 53.41523688790765 setps: 40 count: 74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 58.775471898699465 setps: 51 count: 125\n",
      "reward: 35.03626224833423 setps: 30 count: 155\n",
      "reward: 38.21155243186222 setps: 31 count: 186\n",
      "reward: 38.916633144073415 setps: 32 count: 218\n",
      "reward: 39.59916371165746 setps: 33 count: 251\n",
      "reward: 29.489841469953536 setps: 30 count: 281\n",
      "reward: 48.19095587005431 setps: 42 count: 323\n",
      "reward: 60.4282718732924 setps: 49 count: 372\n",
      "reward: 44.52058408528975 setps: 38 count: 410\n",
      "reward: 49.23842448338982 setps: 43 count: 453\n",
      "reward: 36.585409589625485 setps: 33 count: 486\n",
      "reward: 23.255902309418886 setps: 26 count: 512\n",
      "reward: 51.27446714936814 setps: 40 count: 552\n",
      "reward: 12.97410170624207 setps: 18 count: 570\n",
      "reward: 16.111514421846366 setps: 22 count: 592\n",
      "reward: 49.30614471528243 setps: 37 count: 629\n",
      "reward: 63.2490231341624 setps: 45 count: 674\n",
      "reward: 51.90937023805164 setps: 39 count: 713\n",
      "reward: 50.822801161051025 setps: 38 count: 751\n",
      "reward: 39.39229601946571 setps: 32 count: 783\n",
      "reward: 53.35011840002843 setps: 41 count: 824\n",
      "reward: 54.1621740308794 setps: 48 count: 872\n",
      "reward: 44.074654037012074 setps: 42 count: 914\n",
      "reward: 46.010911318766 setps: 34 count: 948\n",
      "reward: 35.61344264002255 setps: 32 count: 980\n",
      "reward: 16.2189551531701 setps: 20 count: 1000\n",
      "avg rewards: 42.05138206151215\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.42973\n",
      "Epoch:20 Batch:5 Loss:0.21837\n",
      "Epoch:40 Batch:5 Loss:0.16295\n",
      "Epoch:60 Batch:5 Loss:0.13598\n",
      "Epoch:80 Batch:5 Loss:0.10894\n",
      "Epoch:100 Batch:5 Loss:0.09213\n",
      "Epoch:120 Batch:5 Loss:0.08335\n",
      "Epoch:140 Batch:5 Loss:0.07529\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.073\n",
      "Epoch:10 Batch:8 Loss:0.071\n",
      "Epoch:20 Batch:8 Loss:0.074\n",
      "Epoch:30 Batch:8 Loss:0.071\n",
      "Epoch:40 Batch:8 Loss:0.074\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 42.54759366855287 setps: 45 count: 45\n",
      "reward: 28.91151239140599 setps: 37 count: 82\n",
      "reward: 45.46815021044313 setps: 43 count: 125\n",
      "reward: 9.47497793805087 setps: 17 count: 142\n",
      "reward: 20.718566409338383 setps: 34 count: 176\n",
      "reward: 46.398776335957514 setps: 43 count: 219\n",
      "reward: 48.28023475469527 setps: 43 count: 262\n",
      "reward: 78.13093546150922 setps: 66 count: 328\n",
      "reward: 26.419632063951575 setps: 31 count: 359\n",
      "reward: 26.700708359912095 setps: 31 count: 390\n",
      "reward: 52.24629054258984 setps: 44 count: 434\n",
      "reward: 10.646557225196737 setps: 21 count: 455\n",
      "reward: 52.50057266455988 setps: 51 count: 506\n",
      "reward: 53.81108949861664 setps: 41 count: 547\n",
      "reward: 15.571340609627072 setps: 29 count: 576\n",
      "reward: 53.61599985838984 setps: 48 count: 624\n",
      "reward: 14.356452358332168 setps: 18 count: 642\n",
      "reward: 33.34112174264446 setps: 50 count: 692\n",
      "reward: 17.188896195942657 setps: 25 count: 717\n",
      "reward: 47.793107991524444 setps: 50 count: 767\n",
      "reward: 20.23552749044902 setps: 32 count: 799\n",
      "reward: 40.15250908976159 setps: 41 count: 840\n",
      "reward: 41.18831536082871 setps: 38 count: 878\n",
      "reward: 57.218989997696205 setps: 51 count: 929\n",
      "reward: 39.89420262219501 setps: 47 count: 976\n",
      "avg rewards: 36.91248243368685\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.42427\n",
      "Epoch:20 Batch:6 Loss:0.20399\n",
      "Epoch:40 Batch:6 Loss:0.14052\n",
      "Epoch:60 Batch:6 Loss:0.11754\n",
      "Epoch:80 Batch:6 Loss:0.09615\n",
      "Epoch:100 Batch:6 Loss:0.08436\n",
      "Epoch:120 Batch:6 Loss:0.06701\n",
      "Epoch:140 Batch:6 Loss:0.06998\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.058\n",
      "Epoch:10 Batch:8 Loss:0.055\n",
      "Epoch:20 Batch:8 Loss:0.058\n",
      "Epoch:30 Batch:8 Loss:0.058\n",
      "Epoch:40 Batch:8 Loss:0.056\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 34.109131598782554 setps: 38 count: 38\n",
      "reward: 40.474043021672706 setps: 38 count: 76\n",
      "reward: 13.997015672961428 setps: 20 count: 96\n",
      "reward: 46.066121921408914 setps: 39 count: 135\n",
      "reward: 31.26360374795185 setps: 37 count: 172\n",
      "reward: 34.65563409921305 setps: 31 count: 203\n",
      "reward: 33.45013123739191 setps: 34 count: 237\n",
      "reward: 63.3903242576227 setps: 48 count: 285\n",
      "reward: 36.326641258965545 setps: 33 count: 318\n",
      "reward: 31.49507374478562 setps: 33 count: 351\n",
      "reward: 18.056610318060848 setps: 21 count: 372\n",
      "reward: 41.8751937856563 setps: 35 count: 407\n",
      "reward: 58.32836049593461 setps: 45 count: 452\n",
      "reward: 17.840805383356926 setps: 23 count: 475\n",
      "reward: 45.0673577729467 setps: 35 count: 510\n",
      "reward: 16.39458757786924 setps: 19 count: 529\n",
      "reward: 29.527230416059314 setps: 30 count: 559\n",
      "reward: 31.526227944988925 setps: 36 count: 595\n",
      "reward: 39.2130063362114 setps: 36 count: 631\n",
      "reward: 44.51520903900672 setps: 38 count: 669\n",
      "reward: 25.68284514311527 setps: 65 count: 734\n",
      "reward: 40.62417829133628 setps: 33 count: 767\n",
      "reward: 38.67235755121946 setps: 38 count: 805\n",
      "reward: 57.40384946810371 setps: 58 count: 863\n",
      "reward: 60.314781342368235 setps: 49 count: 912\n",
      "reward: 28.04328952086798 setps: 33 count: 945\n",
      "reward: 48.64461968208925 setps: 49 count: 994\n",
      "avg rewards: 37.29474928259065\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.41896\n",
      "Epoch:20 Batch:7 Loss:0.19342\n",
      "Epoch:40 Batch:7 Loss:0.12706\n",
      "Epoch:60 Batch:7 Loss:0.10123\n",
      "Epoch:80 Batch:7 Loss:0.08684\n",
      "Epoch:100 Batch:7 Loss:0.07049\n",
      "Epoch:120 Batch:7 Loss:0.06991\n",
      "Epoch:140 Batch:7 Loss:0.05809\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.058\n",
      "Epoch:10 Batch:8 Loss:0.056\n",
      "Epoch:20 Batch:8 Loss:0.055\n",
      "Epoch:30 Batch:8 Loss:0.054\n",
      "Epoch:40 Batch:8 Loss:0.055\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 37.40783589744241 setps: 37 count: 37\n",
      "reward: 53.452449107571745 setps: 44 count: 81\n",
      "reward: 12.450746656720003 setps: 18 count: 99\n",
      "reward: 50.00719338792842 setps: 42 count: 141\n",
      "reward: 6.610881853627506 setps: 17 count: 158\n",
      "reward: 50.76057311618789 setps: 44 count: 202\n",
      "reward: 61.877526560862314 setps: 45 count: 247\n",
      "reward: 69.00347400729227 setps: 50 count: 297\n",
      "reward: 17.08087498513778 setps: 20 count: 317\n",
      "reward: 55.257595963506894 setps: 48 count: 365\n",
      "reward: 64.6742467794844 setps: 48 count: 413\n",
      "reward: 58.73763853298441 setps: 43 count: 456\n",
      "reward: 37.47668523445172 setps: 31 count: 487\n",
      "reward: 46.197711668509875 setps: 43 count: 530\n",
      "reward: 34.332045593271324 setps: 29 count: 559\n",
      "reward: 52.78069766341941 setps: 50 count: 609\n",
      "reward: 106.06213712637837 setps: 75 count: 684\n",
      "reward: 45.11556531588866 setps: 51 count: 735\n",
      "reward: 16.660156536720752 setps: 20 count: 755\n",
      "reward: 40.145954509498544 setps: 47 count: 802\n",
      "reward: 33.51197222980263 setps: 36 count: 838\n",
      "reward: 61.965156278552605 setps: 53 count: 891\n",
      "reward: 33.95859719997535 setps: 34 count: 925\n",
      "reward: 49.1912006650935 setps: 36 count: 961\n",
      "avg rewards: 45.61328820292954\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.40413\n",
      "Epoch:20 Batch:8 Loss:0.15874\n",
      "Epoch:40 Batch:8 Loss:0.10625\n",
      "Epoch:60 Batch:8 Loss:0.08348\n",
      "Epoch:80 Batch:8 Loss:0.07051\n",
      "Epoch:100 Batch:8 Loss:0.06117\n",
      "Epoch:120 Batch:8 Loss:0.05786\n",
      "Epoch:140 Batch:8 Loss:0.05574\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.055\n",
      "Epoch:10 Batch:8 Loss:0.053\n",
      "Epoch:20 Batch:8 Loss:0.053\n",
      "Epoch:30 Batch:8 Loss:0.052\n",
      "Epoch:40 Batch:8 Loss:0.053\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 56.959095496068755 setps: 41 count: 41\n",
      "reward: 59.85438766728477 setps: 49 count: 90\n",
      "reward: 80.17831382877922 setps: 66 count: 156\n",
      "reward: 20.58946094819985 setps: 23 count: 179\n",
      "reward: 59.959950033097996 setps: 44 count: 223\n",
      "reward: 61.377305679535496 setps: 46 count: 269\n",
      "reward: 17.0639385551156 setps: 20 count: 289\n",
      "reward: 17.026878442804446 setps: 19 count: 308\n",
      "reward: 43.419663246425635 setps: 34 count: 342\n",
      "reward: 50.73282776086854 setps: 37 count: 379\n",
      "reward: 55.82447167678911 setps: 41 count: 420\n",
      "reward: 50.47489175727387 setps: 38 count: 458\n",
      "reward: 53.05265192251973 setps: 38 count: 496\n",
      "reward: 76.66258615676925 setps: 61 count: 557\n",
      "reward: 21.3609799596612 setps: 21 count: 578\n",
      "reward: 40.0620286180274 setps: 33 count: 611\n",
      "reward: 63.192498663436 setps: 56 count: 667\n",
      "reward: 49.77380893340452 setps: 37 count: 704\n",
      "reward: 75.24584928284604 setps: 58 count: 762\n",
      "reward: 39.55109834358736 setps: 33 count: 795\n",
      "reward: 59.52537510409892 setps: 56 count: 851\n",
      "reward: 58.5756049016316 setps: 48 count: 899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 57.440257442675644 setps: 43 count: 942\n",
      "reward: 50.73399544166313 setps: 46 count: 988\n",
      "avg rewards: 50.776579994273504\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.39736\n",
      "Epoch:20 Batch:9 Loss:0.16025\n",
      "Epoch:40 Batch:9 Loss:0.10520\n",
      "Epoch:60 Batch:9 Loss:0.08335\n",
      "Epoch:80 Batch:9 Loss:0.06505\n",
      "Epoch:100 Batch:9 Loss:0.05605\n",
      "Epoch:120 Batch:9 Loss:0.05537\n",
      "Epoch:140 Batch:9 Loss:0.05882\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.049\n",
      "Epoch:10 Batch:8 Loss:0.050\n",
      "Epoch:20 Batch:8 Loss:0.049\n",
      "Epoch:30 Batch:8 Loss:0.048\n",
      "Epoch:40 Batch:8 Loss:0.048\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 60.16968891929866 setps: 51 count: 51\n",
      "reward: 59.822907262135416 setps: 51 count: 102\n",
      "reward: 55.78482886897109 setps: 44 count: 146\n",
      "reward: 50.00070271572768 setps: 40 count: 186\n",
      "reward: 57.89015503499686 setps: 48 count: 234\n",
      "reward: 32.19046489093016 setps: 35 count: 269\n",
      "reward: 58.396938576531824 setps: 43 count: 312\n",
      "reward: 46.43238266529806 setps: 43 count: 355\n",
      "reward: 12.027943523168506 setps: 19 count: 374\n",
      "reward: -7.6192619603738425 setps: 12 count: 386\n",
      "reward: 56.224969002076236 setps: 40 count: 426\n",
      "reward: 22.27454556493176 setps: 25 count: 451\n",
      "reward: 35.78467178982973 setps: 33 count: 484\n",
      "reward: 12.750455694741685 setps: 19 count: 503\n",
      "reward: 51.52778934955132 setps: 47 count: 550\n",
      "reward: 51.12929308031453 setps: 38 count: 588\n",
      "reward: 8.164431722907466 setps: 17 count: 605\n",
      "reward: 19.76850933062524 setps: 22 count: 627\n",
      "reward: 38.21452944177144 setps: 33 count: 660\n",
      "reward: 45.409374599381415 setps: 35 count: 695\n",
      "reward: 55.072667696730065 setps: 46 count: 741\n",
      "reward: 55.95164118284446 setps: 46 count: 787\n",
      "reward: 46.08824373149983 setps: 41 count: 828\n",
      "reward: 30.096238193054162 setps: 30 count: 858\n",
      "reward: 74.30169470214024 setps: 52 count: 910\n",
      "reward: 45.96451591897058 setps: 39 count: 949\n",
      "avg rewards: 41.30078159607902\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.38886\n",
      "Epoch:20 Batch:10 Loss:0.15330\n",
      "Epoch:40 Batch:10 Loss:0.09790\n",
      "Epoch:60 Batch:10 Loss:0.07213\n",
      "Epoch:80 Batch:10 Loss:0.06213\n",
      "Epoch:100 Batch:10 Loss:0.05699\n",
      "Epoch:120 Batch:10 Loss:0.04839\n",
      "Epoch:140 Batch:10 Loss:0.05544\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.048\n",
      "Epoch:10 Batch:8 Loss:0.048\n",
      "Epoch:20 Batch:8 Loss:0.048\n",
      "Epoch:30 Batch:8 Loss:0.049\n",
      "Epoch:40 Batch:8 Loss:0.048\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 49.545560235308955 setps: 40 count: 40\n",
      "reward: 43.93669060930842 setps: 36 count: 76\n",
      "reward: 35.88423115441837 setps: 32 count: 108\n",
      "reward: 36.14611166068062 setps: 32 count: 140\n",
      "reward: 31.476169648389625 setps: 34 count: 174\n",
      "reward: 47.156446309055916 setps: 41 count: 215\n",
      "reward: 33.95148894546728 setps: 32 count: 247\n",
      "reward: 48.560990492292326 setps: 46 count: 293\n",
      "reward: 35.54581967967533 setps: 31 count: 324\n",
      "reward: 40.16583159618748 setps: 35 count: 359\n",
      "reward: 36.92792833413987 setps: 32 count: 391\n",
      "reward: 40.529916390526346 setps: 34 count: 425\n",
      "reward: 30.418895365836217 setps: 35 count: 460\n",
      "reward: 34.70990388922218 setps: 30 count: 490\n",
      "reward: 61.78499357912369 setps: 48 count: 538\n",
      "reward: 35.87629001273599 setps: 31 count: 569\n",
      "reward: 45.629407009872374 setps: 35 count: 604\n",
      "reward: 31.704964122580712 setps: 35 count: 639\n",
      "reward: 20.45184593542799 setps: 22 count: 661\n",
      "reward: 10.412609332577269 setps: 19 count: 680\n",
      "reward: 26.62214328924603 setps: 30 count: 710\n",
      "reward: 55.19130888147775 setps: 53 count: 763\n",
      "reward: 44.17774772702831 setps: 40 count: 803\n",
      "reward: 24.134465021114742 setps: 24 count: 827\n",
      "reward: 36.21434422684252 setps: 36 count: 863\n",
      "reward: 43.51805551479918 setps: 43 count: 906\n",
      "reward: 49.00250303938082 setps: 39 count: 945\n",
      "reward: 16.831757206488692 setps: 19 count: 964\n",
      "avg rewards: 37.37530068604303\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.36017\n",
      "Epoch:20 Batch:11 Loss:0.12190\n",
      "Epoch:40 Batch:11 Loss:0.07513\n",
      "Epoch:60 Batch:11 Loss:0.07007\n",
      "Epoch:80 Batch:11 Loss:0.05712\n",
      "Epoch:100 Batch:11 Loss:0.05298\n",
      "Epoch:120 Batch:11 Loss:0.04550\n",
      "Epoch:140 Batch:11 Loss:0.04561\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.046\n",
      "Epoch:10 Batch:8 Loss:0.044\n",
      "Epoch:20 Batch:8 Loss:0.046\n",
      "Epoch:30 Batch:8 Loss:0.046\n",
      "Epoch:40 Batch:8 Loss:0.045\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 21.179974249834782 setps: 21 count: 21\n",
      "reward: 37.125792737610745 setps: 32 count: 53\n",
      "reward: 35.59928369500267 setps: 33 count: 86\n",
      "reward: 14.252313771822081 setps: 18 count: 104\n",
      "reward: 29.720985087873125 setps: 28 count: 132\n",
      "reward: 36.140593341263596 setps: 35 count: 167\n",
      "reward: 50.85366464805557 setps: 37 count: 204\n",
      "reward: 34.39111080831063 setps: 31 count: 235\n",
      "reward: 30.918169284918985 setps: 31 count: 266\n",
      "reward: 37.60478216905467 setps: 33 count: 299\n",
      "reward: 39.380859732156374 setps: 36 count: 335\n",
      "reward: 37.04793714834522 setps: 33 count: 368\n",
      "reward: 36.08746792831662 setps: 32 count: 400\n",
      "reward: 33.57682599658438 setps: 30 count: 430\n",
      "reward: 34.27945282499568 setps: 32 count: 462\n",
      "reward: 40.13034460438502 setps: 32 count: 494\n",
      "reward: 15.034739479898416 setps: 20 count: 514\n",
      "reward: 44.7010343696209 setps: 35 count: 549\n",
      "reward: 29.82806623378129 setps: 28 count: 577\n",
      "reward: 42.869283487500795 setps: 33 count: 610\n",
      "reward: 10.099367388672544 setps: 18 count: 628\n",
      "reward: 36.00383612227453 setps: 34 count: 662\n",
      "reward: 36.33826395278884 setps: 30 count: 692\n",
      "reward: 28.749615648054167 setps: 28 count: 720\n",
      "reward: 32.06851411484531 setps: 30 count: 750\n",
      "reward: 31.783143295857013 setps: 32 count: 782\n",
      "reward: 34.579640461954114 setps: 31 count: 813\n",
      "reward: 35.65125127007632 setps: 33 count: 846\n",
      "reward: 38.981218204527984 setps: 33 count: 879\n",
      "reward: 41.75893254470138 setps: 33 count: 912\n",
      "reward: 21.116173356515354 setps: 21 count: 933\n",
      "reward: 38.57571473730057 setps: 31 count: 964\n",
      "reward: 31.930465688765977 setps: 28 count: 992\n",
      "avg rewards: 33.28360055714138\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.37509\n",
      "Epoch:20 Batch:12 Loss:0.12779\n",
      "Epoch:40 Batch:12 Loss:0.07498\n",
      "Epoch:60 Batch:12 Loss:0.06602\n",
      "Epoch:80 Batch:12 Loss:0.05294\n",
      "Epoch:100 Batch:12 Loss:0.04790\n",
      "Epoch:120 Batch:12 Loss:0.04670\n",
      "Epoch:140 Batch:12 Loss:0.04970\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.045\n",
      "Epoch:10 Batch:8 Loss:0.043\n",
      "Epoch:20 Batch:8 Loss:0.043\n",
      "Epoch:30 Batch:8 Loss:0.043\n",
      "Epoch:40 Batch:8 Loss:0.043\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 39.4177338504247 setps: 33 count: 33\n",
      "reward: 35.317927834625884 setps: 31 count: 64\n",
      "reward: 31.652540863481413 setps: 29 count: 93\n",
      "reward: 49.48401779340638 setps: 39 count: 132\n",
      "reward: 16.06867602698621 setps: 18 count: 150\n",
      "reward: 34.302365208268746 setps: 30 count: 180\n",
      "reward: 11.253096782692593 setps: 18 count: 198\n",
      "reward: 42.61864790081892 setps: 32 count: 230\n",
      "reward: 38.38410315145447 setps: 32 count: 262\n",
      "reward: 25.593522642122117 setps: 25 count: 287\n",
      "reward: 38.58411114103365 setps: 33 count: 320\n",
      "reward: 31.262444323203816 setps: 31 count: 351\n",
      "reward: 34.889472829304566 setps: 33 count: 384\n",
      "reward: 33.52877229075092 setps: 29 count: 413\n",
      "reward: 39.47543065725039 setps: 31 count: 444\n",
      "reward: 17.885829056991497 setps: 19 count: 463\n",
      "reward: 18.083449078905687 setps: 20 count: 483\n",
      "reward: 35.289212101000885 setps: 30 count: 513\n",
      "reward: 45.02334504921309 setps: 34 count: 547\n",
      "reward: 31.21526318241376 setps: 29 count: 576\n",
      "reward: 36.78250583088666 setps: 32 count: 608\n",
      "reward: 33.306868931243656 setps: 31 count: 639\n",
      "reward: 48.7856049946 setps: 34 count: 673\n",
      "reward: 2.1074303287634386 setps: 16 count: 689\n",
      "reward: 19.37802311698615 setps: 20 count: 709\n",
      "reward: 41.828332094015785 setps: 35 count: 744\n",
      "reward: 43.793140010391646 setps: 34 count: 778\n",
      "reward: 45.35704424026481 setps: 34 count: 812\n",
      "reward: 48.39823640479007 setps: 35 count: 847\n",
      "reward: 39.09715068361111 setps: 32 count: 879\n",
      "reward: 34.776313506141015 setps: 32 count: 911\n",
      "reward: 32.208497421185896 setps: 29 count: 940\n",
      "reward: 37.62643499251134 setps: 33 count: 973\n",
      "avg rewards: 33.72047103999216\n",
      "Done! (13000, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.36830\n",
      "Epoch:20 Batch:13 Loss:0.10783\n",
      "Epoch:40 Batch:13 Loss:0.07269\n",
      "Epoch:60 Batch:13 Loss:0.06169\n",
      "Epoch:80 Batch:13 Loss:0.05259\n",
      "Epoch:100 Batch:13 Loss:0.04615\n",
      "Epoch:120 Batch:13 Loss:0.04887\n",
      "Epoch:140 Batch:13 Loss:0.04349\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.046\n",
      "Epoch:10 Batch:8 Loss:0.046\n",
      "Epoch:20 Batch:8 Loss:0.044\n",
      "Epoch:30 Batch:8 Loss:0.044\n",
      "Epoch:40 Batch:8 Loss:0.045\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 33.99421408330673 setps: 31 count: 31\n",
      "reward: 41.76735904447122 setps: 32 count: 63\n",
      "reward: 52.24196298035531 setps: 40 count: 103\n",
      "reward: 32.38385886125907 setps: 32 count: 135\n",
      "reward: 47.69881302418361 setps: 35 count: 170\n",
      "reward: 23.599914948426882 setps: 28 count: 198\n",
      "reward: 31.918649874879343 setps: 28 count: 226\n",
      "reward: 30.26515280448366 setps: 29 count: 255\n",
      "reward: 27.938053458579823 setps: 28 count: 283\n",
      "reward: 37.41276915864 setps: 32 count: 315\n",
      "reward: 38.53683475845027 setps: 35 count: 350\n",
      "reward: 29.80291736459621 setps: 29 count: 379\n",
      "reward: 39.68986304227438 setps: 32 count: 411\n",
      "reward: 58.243201184963986 setps: 43 count: 454\n",
      "reward: 39.90041662730655 setps: 31 count: 485\n",
      "reward: 32.38232531493123 setps: 30 count: 515\n",
      "reward: 46.74404285781348 setps: 38 count: 553\n",
      "reward: 39.61903562908845 setps: 33 count: 586\n",
      "reward: 37.375164989492625 setps: 37 count: 623\n",
      "reward: 40.707808843856036 setps: 36 count: 659\n",
      "reward: 47.5202427876371 setps: 36 count: 695\n",
      "reward: 39.75268074641353 setps: 37 count: 732\n",
      "reward: 50.186139732210734 setps: 37 count: 769\n",
      "reward: 10.900780605072214 setps: 17 count: 786\n",
      "reward: 30.5678335008386 setps: 28 count: 814\n",
      "reward: 42.70534754512628 setps: 33 count: 847\n",
      "reward: 37.2837277674218 setps: 34 count: 881\n",
      "reward: 49.13982560900331 setps: 34 count: 915\n",
      "reward: 23.546103929149105 setps: 21 count: 936\n",
      "reward: 36.52008291980746 setps: 31 count: 967\n",
      "reward: 32.771095340378814 setps: 30 count: 997\n",
      "avg rewards: 37.51987804304574\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.36686\n",
      "Epoch:20 Batch:14 Loss:0.11025\n",
      "Epoch:40 Batch:14 Loss:0.07491\n",
      "Epoch:60 Batch:14 Loss:0.05994\n",
      "Epoch:80 Batch:14 Loss:0.05179\n",
      "Epoch:100 Batch:14 Loss:0.04587\n",
      "Epoch:120 Batch:14 Loss:0.04394\n",
      "Epoch:140 Batch:14 Loss:0.04166\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.045\n",
      "Epoch:10 Batch:8 Loss:0.045\n",
      "Epoch:20 Batch:8 Loss:0.042\n",
      "Epoch:30 Batch:8 Loss:0.044\n",
      "Epoch:40 Batch:8 Loss:0.043\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 14.562724097332103 setps: 26 count: 26\n",
      "reward: 21.745615991896194 setps: 25 count: 51\n",
      "reward: 23.18723957062902 setps: 27 count: 78\n",
      "reward: 14.497364573967932 setps: 19 count: 97\n",
      "reward: 23.43360230244435 setps: 26 count: 123\n",
      "reward: 19.499841238536472 setps: 26 count: 149\n",
      "reward: 14.485122603387572 setps: 20 count: 169\n",
      "reward: 23.889767847790793 setps: 25 count: 194\n",
      "reward: 21.10897815056814 setps: 24 count: 218\n",
      "reward: 26.399018178033295 setps: 26 count: 244\n",
      "reward: 25.13978122229891 setps: 27 count: 271\n",
      "reward: 22.11840644430049 setps: 25 count: 296\n",
      "reward: 24.134237619845955 setps: 27 count: 323\n",
      "reward: 22.177575224579783 setps: 25 count: 348\n",
      "reward: 24.073590676339514 setps: 27 count: 375\n",
      "reward: 20.935273400208093 setps: 28 count: 403\n",
      "reward: 26.956559917828418 setps: 27 count: 430\n",
      "reward: 20.41576311094104 setps: 28 count: 458\n",
      "reward: 23.39998955526389 setps: 23 count: 481\n",
      "reward: 27.005815355523367 setps: 27 count: 508\n",
      "reward: 25.288167917577088 setps: 25 count: 533\n",
      "reward: 12.596450463091601 setps: 20 count: 553\n",
      "reward: 17.930379053064097 setps: 23 count: 576\n",
      "reward: 15.695957771690153 setps: 29 count: 605\n",
      "reward: 20.216668449403368 setps: 27 count: 632\n",
      "reward: 23.275877631978076 setps: 26 count: 658\n",
      "reward: 17.46469192938966 setps: 20 count: 678\n",
      "reward: 24.75454500099731 setps: 26 count: 704\n",
      "reward: 20.837352844217094 setps: 26 count: 730\n",
      "reward: 20.289368240868498 setps: 26 count: 756\n",
      "reward: 22.571141356934096 setps: 26 count: 782\n",
      "reward: 24.58147922757925 setps: 29 count: 811\n",
      "reward: 23.728292208959466 setps: 27 count: 838\n",
      "reward: 21.415109536441744 setps: 25 count: 863\n",
      "reward: 17.448512957655474 setps: 20 count: 883\n",
      "reward: 24.48027572557766 setps: 26 count: 909\n",
      "reward: 19.960626971624155 setps: 24 count: 933\n",
      "reward: 20.366570844048695 setps: 28 count: 961\n",
      "reward: 23.825678273339875 setps: 27 count: 988\n",
      "avg rewards: 21.43316444836289\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.34418\n",
      "Epoch:20 Batch:15 Loss:0.10215\n",
      "Epoch:40 Batch:15 Loss:0.06940\n",
      "Epoch:60 Batch:15 Loss:0.05204\n",
      "Epoch:80 Batch:15 Loss:0.04876\n",
      "Epoch:100 Batch:15 Loss:0.04647\n",
      "Epoch:120 Batch:15 Loss:0.03928\n",
      "Epoch:140 Batch:15 Loss:0.04113\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.045\n",
      "Epoch:10 Batch:8 Loss:0.045\n",
      "Epoch:20 Batch:8 Loss:0.045\n",
      "Epoch:30 Batch:8 Loss:0.044\n",
      "Epoch:40 Batch:8 Loss:0.043\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 26.808848717107324 setps: 27 count: 27\n",
      "reward: 22.081568966663323 setps: 29 count: 56\n",
      "reward: 27.132347294608188 setps: 27 count: 83\n",
      "reward: 24.799983895811604 setps: 26 count: 109\n",
      "reward: 27.60170071602769 setps: 27 count: 136\n",
      "reward: 23.298050150225755 setps: 29 count: 165\n",
      "reward: 28.000198250582617 setps: 30 count: 195\n",
      "reward: 26.11931856053415 setps: 30 count: 225\n",
      "reward: 31.872775006160374 setps: 29 count: 254\n",
      "reward: 27.016922199561677 setps: 28 count: 282\n",
      "reward: 28.343257635983175 setps: 28 count: 310\n",
      "reward: 29.508741374785313 setps: 27 count: 337\n",
      "reward: 26.077428917007637 setps: 28 count: 365\n",
      "reward: 25.991325361793862 setps: 29 count: 394\n",
      "reward: 26.265509773649683 setps: 27 count: 421\n",
      "reward: 34.096468265225134 setps: 42 count: 463\n",
      "reward: 24.469239191207453 setps: 27 count: 490\n",
      "reward: 30.21779653823178 setps: 29 count: 519\n",
      "reward: 25.012884919288624 setps: 26 count: 545\n",
      "reward: 25.31939692474116 setps: 29 count: 574\n",
      "reward: 24.93169173814531 setps: 22 count: 596\n",
      "reward: 24.17077909119544 setps: 29 count: 625\n",
      "reward: 27.573182579358402 setps: 26 count: 651\n",
      "reward: 25.721395552538162 setps: 28 count: 679\n",
      "reward: 31.8895025862992 setps: 34 count: 713\n",
      "reward: 19.615653974648737 setps: 20 count: 733\n",
      "reward: 26.83144343586028 setps: 29 count: 762\n",
      "reward: 24.49869487488031 setps: 27 count: 789\n",
      "reward: 24.93782805532828 setps: 30 count: 819\n",
      "reward: 25.994482426348263 setps: 29 count: 848\n",
      "reward: 25.19363089129474 setps: 27 count: 875\n",
      "reward: 22.865599194173406 setps: 29 count: 904\n",
      "reward: 27.140068821396557 setps: 29 count: 933\n",
      "reward: 25.529299750857174 setps: 31 count: 964\n",
      "reward: 26.921303970862937 setps: 27 count: 991\n",
      "avg rewards: 26.395666274353818\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.34453\n",
      "Epoch:20 Batch:16 Loss:0.09296\n",
      "Epoch:40 Batch:16 Loss:0.06292\n",
      "Epoch:60 Batch:16 Loss:0.05407\n",
      "Epoch:80 Batch:16 Loss:0.04764\n",
      "Epoch:100 Batch:16 Loss:0.03686\n",
      "Epoch:120 Batch:16 Loss:0.03962\n",
      "Epoch:140 Batch:16 Loss:0.04115\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.040\n",
      "Epoch:10 Batch:8 Loss:0.040\n",
      "Epoch:20 Batch:8 Loss:0.041\n",
      "Epoch:30 Batch:8 Loss:0.041\n",
      "Epoch:40 Batch:8 Loss:0.041\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 19.504873105141453 setps: 20 count: 20\n",
      "reward: 17.300298199012467 setps: 19 count: 39\n",
      "reward: 27.625177925662136 setps: 31 count: 70\n",
      "reward: 32.00739600706292 setps: 35 count: 105\n",
      "reward: 28.592501650449414 setps: 30 count: 135\n",
      "reward: 31.109151012855 setps: 31 count: 166\n",
      "reward: 21.904308844984918 setps: 28 count: 194\n",
      "reward: 32.85917744868056 setps: 29 count: 223\n",
      "reward: 45.01668678821006 setps: 36 count: 259\n",
      "reward: 21.184787895799676 setps: 20 count: 279\n",
      "reward: 22.80130754734127 setps: 30 count: 309\n",
      "reward: 15.678353247798809 setps: 19 count: 328\n",
      "reward: 22.891807681460335 setps: 22 count: 350\n",
      "reward: 28.272063779298335 setps: 28 count: 378\n",
      "reward: 29.11182360829552 setps: 28 count: 406\n",
      "reward: 23.63613171391771 setps: 30 count: 436\n",
      "reward: 19.25790733968606 setps: 37 count: 473\n",
      "reward: 28.60243889183912 setps: 27 count: 500\n",
      "reward: 28.596191553332034 setps: 34 count: 534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 20.314666300833043 setps: 29 count: 563\n",
      "reward: 24.37665051733929 setps: 32 count: 595\n",
      "reward: 17.50268990961049 setps: 20 count: 615\n",
      "reward: 23.820209717622497 setps: 27 count: 642\n",
      "reward: 31.781734170169514 setps: 30 count: 672\n",
      "reward: 31.03531310648686 setps: 29 count: 701\n",
      "reward: 17.936571153056867 setps: 20 count: 721\n",
      "reward: 28.54610057615064 setps: 29 count: 750\n",
      "reward: 30.376875707274312 setps: 27 count: 777\n",
      "reward: 30.59668014730559 setps: 25 count: 802\n",
      "reward: 30.556181690366063 setps: 27 count: 829\n",
      "reward: 26.303896376483316 setps: 31 count: 860\n",
      "reward: 26.516582675294195 setps: 24 count: 884\n",
      "reward: 32.955977086754864 setps: 29 count: 913\n",
      "reward: 29.153834939397353 setps: 29 count: 942\n",
      "reward: 25.93259353371249 setps: 30 count: 972\n",
      "avg rewards: 26.390255481391005\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.34634\n",
      "Epoch:20 Batch:17 Loss:0.08333\n",
      "Epoch:40 Batch:17 Loss:0.06241\n",
      "Epoch:60 Batch:17 Loss:0.04957\n",
      "Epoch:80 Batch:17 Loss:0.04719\n",
      "Epoch:100 Batch:17 Loss:0.04101\n",
      "Epoch:120 Batch:17 Loss:0.03866\n",
      "Epoch:140 Batch:17 Loss:0.04065\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.039\n",
      "Epoch:10 Batch:8 Loss:0.039\n",
      "Epoch:20 Batch:8 Loss:0.038\n",
      "Epoch:30 Batch:8 Loss:0.038\n",
      "Epoch:40 Batch:8 Loss:0.040\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 18.416711221427253 setps: 35 count: 35\n",
      "reward: 22.309365415046344 setps: 36 count: 71\n",
      "reward: 31.34840866822487 setps: 32 count: 103\n",
      "reward: 16.932892774029455 setps: 42 count: 145\n",
      "reward: 19.176951407860905 setps: 21 count: 166\n",
      "reward: 27.637685775736465 setps: 35 count: 201\n",
      "reward: 30.932552212574226 setps: 33 count: 234\n",
      "reward: 28.677087228522577 setps: 28 count: 262\n",
      "reward: 13.017356342599669 setps: 18 count: 280\n",
      "reward: 34.907372689765175 setps: 47 count: 327\n",
      "reward: 19.97068430932122 setps: 41 count: 368\n",
      "reward: 27.641497344357774 setps: 38 count: 406\n",
      "reward: 21.29394175388733 setps: 22 count: 428\n",
      "reward: 27.14526110493316 setps: 29 count: 457\n",
      "reward: 18.160514794729536 setps: 42 count: 499\n",
      "reward: 29.529569692509543 setps: 39 count: 538\n",
      "reward: 20.269254460523367 setps: 44 count: 582\n",
      "reward: 30.505238286698294 setps: 29 count: 611\n",
      "reward: 30.89072204980475 setps: 30 count: 641\n",
      "reward: 44.519088092446324 setps: 53 count: 694\n",
      "reward: 31.490444383733852 setps: 48 count: 742\n",
      "reward: 32.330824408693296 setps: 33 count: 775\n",
      "reward: 35.062916395442144 setps: 34 count: 809\n",
      "reward: 25.579695118796366 setps: 36 count: 845\n",
      "reward: 30.11717445669929 setps: 35 count: 880\n",
      "reward: 17.349530245939974 setps: 36 count: 916\n",
      "reward: 21.85469916549918 setps: 19 count: 935\n",
      "reward: 28.86747165187844 setps: 34 count: 969\n",
      "avg rewards: 26.283389694702883\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.32623\n",
      "Epoch:20 Batch:18 Loss:0.07764\n",
      "Epoch:40 Batch:18 Loss:0.05503\n",
      "Epoch:60 Batch:18 Loss:0.04878\n",
      "Epoch:80 Batch:18 Loss:0.04565\n",
      "Epoch:100 Batch:18 Loss:0.04350\n",
      "Epoch:120 Batch:18 Loss:0.03678\n",
      "Epoch:140 Batch:18 Loss:0.03397\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.042\n",
      "Epoch:10 Batch:8 Loss:0.041\n",
      "Epoch:20 Batch:8 Loss:0.041\n",
      "Epoch:30 Batch:8 Loss:0.041\n",
      "Epoch:40 Batch:8 Loss:0.040\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 37.26911938507983 setps: 39 count: 39\n",
      "reward: 33.91620720690843 setps: 32 count: 71\n",
      "reward: 47.72943353972659 setps: 38 count: 109\n",
      "reward: 18.11956175435771 setps: 22 count: 131\n",
      "reward: 19.43060115707486 setps: 40 count: 171\n",
      "reward: 20.78283848525316 setps: 21 count: 192\n",
      "reward: 18.72481065396714 setps: 19 count: 211\n",
      "reward: 13.682431623793667 setps: 40 count: 251\n",
      "reward: 35.950862224076985 setps: 29 count: 280\n",
      "reward: 19.612996297037178 setps: 41 count: 321\n",
      "reward: 36.78741200267686 setps: 43 count: 364\n",
      "reward: 33.91653524803696 setps: 34 count: 398\n",
      "reward: 25.67408652721934 setps: 27 count: 425\n",
      "reward: 36.16107217100361 setps: 39 count: 464\n",
      "reward: 30.964443764624594 setps: 39 count: 503\n",
      "reward: 21.321287675907662 setps: 20 count: 523\n",
      "reward: 25.98472773310059 setps: 38 count: 561\n",
      "reward: 11.775740066252183 setps: 17 count: 578\n",
      "reward: 44.14711737731704 setps: 41 count: 619\n",
      "reward: 33.662025445542525 setps: 33 count: 652\n",
      "reward: 30.140302692880493 setps: 36 count: 688\n",
      "reward: 41.70332718594874 setps: 40 count: 728\n",
      "reward: 20.637753864176922 setps: 21 count: 749\n",
      "reward: 31.65206501597568 setps: 44 count: 793\n",
      "reward: 43.00923632349586 setps: 47 count: 840\n",
      "reward: 23.237546567172103 setps: 22 count: 862\n",
      "reward: 38.98376778142848 setps: 39 count: 901\n",
      "reward: 25.38437923727325 setps: 36 count: 937\n",
      "reward: 45.21520782860461 setps: 39 count: 976\n",
      "avg rewards: 29.847479201238386\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.32755\n",
      "Epoch:20 Batch:19 Loss:0.07896\n",
      "Epoch:40 Batch:19 Loss:0.05916\n",
      "Epoch:60 Batch:19 Loss:0.04458\n",
      "Epoch:80 Batch:19 Loss:0.04656\n",
      "Epoch:100 Batch:19 Loss:0.03733\n",
      "Epoch:120 Batch:19 Loss:0.03564\n",
      "Epoch:140 Batch:19 Loss:0.03705\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.039\n",
      "Epoch:10 Batch:8 Loss:0.041\n",
      "Epoch:20 Batch:8 Loss:0.041\n",
      "Epoch:30 Batch:8 Loss:0.038\n",
      "Epoch:40 Batch:8 Loss:0.040\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 24.254147034040937 setps: 30 count: 30\n",
      "reward: 15.583930860018882 setps: 28 count: 58\n",
      "reward: 23.33397548050561 setps: 26 count: 84\n",
      "reward: 18.288611795840556 setps: 20 count: 104\n",
      "reward: 22.870493220593197 setps: 40 count: 144\n",
      "reward: 21.176039929747642 setps: 32 count: 176\n",
      "reward: 21.323958805929582 setps: 43 count: 219\n",
      "reward: 22.564848450574203 setps: 34 count: 253\n",
      "reward: 16.921048025216436 setps: 28 count: 281\n",
      "reward: 25.110234099165254 setps: 41 count: 322\n",
      "reward: 15.136403883874301 setps: 39 count: 361\n",
      "reward: 20.192865783112936 setps: 27 count: 388\n",
      "reward: 38.224665458095835 setps: 43 count: 431\n",
      "reward: 26.585865496729095 setps: 37 count: 468\n",
      "reward: 31.911521624625315 setps: 37 count: 505\n",
      "reward: 15.863982718720216 setps: 47 count: 552\n",
      "reward: 18.201845864618374 setps: 33 count: 585\n",
      "reward: 20.407133772408994 setps: 31 count: 616\n",
      "reward: 35.20960378421878 setps: 38 count: 654\n",
      "reward: 19.331077804962124 setps: 38 count: 692\n",
      "reward: 13.956998277094678 setps: 36 count: 728\n",
      "reward: 25.239744840668568 setps: 21 count: 749\n",
      "reward: 18.036188774084437 setps: 42 count: 791\n",
      "reward: 30.88244014049414 setps: 24 count: 815\n",
      "reward: 19.729961563226247 setps: 37 count: 852\n",
      "reward: 14.17361161909648 setps: 35 count: 887\n",
      "reward: 27.57518138221785 setps: 25 count: 912\n",
      "reward: 20.338022651753274 setps: 44 count: 956\n",
      "avg rewards: 22.229442969344067\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.32516\n",
      "Epoch:20 Batch:20 Loss:0.08249\n",
      "Epoch:40 Batch:20 Loss:0.04971\n",
      "Epoch:60 Batch:20 Loss:0.04612\n",
      "Epoch:80 Batch:20 Loss:0.04605\n",
      "Epoch:100 Batch:20 Loss:0.03683\n",
      "Epoch:120 Batch:20 Loss:0.03253\n",
      "Epoch:140 Batch:20 Loss:0.03606\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.038\n",
      "Epoch:10 Batch:8 Loss:0.036\n",
      "Epoch:20 Batch:8 Loss:0.036\n",
      "Epoch:30 Batch:8 Loss:0.034\n",
      "Epoch:40 Batch:8 Loss:0.039\n",
      "Done!\n",
      "############# start BipedalWalker-v3 training ###################\n",
      "(49,)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -44.24513472215952 setps: 800 count: 800\n",
      "reward: -100.77605780164983 setps: 73 count: 873\n",
      "reward: -101.72889737528749 setps: 70 count: 943\n",
      "reward: -118.75422500360696 setps: 56 count: 999\n",
      "avg rewards: -91.37607872567595\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:0.33369\n",
      "Epoch:20 Batch:1 Loss:0.14185\n",
      "Epoch:40 Batch:1 Loss:0.12496\n",
      "Epoch:60 Batch:1 Loss:0.12003\n",
      "Epoch:80 Batch:1 Loss:0.11610\n",
      "Epoch:100 Batch:1 Loss:0.11125\n",
      "Epoch:120 Batch:1 Loss:0.10298\n",
      "Epoch:140 Batch:1 Loss:0.09245\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.214\n",
      "Epoch:10 Batch:9 Loss:0.218\n",
      "Epoch:20 Batch:9 Loss:0.212\n",
      "Epoch:30 Batch:9 Loss:0.215\n",
      "Epoch:40 Batch:9 Loss:0.206\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -100.81486408693095 setps: 153 count: 153\n",
      "reward: -99.2275809181966 setps: 96 count: 249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg rewards: -100.02122250256377\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:0.30102\n",
      "Epoch:20 Batch:2 Loss:0.11391\n",
      "Epoch:40 Batch:2 Loss:0.06953\n",
      "Epoch:60 Batch:2 Loss:0.06733\n",
      "Epoch:80 Batch:2 Loss:0.06217\n",
      "Epoch:100 Batch:2 Loss:0.05567\n",
      "Epoch:120 Batch:2 Loss:0.05129\n",
      "Epoch:140 Batch:2 Loss:0.04295\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.160\n",
      "Epoch:10 Batch:9 Loss:0.149\n",
      "Epoch:20 Batch:9 Loss:0.157\n",
      "Epoch:30 Batch:9 Loss:0.149\n",
      "Epoch:40 Batch:9 Loss:0.149\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -102.62454154609765 setps: 174 count: 174\n",
      "reward: -121.59769614325836 setps: 101 count: 275\n",
      "reward: -129.47958080657818 setps: 122 count: 397\n",
      "reward: -128.39956890332635 setps: 95 count: 492\n",
      "reward: -127.61369860863499 setps: 97 count: 589\n",
      "reward: -126.39461880604985 setps: 100 count: 689\n",
      "avg rewards: -122.68495080232422\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.31046\n",
      "Epoch:20 Batch:3 Loss:0.11678\n",
      "Epoch:40 Batch:3 Loss:0.07538\n",
      "Epoch:60 Batch:3 Loss:0.05651\n",
      "Epoch:80 Batch:3 Loss:0.05006\n",
      "Epoch:100 Batch:3 Loss:0.04279\n",
      "Epoch:120 Batch:3 Loss:0.03758\n",
      "Epoch:140 Batch:3 Loss:0.03395\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.111\n",
      "Epoch:10 Batch:9 Loss:0.110\n",
      "Epoch:20 Batch:9 Loss:0.109\n",
      "Epoch:30 Batch:9 Loss:0.105\n",
      "Epoch:40 Batch:9 Loss:0.102\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -22.31776086212199 setps: 800 count: 800\n",
      "avg rewards: -22.31776086212199\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.31009\n",
      "Epoch:20 Batch:4 Loss:0.08247\n",
      "Epoch:40 Batch:4 Loss:0.05156\n",
      "Epoch:60 Batch:4 Loss:0.04249\n",
      "Epoch:80 Batch:4 Loss:0.03532\n",
      "Epoch:100 Batch:4 Loss:0.03087\n",
      "Epoch:120 Batch:4 Loss:0.02806\n",
      "Epoch:140 Batch:4 Loss:0.02611\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.093\n",
      "Epoch:10 Batch:9 Loss:0.100\n",
      "Epoch:20 Batch:9 Loss:0.100\n",
      "Epoch:30 Batch:9 Loss:0.100\n",
      "Epoch:40 Batch:9 Loss:0.096\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -119.31302181597738 setps: 92 count: 92\n",
      "reward: -118.10386665746073 setps: 96 count: 188\n",
      "reward: -115.72153063723508 setps: 99 count: 287\n",
      "reward: -118.85225155250853 setps: 94 count: 381\n",
      "reward: -120.74369799413842 setps: 89 count: 470\n",
      "reward: -114.256206012529 setps: 98 count: 568\n",
      "reward: -116.78918034009946 setps: 79 count: 647\n",
      "reward: -112.66159380332195 setps: 101 count: 748\n",
      "reward: -112.08275456375132 setps: 87 count: 835\n",
      "reward: -119.94256803514808 setps: 92 count: 927\n",
      "avg rewards: -116.846667141217\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.30481\n",
      "Epoch:20 Batch:5 Loss:0.07342\n",
      "Epoch:40 Batch:5 Loss:0.04882\n",
      "Epoch:60 Batch:5 Loss:0.03794\n",
      "Epoch:80 Batch:5 Loss:0.03359\n",
      "Epoch:100 Batch:5 Loss:0.02903\n",
      "Epoch:120 Batch:5 Loss:0.02659\n",
      "Epoch:140 Batch:5 Loss:0.02474\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.086\n",
      "Epoch:10 Batch:9 Loss:0.088\n",
      "Epoch:20 Batch:9 Loss:0.091\n",
      "Epoch:30 Batch:9 Loss:0.088\n",
      "Epoch:40 Batch:9 Loss:0.088\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -125.31921576104872 setps: 93 count: 93\n",
      "reward: -118.12639754750208 setps: 78 count: 171\n",
      "reward: -118.50484797844416 setps: 69 count: 240\n",
      "reward: -118.79293008134218 setps: 86 count: 326\n",
      "reward: -116.96839520446429 setps: 79 count: 405\n",
      "reward: -114.55476749251287 setps: 79 count: 484\n",
      "reward: -110.23539834865494 setps: 75 count: 559\n",
      "reward: -120.71383199070394 setps: 104 count: 663\n",
      "reward: -124.13861988002684 setps: 108 count: 771\n",
      "reward: -115.37151118244479 setps: 83 count: 854\n",
      "reward: -110.46129333979451 setps: 72 count: 926\n",
      "avg rewards: -117.56247352790356\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.31134\n",
      "Epoch:20 Batch:6 Loss:0.06731\n",
      "Epoch:40 Batch:6 Loss:0.04627\n",
      "Epoch:60 Batch:6 Loss:0.03768\n",
      "Epoch:80 Batch:6 Loss:0.03068\n",
      "Epoch:100 Batch:6 Loss:0.02877\n",
      "Epoch:120 Batch:6 Loss:0.02452\n",
      "Epoch:140 Batch:6 Loss:0.02542\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.085\n",
      "Epoch:10 Batch:9 Loss:0.088\n",
      "Epoch:20 Batch:9 Loss:0.086\n",
      "Epoch:30 Batch:9 Loss:0.082\n",
      "Epoch:40 Batch:9 Loss:0.084\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -23.502763751773813 setps: 800 count: 800\n",
      "avg rewards: -23.502763751773813\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.31511\n",
      "Epoch:20 Batch:7 Loss:0.05623\n",
      "Epoch:40 Batch:7 Loss:0.04227\n",
      "Epoch:60 Batch:7 Loss:0.03258\n",
      "Epoch:80 Batch:7 Loss:0.02922\n",
      "Epoch:100 Batch:7 Loss:0.02620\n",
      "Epoch:120 Batch:7 Loss:0.02529\n",
      "Epoch:140 Batch:7 Loss:0.02320\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.082\n",
      "Epoch:10 Batch:9 Loss:0.080\n",
      "Epoch:20 Batch:9 Loss:0.078\n",
      "Epoch:30 Batch:9 Loss:0.077\n",
      "Epoch:40 Batch:9 Loss:0.072\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -126.56045240734456 setps: 106 count: 106\n",
      "reward: -126.20125214255602 setps: 89 count: 195\n",
      "reward: -125.38883724355512 setps: 84 count: 279\n",
      "reward: -121.51974568930207 setps: 66 count: 345\n",
      "reward: -125.31631007902945 setps: 95 count: 440\n",
      "reward: -124.3258444532423 setps: 74 count: 514\n",
      "reward: -132.17983446160395 setps: 85 count: 599\n",
      "reward: -111.4178565640822 setps: 58 count: 657\n",
      "reward: -125.0804257362131 setps: 110 count: 767\n",
      "reward: -125.37965625835582 setps: 81 count: 848\n",
      "avg rewards: -124.33702150352846\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.31866\n",
      "Epoch:20 Batch:8 Loss:0.06078\n",
      "Epoch:40 Batch:8 Loss:0.04039\n",
      "Epoch:60 Batch:8 Loss:0.03207\n",
      "Epoch:80 Batch:8 Loss:0.02762\n",
      "Epoch:100 Batch:8 Loss:0.02641\n",
      "Epoch:120 Batch:8 Loss:0.02306\n",
      "Epoch:140 Batch:8 Loss:0.02347\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.076\n",
      "Epoch:10 Batch:9 Loss:0.075\n",
      "Epoch:20 Batch:9 Loss:0.075\n",
      "Epoch:30 Batch:9 Loss:0.076\n",
      "Epoch:40 Batch:9 Loss:0.073\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -125.65630109084583 setps: 163 count: 163\n",
      "reward: -124.00852450880285 setps: 107 count: 270\n",
      "reward: -125.73651387751848 setps: 103 count: 373\n",
      "reward: -123.33880741461365 setps: 118 count: 491\n",
      "reward: -127.62470136903474 setps: 100 count: 591\n",
      "reward: -121.50558503887555 setps: 91 count: 682\n",
      "reward: -114.27938083881513 setps: 64 count: 746\n",
      "reward: -123.80629617291379 setps: 90 count: 836\n",
      "reward: -118.82808811506195 setps: 75 count: 911\n",
      "reward: -119.42210389768333 setps: 75 count: 986\n",
      "avg rewards: -122.42063023241653\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.29514\n",
      "Epoch:20 Batch:9 Loss:0.05257\n",
      "Epoch:40 Batch:9 Loss:0.03728\n",
      "Epoch:60 Batch:9 Loss:0.02919\n",
      "Epoch:80 Batch:9 Loss:0.02718\n",
      "Epoch:100 Batch:9 Loss:0.02550\n",
      "Epoch:120 Batch:9 Loss:0.02623\n",
      "Epoch:140 Batch:9 Loss:0.02312\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.069\n",
      "Epoch:10 Batch:9 Loss:0.072\n",
      "Epoch:20 Batch:9 Loss:0.070\n",
      "Epoch:30 Batch:9 Loss:0.069\n",
      "Epoch:40 Batch:9 Loss:0.069\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -124.44781514453453 setps: 113 count: 113\n",
      "reward: -117.32932338026352 setps: 70 count: 183\n",
      "reward: -126.93608269476891 setps: 123 count: 306\n",
      "reward: -113.59668575298599 setps: 60 count: 366\n",
      "reward: -128.88359589336005 setps: 132 count: 498\n",
      "reward: -128.9067737357219 setps: 201 count: 699\n",
      "reward: -119.43707016022876 setps: 97 count: 796\n",
      "avg rewards: -122.79104953740908\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.28778\n",
      "Epoch:20 Batch:10 Loss:0.04712\n",
      "Epoch:40 Batch:10 Loss:0.03364\n",
      "Epoch:60 Batch:10 Loss:0.02777\n",
      "Epoch:80 Batch:10 Loss:0.02746\n",
      "Epoch:100 Batch:10 Loss:0.02543\n",
      "Epoch:120 Batch:10 Loss:0.02296\n",
      "Epoch:140 Batch:10 Loss:0.02302\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.069\n",
      "Epoch:10 Batch:9 Loss:0.068\n",
      "Epoch:20 Batch:9 Loss:0.069\n",
      "Epoch:30 Batch:9 Loss:0.065\n",
      "Epoch:40 Batch:9 Loss:0.064\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -108.21696733151563 setps: 48 count: 48\n",
      "reward: -124.20051327629574 setps: 65 count: 113\n",
      "reward: -110.11471372949767 setps: 49 count: 162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -114.6156412136058 setps: 54 count: 216\n",
      "reward: -110.54752671955526 setps: 51 count: 267\n",
      "reward: -128.03177049240594 setps: 68 count: 335\n",
      "reward: -125.74025489232938 setps: 67 count: 402\n",
      "reward: -119.26912140533142 setps: 57 count: 459\n",
      "reward: -112.78308120824273 setps: 54 count: 513\n",
      "reward: -123.5641482308712 setps: 65 count: 578\n",
      "reward: -124.8332683240318 setps: 66 count: 644\n",
      "reward: -109.48198440851644 setps: 48 count: 692\n",
      "reward: -111.49144453113153 setps: 50 count: 742\n",
      "reward: -107.69357935705223 setps: 46 count: 788\n",
      "reward: -118.37174082916354 setps: 67 count: 855\n",
      "reward: -118.7167212052426 setps: 58 count: 913\n",
      "reward: -109.98982469207235 setps: 50 count: 963\n",
      "avg rewards: -116.33307657922713\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.28193\n",
      "Epoch:20 Batch:11 Loss:0.05375\n",
      "Epoch:40 Batch:11 Loss:0.03468\n",
      "Epoch:60 Batch:11 Loss:0.02975\n",
      "Epoch:80 Batch:11 Loss:0.02513\n",
      "Epoch:100 Batch:11 Loss:0.02646\n",
      "Epoch:120 Batch:11 Loss:0.02261\n",
      "Epoch:140 Batch:11 Loss:0.02308\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.066\n",
      "Epoch:10 Batch:9 Loss:0.060\n",
      "Epoch:20 Batch:9 Loss:0.062\n",
      "Epoch:30 Batch:9 Loss:0.060\n",
      "Epoch:40 Batch:9 Loss:0.068\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -116.73707971792234 setps: 65 count: 65\n",
      "reward: -115.94583795340483 setps: 58 count: 123\n",
      "reward: -117.91165802957303 setps: 63 count: 186\n",
      "reward: -123.67638789937833 setps: 69 count: 255\n",
      "reward: -117.1967742698354 setps: 60 count: 315\n",
      "reward: -120.57506329072577 setps: 68 count: 383\n",
      "reward: -127.75776964730272 setps: 85 count: 468\n",
      "reward: -115.96768788658828 setps: 61 count: 529\n",
      "reward: -121.69495969716336 setps: 92 count: 621\n",
      "reward: -126.71730954046797 setps: 80 count: 701\n",
      "reward: -112.21965919354993 setps: 54 count: 755\n",
      "reward: -118.26410096919474 setps: 63 count: 818\n",
      "reward: -130.96546742987073 setps: 177 count: 995\n",
      "avg rewards: -120.43305811730596\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.31254\n",
      "Epoch:20 Batch:12 Loss:0.04889\n",
      "Epoch:40 Batch:12 Loss:0.03205\n",
      "Epoch:60 Batch:12 Loss:0.03068\n",
      "Epoch:80 Batch:12 Loss:0.02514\n",
      "Epoch:100 Batch:12 Loss:0.02349\n",
      "Epoch:120 Batch:12 Loss:0.02609\n",
      "Epoch:140 Batch:12 Loss:0.02258\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.065\n",
      "Epoch:10 Batch:9 Loss:0.064\n",
      "Epoch:20 Batch:9 Loss:0.060\n",
      "Epoch:30 Batch:9 Loss:0.063\n",
      "Epoch:40 Batch:9 Loss:0.060\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -28.809579061905538 setps: 800 count: 800\n",
      "avg rewards: -28.809579061905538\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.30265\n",
      "Epoch:20 Batch:13 Loss:0.04291\n",
      "Epoch:40 Batch:13 Loss:0.03101\n",
      "Epoch:60 Batch:13 Loss:0.02741\n",
      "Epoch:80 Batch:13 Loss:0.02538\n",
      "Epoch:100 Batch:13 Loss:0.02485\n",
      "Epoch:120 Batch:13 Loss:0.02436\n",
      "Epoch:140 Batch:13 Loss:0.02265\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.053\n",
      "Epoch:10 Batch:9 Loss:0.052\n",
      "Epoch:20 Batch:9 Loss:0.053\n",
      "Epoch:30 Batch:9 Loss:0.053\n",
      "Epoch:40 Batch:9 Loss:0.051\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -23.259050703073555 setps: 800 count: 800\n",
      "avg rewards: -23.259050703073555\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.29029\n",
      "Epoch:20 Batch:14 Loss:0.04391\n",
      "Epoch:40 Batch:14 Loss:0.03312\n",
      "Epoch:60 Batch:14 Loss:0.02853\n",
      "Epoch:80 Batch:14 Loss:0.02484\n",
      "Epoch:100 Batch:14 Loss:0.02560\n",
      "Epoch:120 Batch:14 Loss:0.02217\n",
      "Epoch:140 Batch:14 Loss:0.02053\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.049\n",
      "Epoch:10 Batch:9 Loss:0.049\n",
      "Epoch:20 Batch:9 Loss:0.052\n",
      "Epoch:30 Batch:9 Loss:0.048\n",
      "Epoch:40 Batch:9 Loss:0.050\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -46.72987229694119 setps: 800 count: 800\n",
      "avg rewards: -46.72987229694119\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.24270\n",
      "Epoch:20 Batch:15 Loss:0.04202\n",
      "Epoch:40 Batch:15 Loss:0.03372\n",
      "Epoch:60 Batch:15 Loss:0.03055\n",
      "Epoch:80 Batch:15 Loss:0.02776\n",
      "Epoch:100 Batch:15 Loss:0.02451\n",
      "Epoch:120 Batch:15 Loss:0.02426\n",
      "Epoch:140 Batch:15 Loss:0.02309\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.051\n",
      "Epoch:10 Batch:9 Loss:0.048\n",
      "Epoch:20 Batch:9 Loss:0.050\n",
      "Epoch:30 Batch:9 Loss:0.046\n",
      "Epoch:40 Batch:9 Loss:0.053\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -37.68367565952365 setps: 800 count: 800\n",
      "avg rewards: -37.68367565952365\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.26394\n",
      "Epoch:20 Batch:16 Loss:0.04282\n",
      "Epoch:40 Batch:16 Loss:0.03187\n",
      "Epoch:60 Batch:16 Loss:0.02938\n",
      "Epoch:80 Batch:16 Loss:0.02656\n",
      "Epoch:100 Batch:16 Loss:0.02729\n",
      "Epoch:120 Batch:16 Loss:0.02306\n",
      "Epoch:140 Batch:16 Loss:0.02393\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.047\n",
      "Epoch:10 Batch:9 Loss:0.049\n",
      "Epoch:20 Batch:9 Loss:0.048\n",
      "Epoch:30 Batch:9 Loss:0.046\n",
      "Epoch:40 Batch:9 Loss:0.047\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -18.96646082390719 setps: 800 count: 800\n",
      "avg rewards: -18.96646082390719\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.26943\n",
      "Epoch:20 Batch:17 Loss:0.04246\n",
      "Epoch:40 Batch:17 Loss:0.03233\n",
      "Epoch:60 Batch:17 Loss:0.02803\n",
      "Epoch:80 Batch:17 Loss:0.02629\n",
      "Epoch:100 Batch:17 Loss:0.02617\n",
      "Epoch:120 Batch:17 Loss:0.02447\n",
      "Epoch:140 Batch:17 Loss:0.02488\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.047\n",
      "Epoch:10 Batch:9 Loss:0.042\n",
      "Epoch:20 Batch:9 Loss:0.044\n",
      "Epoch:30 Batch:9 Loss:0.045\n",
      "Epoch:40 Batch:9 Loss:0.042\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 28.98569324467644 setps: 800 count: 800\n",
      "avg rewards: 28.98569324467644\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.24619\n",
      "Epoch:20 Batch:18 Loss:0.03904\n",
      "Epoch:40 Batch:18 Loss:0.03168\n",
      "Epoch:60 Batch:18 Loss:0.02728\n",
      "Epoch:80 Batch:18 Loss:0.02892\n",
      "Epoch:100 Batch:18 Loss:0.02816\n",
      "Epoch:120 Batch:18 Loss:0.02494\n",
      "Epoch:140 Batch:18 Loss:0.02511\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.040\n",
      "Epoch:10 Batch:9 Loss:0.039\n",
      "Epoch:20 Batch:9 Loss:0.038\n",
      "Epoch:30 Batch:9 Loss:0.035\n",
      "Epoch:40 Batch:9 Loss:0.038\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 33.75402129999857 setps: 800 count: 800\n",
      "avg rewards: 33.75402129999857\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.23663\n",
      "Epoch:20 Batch:19 Loss:0.04314\n",
      "Epoch:40 Batch:19 Loss:0.03263\n",
      "Epoch:60 Batch:19 Loss:0.02997\n",
      "Epoch:80 Batch:19 Loss:0.02892\n",
      "Epoch:100 Batch:19 Loss:0.02373\n",
      "Epoch:120 Batch:19 Loss:0.02628\n",
      "Epoch:140 Batch:19 Loss:0.02490\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.040\n",
      "Epoch:10 Batch:9 Loss:0.041\n",
      "Epoch:20 Batch:9 Loss:0.038\n",
      "Epoch:30 Batch:9 Loss:0.037\n",
      "Epoch:40 Batch:9 Loss:0.036\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -117.95066830111047 setps: 61 count: 61\n",
      "reward: 148.762256443038 setps: 800 count: 861\n",
      "avg rewards: 15.405794070963772\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.20724\n",
      "Epoch:20 Batch:20 Loss:0.03791\n",
      "Epoch:40 Batch:20 Loss:0.03209\n",
      "Epoch:60 Batch:20 Loss:0.02767\n",
      "Epoch:80 Batch:20 Loss:0.02640\n",
      "Epoch:100 Batch:20 Loss:0.02685\n",
      "Epoch:120 Batch:20 Loss:0.02367\n",
      "Epoch:140 Batch:20 Loss:0.02434\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.030\n",
      "Epoch:10 Batch:9 Loss:0.033\n",
      "Epoch:20 Batch:9 Loss:0.032\n",
      "Epoch:30 Batch:9 Loss:0.033\n",
      "Epoch:40 Batch:9 Loss:0.031\n",
      "Done!\n",
      "############# start Walker2DBulletEnv-v0 training ###################\n",
      "(49, 1000, 22)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 17.628482255975538 setps: 13 count: 13\n",
      "reward: 15.295545780273098 setps: 12 count: 25\n",
      "reward: 14.60132241650281 setps: 9 count: 34\n",
      "reward: 16.80427010738058 setps: 14 count: 48\n",
      "reward: 13.327445565631205 setps: 8 count: 56\n",
      "reward: 10.250701255020976 setps: 8 count: 64\n",
      "reward: 15.136430656119773 setps: 13 count: 77\n",
      "reward: 11.756486017561109 setps: 7 count: 84\n",
      "reward: 16.824948083671917 setps: 12 count: 96\n",
      "reward: 19.49326231560699 setps: 17 count: 113\n",
      "reward: 27.06296331273333 setps: 26 count: 139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 25.434329342700945 setps: 27 count: 166\n",
      "reward: 14.202319174558214 setps: 8 count: 174\n",
      "reward: 16.054074167239012 setps: 17 count: 191\n",
      "reward: 20.110619779252737 setps: 18 count: 209\n",
      "reward: 15.33067732019408 setps: 12 count: 221\n",
      "reward: 19.559810573591676 setps: 21 count: 242\n",
      "reward: 22.055368437802823 setps: 23 count: 265\n",
      "reward: 16.328318490931995 setps: 16 count: 281\n",
      "reward: 18.166326788424335 setps: 18 count: 299\n",
      "reward: 14.575148098405041 setps: 14 count: 313\n",
      "reward: 22.141886228630028 setps: 22 count: 335\n",
      "reward: 11.09307995052659 setps: 6 count: 341\n",
      "reward: 16.746675539205896 setps: 14 count: 355\n",
      "reward: 14.491405984517769 setps: 10 count: 365\n",
      "reward: 12.139916922258271 setps: 8 count: 373\n",
      "reward: 27.976969660820036 setps: 30 count: 403\n",
      "reward: 15.532611891254785 setps: 12 count: 415\n",
      "reward: 16.336029688832056 setps: 13 count: 428\n",
      "reward: 17.608039007104523 setps: 14 count: 442\n",
      "reward: 17.625913934643904 setps: 21 count: 463\n",
      "reward: 24.651218557333053 setps: 24 count: 487\n",
      "reward: 16.44466511144419 setps: 12 count: 499\n",
      "reward: 14.41406574047578 setps: 10 count: 509\n",
      "reward: 17.58412711729761 setps: 18 count: 527\n",
      "reward: 20.553306586608233 setps: 16 count: 543\n",
      "reward: 15.736016320492489 setps: 11 count: 554\n",
      "reward: 17.819669316033828 setps: 15 count: 569\n",
      "reward: 25.819218799834196 setps: 29 count: 598\n",
      "reward: 14.98679081998998 setps: 12 count: 610\n",
      "reward: 13.74165762893972 setps: 9 count: 619\n",
      "reward: 13.458624790895556 setps: 11 count: 630\n",
      "reward: 15.374212534478284 setps: 13 count: 643\n",
      "reward: 19.89335080527089 setps: 19 count: 662\n",
      "reward: 15.841128521163771 setps: 13 count: 675\n",
      "reward: 18.11115311489557 setps: 16 count: 691\n",
      "reward: 15.432769860027474 setps: 15 count: 706\n",
      "reward: 22.455822799970335 setps: 19 count: 725\n",
      "reward: 18.785966558349898 setps: 19 count: 744\n",
      "reward: 16.5050942114176 setps: 12 count: 756\n",
      "reward: 20.38701897207065 setps: 19 count: 775\n",
      "reward: 13.945260211777349 setps: 11 count: 786\n",
      "reward: 19.516826823129666 setps: 17 count: 803\n",
      "reward: 13.20895221870014 setps: 9 count: 812\n",
      "reward: 19.348339649602714 setps: 20 count: 832\n",
      "reward: 14.918982877806412 setps: 10 count: 842\n",
      "reward: 22.118211751247873 setps: 20 count: 862\n",
      "reward: 15.251755501983283 setps: 11 count: 873\n",
      "reward: 14.787219608628948 setps: 12 count: 885\n",
      "reward: 18.87202530757495 setps: 18 count: 903\n",
      "reward: 14.7494128306731 setps: 11 count: 914\n",
      "reward: 15.395560012491478 setps: 15 count: 929\n",
      "reward: 16.313154007004048 setps: 11 count: 940\n",
      "reward: 17.376869249681473 setps: 14 count: 954\n",
      "reward: 20.31432603583671 setps: 17 count: 971\n",
      "reward: 14.990424769197123 setps: 10 count: 981\n",
      "reward: 19.57561119704624 setps: 18 count: 999\n",
      "avg rewards: 17.31895804427974\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:0.33397\n",
      "Epoch:20 Batch:1 Loss:0.13413\n",
      "Epoch:40 Batch:1 Loss:0.11520\n",
      "Epoch:60 Batch:1 Loss:0.10690\n",
      "Epoch:80 Batch:1 Loss:0.09472\n",
      "Epoch:100 Batch:1 Loss:0.07404\n",
      "Epoch:120 Batch:1 Loss:0.05783\n",
      "Epoch:140 Batch:1 Loss:0.05133\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.123\n",
      "Epoch:10 Batch:10 Loss:0.123\n",
      "Epoch:20 Batch:10 Loss:0.121\n",
      "Epoch:30 Batch:10 Loss:0.122\n",
      "Epoch:40 Batch:10 Loss:0.121\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 18.14788112898823 setps: 66 count: 66\n",
      "reward: 25.812530248185798 setps: 84 count: 150\n",
      "reward: 27.452162486790613 setps: 88 count: 238\n",
      "reward: 32.95006864361203 setps: 96 count: 334\n",
      "reward: 36.62148466068758 setps: 94 count: 428\n",
      "reward: 27.0053531931553 setps: 81 count: 509\n",
      "reward: 20.361457584149328 setps: 73 count: 582\n",
      "avg rewards: 26.907276849366983\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:0.33720\n",
      "Epoch:20 Batch:2 Loss:0.08758\n",
      "Epoch:40 Batch:2 Loss:0.07539\n",
      "Epoch:60 Batch:2 Loss:0.05684\n",
      "Epoch:80 Batch:2 Loss:0.04382\n",
      "Epoch:100 Batch:2 Loss:0.03496\n",
      "Epoch:120 Batch:2 Loss:0.02922\n",
      "Epoch:140 Batch:2 Loss:0.02488\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.104\n",
      "Epoch:10 Batch:10 Loss:0.101\n",
      "Epoch:20 Batch:10 Loss:0.100\n",
      "Epoch:30 Batch:10 Loss:0.102\n",
      "Epoch:40 Batch:10 Loss:0.100\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 17.30682139479177 setps: 63 count: 63\n",
      "reward: 14.097364179912251 setps: 64 count: 127\n",
      "reward: 13.13120802229096 setps: 58 count: 185\n",
      "reward: 65.69080766726984 setps: 85 count: 270\n",
      "reward: 17.240688546441373 setps: 71 count: 341\n",
      "reward: 19.986652581053203 setps: 77 count: 418\n",
      "reward: 19.11810577318975 setps: 72 count: 490\n",
      "reward: 15.189006306574449 setps: 58 count: 548\n",
      "reward: 18.454312854062298 setps: 61 count: 609\n",
      "reward: 11.023064622857778 setps: 59 count: 668\n",
      "reward: 13.96325936270732 setps: 62 count: 730\n",
      "reward: 56.41421333758189 setps: 85 count: 815\n",
      "reward: 21.215930669700903 setps: 82 count: 897\n",
      "reward: 16.439162334751746 setps: 73 count: 970\n",
      "avg rewards: 22.805042689513254\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.33691\n",
      "Epoch:20 Batch:3 Loss:0.06594\n",
      "Epoch:40 Batch:3 Loss:0.05296\n",
      "Epoch:60 Batch:3 Loss:0.03508\n",
      "Epoch:80 Batch:3 Loss:0.02665\n",
      "Epoch:100 Batch:3 Loss:0.02040\n",
      "Epoch:120 Batch:3 Loss:0.02031\n",
      "Epoch:140 Batch:3 Loss:0.01787\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.093\n",
      "Epoch:10 Batch:10 Loss:0.095\n",
      "Epoch:20 Batch:10 Loss:0.091\n",
      "Epoch:30 Batch:10 Loss:0.091\n",
      "Epoch:40 Batch:10 Loss:0.094\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 11.631045260334215 setps: 55 count: 55\n",
      "reward: 49.550275599291474 setps: 132 count: 187\n",
      "reward: 22.315850045670224 setps: 75 count: 262\n",
      "reward: 24.73080618485693 setps: 84 count: 346\n",
      "reward: 18.329645536700273 setps: 62 count: 408\n",
      "reward: 13.69525050830417 setps: 60 count: 468\n",
      "reward: 12.003621543645572 setps: 57 count: 525\n",
      "reward: 24.430167651476218 setps: 81 count: 606\n",
      "reward: 16.410013676846585 setps: 61 count: 667\n",
      "reward: 21.286498566804216 setps: 80 count: 747\n",
      "reward: 11.332878434476152 setps: 54 count: 801\n",
      "reward: 13.807765456722699 setps: 67 count: 868\n",
      "reward: 17.555274667940086 setps: 70 count: 938\n",
      "avg rewards: 19.77531485638991\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.31680\n",
      "Epoch:20 Batch:4 Loss:0.05962\n",
      "Epoch:40 Batch:4 Loss:0.03592\n",
      "Epoch:60 Batch:4 Loss:0.02389\n",
      "Epoch:80 Batch:4 Loss:0.01829\n",
      "Epoch:100 Batch:4 Loss:0.01558\n",
      "Epoch:120 Batch:4 Loss:0.01594\n",
      "Epoch:140 Batch:4 Loss:0.01481\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.080\n",
      "Epoch:10 Batch:10 Loss:0.082\n",
      "Epoch:20 Batch:10 Loss:0.081\n",
      "Epoch:30 Batch:10 Loss:0.079\n",
      "Epoch:40 Batch:10 Loss:0.079\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 15.409695997777451 setps: 73 count: 73\n",
      "reward: 10.834034624017661 setps: 63 count: 136\n",
      "reward: 20.15254753132903 setps: 84 count: 220\n",
      "reward: 10.835861618648048 setps: 73 count: 293\n",
      "reward: 25.466047039411194 setps: 108 count: 401\n",
      "reward: 10.535676527106258 setps: 72 count: 473\n",
      "reward: 12.63017519588611 setps: 69 count: 542\n",
      "reward: 10.89396877685213 setps: 81 count: 623\n",
      "reward: 12.742235595235261 setps: 70 count: 693\n",
      "reward: 14.941510864753223 setps: 83 count: 776\n",
      "reward: 22.66546776563627 setps: 99 count: 875\n",
      "reward: 14.392346886536684 setps: 68 count: 943\n",
      "avg rewards: 15.124964035265776\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.30812\n",
      "Epoch:20 Batch:5 Loss:0.05001\n",
      "Epoch:40 Batch:5 Loss:0.02736\n",
      "Epoch:60 Batch:5 Loss:0.01771\n",
      "Epoch:80 Batch:5 Loss:0.01389\n",
      "Epoch:100 Batch:5 Loss:0.01315\n",
      "Epoch:120 Batch:5 Loss:0.01325\n",
      "Epoch:140 Batch:5 Loss:0.01114\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.080\n",
      "Epoch:10 Batch:10 Loss:0.080\n",
      "Epoch:20 Batch:10 Loss:0.077\n",
      "Epoch:30 Batch:10 Loss:0.077\n",
      "Epoch:40 Batch:10 Loss:0.076\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 16.017661181867872 setps: 62 count: 62\n",
      "reward: 32.02482931914564 setps: 89 count: 151\n",
      "reward: 25.444497206702355 setps: 81 count: 232\n",
      "reward: 36.38576339887511 setps: 86 count: 318\n",
      "reward: 40.52116455919747 setps: 97 count: 415\n",
      "reward: 31.76842387353389 setps: 90 count: 505\n",
      "reward: 23.511722166241096 setps: 77 count: 582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 47.82744790420695 setps: 119 count: 701\n",
      "reward: 43.0046039045905 setps: 103 count: 804\n",
      "reward: 14.219570716722222 setps: 57 count: 861\n",
      "reward: 19.414702803221008 setps: 66 count: 927\n",
      "reward: 17.29239784474195 setps: 65 count: 992\n",
      "avg rewards: 28.95273207325384\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.30472\n",
      "Epoch:20 Batch:6 Loss:0.04028\n",
      "Epoch:40 Batch:6 Loss:0.02204\n",
      "Epoch:60 Batch:6 Loss:0.01530\n",
      "Epoch:80 Batch:6 Loss:0.01338\n",
      "Epoch:100 Batch:6 Loss:0.01132\n",
      "Epoch:120 Batch:6 Loss:0.01017\n",
      "Epoch:140 Batch:6 Loss:0.01058\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.077\n",
      "Epoch:10 Batch:10 Loss:0.077\n",
      "Epoch:20 Batch:10 Loss:0.076\n",
      "Epoch:30 Batch:10 Loss:0.075\n",
      "Epoch:40 Batch:10 Loss:0.077\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 19.670265736090364 setps: 69 count: 69\n",
      "reward: 24.92520453564212 setps: 75 count: 144\n",
      "reward: 17.51094406805496 setps: 68 count: 212\n",
      "reward: 22.094658589625038 setps: 79 count: 291\n",
      "reward: 21.76822148437204 setps: 74 count: 365\n",
      "reward: 34.468014932439836 setps: 90 count: 455\n",
      "reward: 11.587729939559354 setps: 53 count: 508\n",
      "avg rewards: 21.71786275511196\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.29326\n",
      "Epoch:20 Batch:7 Loss:0.03615\n",
      "Epoch:40 Batch:7 Loss:0.01713\n",
      "Epoch:60 Batch:7 Loss:0.01310\n",
      "Epoch:80 Batch:7 Loss:0.01087\n",
      "Epoch:100 Batch:7 Loss:0.01037\n",
      "Epoch:120 Batch:7 Loss:0.01020\n",
      "Epoch:140 Batch:7 Loss:0.00886\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.074\n",
      "Epoch:10 Batch:10 Loss:0.076\n",
      "Epoch:20 Batch:10 Loss:0.074\n",
      "Epoch:30 Batch:10 Loss:0.075\n",
      "Epoch:40 Batch:10 Loss:0.075\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 518.6704135134669 setps: 800 count: 800\n",
      "avg rewards: 518.6704135134669\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.30715\n",
      "Epoch:20 Batch:8 Loss:0.02677\n",
      "Epoch:40 Batch:8 Loss:0.01662\n",
      "Epoch:60 Batch:8 Loss:0.01083\n",
      "Epoch:80 Batch:8 Loss:0.00892\n",
      "Epoch:100 Batch:8 Loss:0.00905\n",
      "Epoch:120 Batch:8 Loss:0.00783\n",
      "Epoch:140 Batch:8 Loss:0.00799\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.090\n",
      "Epoch:10 Batch:10 Loss:0.089\n",
      "Epoch:20 Batch:10 Loss:0.085\n",
      "Epoch:30 Batch:10 Loss:0.085\n",
      "Epoch:40 Batch:10 Loss:0.085\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 22.65781092646066 setps: 63 count: 63\n",
      "reward: 22.697690184296505 setps: 75 count: 138\n",
      "reward: 31.82461510663415 setps: 78 count: 216\n",
      "reward: 22.758335463343236 setps: 66 count: 282\n",
      "reward: 10.733336453961961 setps: 48 count: 330\n",
      "reward: 35.77574646904397 setps: 94 count: 424\n",
      "reward: 68.25920328810248 setps: 133 count: 557\n",
      "reward: 25.870036026973693 setps: 77 count: 634\n",
      "reward: 18.742848499053807 setps: 67 count: 701\n",
      "reward: 26.730805350246374 setps: 69 count: 770\n",
      "reward: 28.073388316224737 setps: 77 count: 847\n",
      "reward: 85.85059271741919 setps: 102 count: 949\n",
      "avg rewards: 33.33120073348007\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.27425\n",
      "Epoch:20 Batch:9 Loss:0.02429\n",
      "Epoch:40 Batch:9 Loss:0.01713\n",
      "Epoch:60 Batch:9 Loss:0.01223\n",
      "Epoch:80 Batch:9 Loss:0.00947\n",
      "Epoch:100 Batch:9 Loss:0.00790\n",
      "Epoch:120 Batch:9 Loss:0.00808\n",
      "Epoch:140 Batch:9 Loss:0.00721\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.077\n",
      "Epoch:10 Batch:10 Loss:0.076\n",
      "Epoch:20 Batch:10 Loss:0.076\n",
      "Epoch:30 Batch:10 Loss:0.076\n",
      "Epoch:40 Batch:10 Loss:0.075\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 28.564429988844502 setps: 85 count: 85\n",
      "reward: 37.604251865786516 setps: 98 count: 183\n",
      "reward: 20.48576581592934 setps: 66 count: 249\n",
      "reward: 10.274401772014972 setps: 48 count: 297\n",
      "reward: 33.982612046974815 setps: 82 count: 379\n",
      "reward: 31.74747871479631 setps: 82 count: 461\n",
      "reward: 25.144564900736437 setps: 78 count: 539\n",
      "avg rewards: 26.829072157868982\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.24662\n",
      "Epoch:20 Batch:10 Loss:0.02200\n",
      "Epoch:40 Batch:10 Loss:0.01301\n",
      "Epoch:60 Batch:10 Loss:0.00910\n",
      "Epoch:80 Batch:10 Loss:0.00952\n",
      "Epoch:100 Batch:10 Loss:0.00808\n",
      "Epoch:120 Batch:10 Loss:0.00794\n",
      "Epoch:140 Batch:10 Loss:0.00741\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.079\n",
      "Epoch:10 Batch:10 Loss:0.079\n",
      "Epoch:20 Batch:10 Loss:0.081\n",
      "Epoch:30 Batch:10 Loss:0.079\n",
      "Epoch:40 Batch:10 Loss:0.079\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 44.50136002686193 setps: 103 count: 103\n",
      "reward: 48.61597618508677 setps: 112 count: 215\n",
      "reward: 42.315541075682276 setps: 98 count: 313\n",
      "reward: 35.80114550108847 setps: 94 count: 407\n",
      "reward: 18.857582827830626 setps: 65 count: 472\n",
      "reward: 45.56467873162473 setps: 101 count: 573\n",
      "avg rewards: 39.27604739136246\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.19862\n",
      "Epoch:20 Batch:11 Loss:0.02133\n",
      "Epoch:40 Batch:11 Loss:0.01151\n",
      "Epoch:60 Batch:11 Loss:0.00851\n",
      "Epoch:80 Batch:11 Loss:0.00847\n",
      "Epoch:100 Batch:11 Loss:0.00728\n",
      "Epoch:120 Batch:11 Loss:0.00643\n",
      "Epoch:140 Batch:11 Loss:0.00644\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.080\n",
      "Epoch:10 Batch:10 Loss:0.082\n",
      "Epoch:20 Batch:10 Loss:0.080\n",
      "Epoch:30 Batch:10 Loss:0.079\n",
      "Epoch:40 Batch:10 Loss:0.080\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 41.762651806003106 setps: 73 count: 73\n",
      "reward: 66.75950527817189 setps: 73 count: 146\n",
      "reward: 68.17082470021413 setps: 81 count: 227\n",
      "avg rewards: 58.89766059479638\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.25865\n",
      "Epoch:20 Batch:12 Loss:0.01929\n",
      "Epoch:40 Batch:12 Loss:0.01007\n",
      "Epoch:60 Batch:12 Loss:0.00756\n",
      "Epoch:80 Batch:12 Loss:0.00768\n",
      "Epoch:100 Batch:12 Loss:0.00617\n",
      "Epoch:120 Batch:12 Loss:0.00597\n",
      "Epoch:140 Batch:12 Loss:0.00619\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.080\n",
      "Epoch:10 Batch:10 Loss:0.080\n",
      "Epoch:20 Batch:10 Loss:0.079\n",
      "Epoch:30 Batch:10 Loss:0.079\n",
      "Epoch:40 Batch:10 Loss:0.079\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 83.483153708218 setps: 92 count: 92\n",
      "reward: 56.77426124528573 setps: 126 count: 218\n",
      "reward: 287.15277110355095 setps: 506 count: 724\n",
      "reward: 97.31850814344072 setps: 121 count: 845\n",
      "reward: 69.59130924933271 setps: 75 count: 920\n",
      "reward: 56.95671073500271 setps: 59 count: 979\n",
      "avg rewards: 108.54611903080514\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.25677\n",
      "Epoch:20 Batch:13 Loss:0.01896\n",
      "Epoch:40 Batch:13 Loss:0.00920\n",
      "Epoch:60 Batch:13 Loss:0.00826\n",
      "Epoch:80 Batch:13 Loss:0.00712\n",
      "Epoch:100 Batch:13 Loss:0.00741\n",
      "Epoch:120 Batch:13 Loss:0.00665\n",
      "Epoch:140 Batch:13 Loss:0.00564\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.075\n",
      "Epoch:10 Batch:10 Loss:0.074\n",
      "Epoch:20 Batch:10 Loss:0.075\n",
      "Epoch:30 Batch:10 Loss:0.074\n",
      "Epoch:40 Batch:10 Loss:0.076\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 52.09939206831913 setps: 93 count: 93\n",
      "reward: 488.82710280084257 setps: 800 count: 893\n",
      "reward: 24.89176142618816 setps: 73 count: 966\n",
      "avg rewards: 188.6060854317833\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.16614\n",
      "Epoch:20 Batch:14 Loss:0.01376\n",
      "Epoch:40 Batch:14 Loss:0.00946\n",
      "Epoch:60 Batch:14 Loss:0.00670\n",
      "Epoch:80 Batch:14 Loss:0.00610\n",
      "Epoch:100 Batch:14 Loss:0.00607\n",
      "Epoch:120 Batch:14 Loss:0.00548\n",
      "Epoch:140 Batch:14 Loss:0.00606\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.087\n",
      "Epoch:10 Batch:10 Loss:0.085\n",
      "Epoch:20 Batch:10 Loss:0.084\n",
      "Epoch:30 Batch:10 Loss:0.084\n",
      "Epoch:40 Batch:10 Loss:0.084\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 363.7531044857066 setps: 520 count: 520\n",
      "avg rewards: 363.7531044857066\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.11053\n",
      "Epoch:20 Batch:15 Loss:0.01610\n",
      "Epoch:40 Batch:15 Loss:0.00961\n",
      "Epoch:60 Batch:15 Loss:0.00726\n",
      "Epoch:80 Batch:15 Loss:0.00643\n",
      "Epoch:100 Batch:15 Loss:0.00559\n",
      "Epoch:120 Batch:15 Loss:0.00548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:140 Batch:15 Loss:0.00447\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.078\n",
      "Epoch:10 Batch:10 Loss:0.079\n",
      "Epoch:20 Batch:10 Loss:0.079\n",
      "Epoch:30 Batch:10 Loss:0.078\n",
      "Epoch:40 Batch:10 Loss:0.080\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 53.86484340174794 setps: 105 count: 105\n",
      "reward: 307.0809620828653 setps: 547 count: 652\n",
      "reward: 31.34005841136387 setps: 85 count: 737\n",
      "reward: 80.77498327542853 setps: 86 count: 823\n",
      "reward: 54.41607934829665 setps: 114 count: 937\n",
      "avg rewards: 105.49538530394045\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.08574\n",
      "Epoch:20 Batch:16 Loss:0.01295\n",
      "Epoch:40 Batch:16 Loss:0.00951\n",
      "Epoch:60 Batch:16 Loss:0.00529\n",
      "Epoch:80 Batch:16 Loss:0.00489\n",
      "Epoch:100 Batch:16 Loss:0.00475\n",
      "Epoch:120 Batch:16 Loss:0.00548\n",
      "Epoch:140 Batch:16 Loss:0.00437\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.073\n",
      "Epoch:10 Batch:10 Loss:0.072\n",
      "Epoch:20 Batch:10 Loss:0.073\n",
      "Epoch:30 Batch:10 Loss:0.074\n",
      "Epoch:40 Batch:10 Loss:0.072\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 37.71459607454746 setps: 72 count: 72\n",
      "reward: 23.2252995013827 setps: 59 count: 131\n",
      "reward: 33.45672565643471 setps: 70 count: 201\n",
      "reward: 24.583651538884446 setps: 64 count: 265\n",
      "reward: 34.34802460506907 setps: 73 count: 338\n",
      "reward: 112.43100660677739 setps: 122 count: 460\n",
      "reward: 22.503252558787068 setps: 60 count: 520\n",
      "avg rewards: 41.18036522026898\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.14422\n",
      "Epoch:20 Batch:17 Loss:0.01224\n",
      "Epoch:40 Batch:17 Loss:0.00734\n",
      "Epoch:60 Batch:17 Loss:0.00619\n",
      "Epoch:80 Batch:17 Loss:0.00710\n",
      "Epoch:100 Batch:17 Loss:0.00511\n",
      "Epoch:120 Batch:17 Loss:0.00557\n",
      "Epoch:140 Batch:17 Loss:0.00445\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.073\n",
      "Epoch:10 Batch:10 Loss:0.072\n",
      "Epoch:20 Batch:10 Loss:0.073\n",
      "Epoch:30 Batch:10 Loss:0.072\n",
      "Epoch:40 Batch:10 Loss:0.071\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 43.15381444234226 setps: 100 count: 100\n",
      "reward: 50.79806773914752 setps: 111 count: 211\n",
      "avg rewards: 46.97594109074489\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.07996\n",
      "Epoch:20 Batch:18 Loss:0.01224\n",
      "Epoch:40 Batch:18 Loss:0.00774\n",
      "Epoch:60 Batch:18 Loss:0.00779\n",
      "Epoch:80 Batch:18 Loss:0.00590\n",
      "Epoch:100 Batch:18 Loss:0.00557\n",
      "Epoch:120 Batch:18 Loss:0.00534\n",
      "Epoch:140 Batch:18 Loss:0.00421\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.072\n",
      "Epoch:10 Batch:10 Loss:0.071\n",
      "Epoch:20 Batch:10 Loss:0.074\n",
      "Epoch:30 Batch:10 Loss:0.074\n",
      "Epoch:40 Batch:10 Loss:0.070\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 475.1624853061312 setps: 800 count: 800\n",
      "avg rewards: 475.1624853061312\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.08942\n",
      "Epoch:20 Batch:19 Loss:0.01269\n",
      "Epoch:40 Batch:19 Loss:0.00747\n",
      "Epoch:60 Batch:19 Loss:0.00492\n",
      "Epoch:80 Batch:19 Loss:0.00529\n",
      "Epoch:100 Batch:19 Loss:0.00528\n",
      "Epoch:120 Batch:19 Loss:0.00427\n",
      "Epoch:140 Batch:19 Loss:0.00440\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.072\n",
      "Epoch:10 Batch:10 Loss:0.070\n",
      "Epoch:20 Batch:10 Loss:0.072\n",
      "Epoch:30 Batch:10 Loss:0.070\n",
      "Epoch:40 Batch:10 Loss:0.070\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 82.62064021699044 setps: 67 count: 67\n",
      "reward: 43.18261750713574 setps: 68 count: 135\n",
      "reward: 107.21192237447539 setps: 93 count: 228\n",
      "reward: 67.69651045164498 setps: 62 count: 290\n",
      "reward: 65.59488584260399 setps: 99 count: 389\n",
      "reward: 105.48496614401812 setps: 92 count: 481\n",
      "reward: 76.47781751333531 setps: 60 count: 541\n",
      "reward: 74.51041397506047 setps: 84 count: 625\n",
      "reward: 60.577446672767124 setps: 53 count: 678\n",
      "reward: 70.7237150510453 setps: 49 count: 727\n",
      "reward: 89.47972911904363 setps: 71 count: 798\n",
      "reward: 78.8420005447697 setps: 59 count: 857\n",
      "reward: 60.29450913733308 setps: 100 count: 957\n",
      "avg rewards: 75.59209035001717\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.05111\n",
      "Epoch:20 Batch:20 Loss:0.01319\n",
      "Epoch:40 Batch:20 Loss:0.00802\n",
      "Epoch:60 Batch:20 Loss:0.00668\n",
      "Epoch:80 Batch:20 Loss:0.00487\n",
      "Epoch:100 Batch:20 Loss:0.00495\n",
      "Epoch:120 Batch:20 Loss:0.00457\n",
      "Epoch:140 Batch:20 Loss:0.00376\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.062\n",
      "Epoch:10 Batch:10 Loss:0.061\n",
      "Epoch:20 Batch:10 Loss:0.061\n",
      "Epoch:30 Batch:10 Loss:0.061\n",
      "Epoch:40 Batch:10 Loss:0.062\n",
      "Done!\n",
      "############# start HopperBulletEnv-v0 training ###################\n",
      "(49,)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 18.64622586793121 setps: 17 count: 17\n",
      "reward: 17.40818309659371 setps: 7 count: 24\n",
      "reward: 29.16112769078754 setps: 19 count: 43\n",
      "reward: 19.706662755335856 setps: 6 count: 49\n",
      "reward: 17.834163890368654 setps: 11 count: 60\n",
      "reward: 15.414542757795425 setps: 5 count: 65\n",
      "reward: 22.750892984151143 setps: 12 count: 77\n",
      "reward: 21.80796264240198 setps: 12 count: 89\n",
      "reward: 19.568133132855294 setps: 6 count: 95\n",
      "reward: 18.128844645718345 setps: 8 count: 103\n",
      "reward: 17.84680500954273 setps: 7 count: 110\n",
      "reward: 19.310564171876468 setps: 10 count: 120\n",
      "reward: 19.86305069273367 setps: 6 count: 126\n",
      "reward: 19.241151433065532 setps: 8 count: 134\n",
      "reward: 20.308980393574164 setps: 15 count: 149\n",
      "reward: 20.214840676484165 setps: 9 count: 158\n",
      "reward: 16.789200878505653 setps: 5 count: 163\n",
      "reward: 21.39787101899128 setps: 7 count: 170\n",
      "reward: 19.60174560581654 setps: 9 count: 179\n",
      "reward: 17.827535326604266 setps: 9 count: 188\n",
      "reward: 24.887277609622107 setps: 11 count: 199\n",
      "reward: 21.393940396537072 setps: 14 count: 213\n",
      "reward: 18.002829718199795 setps: 11 count: 224\n",
      "reward: 20.501069520310555 setps: 10 count: 234\n",
      "reward: 16.844627722518634 setps: 18 count: 252\n",
      "reward: 25.165394061084953 setps: 15 count: 267\n",
      "reward: 21.528627748241703 setps: 9 count: 276\n",
      "reward: 20.395198861430984 setps: 11 count: 287\n",
      "reward: 21.722299501718958 setps: 17 count: 304\n",
      "reward: 22.09144142128935 setps: 21 count: 325\n",
      "reward: 21.126231244151132 setps: 14 count: 339\n",
      "reward: 21.787212841423752 setps: 7 count: 346\n",
      "reward: 20.266550721086972 setps: 11 count: 357\n",
      "reward: 17.30798618016561 setps: 12 count: 369\n",
      "reward: 18.20786339586193 setps: 12 count: 381\n",
      "reward: 31.364673500433856 setps: 27 count: 408\n",
      "reward: 27.293263943678177 setps: 22 count: 430\n",
      "reward: 17.19448577382427 setps: 11 count: 441\n",
      "reward: 21.545916226369446 setps: 11 count: 452\n",
      "reward: 25.312593153957277 setps: 18 count: 470\n",
      "reward: 20.23581227164104 setps: 16 count: 486\n",
      "reward: 24.591501283783874 setps: 24 count: 510\n",
      "reward: 23.234750680532308 setps: 16 count: 526\n",
      "reward: 19.825682194056572 setps: 8 count: 534\n",
      "reward: 17.99982078739849 setps: 12 count: 546\n",
      "reward: 21.059514472233424 setps: 12 count: 558\n",
      "reward: 21.487991855837755 setps: 12 count: 570\n",
      "reward: 21.60700323674537 setps: 13 count: 583\n",
      "reward: 19.794143930661086 setps: 12 count: 595\n",
      "reward: 20.992879474215442 setps: 14 count: 609\n",
      "reward: 18.884208956078506 setps: 12 count: 621\n",
      "reward: 16.720906671618287 setps: 5 count: 626\n",
      "reward: 23.02920197494241 setps: 10 count: 636\n",
      "reward: 22.633363741256467 setps: 10 count: 646\n",
      "reward: 16.348403305921238 setps: 14 count: 660\n",
      "reward: 20.74376247452456 setps: 8 count: 668\n",
      "reward: 18.945885027166515 setps: 7 count: 675\n",
      "reward: 17.206041958720018 setps: 7 count: 682\n",
      "reward: 26.136647380510112 setps: 18 count: 700\n",
      "reward: 17.05585997849121 setps: 6 count: 706\n",
      "reward: 19.712622070475483 setps: 11 count: 717\n",
      "reward: 17.732156160479647 setps: 8 count: 725\n",
      "reward: 15.238716689778084 setps: 7 count: 732\n",
      "reward: 17.194037004568962 setps: 7 count: 739\n",
      "reward: 16.19925266330392 setps: 6 count: 745\n",
      "reward: 29.07830599426961 setps: 20 count: 765\n",
      "reward: 19.41357452069642 setps: 9 count: 774\n",
      "reward: 20.653941987246796 setps: 13 count: 787\n",
      "reward: 25.181756244999995 setps: 13 count: 800\n",
      "reward: 19.679301787279833 setps: 15 count: 815\n",
      "reward: 24.50541133024963 setps: 12 count: 827\n",
      "reward: 17.203685440489792 setps: 5 count: 832\n",
      "reward: 22.56045434886182 setps: 12 count: 844\n",
      "reward: 14.628545155499886 setps: 12 count: 856\n",
      "reward: 24.96474900062022 setps: 16 count: 872\n",
      "reward: 16.888685974825055 setps: 7 count: 879\n",
      "reward: 19.432054585524025 setps: 16 count: 895\n",
      "reward: 21.920430831357955 setps: 15 count: 910\n",
      "reward: 17.444797522101723 setps: 17 count: 927\n",
      "reward: 26.620541610402867 setps: 20 count: 947\n",
      "reward: 26.090298064738462 setps: 20 count: 967\n",
      "reward: 18.495343158606556 setps: 13 count: 980\n",
      "reward: 28.108907147754508 setps: 20 count: 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg rewards: 20.653601435704893\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:0.38996\n",
      "Epoch:20 Batch:1 Loss:0.17718\n",
      "Epoch:40 Batch:1 Loss:0.14343\n",
      "Epoch:60 Batch:1 Loss:0.11031\n",
      "Epoch:80 Batch:1 Loss:0.07865\n",
      "Epoch:100 Batch:1 Loss:0.06314\n",
      "Epoch:120 Batch:1 Loss:0.05615\n",
      "Epoch:140 Batch:1 Loss:0.05115\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.137\n",
      "Epoch:10 Batch:10 Loss:0.125\n",
      "Epoch:20 Batch:10 Loss:0.127\n",
      "Epoch:30 Batch:10 Loss:0.122\n",
      "Epoch:40 Batch:10 Loss:0.123\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 20.2098911152425 setps: 47 count: 47\n",
      "reward: 19.89956382952951 setps: 47 count: 94\n",
      "reward: 20.47113093973639 setps: 51 count: 145\n",
      "reward: 22.134864703890347 setps: 55 count: 200\n",
      "reward: 19.03084999887797 setps: 48 count: 248\n",
      "reward: 24.82347257664805 setps: 42 count: 290\n",
      "reward: 22.317326837713345 setps: 51 count: 341\n",
      "reward: 26.680337324659916 setps: 56 count: 397\n",
      "reward: 19.155206466099468 setps: 43 count: 440\n",
      "reward: 19.765972392664118 setps: 45 count: 485\n",
      "reward: 18.840066794976885 setps: 48 count: 533\n",
      "reward: 17.055544663757615 setps: 41 count: 574\n",
      "reward: 15.613439207529883 setps: 39 count: 613\n",
      "reward: 15.1650387483518 setps: 41 count: 654\n",
      "reward: 21.358556448925803 setps: 40 count: 694\n",
      "reward: 30.221509760813202 setps: 46 count: 740\n",
      "reward: 21.125516349355166 setps: 50 count: 790\n",
      "reward: 20.230030770270965 setps: 50 count: 840\n",
      "reward: 16.078610636058148 setps: 41 count: 881\n",
      "reward: 40.62792164423008 setps: 44 count: 925\n",
      "reward: 13.577472623609356 setps: 39 count: 964\n",
      "avg rewards: 21.161063039663834\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:0.34964\n",
      "Epoch:20 Batch:2 Loss:0.12737\n",
      "Epoch:40 Batch:2 Loss:0.07991\n",
      "Epoch:60 Batch:2 Loss:0.04719\n",
      "Epoch:80 Batch:2 Loss:0.03176\n",
      "Epoch:100 Batch:2 Loss:0.02852\n",
      "Epoch:120 Batch:2 Loss:0.02669\n",
      "Epoch:140 Batch:2 Loss:0.02604\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.078\n",
      "Epoch:10 Batch:10 Loss:0.086\n",
      "Epoch:20 Batch:10 Loss:0.084\n",
      "Epoch:30 Batch:10 Loss:0.079\n",
      "Epoch:40 Batch:10 Loss:0.080\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 25.203638382190547 setps: 39 count: 39\n",
      "reward: 39.119834814409856 setps: 42 count: 81\n",
      "reward: 30.88106913952069 setps: 39 count: 120\n",
      "reward: 28.788325861215704 setps: 37 count: 157\n",
      "reward: 34.63335941724217 setps: 39 count: 196\n",
      "reward: 37.070004813253775 setps: 44 count: 240\n",
      "reward: 37.58036197087785 setps: 41 count: 281\n",
      "reward: 27.485542275772605 setps: 36 count: 317\n",
      "reward: 39.4354871819829 setps: 43 count: 360\n",
      "reward: 30.660161808358676 setps: 36 count: 396\n",
      "reward: 35.897744954637886 setps: 41 count: 437\n",
      "reward: 31.77670046826388 setps: 37 count: 474\n",
      "reward: 29.261201203959352 setps: 36 count: 510\n",
      "reward: 37.70726899340516 setps: 43 count: 553\n",
      "reward: 29.29564574729448 setps: 36 count: 589\n",
      "reward: 32.516921816792454 setps: 37 count: 626\n",
      "reward: 31.387857755410366 setps: 38 count: 664\n",
      "reward: 37.86247462015018 setps: 43 count: 707\n",
      "reward: 32.53355266291765 setps: 38 count: 745\n",
      "reward: 30.857650320480754 setps: 36 count: 781\n",
      "reward: 31.219134826524535 setps: 40 count: 821\n",
      "reward: 30.806313420536753 setps: 38 count: 859\n",
      "reward: 34.24793515945785 setps: 40 count: 899\n",
      "reward: 35.76319505383581 setps: 40 count: 939\n",
      "reward: 32.654935107812356 setps: 37 count: 976\n",
      "avg rewards: 32.98585271105217\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.33882\n",
      "Epoch:20 Batch:3 Loss:0.10065\n",
      "Epoch:40 Batch:3 Loss:0.04631\n",
      "Epoch:60 Batch:3 Loss:0.02912\n",
      "Epoch:80 Batch:3 Loss:0.02372\n",
      "Epoch:100 Batch:3 Loss:0.02369\n",
      "Epoch:120 Batch:3 Loss:0.02130\n",
      "Epoch:140 Batch:3 Loss:0.01881\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.075\n",
      "Epoch:10 Batch:10 Loss:0.070\n",
      "Epoch:20 Batch:10 Loss:0.070\n",
      "Epoch:30 Batch:10 Loss:0.072\n",
      "Epoch:40 Batch:10 Loss:0.071\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 28.914546368973973 setps: 36 count: 36\n",
      "reward: 33.17123602791835 setps: 39 count: 75\n",
      "reward: 30.389682249154433 setps: 38 count: 113\n",
      "reward: 37.13065101482789 setps: 42 count: 155\n",
      "reward: 35.42629633191682 setps: 41 count: 196\n",
      "reward: 40.67032574405166 setps: 45 count: 241\n",
      "reward: 32.30623880694039 setps: 39 count: 280\n",
      "reward: 31.881889043909904 setps: 38 count: 318\n",
      "reward: 35.640770040602355 setps: 42 count: 360\n",
      "reward: 25.50828130396986 setps: 33 count: 393\n",
      "reward: 32.248261314499544 setps: 38 count: 431\n",
      "reward: 35.5577034193513 setps: 41 count: 472\n",
      "reward: 30.712588283348307 setps: 36 count: 508\n",
      "reward: 34.269473063692566 setps: 40 count: 548\n",
      "reward: 32.34112526721728 setps: 39 count: 587\n",
      "reward: 41.19794295894826 setps: 43 count: 630\n",
      "reward: 36.59432143655577 setps: 42 count: 672\n",
      "reward: 33.0628110961261 setps: 39 count: 711\n",
      "reward: 28.19447349980765 setps: 35 count: 746\n",
      "reward: 33.82335805930342 setps: 41 count: 787\n",
      "reward: 30.448222404504488 setps: 37 count: 824\n",
      "reward: 34.06490599850366 setps: 40 count: 864\n",
      "reward: 36.664594803581714 setps: 43 count: 907\n",
      "reward: 35.28213105253381 setps: 40 count: 947\n",
      "reward: 36.14182973700663 setps: 41 count: 988\n",
      "avg rewards: 33.66574637308985\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.32432\n",
      "Epoch:20 Batch:4 Loss:0.06506\n",
      "Epoch:40 Batch:4 Loss:0.03667\n",
      "Epoch:60 Batch:4 Loss:0.02599\n",
      "Epoch:80 Batch:4 Loss:0.02009\n",
      "Epoch:100 Batch:4 Loss:0.01868\n",
      "Epoch:120 Batch:4 Loss:0.01898\n",
      "Epoch:140 Batch:4 Loss:0.01811\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.081\n",
      "Epoch:10 Batch:10 Loss:0.072\n",
      "Epoch:20 Batch:10 Loss:0.061\n",
      "Epoch:30 Batch:10 Loss:0.066\n",
      "Epoch:40 Batch:10 Loss:0.074\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 42.113275957266154 setps: 35 count: 35\n",
      "reward: 41.8351145093111 setps: 32 count: 67\n",
      "reward: 41.476070733807866 setps: 31 count: 98\n",
      "reward: 38.08178689389606 setps: 30 count: 128\n",
      "reward: 38.96656393738085 setps: 29 count: 157\n",
      "reward: 39.753242157456405 setps: 29 count: 186\n",
      "reward: 39.61512565426092 setps: 31 count: 217\n",
      "reward: 38.89487116984092 setps: 30 count: 247\n",
      "reward: 41.90929561757949 setps: 28 count: 275\n",
      "reward: 40.92564411579078 setps: 29 count: 304\n",
      "reward: 39.07617242959675 setps: 32 count: 336\n",
      "reward: 40.414087899957664 setps: 28 count: 364\n",
      "reward: 39.37103906385747 setps: 28 count: 392\n",
      "reward: 41.946319842430235 setps: 30 count: 422\n",
      "reward: 40.277661696987344 setps: 29 count: 451\n",
      "reward: 39.922153195136346 setps: 28 count: 479\n",
      "reward: 40.35538776132598 setps: 29 count: 508\n",
      "reward: 39.2926798163433 setps: 31 count: 539\n",
      "reward: 40.67238570188493 setps: 27 count: 566\n",
      "reward: 38.67622768153378 setps: 29 count: 595\n",
      "reward: 38.689285080401184 setps: 26 count: 621\n",
      "reward: 39.59714607151254 setps: 29 count: 650\n",
      "reward: 38.00233581928915 setps: 27 count: 677\n",
      "reward: 39.95805458292162 setps: 31 count: 708\n",
      "reward: 38.29780922584468 setps: 27 count: 735\n",
      "reward: 40.83510931720521 setps: 28 count: 763\n",
      "reward: 37.750307150755546 setps: 25 count: 788\n",
      "reward: 38.675879700931546 setps: 30 count: 818\n",
      "reward: 38.546653685563065 setps: 28 count: 846\n",
      "reward: 38.36133172724803 setps: 26 count: 872\n",
      "reward: 40.31127478378913 setps: 28 count: 900\n",
      "reward: 38.24636505796662 setps: 28 count: 928\n",
      "reward: 38.36093560353474 setps: 30 count: 958\n",
      "reward: 38.6082102016968 setps: 27 count: 985\n",
      "avg rewards: 39.641641289538356\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.30499\n",
      "Epoch:20 Batch:5 Loss:0.05697\n",
      "Epoch:40 Batch:5 Loss:0.03221\n",
      "Epoch:60 Batch:5 Loss:0.02068\n",
      "Epoch:80 Batch:5 Loss:0.01814\n",
      "Epoch:100 Batch:5 Loss:0.01671\n",
      "Epoch:120 Batch:5 Loss:0.01678\n",
      "Epoch:140 Batch:5 Loss:0.01529\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.064\n",
      "Epoch:10 Batch:10 Loss:0.058\n",
      "Epoch:20 Batch:10 Loss:0.062\n",
      "Epoch:30 Batch:10 Loss:0.059\n",
      "Epoch:40 Batch:10 Loss:0.054\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 43.11434422205057 setps: 54 count: 54\n",
      "reward: 37.3787323946075 setps: 52 count: 106\n",
      "reward: 44.04624384556083 setps: 52 count: 158\n",
      "reward: 39.01083700535382 setps: 55 count: 213\n",
      "reward: 62.009650791442255 setps: 62 count: 275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 48.51313162594087 setps: 59 count: 334\n",
      "reward: 29.121268267526467 setps: 36 count: 370\n",
      "reward: 33.480586604619745 setps: 55 count: 425\n",
      "reward: 53.36125053801953 setps: 64 count: 489\n",
      "reward: 38.20054793818271 setps: 40 count: 529\n",
      "reward: 36.249601800531664 setps: 33 count: 562\n",
      "reward: 24.722505473499773 setps: 97 count: 659\n",
      "reward: 46.724212790772434 setps: 41 count: 700\n",
      "reward: 41.73115459099644 setps: 54 count: 754\n",
      "reward: 51.47878696018853 setps: 55 count: 809\n",
      "reward: 37.97325651948777 setps: 32 count: 841\n",
      "reward: 45.13083799544983 setps: 56 count: 897\n",
      "reward: 64.36427152088001 setps: 64 count: 961\n",
      "avg rewards: 43.145067826950594\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.30435\n",
      "Epoch:20 Batch:6 Loss:0.04499\n",
      "Epoch:40 Batch:6 Loss:0.02468\n",
      "Epoch:60 Batch:6 Loss:0.02108\n",
      "Epoch:80 Batch:6 Loss:0.01821\n",
      "Epoch:100 Batch:6 Loss:0.01554\n",
      "Epoch:120 Batch:6 Loss:0.01431\n",
      "Epoch:140 Batch:6 Loss:0.01699\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.036\n",
      "Epoch:10 Batch:10 Loss:0.034\n",
      "Epoch:20 Batch:10 Loss:0.033\n",
      "Epoch:30 Batch:10 Loss:0.034\n",
      "Epoch:40 Batch:10 Loss:0.031\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 83.08457049057324 setps: 72 count: 72\n",
      "reward: 76.9824997428295 setps: 64 count: 136\n",
      "reward: 86.65674680541417 setps: 89 count: 225\n",
      "reward: 79.571640458623 setps: 68 count: 293\n",
      "reward: 79.0753524671658 setps: 69 count: 362\n",
      "reward: 95.12602089866844 setps: 99 count: 461\n",
      "reward: 102.7795450472302 setps: 97 count: 558\n",
      "reward: 75.84801788209734 setps: 77 count: 635\n",
      "reward: 90.4814926756662 setps: 96 count: 731\n",
      "reward: 92.27048033469472 setps: 98 count: 829\n",
      "reward: 37.511799057897605 setps: 70 count: 899\n",
      "reward: 76.8795738992092 setps: 65 count: 964\n",
      "avg rewards: 81.35564498000579\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.29330\n",
      "Epoch:20 Batch:7 Loss:0.04080\n",
      "Epoch:40 Batch:7 Loss:0.02346\n",
      "Epoch:60 Batch:7 Loss:0.01938\n",
      "Epoch:80 Batch:7 Loss:0.01440\n",
      "Epoch:100 Batch:7 Loss:0.01513\n",
      "Epoch:120 Batch:7 Loss:0.01382\n",
      "Epoch:140 Batch:7 Loss:0.01246\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.030\n",
      "Epoch:10 Batch:10 Loss:0.026\n",
      "Epoch:20 Batch:10 Loss:0.030\n",
      "Epoch:30 Batch:10 Loss:0.033\n",
      "Epoch:40 Batch:10 Loss:0.029\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 43.49954750273464 setps: 47 count: 47\n",
      "reward: 56.67402769311304 setps: 52 count: 99\n",
      "reward: 53.140956466854554 setps: 49 count: 148\n",
      "reward: 58.71802628720906 setps: 55 count: 203\n",
      "reward: 44.642544294343665 setps: 48 count: 251\n",
      "reward: 52.35020631643128 setps: 50 count: 301\n",
      "reward: 46.315585432923406 setps: 48 count: 349\n",
      "reward: 47.79929975338452 setps: 49 count: 398\n",
      "reward: 55.2944792830007 setps: 52 count: 450\n",
      "reward: 52.55752032165183 setps: 49 count: 499\n",
      "reward: 53.05206922750221 setps: 51 count: 550\n",
      "reward: 59.49616096979736 setps: 54 count: 604\n",
      "reward: 55.736102804630356 setps: 51 count: 655\n",
      "reward: 42.49881404867046 setps: 49 count: 704\n",
      "reward: 55.44085606390145 setps: 50 count: 754\n",
      "reward: 55.50587920103862 setps: 52 count: 806\n",
      "reward: 60.9087507860706 setps: 55 count: 861\n",
      "reward: 57.939680035451616 setps: 53 count: 914\n",
      "reward: 52.10308780834019 setps: 49 count: 963\n",
      "avg rewards: 52.8249260156342\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.25998\n",
      "Epoch:20 Batch:8 Loss:0.03752\n",
      "Epoch:40 Batch:8 Loss:0.01908\n",
      "Epoch:60 Batch:8 Loss:0.01728\n",
      "Epoch:80 Batch:8 Loss:0.01450\n",
      "Epoch:100 Batch:8 Loss:0.01370\n",
      "Epoch:120 Batch:8 Loss:0.01247\n",
      "Epoch:140 Batch:8 Loss:0.01228\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.033\n",
      "Epoch:10 Batch:10 Loss:0.029\n",
      "Epoch:20 Batch:10 Loss:0.034\n",
      "Epoch:30 Batch:10 Loss:0.029\n",
      "Epoch:40 Batch:10 Loss:0.030\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 56.2107762012296 setps: 52 count: 52\n",
      "reward: 56.68670582980266 setps: 54 count: 106\n",
      "reward: 46.591619641524424 setps: 46 count: 152\n",
      "reward: 53.31077281678445 setps: 51 count: 203\n",
      "reward: 58.106687030543966 setps: 54 count: 257\n",
      "reward: 63.176919720950536 setps: 59 count: 316\n",
      "reward: 45.99592160998583 setps: 46 count: 362\n",
      "reward: 54.82499039596441 setps: 51 count: 413\n",
      "reward: 45.37322076386773 setps: 46 count: 459\n",
      "reward: 60.26794950929181 setps: 57 count: 516\n",
      "reward: 61.381196374371946 setps: 57 count: 573\n",
      "reward: 51.99788047998736 setps: 49 count: 622\n",
      "reward: 51.11975494815414 setps: 48 count: 670\n",
      "reward: 49.81061689210473 setps: 48 count: 718\n",
      "reward: 61.63681172565993 setps: 57 count: 775\n",
      "reward: 53.99723638065771 setps: 51 count: 826\n",
      "reward: 50.08500267979979 setps: 48 count: 874\n",
      "reward: 46.38868043089605 setps: 47 count: 921\n",
      "reward: 51.82919293375308 setps: 49 count: 970\n",
      "avg rewards: 53.620628229754224\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.22493\n",
      "Epoch:20 Batch:9 Loss:0.03108\n",
      "Epoch:40 Batch:9 Loss:0.01887\n",
      "Epoch:60 Batch:9 Loss:0.01780\n",
      "Epoch:80 Batch:9 Loss:0.01274\n",
      "Epoch:100 Batch:9 Loss:0.01230\n",
      "Epoch:120 Batch:9 Loss:0.01371\n",
      "Epoch:140 Batch:9 Loss:0.01174\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.028\n",
      "Epoch:10 Batch:10 Loss:0.031\n",
      "Epoch:20 Batch:10 Loss:0.028\n",
      "Epoch:30 Batch:10 Loss:0.030\n",
      "Epoch:40 Batch:10 Loss:0.031\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 67.60306700535439 setps: 64 count: 64\n",
      "reward: 80.3332244820471 setps: 76 count: 140\n",
      "reward: 63.75124351412668 setps: 60 count: 200\n",
      "reward: 83.76662186307364 setps: 85 count: 285\n",
      "reward: 64.56674386181692 setps: 63 count: 348\n",
      "reward: 71.52679353613567 setps: 67 count: 415\n",
      "reward: 60.68161332363816 setps: 60 count: 475\n",
      "reward: 59.485547747429514 setps: 56 count: 531\n",
      "reward: 58.51316254926496 setps: 60 count: 591\n",
      "reward: 71.18358796250395 setps: 69 count: 660\n",
      "reward: 68.44973470052648 setps: 65 count: 725\n",
      "reward: 65.15725529039192 setps: 69 count: 794\n",
      "reward: 80.98058264358842 setps: 80 count: 874\n",
      "reward: 57.27899676239323 setps: 59 count: 933\n",
      "avg rewards: 68.09129823159222\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.25129\n",
      "Epoch:20 Batch:10 Loss:0.03059\n",
      "Epoch:40 Batch:10 Loss:0.01827\n",
      "Epoch:60 Batch:10 Loss:0.01646\n",
      "Epoch:80 Batch:10 Loss:0.01182\n",
      "Epoch:100 Batch:10 Loss:0.01208\n",
      "Epoch:120 Batch:10 Loss:0.01102\n",
      "Epoch:140 Batch:10 Loss:0.01146\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.033\n",
      "Epoch:10 Batch:10 Loss:0.029\n",
      "Epoch:20 Batch:10 Loss:0.029\n",
      "Epoch:30 Batch:10 Loss:0.029\n",
      "Epoch:40 Batch:10 Loss:0.028\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 40.66996451982849 setps: 53 count: 53\n",
      "reward: 26.654047831344364 setps: 65 count: 118\n",
      "reward: 55.02962367572038 setps: 57 count: 175\n",
      "reward: 36.99762567229482 setps: 56 count: 231\n",
      "reward: 43.42558976706644 setps: 57 count: 288\n",
      "reward: 58.3840869030042 setps: 59 count: 347\n",
      "reward: 50.53834759772727 setps: 54 count: 401\n",
      "reward: 44.9144160897704 setps: 56 count: 457\n",
      "reward: 40.63697512342333 setps: 56 count: 513\n",
      "reward: 58.347000168485096 setps: 59 count: 572\n",
      "reward: 30.70724594634812 setps: 58 count: 630\n",
      "reward: 48.93217682265387 setps: 49 count: 679\n",
      "reward: 30.876183017996667 setps: 61 count: 740\n",
      "reward: 40.9453737863907 setps: 53 count: 793\n",
      "reward: 80.37230475932809 setps: 98 count: 891\n",
      "reward: 32.85606908912016 setps: 56 count: 947\n",
      "avg rewards: 45.0179394231564\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.18380\n",
      "Epoch:20 Batch:11 Loss:0.02816\n",
      "Epoch:40 Batch:11 Loss:0.01844\n",
      "Epoch:60 Batch:11 Loss:0.01482\n",
      "Epoch:80 Batch:11 Loss:0.01251\n",
      "Epoch:100 Batch:11 Loss:0.01051\n",
      "Epoch:120 Batch:11 Loss:0.01252\n",
      "Epoch:140 Batch:11 Loss:0.01060\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.025\n",
      "Epoch:10 Batch:10 Loss:0.026\n",
      "Epoch:20 Batch:10 Loss:0.025\n",
      "Epoch:30 Batch:10 Loss:0.026\n",
      "Epoch:40 Batch:10 Loss:0.024\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 51.62742055506969 setps: 88 count: 88\n",
      "reward: 138.90831875293117 setps: 156 count: 244\n",
      "avg rewards: 95.26786965400044\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.21669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 Batch:12 Loss:0.02355\n",
      "Epoch:40 Batch:12 Loss:0.01414\n",
      "Epoch:60 Batch:12 Loss:0.01195\n",
      "Epoch:80 Batch:12 Loss:0.01134\n",
      "Epoch:100 Batch:12 Loss:0.01139\n",
      "Epoch:120 Batch:12 Loss:0.01044\n",
      "Epoch:140 Batch:12 Loss:0.01105\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.017\n",
      "Epoch:10 Batch:10 Loss:0.017\n",
      "Epoch:20 Batch:10 Loss:0.020\n",
      "Epoch:30 Batch:10 Loss:0.020\n",
      "Epoch:40 Batch:10 Loss:0.020\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 74.41632388378348 setps: 68 count: 68\n",
      "reward: 65.7808023733989 setps: 59 count: 127\n",
      "reward: 60.006037252879466 setps: 57 count: 184\n",
      "reward: 60.661868068961475 setps: 56 count: 240\n",
      "reward: 85.62778379586962 setps: 77 count: 317\n",
      "reward: 57.49882743620837 setps: 53 count: 370\n",
      "reward: 71.73436368297554 setps: 65 count: 435\n",
      "reward: 74.50011231816025 setps: 64 count: 499\n",
      "reward: 63.366261093658984 setps: 57 count: 556\n",
      "reward: 98.24189367818762 setps: 77 count: 633\n",
      "reward: 71.90415229925418 setps: 72 count: 705\n",
      "reward: 76.65854472393112 setps: 72 count: 777\n",
      "reward: 84.5912702303176 setps: 75 count: 852\n",
      "reward: 126.42322265669505 setps: 96 count: 948\n",
      "avg rewards: 76.52939024959154\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.21233\n",
      "Epoch:20 Batch:13 Loss:0.02269\n",
      "Epoch:40 Batch:13 Loss:0.01337\n",
      "Epoch:60 Batch:13 Loss:0.01173\n",
      "Epoch:80 Batch:13 Loss:0.01007\n",
      "Epoch:100 Batch:13 Loss:0.01050\n",
      "Epoch:120 Batch:13 Loss:0.01131\n",
      "Epoch:140 Batch:13 Loss:0.00908\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.016\n",
      "Epoch:10 Batch:10 Loss:0.014\n",
      "Epoch:20 Batch:10 Loss:0.013\n",
      "Epoch:30 Batch:10 Loss:0.014\n",
      "Epoch:40 Batch:10 Loss:0.013\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 87.31255822644015 setps: 79 count: 79\n",
      "reward: 68.67879096965261 setps: 60 count: 139\n",
      "reward: 65.91446610757812 setps: 57 count: 196\n",
      "reward: 70.6881883786511 setps: 65 count: 261\n",
      "reward: 57.97886861969601 setps: 50 count: 311\n",
      "reward: 71.96285404355729 setps: 61 count: 372\n",
      "reward: 65.33546820133053 setps: 56 count: 428\n",
      "reward: 81.8887986181886 setps: 74 count: 502\n",
      "reward: 62.60672939510951 setps: 54 count: 556\n",
      "reward: 63.49107708698284 setps: 52 count: 608\n",
      "reward: 71.02705893937164 setps: 60 count: 668\n",
      "reward: 68.9569662902548 setps: 58 count: 726\n",
      "reward: 85.93391706217513 setps: 74 count: 800\n",
      "reward: 68.44849233892391 setps: 60 count: 860\n",
      "reward: 62.667725277544726 setps: 60 count: 920\n",
      "reward: 69.22420876134494 setps: 58 count: 978\n",
      "avg rewards: 70.13226051980013\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.15470\n",
      "Epoch:20 Batch:14 Loss:0.02060\n",
      "Epoch:40 Batch:14 Loss:0.01313\n",
      "Epoch:60 Batch:14 Loss:0.01157\n",
      "Epoch:80 Batch:14 Loss:0.00997\n",
      "Epoch:100 Batch:14 Loss:0.01142\n",
      "Epoch:120 Batch:14 Loss:0.00852\n",
      "Epoch:140 Batch:14 Loss:0.00803\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.015\n",
      "Epoch:10 Batch:10 Loss:0.012\n",
      "Epoch:20 Batch:10 Loss:0.014\n",
      "Epoch:30 Batch:10 Loss:0.016\n",
      "Epoch:40 Batch:10 Loss:0.012\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 77.05852953276556 setps: 64 count: 64\n",
      "reward: 118.39866289419876 setps: 120 count: 184\n",
      "reward: 69.80375760423848 setps: 61 count: 245\n",
      "reward: 56.05334713073972 setps: 51 count: 296\n",
      "reward: 79.38426514373351 setps: 68 count: 364\n",
      "reward: 62.09645349062338 setps: 55 count: 419\n",
      "reward: 73.54567396033491 setps: 62 count: 481\n",
      "reward: 62.189267311847644 setps: 54 count: 535\n",
      "reward: 66.51657163356396 setps: 61 count: 596\n",
      "reward: 64.13754824629066 setps: 58 count: 654\n",
      "reward: 66.44574030960939 setps: 60 count: 714\n",
      "reward: 64.72482302417812 setps: 58 count: 772\n",
      "reward: 66.99433096977737 setps: 57 count: 829\n",
      "reward: 73.92766021380741 setps: 61 count: 890\n",
      "reward: 66.78763608883746 setps: 60 count: 950\n",
      "avg rewards: 71.20428450363642\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.11438\n",
      "Epoch:20 Batch:15 Loss:0.01868\n",
      "Epoch:40 Batch:15 Loss:0.01294\n",
      "Epoch:60 Batch:15 Loss:0.01015\n",
      "Epoch:80 Batch:15 Loss:0.01015\n",
      "Epoch:100 Batch:15 Loss:0.00974\n",
      "Epoch:120 Batch:15 Loss:0.00815\n",
      "Epoch:140 Batch:15 Loss:0.00786\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.012\n",
      "Epoch:10 Batch:10 Loss:0.013\n",
      "Epoch:20 Batch:10 Loss:0.013\n",
      "Epoch:30 Batch:10 Loss:0.013\n",
      "Epoch:40 Batch:10 Loss:0.012\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 66.3458721617237 setps: 57 count: 57\n",
      "reward: 74.28089314007522 setps: 62 count: 119\n",
      "reward: 69.46805025701467 setps: 63 count: 182\n",
      "reward: 60.16293397182162 setps: 51 count: 233\n",
      "reward: 60.24905266124696 setps: 53 count: 286\n",
      "reward: 70.54980287775687 setps: 65 count: 351\n",
      "reward: 59.22256433482543 setps: 50 count: 401\n",
      "reward: 53.08511172356828 setps: 49 count: 450\n",
      "reward: 54.184900089822605 setps: 51 count: 501\n",
      "reward: 56.31648960103193 setps: 51 count: 552\n",
      "reward: 65.0609690210578 setps: 54 count: 606\n",
      "reward: 66.35081384032165 setps: 56 count: 662\n",
      "reward: 62.2572745524987 setps: 54 count: 716\n",
      "reward: 53.53095268502073 setps: 49 count: 765\n",
      "reward: 56.73297329272752 setps: 55 count: 820\n",
      "reward: 69.74043822396196 setps: 58 count: 878\n",
      "reward: 71.94451418762618 setps: 60 count: 938\n",
      "reward: 57.688283203104206 setps: 54 count: 992\n",
      "avg rewards: 62.620660545844785\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.12679\n",
      "Epoch:20 Batch:16 Loss:0.02195\n",
      "Epoch:40 Batch:16 Loss:0.01144\n",
      "Epoch:60 Batch:16 Loss:0.01041\n",
      "Epoch:80 Batch:16 Loss:0.00917\n",
      "Epoch:100 Batch:16 Loss:0.00958\n",
      "Epoch:120 Batch:16 Loss:0.00748\n",
      "Epoch:140 Batch:16 Loss:0.00971\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.016\n",
      "Epoch:10 Batch:10 Loss:0.014\n",
      "Epoch:20 Batch:10 Loss:0.014\n",
      "Epoch:30 Batch:10 Loss:0.015\n",
      "Epoch:40 Batch:10 Loss:0.014\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 98.60749516974901 setps: 88 count: 88\n",
      "reward: 69.87359400788374 setps: 58 count: 146\n",
      "reward: 137.2642293163808 setps: 136 count: 282\n",
      "reward: 88.25192107198674 setps: 76 count: 358\n",
      "reward: 74.07749076806358 setps: 65 count: 423\n",
      "reward: 89.92380511411025 setps: 79 count: 502\n",
      "reward: 74.77496745270692 setps: 64 count: 566\n",
      "reward: 122.28254705031287 setps: 114 count: 680\n",
      "reward: 68.06149213765022 setps: 57 count: 737\n",
      "reward: 86.79686530770998 setps: 95 count: 832\n",
      "reward: 122.32347062411574 setps: 110 count: 942\n",
      "avg rewards: 93.83980709278816\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.13990\n",
      "Epoch:20 Batch:17 Loss:0.02260\n",
      "Epoch:40 Batch:17 Loss:0.01240\n",
      "Epoch:60 Batch:17 Loss:0.00848\n",
      "Epoch:80 Batch:17 Loss:0.00808\n",
      "Epoch:100 Batch:17 Loss:0.00864\n",
      "Epoch:120 Batch:17 Loss:0.00828\n",
      "Epoch:140 Batch:17 Loss:0.00928\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.013\n",
      "Epoch:10 Batch:10 Loss:0.012\n",
      "Epoch:20 Batch:10 Loss:0.011\n",
      "Epoch:30 Batch:10 Loss:0.013\n",
      "Epoch:40 Batch:10 Loss:0.012\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 121.9699741293152 setps: 74 count: 74\n",
      "reward: 86.98344297274164 setps: 75 count: 149\n",
      "reward: 62.90152916010411 setps: 56 count: 205\n",
      "reward: 67.0928369029818 setps: 57 count: 262\n",
      "reward: 96.82082795340104 setps: 89 count: 351\n",
      "reward: 74.55362935885935 setps: 67 count: 418\n",
      "reward: 92.41618981817734 setps: 71 count: 489\n",
      "reward: 135.1747411817195 setps: 138 count: 627\n",
      "reward: 89.66435157087545 setps: 76 count: 703\n",
      "reward: 68.9619366234052 setps: 70 count: 773\n",
      "reward: 86.5354770835271 setps: 78 count: 851\n",
      "reward: 111.37290487561552 setps: 80 count: 931\n",
      "reward: 82.84586454836683 setps: 68 count: 999\n",
      "avg rewards: 90.56105432146848\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.08271\n",
      "Epoch:20 Batch:18 Loss:0.01717\n",
      "Epoch:40 Batch:18 Loss:0.00827\n",
      "Epoch:60 Batch:18 Loss:0.00967\n",
      "Epoch:80 Batch:18 Loss:0.00851\n",
      "Epoch:100 Batch:18 Loss:0.00860\n",
      "Epoch:120 Batch:18 Loss:0.00717\n",
      "Epoch:140 Batch:18 Loss:0.00735\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.012\n",
      "Epoch:10 Batch:10 Loss:0.012\n",
      "Epoch:20 Batch:10 Loss:0.011\n",
      "Epoch:30 Batch:10 Loss:0.014\n",
      "Epoch:40 Batch:10 Loss:0.011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 68.32711742146205 setps: 60 count: 60\n",
      "reward: 67.9183911690343 setps: 61 count: 121\n",
      "reward: 60.480521757643146 setps: 55 count: 176\n",
      "reward: 76.7076728325759 setps: 68 count: 244\n",
      "reward: 69.03011599168241 setps: 63 count: 307\n",
      "reward: 66.86766237700213 setps: 63 count: 370\n",
      "reward: 60.377527722285606 setps: 52 count: 422\n",
      "reward: 67.6998742468277 setps: 60 count: 482\n",
      "reward: 61.645826168553334 setps: 56 count: 538\n",
      "reward: 62.35650244027638 setps: 55 count: 593\n",
      "reward: 57.01683143808915 setps: 53 count: 646\n",
      "reward: 75.81317391952909 setps: 66 count: 712\n",
      "reward: 43.37428707381768 setps: 50 count: 762\n",
      "reward: 51.09246386138111 setps: 49 count: 811\n",
      "reward: 72.78533853040136 setps: 62 count: 873\n",
      "reward: 67.13085511298632 setps: 60 count: 933\n",
      "reward: 50.93111394191218 setps: 48 count: 981\n",
      "avg rewards: 63.50325152973294\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.07967\n",
      "Epoch:20 Batch:19 Loss:0.01287\n",
      "Epoch:40 Batch:19 Loss:0.00971\n",
      "Epoch:60 Batch:19 Loss:0.01061\n",
      "Epoch:80 Batch:19 Loss:0.00872\n",
      "Epoch:100 Batch:19 Loss:0.00780\n",
      "Epoch:120 Batch:19 Loss:0.00763\n",
      "Epoch:140 Batch:19 Loss:0.00773\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.014\n",
      "Epoch:10 Batch:10 Loss:0.013\n",
      "Epoch:20 Batch:10 Loss:0.013\n",
      "Epoch:30 Batch:10 Loss:0.014\n",
      "Epoch:40 Batch:10 Loss:0.015\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 106.29466683902284 setps: 79 count: 79\n",
      "reward: 180.5389852126885 setps: 167 count: 246\n",
      "reward: 403.6757054366154 setps: 230 count: 476\n",
      "reward: 104.31302022164894 setps: 136 count: 612\n",
      "reward: 85.42491680718523 setps: 91 count: 703\n",
      "reward: 86.93242177891372 setps: 81 count: 784\n",
      "reward: 137.60761681806474 setps: 104 count: 888\n",
      "avg rewards: 157.82676187344848\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.08196\n",
      "Epoch:20 Batch:20 Loss:0.01330\n",
      "Epoch:40 Batch:20 Loss:0.01165\n",
      "Epoch:60 Batch:20 Loss:0.00902\n",
      "Epoch:80 Batch:20 Loss:0.00849\n",
      "Epoch:100 Batch:20 Loss:0.00730\n",
      "Epoch:120 Batch:20 Loss:0.00599\n",
      "Epoch:140 Batch:20 Loss:0.00683\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.011\n",
      "Epoch:10 Batch:10 Loss:0.011\n",
      "Epoch:20 Batch:10 Loss:0.010\n",
      "Epoch:30 Batch:10 Loss:0.010\n",
      "Epoch:40 Batch:10 Loss:0.011\n",
      "Done!\n",
      "############# start HalfCheetahBulletEnv-v0 training ###################\n",
      "(50, 1000, 26)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -980.7297681761808 setps: 800 count: 800\n",
      "avg rewards: -980.7297681761808\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:0.36913\n",
      "Epoch:20 Batch:1 Loss:0.28979\n",
      "Epoch:40 Batch:1 Loss:0.24146\n",
      "Epoch:60 Batch:1 Loss:0.22555\n",
      "Epoch:80 Batch:1 Loss:0.20282\n",
      "Epoch:100 Batch:1 Loss:0.17083\n",
      "Epoch:120 Batch:1 Loss:0.14239\n",
      "Epoch:140 Batch:1 Loss:0.12557\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.207\n",
      "Epoch:10 Batch:10 Loss:0.203\n",
      "Epoch:20 Batch:10 Loss:0.198\n",
      "Epoch:30 Batch:10 Loss:0.199\n",
      "Epoch:40 Batch:10 Loss:0.196\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1091.407040460379 setps: 800 count: 800\n",
      "avg rewards: -1091.407040460379\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:0.39001\n",
      "Epoch:20 Batch:2 Loss:0.15500\n",
      "Epoch:40 Batch:2 Loss:0.11939\n",
      "Epoch:60 Batch:2 Loss:0.10835\n",
      "Epoch:80 Batch:2 Loss:0.09840\n",
      "Epoch:100 Batch:2 Loss:0.08202\n",
      "Epoch:120 Batch:2 Loss:0.06938\n",
      "Epoch:140 Batch:2 Loss:0.06058\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.141\n",
      "Epoch:10 Batch:10 Loss:0.148\n",
      "Epoch:20 Batch:10 Loss:0.146\n",
      "Epoch:30 Batch:10 Loss:0.148\n",
      "Epoch:40 Batch:10 Loss:0.142\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -993.0541274186418 setps: 800 count: 800\n",
      "avg rewards: -993.0541274186418\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.37233\n",
      "Epoch:20 Batch:3 Loss:0.12760\n",
      "Epoch:40 Batch:3 Loss:0.08311\n",
      "Epoch:60 Batch:3 Loss:0.06756\n",
      "Epoch:80 Batch:3 Loss:0.05467\n",
      "Epoch:100 Batch:3 Loss:0.04823\n",
      "Epoch:120 Batch:3 Loss:0.04227\n",
      "Epoch:140 Batch:3 Loss:0.04106\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.122\n",
      "Epoch:10 Batch:10 Loss:0.121\n",
      "Epoch:20 Batch:10 Loss:0.123\n",
      "Epoch:30 Batch:10 Loss:0.116\n",
      "Epoch:40 Batch:10 Loss:0.116\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1074.577513052334 setps: 800 count: 800\n",
      "avg rewards: -1074.577513052334\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.40557\n",
      "Epoch:20 Batch:4 Loss:0.10816\n",
      "Epoch:40 Batch:4 Loss:0.07721\n",
      "Epoch:60 Batch:4 Loss:0.05286\n",
      "Epoch:80 Batch:4 Loss:0.04018\n",
      "Epoch:100 Batch:4 Loss:0.03554\n",
      "Epoch:120 Batch:4 Loss:0.03136\n",
      "Epoch:140 Batch:4 Loss:0.03162\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.102\n",
      "Epoch:10 Batch:10 Loss:0.101\n",
      "Epoch:20 Batch:10 Loss:0.096\n",
      "Epoch:30 Batch:10 Loss:0.100\n",
      "Epoch:40 Batch:10 Loss:0.096\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1114.9777973111027 setps: 800 count: 800\n",
      "avg rewards: -1114.9777973111027\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.41038\n",
      "Epoch:20 Batch:5 Loss:0.08919\n",
      "Epoch:40 Batch:5 Loss:0.05465\n",
      "Epoch:60 Batch:5 Loss:0.04313\n",
      "Epoch:80 Batch:5 Loss:0.03237\n",
      "Epoch:100 Batch:5 Loss:0.02894\n",
      "Epoch:120 Batch:5 Loss:0.02691\n",
      "Epoch:140 Batch:5 Loss:0.02719\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.087\n",
      "Epoch:10 Batch:10 Loss:0.083\n",
      "Epoch:20 Batch:10 Loss:0.084\n",
      "Epoch:30 Batch:10 Loss:0.082\n",
      "Epoch:40 Batch:10 Loss:0.085\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1052.7127034088298 setps: 800 count: 800\n",
      "avg rewards: -1052.7127034088298\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.38381\n",
      "Epoch:20 Batch:6 Loss:0.08292\n",
      "Epoch:40 Batch:6 Loss:0.04980\n",
      "Epoch:60 Batch:6 Loss:0.03609\n",
      "Epoch:80 Batch:6 Loss:0.02942\n",
      "Epoch:100 Batch:6 Loss:0.02504\n",
      "Epoch:120 Batch:6 Loss:0.02058\n",
      "Epoch:140 Batch:6 Loss:0.02317\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.074\n",
      "Epoch:10 Batch:10 Loss:0.076\n",
      "Epoch:20 Batch:10 Loss:0.076\n",
      "Epoch:30 Batch:10 Loss:0.073\n",
      "Epoch:40 Batch:10 Loss:0.075\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -783.1042044731473 setps: 800 count: 800\n",
      "avg rewards: -783.1042044731473\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.41013\n",
      "Epoch:20 Batch:7 Loss:0.06623\n",
      "Epoch:40 Batch:7 Loss:0.03740\n",
      "Epoch:60 Batch:7 Loss:0.02960\n",
      "Epoch:80 Batch:7 Loss:0.02689\n",
      "Epoch:100 Batch:7 Loss:0.01960\n",
      "Epoch:120 Batch:7 Loss:0.01860\n",
      "Epoch:140 Batch:7 Loss:0.01951\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.077\n",
      "Epoch:10 Batch:10 Loss:0.076\n",
      "Epoch:20 Batch:10 Loss:0.077\n",
      "Epoch:30 Batch:10 Loss:0.072\n",
      "Epoch:40 Batch:10 Loss:0.076\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 431.89490009019676 setps: 800 count: 800\n",
      "avg rewards: 431.89490009019676\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.37062\n",
      "Epoch:20 Batch:8 Loss:0.06122\n",
      "Epoch:40 Batch:8 Loss:0.03483\n",
      "Epoch:60 Batch:8 Loss:0.02725\n",
      "Epoch:80 Batch:8 Loss:0.02137\n",
      "Epoch:100 Batch:8 Loss:0.01935\n",
      "Epoch:120 Batch:8 Loss:0.01815\n",
      "Epoch:140 Batch:8 Loss:0.01737\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.077\n",
      "Epoch:10 Batch:10 Loss:0.074\n",
      "Epoch:20 Batch:10 Loss:0.068\n",
      "Epoch:30 Batch:10 Loss:0.071\n",
      "Epoch:40 Batch:10 Loss:0.070\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -948.9693008938973 setps: 800 count: 800\n",
      "avg rewards: -948.9693008938973\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.32535\n",
      "Epoch:20 Batch:9 Loss:0.05205\n",
      "Epoch:40 Batch:9 Loss:0.02902\n",
      "Epoch:60 Batch:9 Loss:0.02345\n",
      "Epoch:80 Batch:9 Loss:0.01719\n",
      "Epoch:100 Batch:9 Loss:0.01794\n",
      "Epoch:120 Batch:9 Loss:0.01823\n",
      "Epoch:140 Batch:9 Loss:0.01523\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:10 Loss:0.058\n",
      "Epoch:10 Batch:10 Loss:0.055\n",
      "Epoch:20 Batch:10 Loss:0.055\n",
      "Epoch:30 Batch:10 Loss:0.057\n",
      "Epoch:40 Batch:10 Loss:0.053\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1167.1431482157902 setps: 800 count: 800\n",
      "avg rewards: -1167.1431482157902\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.33186\n",
      "Epoch:20 Batch:10 Loss:0.05312\n",
      "Epoch:40 Batch:10 Loss:0.03006\n",
      "Epoch:60 Batch:10 Loss:0.01967\n",
      "Epoch:80 Batch:10 Loss:0.02018\n",
      "Epoch:100 Batch:10 Loss:0.01747\n",
      "Epoch:120 Batch:10 Loss:0.01755\n",
      "Epoch:140 Batch:10 Loss:0.01628\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.046\n",
      "Epoch:10 Batch:10 Loss:0.047\n",
      "Epoch:20 Batch:10 Loss:0.046\n",
      "Epoch:30 Batch:10 Loss:0.048\n",
      "Epoch:40 Batch:10 Loss:0.046\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1186.47088662314 setps: 800 count: 800\n",
      "avg rewards: -1186.47088662314\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.32509\n",
      "Epoch:20 Batch:11 Loss:0.05362\n",
      "Epoch:40 Batch:11 Loss:0.03199\n",
      "Epoch:60 Batch:11 Loss:0.02307\n",
      "Epoch:80 Batch:11 Loss:0.01950\n",
      "Epoch:100 Batch:11 Loss:0.01979\n",
      "Epoch:120 Batch:11 Loss:0.01865\n",
      "Epoch:140 Batch:11 Loss:0.01450\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.046\n",
      "Epoch:10 Batch:10 Loss:0.045\n",
      "Epoch:20 Batch:10 Loss:0.046\n",
      "Epoch:30 Batch:10 Loss:0.046\n",
      "Epoch:40 Batch:10 Loss:0.044\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -987.9101315718785 setps: 800 count: 800\n",
      "avg rewards: -987.9101315718785\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.36745\n",
      "Epoch:20 Batch:12 Loss:0.05029\n",
      "Epoch:40 Batch:12 Loss:0.02361\n",
      "Epoch:60 Batch:12 Loss:0.02162\n",
      "Epoch:80 Batch:12 Loss:0.01757\n",
      "Epoch:100 Batch:12 Loss:0.01416\n",
      "Epoch:120 Batch:12 Loss:0.01641\n",
      "Epoch:140 Batch:12 Loss:0.01367\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.047\n",
      "Epoch:10 Batch:10 Loss:0.045\n",
      "Epoch:20 Batch:10 Loss:0.048\n",
      "Epoch:30 Batch:10 Loss:0.042\n",
      "Epoch:40 Batch:10 Loss:0.046\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -752.2726520921474 setps: 800 count: 800\n",
      "avg rewards: -752.2726520921474\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.31063\n",
      "Epoch:20 Batch:13 Loss:0.04330\n",
      "Epoch:40 Batch:13 Loss:0.02464\n",
      "Epoch:60 Batch:13 Loss:0.02083\n",
      "Epoch:80 Batch:13 Loss:0.01595\n",
      "Epoch:100 Batch:13 Loss:0.01717\n",
      "Epoch:120 Batch:13 Loss:0.01566\n",
      "Epoch:140 Batch:13 Loss:0.01689\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.041\n",
      "Epoch:10 Batch:10 Loss:0.041\n",
      "Epoch:20 Batch:10 Loss:0.040\n",
      "Epoch:30 Batch:10 Loss:0.040\n",
      "Epoch:40 Batch:10 Loss:0.040\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1164.132539032525 setps: 800 count: 800\n",
      "avg rewards: -1164.132539032525\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.32483\n",
      "Epoch:20 Batch:14 Loss:0.03604\n",
      "Epoch:40 Batch:14 Loss:0.02352\n",
      "Epoch:60 Batch:14 Loss:0.01707\n",
      "Epoch:80 Batch:14 Loss:0.01475\n",
      "Epoch:100 Batch:14 Loss:0.01721\n",
      "Epoch:120 Batch:14 Loss:0.01426\n",
      "Epoch:140 Batch:14 Loss:0.01251\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.038\n",
      "Epoch:10 Batch:10 Loss:0.036\n",
      "Epoch:20 Batch:10 Loss:0.038\n",
      "Epoch:30 Batch:10 Loss:0.038\n",
      "Epoch:40 Batch:10 Loss:0.036\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -782.777056384864 setps: 800 count: 800\n",
      "avg rewards: -782.777056384864\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.30930\n",
      "Epoch:20 Batch:15 Loss:0.03415\n",
      "Epoch:40 Batch:15 Loss:0.02150\n",
      "Epoch:60 Batch:15 Loss:0.01518\n",
      "Epoch:80 Batch:15 Loss:0.01534\n",
      "Epoch:100 Batch:15 Loss:0.01698\n",
      "Epoch:120 Batch:15 Loss:0.01449\n",
      "Epoch:140 Batch:15 Loss:0.01482\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.039\n",
      "Epoch:10 Batch:10 Loss:0.036\n",
      "Epoch:20 Batch:10 Loss:0.037\n",
      "Epoch:30 Batch:10 Loss:0.035\n",
      "Epoch:40 Batch:10 Loss:0.037\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1001.6068264395581 setps: 800 count: 800\n",
      "avg rewards: -1001.6068264395581\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.23566\n",
      "Epoch:20 Batch:16 Loss:0.03587\n",
      "Epoch:40 Batch:16 Loss:0.02171\n",
      "Epoch:60 Batch:16 Loss:0.01709\n",
      "Epoch:80 Batch:16 Loss:0.01795\n",
      "Epoch:100 Batch:16 Loss:0.01440\n",
      "Epoch:120 Batch:16 Loss:0.01383\n",
      "Epoch:140 Batch:16 Loss:0.01706\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.039\n",
      "Epoch:10 Batch:10 Loss:0.035\n",
      "Epoch:20 Batch:10 Loss:0.035\n",
      "Epoch:30 Batch:10 Loss:0.036\n",
      "Epoch:40 Batch:10 Loss:0.035\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1019.4432435806847 setps: 800 count: 800\n",
      "avg rewards: -1019.4432435806847\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.30737\n",
      "Epoch:20 Batch:17 Loss:0.03498\n",
      "Epoch:40 Batch:17 Loss:0.02143\n",
      "Epoch:60 Batch:17 Loss:0.01739\n",
      "Epoch:80 Batch:17 Loss:0.01640\n",
      "Epoch:100 Batch:17 Loss:0.01531\n",
      "Epoch:120 Batch:17 Loss:0.01228\n",
      "Epoch:140 Batch:17 Loss:0.01122\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.038\n",
      "Epoch:10 Batch:10 Loss:0.038\n",
      "Epoch:20 Batch:10 Loss:0.038\n",
      "Epoch:30 Batch:10 Loss:0.037\n",
      "Epoch:40 Batch:10 Loss:0.037\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1014.3633110526603 setps: 800 count: 800\n",
      "avg rewards: -1014.3633110526603\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.23303\n",
      "Epoch:20 Batch:18 Loss:0.02582\n",
      "Epoch:40 Batch:18 Loss:0.02242\n",
      "Epoch:60 Batch:18 Loss:0.01537\n",
      "Epoch:80 Batch:18 Loss:0.01540\n",
      "Epoch:100 Batch:18 Loss:0.01159\n",
      "Epoch:120 Batch:18 Loss:0.01165\n",
      "Epoch:140 Batch:18 Loss:0.01241\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.037\n",
      "Epoch:10 Batch:10 Loss:0.036\n",
      "Epoch:20 Batch:10 Loss:0.038\n",
      "Epoch:30 Batch:10 Loss:0.035\n",
      "Epoch:40 Batch:10 Loss:0.036\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1040.3454051479507 setps: 800 count: 800\n",
      "avg rewards: -1040.3454051479507\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.18388\n",
      "Epoch:20 Batch:19 Loss:0.03028\n",
      "Epoch:40 Batch:19 Loss:0.01825\n",
      "Epoch:60 Batch:19 Loss:0.01673\n",
      "Epoch:80 Batch:19 Loss:0.01723\n",
      "Epoch:100 Batch:19 Loss:0.01270\n",
      "Epoch:120 Batch:19 Loss:0.01289\n",
      "Epoch:140 Batch:19 Loss:0.01093\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.036\n",
      "Epoch:10 Batch:10 Loss:0.037\n",
      "Epoch:20 Batch:10 Loss:0.034\n",
      "Epoch:30 Batch:10 Loss:0.035\n",
      "Epoch:40 Batch:10 Loss:0.034\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1108.9364622802286 setps: 800 count: 800\n",
      "avg rewards: -1108.9364622802286\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.17143\n",
      "Epoch:20 Batch:20 Loss:0.03079\n",
      "Epoch:40 Batch:20 Loss:0.02024\n",
      "Epoch:60 Batch:20 Loss:0.01794\n",
      "Epoch:80 Batch:20 Loss:0.01476\n",
      "Epoch:100 Batch:20 Loss:0.01270\n",
      "Epoch:120 Batch:20 Loss:0.01247\n",
      "Epoch:140 Batch:20 Loss:0.01129\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.037\n",
      "Epoch:10 Batch:10 Loss:0.036\n",
      "Epoch:20 Batch:10 Loss:0.037\n",
      "Epoch:30 Batch:10 Loss:0.037\n",
      "Epoch:40 Batch:10 Loss:0.034\n",
      "Done!\n",
      "############# start AntBulletEnv-v0 training ###################\n",
      "(50, 1000, 28)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 398.48683142906646 setps: 800 count: 800\n",
      "avg rewards: 398.48683142906646\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:0.41885\n",
      "Epoch:20 Batch:1 Loss:0.08404\n",
      "Epoch:40 Batch:1 Loss:0.05293\n",
      "Epoch:60 Batch:1 Loss:0.04963\n",
      "Epoch:80 Batch:1 Loss:0.04666\n",
      "Epoch:100 Batch:1 Loss:0.04210\n",
      "Epoch:120 Batch:1 Loss:0.03423\n",
      "Epoch:140 Batch:1 Loss:0.02657\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.325\n",
      "Epoch:10 Batch:10 Loss:0.324\n",
      "Epoch:20 Batch:10 Loss:0.322\n",
      "Epoch:30 Batch:10 Loss:0.323\n",
      "Epoch:40 Batch:10 Loss:0.317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 6.380802997943829 setps: 21 count: 21\n",
      "reward: 109.6775662861764 setps: 226 count: 247\n",
      "reward: 25.018028427711403 setps: 56 count: 303\n",
      "reward: 98.01904028579743 setps: 207 count: 510\n",
      "reward: 7.806630033312828 setps: 23 count: 533\n",
      "reward: 16.53936746416584 setps: 35 count: 568\n",
      "reward: 22.032213032603607 setps: 56 count: 624\n",
      "reward: 115.0315630459692 setps: 240 count: 864\n",
      "reward: 7.280475045724595 setps: 21 count: 885\n",
      "reward: 14.970958377364148 setps: 40 count: 925\n",
      "avg rewards: 42.27566449967692\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:0.49840\n",
      "Epoch:20 Batch:2 Loss:0.08765\n",
      "Epoch:40 Batch:2 Loss:0.04034\n",
      "Epoch:60 Batch:2 Loss:0.03344\n",
      "Epoch:80 Batch:2 Loss:0.02058\n",
      "Epoch:100 Batch:2 Loss:0.01332\n",
      "Epoch:120 Batch:2 Loss:0.01178\n",
      "Epoch:140 Batch:2 Loss:0.01107\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.300\n",
      "Epoch:10 Batch:10 Loss:0.290\n",
      "Epoch:20 Batch:10 Loss:0.291\n",
      "Epoch:30 Batch:10 Loss:0.287\n",
      "Epoch:40 Batch:10 Loss:0.288\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 364.03392045768527 setps: 800 count: 800\n",
      "reward: 60.96767763917013 setps: 94 count: 894\n",
      "avg rewards: 212.5007990484277\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.50510\n",
      "Epoch:20 Batch:3 Loss:0.05088\n",
      "Epoch:40 Batch:3 Loss:0.02783\n",
      "Epoch:60 Batch:3 Loss:0.01658\n",
      "Epoch:80 Batch:3 Loss:0.01154\n",
      "Epoch:100 Batch:3 Loss:0.00872\n",
      "Epoch:120 Batch:3 Loss:0.00923\n",
      "Epoch:140 Batch:3 Loss:0.00861\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.295\n",
      "Epoch:10 Batch:10 Loss:0.288\n",
      "Epoch:20 Batch:10 Loss:0.284\n",
      "Epoch:30 Batch:10 Loss:0.286\n",
      "Epoch:40 Batch:10 Loss:0.284\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 20.57255893662368 setps: 31 count: 31\n",
      "reward: 14.011174092248254 setps: 24 count: 55\n",
      "reward: 51.572003820847016 setps: 69 count: 124\n",
      "reward: 13.258824450832618 setps: 24 count: 148\n",
      "reward: 19.728365339234006 setps: 29 count: 177\n",
      "reward: 9.934185534634162 setps: 21 count: 198\n",
      "reward: 11.730663075082703 setps: 22 count: 220\n",
      "reward: 25.32941468408099 setps: 37 count: 257\n",
      "reward: 11.810884929065653 setps: 22 count: 279\n",
      "reward: 86.00244485813892 setps: 157 count: 436\n",
      "reward: 10.438434265118847 setps: 21 count: 457\n",
      "reward: 11.094290239813564 setps: 23 count: 480\n",
      "reward: 9.649106499296613 setps: 21 count: 501\n",
      "reward: 13.92333085034188 setps: 24 count: 525\n",
      "reward: 210.29077081417498 setps: 288 count: 813\n",
      "reward: 9.996000690264918 setps: 21 count: 834\n",
      "reward: 105.10694191857912 setps: 152 count: 986\n",
      "avg rewards: 37.32055264696341\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.50124\n",
      "Epoch:20 Batch:4 Loss:0.05650\n",
      "Epoch:40 Batch:4 Loss:0.01878\n",
      "Epoch:60 Batch:4 Loss:0.01331\n",
      "Epoch:80 Batch:4 Loss:0.01093\n",
      "Epoch:100 Batch:4 Loss:0.00937\n",
      "Epoch:120 Batch:4 Loss:0.00778\n",
      "Epoch:140 Batch:4 Loss:0.00764\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.305\n",
      "Epoch:10 Batch:10 Loss:0.297\n",
      "Epoch:20 Batch:10 Loss:0.292\n",
      "Epoch:30 Batch:10 Loss:0.288\n",
      "Epoch:40 Batch:10 Loss:0.287\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 7.310426850630028 setps: 20 count: 20\n",
      "reward: 6.81236145850562 setps: 20 count: 40\n",
      "reward: 7.418197696008429 setps: 20 count: 60\n",
      "reward: 7.621643011628477 setps: 20 count: 80\n",
      "reward: 7.407436706275621 setps: 20 count: 100\n",
      "reward: 7.618056658188287 setps: 20 count: 120\n",
      "reward: 6.423682752349122 setps: 20 count: 140\n",
      "reward: 7.735813639282425 setps: 20 count: 160\n",
      "reward: 7.3894028484253775 setps: 20 count: 180\n",
      "reward: 7.8037307121296156 setps: 20 count: 200\n",
      "reward: 6.857766341298699 setps: 20 count: 220\n",
      "reward: 6.784333000556215 setps: 20 count: 240\n",
      "reward: 7.515213857729394 setps: 20 count: 260\n",
      "reward: 7.488058913557326 setps: 20 count: 280\n",
      "reward: 7.667532203423616 setps: 20 count: 300\n",
      "reward: 7.5335707264952365 setps: 20 count: 320\n",
      "reward: 7.625234800846374 setps: 20 count: 340\n",
      "reward: 7.285889174712065 setps: 20 count: 360\n",
      "reward: 7.641342117069872 setps: 20 count: 380\n",
      "reward: 7.573448569056927 setps: 20 count: 400\n",
      "reward: 7.639946870280255 setps: 20 count: 420\n",
      "reward: 7.871294883146765 setps: 20 count: 440\n",
      "reward: 7.683191428643478 setps: 20 count: 460\n",
      "reward: 7.741898360483173 setps: 20 count: 480\n",
      "reward: 7.3359147717637825 setps: 20 count: 500\n",
      "reward: 6.88786165142228 setps: 20 count: 520\n",
      "reward: 6.6306305906633485 setps: 20 count: 540\n",
      "reward: 7.472553715099638 setps: 20 count: 560\n",
      "reward: 7.980285127814566 setps: 20 count: 580\n",
      "reward: 7.487601937017462 setps: 20 count: 600\n",
      "reward: 7.136606696489615 setps: 20 count: 620\n",
      "reward: 7.029263105351128 setps: 20 count: 640\n",
      "reward: 6.988664583339413 setps: 20 count: 660\n",
      "reward: 6.528501739796775 setps: 20 count: 680\n",
      "reward: 7.715759251640702 setps: 20 count: 700\n",
      "reward: 7.591531867734738 setps: 20 count: 720\n",
      "reward: 7.466988838723043 setps: 20 count: 740\n",
      "reward: 7.308301509288142 setps: 20 count: 760\n",
      "reward: 6.9890545844056735 setps: 20 count: 780\n",
      "reward: 7.367424458832829 setps: 20 count: 800\n",
      "reward: 7.555950161702638 setps: 20 count: 820\n",
      "reward: 7.897058369147999 setps: 20 count: 840\n",
      "reward: 7.525037852754757 setps: 20 count: 860\n",
      "reward: 7.455104697470959 setps: 20 count: 880\n",
      "reward: 7.55081865608954 setps: 20 count: 900\n",
      "reward: 7.026526418711001 setps: 20 count: 920\n",
      "reward: 7.525027456197131 setps: 20 count: 940\n",
      "reward: 7.705571930164297 setps: 20 count: 960\n",
      "reward: 6.985998146123891 setps: 20 count: 980\n",
      "reward: 7.634988139411144 setps: 20 count: 1000\n",
      "avg rewards: 7.384569996757578\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.47406\n",
      "Epoch:20 Batch:5 Loss:0.03638\n",
      "Epoch:40 Batch:5 Loss:0.01817\n",
      "Epoch:60 Batch:5 Loss:0.01445\n",
      "Epoch:80 Batch:5 Loss:0.01090\n",
      "Epoch:100 Batch:5 Loss:0.00879\n",
      "Epoch:120 Batch:5 Loss:0.00846\n",
      "Epoch:140 Batch:5 Loss:0.00766\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.297\n",
      "Epoch:10 Batch:10 Loss:0.284\n",
      "Epoch:20 Batch:10 Loss:0.284\n",
      "Epoch:30 Batch:10 Loss:0.279\n",
      "Epoch:40 Batch:10 Loss:0.283\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 7.719902636998449 setps: 20 count: 20\n",
      "reward: 6.30178291166958 setps: 20 count: 40\n",
      "reward: 7.480270767809996 setps: 20 count: 60\n",
      "reward: 7.342301461687019 setps: 20 count: 80\n",
      "reward: 6.003543546551374 setps: 20 count: 100\n",
      "reward: 6.422776283338314 setps: 20 count: 120\n",
      "reward: 7.465827174475996 setps: 20 count: 140\n",
      "reward: 27.644825343495178 setps: 61 count: 201\n",
      "reward: 16.456171908009857 setps: 44 count: 245\n",
      "reward: 7.772133266400488 setps: 20 count: 265\n",
      "reward: 7.0707138137120635 setps: 20 count: 285\n",
      "reward: 7.269151821160632 setps: 20 count: 305\n",
      "reward: 7.059665550751378 setps: 20 count: 325\n",
      "reward: 7.545901330822378 setps: 20 count: 345\n",
      "reward: 6.827220915192447 setps: 20 count: 365\n",
      "reward: 6.635186244845681 setps: 20 count: 385\n",
      "reward: 6.7300215858253045 setps: 20 count: 405\n",
      "reward: 7.113791554646742 setps: 20 count: 425\n",
      "reward: 6.9840698542233435 setps: 20 count: 445\n",
      "reward: 6.107071117371379 setps: 20 count: 465\n",
      "reward: 6.990787513369288 setps: 20 count: 485\n",
      "reward: 6.300078146884333 setps: 20 count: 505\n",
      "reward: 6.185508175942232 setps: 20 count: 525\n",
      "reward: 7.528054412038182 setps: 20 count: 545\n",
      "reward: 7.347310523204213 setps: 20 count: 565\n",
      "reward: 7.108902877262152 setps: 20 count: 585\n",
      "reward: 7.75689136267174 setps: 20 count: 605\n",
      "reward: 6.868193792944657 setps: 20 count: 625\n",
      "reward: 6.621581442438763 setps: 20 count: 645\n",
      "reward: 6.723878163287009 setps: 20 count: 665\n",
      "reward: 7.055036190047395 setps: 20 count: 685\n",
      "reward: 7.163335389734128 setps: 20 count: 705\n",
      "reward: 7.138379585620715 setps: 20 count: 725\n",
      "reward: 7.234025043815198 setps: 20 count: 745\n",
      "reward: 7.352777187064929 setps: 20 count: 765\n",
      "reward: 7.081284980267809 setps: 20 count: 785\n",
      "reward: 7.503520584107901 setps: 20 count: 805\n",
      "reward: 7.532828486931978 setps: 20 count: 825\n",
      "reward: 6.48719652245927 setps: 20 count: 845\n",
      "reward: 6.3432538805282075 setps: 20 count: 865\n",
      "reward: 6.561465588821736 setps: 20 count: 885\n",
      "reward: 7.354381727750296 setps: 20 count: 905\n",
      "reward: 6.0576129693276 setps: 20 count: 925\n",
      "reward: 7.284843433459174 setps: 20 count: 945\n",
      "reward: 7.390412959904642 setps: 20 count: 965\n",
      "reward: 7.527621102528065 setps: 20 count: 985\n",
      "avg rewards: 7.661988937639114\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:6 Loss:0.47643\n",
      "Epoch:20 Batch:6 Loss:0.02843\n",
      "Epoch:40 Batch:6 Loss:0.01823\n",
      "Epoch:60 Batch:6 Loss:0.01241\n",
      "Epoch:80 Batch:6 Loss:0.01033\n",
      "Epoch:100 Batch:6 Loss:0.00829\n",
      "Epoch:120 Batch:6 Loss:0.00798\n",
      "Epoch:140 Batch:6 Loss:0.00731\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.294\n",
      "Epoch:10 Batch:10 Loss:0.288\n",
      "Epoch:20 Batch:10 Loss:0.280\n",
      "Epoch:30 Batch:10 Loss:0.275\n",
      "Epoch:40 Batch:10 Loss:0.278\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 287.77431837186475 setps: 800 count: 800\n",
      "avg rewards: 287.77431837186475\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.51140\n",
      "Epoch:20 Batch:7 Loss:0.02748\n",
      "Epoch:40 Batch:7 Loss:0.01753\n",
      "Epoch:60 Batch:7 Loss:0.01223\n",
      "Epoch:80 Batch:7 Loss:0.00953\n",
      "Epoch:100 Batch:7 Loss:0.00814\n",
      "Epoch:120 Batch:7 Loss:0.00794\n",
      "Epoch:140 Batch:7 Loss:0.00716\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.241\n",
      "Epoch:10 Batch:10 Loss:0.239\n",
      "Epoch:20 Batch:10 Loss:0.238\n",
      "Epoch:30 Batch:10 Loss:0.237\n",
      "Epoch:40 Batch:10 Loss:0.235\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 388.4708373841834 setps: 800 count: 800\n",
      "avg rewards: 388.4708373841834\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.50980\n",
      "Epoch:20 Batch:8 Loss:0.02770\n",
      "Epoch:40 Batch:8 Loss:0.01487\n",
      "Epoch:60 Batch:8 Loss:0.01253\n",
      "Epoch:80 Batch:8 Loss:0.00949\n",
      "Epoch:100 Batch:8 Loss:0.00866\n",
      "Epoch:120 Batch:8 Loss:0.00779\n",
      "Epoch:140 Batch:8 Loss:0.00795\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.191\n",
      "Epoch:10 Batch:10 Loss:0.188\n",
      "Epoch:20 Batch:10 Loss:0.184\n",
      "Epoch:30 Batch:10 Loss:0.182\n",
      "Epoch:40 Batch:10 Loss:0.181\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 378.71794250439723 setps: 800 count: 800\n",
      "avg rewards: 378.71794250439723\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.52319\n",
      "Epoch:20 Batch:9 Loss:0.02398\n",
      "Epoch:40 Batch:9 Loss:0.01381\n",
      "Epoch:60 Batch:9 Loss:0.01055\n",
      "Epoch:80 Batch:9 Loss:0.00837\n",
      "Epoch:100 Batch:9 Loss:0.00784\n",
      "Epoch:120 Batch:9 Loss:0.00757\n",
      "Epoch:140 Batch:9 Loss:0.00672\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.202\n",
      "Epoch:10 Batch:10 Loss:0.190\n",
      "Epoch:20 Batch:10 Loss:0.189\n",
      "Epoch:30 Batch:10 Loss:0.188\n",
      "Epoch:40 Batch:10 Loss:0.189\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 382.34350731677716 setps: 800 count: 800\n",
      "avg rewards: 382.34350731677716\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.45762\n",
      "Epoch:20 Batch:10 Loss:0.02115\n",
      "Epoch:40 Batch:10 Loss:0.01326\n",
      "Epoch:60 Batch:10 Loss:0.00966\n",
      "Epoch:80 Batch:10 Loss:0.00772\n",
      "Epoch:100 Batch:10 Loss:0.00703\n",
      "Epoch:120 Batch:10 Loss:0.00667\n",
      "Epoch:140 Batch:10 Loss:0.00658\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.171\n",
      "Epoch:10 Batch:10 Loss:0.170\n",
      "Epoch:20 Batch:10 Loss:0.167\n",
      "Epoch:30 Batch:10 Loss:0.166\n",
      "Epoch:40 Batch:10 Loss:0.164\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 385.62222611718187 setps: 800 count: 800\n",
      "avg rewards: 385.62222611718187\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.42008\n",
      "Epoch:20 Batch:11 Loss:0.01997\n",
      "Epoch:40 Batch:11 Loss:0.01095\n",
      "Epoch:60 Batch:11 Loss:0.00768\n",
      "Epoch:80 Batch:11 Loss:0.00714\n",
      "Epoch:100 Batch:11 Loss:0.00631\n",
      "Epoch:120 Batch:11 Loss:0.00697\n",
      "Epoch:140 Batch:11 Loss:0.00546\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.163\n",
      "Epoch:10 Batch:10 Loss:0.163\n",
      "Epoch:20 Batch:10 Loss:0.160\n",
      "Epoch:30 Batch:10 Loss:0.159\n",
      "Epoch:40 Batch:10 Loss:0.160\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 380.6802577539013 setps: 800 count: 800\n",
      "avg rewards: 380.6802577539013\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.44391\n",
      "Epoch:20 Batch:12 Loss:0.01704\n",
      "Epoch:40 Batch:12 Loss:0.01043\n",
      "Epoch:60 Batch:12 Loss:0.00712\n",
      "Epoch:80 Batch:12 Loss:0.00712\n",
      "Epoch:100 Batch:12 Loss:0.00677\n",
      "Epoch:120 Batch:12 Loss:0.00592\n",
      "Epoch:140 Batch:12 Loss:0.00553\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.177\n",
      "Epoch:10 Batch:10 Loss:0.172\n",
      "Epoch:20 Batch:10 Loss:0.173\n",
      "Epoch:30 Batch:10 Loss:0.173\n",
      "Epoch:40 Batch:10 Loss:0.172\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 375.36603466360293 setps: 800 count: 800\n",
      "avg rewards: 375.36603466360293\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.38490\n",
      "Epoch:20 Batch:13 Loss:0.01446\n",
      "Epoch:40 Batch:13 Loss:0.01056\n",
      "Epoch:60 Batch:13 Loss:0.00698\n",
      "Epoch:80 Batch:13 Loss:0.00626\n",
      "Epoch:100 Batch:13 Loss:0.00615\n",
      "Epoch:120 Batch:13 Loss:0.00487\n",
      "Epoch:140 Batch:13 Loss:0.00536\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.172\n",
      "Epoch:10 Batch:10 Loss:0.168\n",
      "Epoch:20 Batch:10 Loss:0.168\n",
      "Epoch:30 Batch:10 Loss:0.164\n",
      "Epoch:40 Batch:10 Loss:0.165\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 485.8257484562971 setps: 800 count: 800\n",
      "avg rewards: 485.8257484562971\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.35864\n",
      "Epoch:20 Batch:14 Loss:0.01463\n",
      "Epoch:40 Batch:14 Loss:0.00895\n",
      "Epoch:60 Batch:14 Loss:0.00685\n",
      "Epoch:80 Batch:14 Loss:0.00613\n",
      "Epoch:100 Batch:14 Loss:0.00512\n",
      "Epoch:120 Batch:14 Loss:0.00448\n",
      "Epoch:140 Batch:14 Loss:0.00531\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.157\n",
      "Epoch:10 Batch:10 Loss:0.154\n",
      "Epoch:20 Batch:10 Loss:0.152\n",
      "Epoch:30 Batch:10 Loss:0.154\n",
      "Epoch:40 Batch:10 Loss:0.152\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 374.583201018331 setps: 800 count: 800\n",
      "avg rewards: 374.583201018331\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.34109\n",
      "Epoch:20 Batch:15 Loss:0.01403\n",
      "Epoch:40 Batch:15 Loss:0.00856\n",
      "Epoch:60 Batch:15 Loss:0.00705\n",
      "Epoch:80 Batch:15 Loss:0.00631\n",
      "Epoch:100 Batch:15 Loss:0.00557\n",
      "Epoch:120 Batch:15 Loss:0.00516\n",
      "Epoch:140 Batch:15 Loss:0.00440\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.170\n",
      "Epoch:10 Batch:10 Loss:0.165\n",
      "Epoch:20 Batch:10 Loss:0.162\n",
      "Epoch:30 Batch:10 Loss:0.166\n",
      "Epoch:40 Batch:10 Loss:0.162\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 1.9370655458551247 setps: 800 count: 800\n",
      "avg rewards: 1.9370655458551247\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.27944\n",
      "Epoch:20 Batch:16 Loss:0.01402\n",
      "Epoch:40 Batch:16 Loss:0.00900\n",
      "Epoch:60 Batch:16 Loss:0.00689\n",
      "Epoch:80 Batch:16 Loss:0.00647\n",
      "Epoch:100 Batch:16 Loss:0.00503\n",
      "Epoch:120 Batch:16 Loss:0.00449\n",
      "Epoch:140 Batch:16 Loss:0.00432\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.176\n",
      "Epoch:10 Batch:10 Loss:0.167\n",
      "Epoch:20 Batch:10 Loss:0.167\n",
      "Epoch:30 Batch:10 Loss:0.167\n",
      "Epoch:40 Batch:10 Loss:0.164\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 169.36697867402538 setps: 800 count: 800\n",
      "avg rewards: 169.36697867402538\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.34970\n",
      "Epoch:20 Batch:17 Loss:0.01242\n",
      "Epoch:40 Batch:17 Loss:0.00849\n",
      "Epoch:60 Batch:17 Loss:0.00649\n",
      "Epoch:80 Batch:17 Loss:0.00536\n",
      "Epoch:100 Batch:17 Loss:0.00503\n",
      "Epoch:120 Batch:17 Loss:0.00386\n",
      "Epoch:140 Batch:17 Loss:0.00503\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.160\n",
      "Epoch:10 Batch:10 Loss:0.154\n",
      "Epoch:20 Batch:10 Loss:0.155\n",
      "Epoch:30 Batch:10 Loss:0.157\n",
      "Epoch:40 Batch:10 Loss:0.153\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 202.8305960161027 setps: 800 count: 800\n",
      "avg rewards: 202.8305960161027\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.34668\n",
      "Epoch:20 Batch:18 Loss:0.01238\n",
      "Epoch:40 Batch:18 Loss:0.00794\n",
      "Epoch:60 Batch:18 Loss:0.00647\n",
      "Epoch:80 Batch:18 Loss:0.00524\n",
      "Epoch:100 Batch:18 Loss:0.00470\n",
      "Epoch:120 Batch:18 Loss:0.00491\n",
      "Epoch:140 Batch:18 Loss:0.00474\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:10 Loss:0.173\n",
      "Epoch:10 Batch:10 Loss:0.170\n",
      "Epoch:20 Batch:10 Loss:0.165\n",
      "Epoch:30 Batch:10 Loss:0.167\n",
      "Epoch:40 Batch:10 Loss:0.168\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 344.75604106956285 setps: 800 count: 800\n",
      "avg rewards: 344.75604106956285\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.25281\n",
      "Epoch:20 Batch:19 Loss:0.01185\n",
      "Epoch:40 Batch:19 Loss:0.00840\n",
      "Epoch:60 Batch:19 Loss:0.00649\n",
      "Epoch:80 Batch:19 Loss:0.00586\n",
      "Epoch:100 Batch:19 Loss:0.00472\n",
      "Epoch:120 Batch:19 Loss:0.00409\n",
      "Epoch:140 Batch:19 Loss:0.00457\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.169\n",
      "Epoch:10 Batch:10 Loss:0.167\n",
      "Epoch:20 Batch:10 Loss:0.167\n",
      "Epoch:30 Batch:10 Loss:0.165\n",
      "Epoch:40 Batch:10 Loss:0.164\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 49.87752588962027 setps: 800 count: 800\n",
      "avg rewards: 49.87752588962027\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.26574\n",
      "Epoch:20 Batch:20 Loss:0.01079\n",
      "Epoch:40 Batch:20 Loss:0.00672\n",
      "Epoch:60 Batch:20 Loss:0.00664\n",
      "Epoch:80 Batch:20 Loss:0.00487\n",
      "Epoch:100 Batch:20 Loss:0.00458\n",
      "Epoch:120 Batch:20 Loss:0.00433\n",
      "Epoch:140 Batch:20 Loss:0.00390\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.151\n",
      "Epoch:10 Batch:10 Loss:0.142\n",
      "Epoch:20 Batch:10 Loss:0.140\n",
      "Epoch:30 Batch:10 Loss:0.143\n",
      "Epoch:40 Batch:10 Loss:0.141\n",
      "Done!\n",
      "############# start HumanoidBulletEnv-v0 training ###################\n",
      "(49,)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -31.670611619416743 setps: 19 count: 19\n",
      "reward: -31.887152813519066 setps: 19 count: 38\n",
      "reward: -28.64125677267438 setps: 18 count: 56\n",
      "reward: -24.909012063659613 setps: 19 count: 75\n",
      "reward: -41.29290771828528 setps: 20 count: 95\n",
      "reward: -51.99106353911048 setps: 20 count: 115\n",
      "reward: -39.31853355359345 setps: 20 count: 135\n",
      "reward: -28.79578133157338 setps: 18 count: 153\n",
      "reward: -31.628607562232357 setps: 20 count: 173\n",
      "reward: -17.75812217401544 setps: 19 count: 192\n",
      "reward: -35.16423495947384 setps: 18 count: 210\n",
      "reward: -30.055821837983967 setps: 21 count: 231\n",
      "reward: -25.62942155986966 setps: 19 count: 250\n",
      "reward: -32.761298733825974 setps: 20 count: 270\n",
      "reward: -38.50627379091165 setps: 19 count: 289\n",
      "reward: -21.899687603708298 setps: 19 count: 308\n",
      "reward: -32.51787980311201 setps: 20 count: 328\n",
      "reward: -37.147549563650685 setps: 19 count: 347\n",
      "reward: -36.271225149756354 setps: 20 count: 367\n",
      "reward: -28.97792792861874 setps: 19 count: 386\n",
      "reward: -43.6678331704199 setps: 20 count: 406\n",
      "reward: -32.592590161877155 setps: 20 count: 426\n",
      "reward: -30.57825719642569 setps: 19 count: 445\n",
      "reward: -25.044538317006662 setps: 21 count: 466\n",
      "reward: -32.29310693947482 setps: 19 count: 485\n",
      "reward: -44.71703968606743 setps: 19 count: 504\n",
      "reward: -32.2655731237668 setps: 31 count: 535\n",
      "reward: -29.846799830935193 setps: 20 count: 555\n",
      "reward: -33.898694592404354 setps: 23 count: 578\n",
      "reward: -30.640793769559238 setps: 18 count: 596\n",
      "reward: -38.273901971895256 setps: 20 count: 616\n",
      "reward: -31.58877532303595 setps: 18 count: 634\n",
      "reward: -20.958476304473873 setps: 24 count: 658\n",
      "reward: -26.97668673825101 setps: 17 count: 675\n",
      "reward: -20.09804180001374 setps: 18 count: 693\n",
      "reward: -19.265679680260654 setps: 19 count: 712\n",
      "reward: -36.24132778493222 setps: 20 count: 732\n",
      "reward: -35.27037726789567 setps: 19 count: 751\n",
      "reward: -23.978927737307096 setps: 20 count: 771\n",
      "reward: -21.421806157306122 setps: 18 count: 789\n",
      "reward: -36.311412511651 setps: 19 count: 808\n",
      "reward: -40.74404378553736 setps: 23 count: 831\n",
      "reward: -26.159285747693506 setps: 19 count: 850\n",
      "reward: -31.721262937643044 setps: 19 count: 869\n",
      "reward: -29.72960160078656 setps: 19 count: 888\n",
      "reward: -41.219347612689305 setps: 20 count: 908\n",
      "reward: -12.25031010333478 setps: 20 count: 928\n",
      "reward: -23.72108948078385 setps: 20 count: 948\n",
      "reward: -36.536707819423455 setps: 23 count: 971\n",
      "reward: -38.589419054178876 setps: 25 count: 996\n",
      "avg rewards: -31.46852156572044\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:0.59861\n",
      "Epoch:20 Batch:1 Loss:0.56873\n",
      "Epoch:40 Batch:1 Loss:0.53206\n",
      "Epoch:60 Batch:1 Loss:0.50608\n",
      "Epoch:80 Batch:1 Loss:0.48599\n",
      "Epoch:100 Batch:1 Loss:0.45712\n",
      "Epoch:120 Batch:1 Loss:0.41611\n",
      "Epoch:140 Batch:1 Loss:0.36471\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.259\n",
      "Epoch:10 Batch:8 Loss:0.257\n",
      "Epoch:20 Batch:8 Loss:0.258\n",
      "Epoch:30 Batch:8 Loss:0.255\n",
      "Epoch:40 Batch:8 Loss:0.255\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 8.414773900507136 setps: 17 count: 17\n",
      "reward: 16.613046765865878 setps: 19 count: 36\n",
      "reward: 51.56178844942623 setps: 48 count: 84\n",
      "reward: 67.74260314666171 setps: 52 count: 136\n",
      "reward: 12.457849302602698 setps: 18 count: 154\n",
      "reward: 36.063627466362966 setps: 33 count: 187\n",
      "reward: 36.49465414083532 setps: 33 count: 220\n",
      "reward: 46.70484570957051 setps: 46 count: 266\n",
      "reward: 27.90677656700136 setps: 27 count: 293\n",
      "reward: 97.98718371154101 setps: 61 count: 354\n",
      "reward: 44.89235205042934 setps: 45 count: 399\n",
      "reward: 30.566935109498445 setps: 30 count: 429\n",
      "reward: 26.35370368744479 setps: 31 count: 460\n",
      "reward: 52.80385158116406 setps: 45 count: 505\n",
      "reward: 68.92064257072778 setps: 51 count: 556\n",
      "reward: 28.654353028410693 setps: 28 count: 584\n",
      "reward: 26.603356612290373 setps: 29 count: 613\n",
      "reward: 43.37186622556764 setps: 37 count: 650\n",
      "reward: 46.0184161190875 setps: 35 count: 685\n",
      "reward: 75.99500115866249 setps: 55 count: 740\n",
      "reward: 23.80022523406951 setps: 28 count: 768\n",
      "reward: 63.88068419889606 setps: 54 count: 822\n",
      "reward: 75.1313003194737 setps: 49 count: 871\n",
      "reward: 80.53498904573063 setps: 55 count: 926\n",
      "reward: 8.379504729545442 setps: 18 count: 944\n",
      "reward: 68.41796192022305 setps: 55 count: 999\n",
      "avg rewards: 44.85662664429216\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:2 Loss:0.46938\n",
      "Epoch:20 Batch:2 Loss:0.38441\n",
      "Epoch:40 Batch:2 Loss:0.33870\n",
      "Epoch:60 Batch:2 Loss:0.29899\n",
      "Epoch:80 Batch:2 Loss:0.26573\n",
      "Epoch:100 Batch:2 Loss:0.22938\n",
      "Epoch:120 Batch:2 Loss:0.19845\n",
      "Epoch:140 Batch:2 Loss:0.17367\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.129\n",
      "Epoch:10 Batch:8 Loss:0.128\n",
      "Epoch:20 Batch:8 Loss:0.125\n",
      "Epoch:30 Batch:8 Loss:0.124\n",
      "Epoch:40 Batch:8 Loss:0.124\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 7.287983219590388 setps: 19 count: 19\n",
      "reward: 8.505507567919267 setps: 19 count: 38\n",
      "reward: 7.16861396683089 setps: 19 count: 57\n",
      "reward: 8.083257709398456 setps: 19 count: 76\n",
      "reward: 8.040535994051607 setps: 19 count: 95\n",
      "reward: 30.623222979916317 setps: 30 count: 125\n",
      "reward: 7.847771438347992 setps: 19 count: 144\n",
      "reward: 7.1914957699395 setps: 19 count: 163\n",
      "reward: 7.6830377416830755 setps: 19 count: 182\n",
      "reward: 7.816505822268661 setps: 19 count: 201\n",
      "reward: 7.529631206944757 setps: 19 count: 220\n",
      "reward: 6.1351627001233275 setps: 19 count: 239\n",
      "reward: -1.6035536230338043 setps: 19 count: 258\n",
      "reward: 8.05061966858775 setps: 19 count: 277\n",
      "reward: 9.992923064820932 setps: 19 count: 296\n",
      "reward: 3.468057878028776 setps: 18 count: 314\n",
      "reward: 8.972617444321804 setps: 19 count: 333\n",
      "reward: 4.3771522198017925 setps: 18 count: 351\n",
      "reward: 8.808643744296567 setps: 19 count: 370\n",
      "reward: 8.76230494076444 setps: 19 count: 389\n",
      "reward: 8.468244762858376 setps: 19 count: 408\n",
      "reward: 8.322289680011451 setps: 19 count: 427\n",
      "reward: 8.57316910081281 setps: 19 count: 446\n",
      "reward: 7.088537714835545 setps: 19 count: 465\n",
      "reward: 7.819351435027783 setps: 20 count: 485\n",
      "reward: 9.570810879509372 setps: 19 count: 504\n",
      "reward: 5.240865397652669 setps: 18 count: 522\n",
      "reward: 7.227151104214136 setps: 19 count: 541\n",
      "reward: 7.478195272298764 setps: 19 count: 560\n",
      "reward: 3.150637969601667 setps: 17 count: 577\n",
      "reward: 6.421304175374098 setps: 18 count: 595\n",
      "reward: 7.655556179342964 setps: 19 count: 614\n",
      "reward: 7.034636386108468 setps: 18 count: 632\n",
      "reward: 7.410116681108776 setps: 19 count: 651\n",
      "reward: 8.286737499615992 setps: 19 count: 670\n",
      "reward: 6.748972703577602 setps: 19 count: 689\n",
      "reward: 8.713067030956152 setps: 19 count: 708\n",
      "reward: 8.334692847859698 setps: 19 count: 727\n",
      "reward: 6.271419644026901 setps: 19 count: 746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 9.052507793244146 setps: 19 count: 765\n",
      "reward: 8.31184507417347 setps: 19 count: 784\n",
      "reward: 1.6184534458632687 setps: 18 count: 802\n",
      "reward: 8.878184109585705 setps: 19 count: 821\n",
      "reward: 8.442457101494075 setps: 19 count: 840\n",
      "reward: 8.977592535552683 setps: 19 count: 859\n",
      "reward: -0.5084867378987845 setps: 19 count: 878\n",
      "reward: 9.997243058143068 setps: 19 count: 897\n",
      "reward: 8.864848374832945 setps: 19 count: 916\n",
      "reward: 10.481499523390085 setps: 19 count: 935\n",
      "reward: 6.492303885011644 setps: 19 count: 954\n",
      "reward: -4.492353088481469 setps: 14 count: 968\n",
      "reward: 8.094010614465516 setps: 20 count: 988\n",
      "avg rewards: 7.476295261707155\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.43684\n",
      "Epoch:20 Batch:3 Loss:0.29463\n",
      "Epoch:40 Batch:3 Loss:0.24986\n",
      "Epoch:60 Batch:3 Loss:0.21489\n",
      "Epoch:80 Batch:3 Loss:0.18082\n",
      "Epoch:100 Batch:3 Loss:0.14644\n",
      "Epoch:120 Batch:3 Loss:0.13345\n",
      "Epoch:140 Batch:3 Loss:0.10919\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.118\n",
      "Epoch:10 Batch:8 Loss:0.112\n",
      "Epoch:20 Batch:8 Loss:0.111\n",
      "Epoch:30 Batch:8 Loss:0.109\n",
      "Epoch:40 Batch:8 Loss:0.110\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 21.32795663620636 setps: 29 count: 29\n",
      "reward: 30.97707754514413 setps: 30 count: 59\n",
      "reward: 27.17726144635963 setps: 28 count: 87\n",
      "reward: 18.192251791391758 setps: 24 count: 111\n",
      "reward: 23.656870672894 setps: 27 count: 138\n",
      "reward: 26.55799856111552 setps: 28 count: 166\n",
      "reward: 22.446365735256403 setps: 26 count: 192\n",
      "reward: 19.38818706403981 setps: 24 count: 216\n",
      "reward: 12.382599671839852 setps: 17 count: 233\n",
      "reward: 18.77960507449316 setps: 24 count: 257\n",
      "reward: 26.13294276300439 setps: 27 count: 284\n",
      "reward: 20.44072996623872 setps: 25 count: 309\n",
      "reward: 22.929165611251666 setps: 26 count: 335\n",
      "reward: 14.345146140138967 setps: 19 count: 354\n",
      "reward: 26.68121987716586 setps: 26 count: 380\n",
      "reward: 23.544697658022056 setps: 29 count: 409\n",
      "reward: 20.321159456043098 setps: 25 count: 434\n",
      "reward: 26.065623686859904 setps: 29 count: 463\n",
      "reward: 30.733769258411478 setps: 37 count: 500\n",
      "reward: 19.852711276129412 setps: 24 count: 524\n",
      "reward: 15.419446931318088 setps: 20 count: 544\n",
      "reward: 24.935010536601478 setps: 29 count: 573\n",
      "reward: 22.05347005242657 setps: 24 count: 597\n",
      "reward: 19.656156754565014 setps: 24 count: 621\n",
      "reward: 26.52640897945966 setps: 30 count: 651\n",
      "reward: 22.786057782944404 setps: 25 count: 676\n",
      "reward: 23.651919413443828 setps: 27 count: 703\n",
      "reward: 21.869993297089238 setps: 23 count: 726\n",
      "reward: 21.512034931388914 setps: 24 count: 750\n",
      "reward: 32.426500029434095 setps: 33 count: 783\n",
      "reward: 6.91168288898043 setps: 17 count: 800\n",
      "reward: 24.562268177757502 setps: 25 count: 825\n",
      "reward: 26.321735306289344 setps: 28 count: 853\n",
      "reward: 24.363260352671205 setps: 26 count: 879\n",
      "reward: 24.168463146066646 setps: 27 count: 906\n",
      "reward: 22.52747711579141 setps: 27 count: 933\n",
      "reward: 16.998363724193766 setps: 24 count: 957\n",
      "reward: 29.612023661959395 setps: 28 count: 985\n",
      "avg rewards: 22.585147709852293\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:4 Loss:0.41215\n",
      "Epoch:20 Batch:4 Loss:0.26020\n",
      "Epoch:40 Batch:4 Loss:0.19734\n",
      "Epoch:60 Batch:4 Loss:0.17130\n",
      "Epoch:80 Batch:4 Loss:0.13748\n",
      "Epoch:100 Batch:4 Loss:0.10953\n",
      "Epoch:120 Batch:4 Loss:0.10246\n",
      "Epoch:140 Batch:4 Loss:0.09319\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.105\n",
      "Epoch:10 Batch:8 Loss:0.101\n",
      "Epoch:20 Batch:8 Loss:0.102\n",
      "Epoch:30 Batch:8 Loss:0.103\n",
      "Epoch:40 Batch:8 Loss:0.105\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 30.306478663622695 setps: 34 count: 34\n",
      "reward: 30.562605692770738 setps: 35 count: 69\n",
      "reward: 26.030415190380875 setps: 28 count: 97\n",
      "reward: 28.33249610155617 setps: 29 count: 126\n",
      "reward: 22.38760494807211 setps: 25 count: 151\n",
      "reward: 16.046544957798325 setps: 18 count: 169\n",
      "reward: 22.23273491687869 setps: 26 count: 195\n",
      "reward: 20.659423043510472 setps: 24 count: 219\n",
      "reward: 23.841524535052304 setps: 27 count: 246\n",
      "reward: 25.594924236809312 setps: 26 count: 272\n",
      "reward: 22.999184783085365 setps: 25 count: 297\n",
      "reward: 30.931596257101045 setps: 31 count: 328\n",
      "reward: 24.590530585446686 setps: 26 count: 354\n",
      "reward: 32.74807357953396 setps: 31 count: 385\n",
      "reward: 23.58758797131159 setps: 24 count: 409\n",
      "reward: 30.865107949590307 setps: 30 count: 439\n",
      "reward: 24.423652903085163 setps: 32 count: 471\n",
      "reward: 26.726139645910006 setps: 26 count: 497\n",
      "reward: 27.366142175502315 setps: 26 count: 523\n",
      "reward: 24.076849922894326 setps: 26 count: 549\n",
      "reward: 32.598006379859 setps: 31 count: 580\n",
      "reward: 23.760294330974286 setps: 24 count: 604\n",
      "reward: 29.351136312620653 setps: 28 count: 632\n",
      "reward: 26.577202568632494 setps: 27 count: 659\n",
      "reward: 12.16422529788397 setps: 19 count: 678\n",
      "reward: 22.43233109200228 setps: 25 count: 703\n",
      "reward: 20.005824381063583 setps: 27 count: 730\n",
      "reward: 21.973054956678236 setps: 24 count: 754\n",
      "reward: 23.588803642480347 setps: 27 count: 781\n",
      "reward: 23.358852241636484 setps: 23 count: 804\n",
      "reward: 25.334213705195 setps: 29 count: 833\n",
      "reward: 13.07973199322878 setps: 18 count: 851\n",
      "reward: 22.730081272372633 setps: 26 count: 877\n",
      "reward: 21.7488387644582 setps: 34 count: 911\n",
      "reward: 22.3492470396799 setps: 23 count: 934\n",
      "reward: 14.19854263630841 setps: 17 count: 951\n",
      "reward: 22.556105142179874 setps: 25 count: 976\n",
      "avg rewards: 24.11124621127477\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:5 Loss:0.39623\n",
      "Epoch:20 Batch:5 Loss:0.22888\n",
      "Epoch:40 Batch:5 Loss:0.16013\n",
      "Epoch:60 Batch:5 Loss:0.13020\n",
      "Epoch:80 Batch:5 Loss:0.10669\n",
      "Epoch:100 Batch:5 Loss:0.09084\n",
      "Epoch:120 Batch:5 Loss:0.07793\n",
      "Epoch:140 Batch:5 Loss:0.07484\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.105\n",
      "Epoch:10 Batch:8 Loss:0.103\n",
      "Epoch:20 Batch:8 Loss:0.102\n",
      "Epoch:30 Batch:8 Loss:0.101\n",
      "Epoch:40 Batch:8 Loss:0.103\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 48.92755058396287 setps: 32 count: 32\n",
      "reward: 48.542758727281765 setps: 34 count: 66\n",
      "reward: 59.48784035988502 setps: 37 count: 103\n",
      "reward: 30.404132795993057 setps: 28 count: 131\n",
      "reward: 14.32346338933858 setps: 18 count: 149\n",
      "reward: 52.614594580908296 setps: 37 count: 186\n",
      "reward: 58.647173077386114 setps: 39 count: 225\n",
      "reward: 44.569229849726256 setps: 37 count: 262\n",
      "reward: 39.98257260909159 setps: 30 count: 292\n",
      "reward: 57.16469350958214 setps: 36 count: 328\n",
      "reward: 19.7809570338155 setps: 21 count: 349\n",
      "reward: 22.4745043273404 setps: 22 count: 371\n",
      "reward: 68.73119719867829 setps: 61 count: 432\n",
      "reward: 54.16818348257365 setps: 41 count: 473\n",
      "reward: 84.24794764201216 setps: 56 count: 529\n",
      "reward: 13.935885936014527 setps: 18 count: 547\n",
      "reward: 69.29992717890418 setps: 41 count: 588\n",
      "reward: 47.08343848727236 setps: 41 count: 629\n",
      "reward: 66.83299479628623 setps: 44 count: 673\n",
      "reward: 74.7651930229724 setps: 46 count: 719\n",
      "reward: 52.33797898651391 setps: 33 count: 752\n",
      "reward: 42.87177858525393 setps: 30 count: 782\n",
      "reward: 47.94723741634225 setps: 31 count: 813\n",
      "reward: 45.228365188805036 setps: 33 count: 846\n",
      "reward: 62.65237261467847 setps: 45 count: 891\n",
      "reward: 56.46102481388516 setps: 36 count: 927\n",
      "reward: 19.076918187073897 setps: 20 count: 947\n",
      "reward: 69.06080402531515 setps: 44 count: 991\n",
      "avg rewards: 48.98645422881761\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.38525\n",
      "Epoch:20 Batch:6 Loss:0.21083\n",
      "Epoch:40 Batch:6 Loss:0.14090\n",
      "Epoch:60 Batch:6 Loss:0.11391\n",
      "Epoch:80 Batch:6 Loss:0.09245\n",
      "Epoch:100 Batch:6 Loss:0.07377\n",
      "Epoch:120 Batch:6 Loss:0.06704\n",
      "Epoch:140 Batch:6 Loss:0.06554\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.099\n",
      "Epoch:10 Batch:8 Loss:0.097\n",
      "Epoch:20 Batch:8 Loss:0.096\n",
      "Epoch:30 Batch:8 Loss:0.095\n",
      "Epoch:40 Batch:8 Loss:0.096\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 47.54955460613418 setps: 38 count: 38\n",
      "reward: 41.619085989559245 setps: 37 count: 75\n",
      "reward: 61.46843097635282 setps: 63 count: 138\n",
      "reward: 54.811314319315706 setps: 42 count: 180\n",
      "reward: 61.744070892363375 setps: 43 count: 223\n",
      "reward: 38.77820969079184 setps: 35 count: 258\n",
      "reward: 53.39344456150864 setps: 40 count: 298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 72.13873832898246 setps: 49 count: 347\n",
      "reward: 48.26883309737459 setps: 41 count: 388\n",
      "reward: 49.69706993629515 setps: 57 count: 445\n",
      "reward: 54.18855581947429 setps: 37 count: 482\n",
      "reward: 43.75154395715071 setps: 37 count: 519\n",
      "reward: 59.18661519575543 setps: 38 count: 557\n",
      "reward: 47.7068394578615 setps: 36 count: 593\n",
      "reward: 26.512616248628184 setps: 26 count: 619\n",
      "reward: 45.8102166253171 setps: 36 count: 655\n",
      "reward: 49.376016816824276 setps: 39 count: 694\n",
      "reward: 41.43039001653815 setps: 35 count: 729\n",
      "reward: 50.91364345286419 setps: 37 count: 766\n",
      "reward: 50.210692174414 setps: 37 count: 803\n",
      "reward: 71.74058947145822 setps: 49 count: 852\n",
      "reward: 72.58931353058144 setps: 44 count: 896\n",
      "reward: 39.459581374772824 setps: 34 count: 930\n",
      "reward: 51.27297554184044 setps: 37 count: 967\n",
      "avg rewards: 51.40076425342328\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:7 Loss:0.38952\n",
      "Epoch:20 Batch:7 Loss:0.19444\n",
      "Epoch:40 Batch:7 Loss:0.11793\n",
      "Epoch:60 Batch:7 Loss:0.09685\n",
      "Epoch:80 Batch:7 Loss:0.07940\n",
      "Epoch:100 Batch:7 Loss:0.06460\n",
      "Epoch:120 Batch:7 Loss:0.06500\n",
      "Epoch:140 Batch:7 Loss:0.05816\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.095\n",
      "Epoch:10 Batch:8 Loss:0.096\n",
      "Epoch:20 Batch:8 Loss:0.095\n",
      "Epoch:30 Batch:8 Loss:0.097\n",
      "Epoch:40 Batch:8 Loss:0.095\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 40.97751204488304 setps: 35 count: 35\n",
      "reward: 12.551917267312819 setps: 20 count: 55\n",
      "reward: 59.325086923783225 setps: 42 count: 97\n",
      "reward: 48.46320170478574 setps: 42 count: 139\n",
      "reward: 42.79364052647725 setps: 36 count: 175\n",
      "reward: 51.56591433177093 setps: 45 count: 220\n",
      "reward: 52.38419912537647 setps: 43 count: 263\n",
      "reward: 13.82319961726025 setps: 18 count: 281\n",
      "reward: 42.74562786319438 setps: 34 count: 315\n",
      "reward: 31.762453552694925 setps: 39 count: 354\n",
      "reward: 50.57718593167081 setps: 38 count: 392\n",
      "reward: 46.129348968624264 setps: 37 count: 429\n",
      "reward: 31.485782146251587 setps: 28 count: 457\n",
      "reward: 43.264738032425505 setps: 37 count: 494\n",
      "reward: 46.19320031716633 setps: 37 count: 531\n",
      "reward: 45.45438703389227 setps: 39 count: 570\n",
      "reward: 47.21091664853592 setps: 51 count: 621\n",
      "reward: 59.187695333592984 setps: 49 count: 670\n",
      "reward: 42.59826646175207 setps: 34 count: 704\n",
      "reward: 43.065594306461584 setps: 40 count: 744\n",
      "reward: 86.20913281481187 setps: 58 count: 802\n",
      "reward: 49.27219883275249 setps: 39 count: 841\n",
      "reward: 61.005432900617606 setps: 78 count: 919\n",
      "reward: 16.15389593680593 setps: 19 count: 938\n",
      "reward: 48.94798933823478 setps: 41 count: 979\n",
      "reward: 11.768235821310373 setps: 17 count: 996\n",
      "avg rewards: 43.26602899163251\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:8 Loss:0.37855\n",
      "Epoch:20 Batch:8 Loss:0.15244\n",
      "Epoch:40 Batch:8 Loss:0.11365\n",
      "Epoch:60 Batch:8 Loss:0.08543\n",
      "Epoch:80 Batch:8 Loss:0.06937\n",
      "Epoch:100 Batch:8 Loss:0.05964\n",
      "Epoch:120 Batch:8 Loss:0.05357\n",
      "Epoch:140 Batch:8 Loss:0.05160\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.081\n",
      "Epoch:10 Batch:8 Loss:0.082\n",
      "Epoch:20 Batch:8 Loss:0.082\n",
      "Epoch:30 Batch:8 Loss:0.082\n",
      "Epoch:40 Batch:8 Loss:0.080\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 109.50263896545368 setps: 75 count: 75\n",
      "reward: 17.89005864183273 setps: 20 count: 95\n",
      "reward: 64.20993847641803 setps: 52 count: 147\n",
      "reward: 67.61667468217055 setps: 56 count: 203\n",
      "reward: 97.33525966937304 setps: 59 count: 262\n",
      "reward: 64.51311536866703 setps: 50 count: 312\n",
      "reward: 87.34298501573797 setps: 58 count: 370\n",
      "reward: 22.740179650024217 setps: 27 count: 397\n",
      "reward: 98.43328223670544 setps: 62 count: 459\n",
      "reward: 44.757886321812116 setps: 41 count: 500\n",
      "reward: 17.439906751237867 setps: 20 count: 520\n",
      "reward: 46.86324998362107 setps: 37 count: 557\n",
      "reward: 14.876159300732251 setps: 19 count: 576\n",
      "reward: 47.59970704898297 setps: 44 count: 620\n",
      "reward: 108.05654355867736 setps: 64 count: 684\n",
      "reward: 14.78467802965897 setps: 21 count: 705\n",
      "reward: 80.7879141054451 setps: 53 count: 758\n",
      "reward: 32.17496532265505 setps: 45 count: 803\n",
      "reward: 20.129790717781 setps: 22 count: 825\n",
      "reward: 20.232827578486464 setps: 20 count: 845\n",
      "reward: 88.25733730972425 setps: 57 count: 902\n",
      "reward: 88.8188900876499 setps: 57 count: 959\n",
      "avg rewards: 57.01654494649304\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:9 Loss:0.37195\n",
      "Epoch:20 Batch:9 Loss:0.15484\n",
      "Epoch:40 Batch:9 Loss:0.10122\n",
      "Epoch:60 Batch:9 Loss:0.07608\n",
      "Epoch:80 Batch:9 Loss:0.06579\n",
      "Epoch:100 Batch:9 Loss:0.06407\n",
      "Epoch:120 Batch:9 Loss:0.05412\n",
      "Epoch:140 Batch:9 Loss:0.05227\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.077\n",
      "Epoch:10 Batch:8 Loss:0.074\n",
      "Epoch:20 Batch:8 Loss:0.073\n",
      "Epoch:30 Batch:8 Loss:0.074\n",
      "Epoch:40 Batch:8 Loss:0.074\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 44.02007919833123 setps: 40 count: 40\n",
      "reward: 24.30412976564839 setps: 33 count: 73\n",
      "reward: 37.306903679399696 setps: 29 count: 102\n",
      "reward: 59.38292156941025 setps: 52 count: 154\n",
      "reward: 20.79739300851797 setps: 24 count: 178\n",
      "reward: 25.255020513522318 setps: 28 count: 206\n",
      "reward: 26.37746722686133 setps: 28 count: 234\n",
      "reward: 37.311923463335546 setps: 43 count: 277\n",
      "reward: 34.19129256707384 setps: 41 count: 318\n",
      "reward: 46.13179787561238 setps: 37 count: 355\n",
      "reward: 32.53891624917887 setps: 27 count: 382\n",
      "reward: 16.40264497761236 setps: 19 count: 401\n",
      "reward: 63.96628424521187 setps: 53 count: 454\n",
      "reward: 70.73183839393168 setps: 59 count: 513\n",
      "reward: 62.811074073311474 setps: 51 count: 564\n",
      "reward: 64.47877734795183 setps: 55 count: 619\n",
      "reward: 95.84071908820334 setps: 70 count: 689\n",
      "reward: 65.09554890456349 setps: 48 count: 737\n",
      "reward: 56.99800178055156 setps: 46 count: 783\n",
      "reward: 45.967524731173775 setps: 36 count: 819\n",
      "reward: 43.27952198924467 setps: 46 count: 865\n",
      "reward: 38.89433794301731 setps: 44 count: 909\n",
      "reward: 57.51514724173177 setps: 50 count: 959\n",
      "reward: 36.69033868309051 setps: 40 count: 999\n",
      "avg rewards: 46.09540018818698\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.34994\n",
      "Epoch:20 Batch:10 Loss:0.13217\n",
      "Epoch:40 Batch:10 Loss:0.09041\n",
      "Epoch:60 Batch:10 Loss:0.07068\n",
      "Epoch:80 Batch:10 Loss:0.06035\n",
      "Epoch:100 Batch:10 Loss:0.05316\n",
      "Epoch:120 Batch:10 Loss:0.04574\n",
      "Epoch:140 Batch:10 Loss:0.04344\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.056\n",
      "Epoch:10 Batch:8 Loss:0.057\n",
      "Epoch:20 Batch:8 Loss:0.057\n",
      "Epoch:30 Batch:8 Loss:0.054\n",
      "Epoch:40 Batch:8 Loss:0.056\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 47.112022916571 setps: 42 count: 42\n",
      "reward: 48.436516562473834 setps: 38 count: 80\n",
      "reward: 34.4233937835641 setps: 44 count: 124\n",
      "reward: 54.242198222201836 setps: 51 count: 175\n",
      "reward: 40.27869823305518 setps: 46 count: 221\n",
      "reward: 42.22036027272261 setps: 36 count: 257\n",
      "reward: 38.68787297707023 setps: 40 count: 297\n",
      "reward: 44.4018206158973 setps: 38 count: 335\n",
      "reward: 61.00677028928769 setps: 57 count: 392\n",
      "reward: 41.36476796386124 setps: 39 count: 431\n",
      "reward: 47.68496701458935 setps: 46 count: 477\n",
      "reward: 51.25070807051815 setps: 58 count: 535\n",
      "reward: 17.228423833777192 setps: 21 count: 556\n",
      "reward: 13.105891082178278 setps: 19 count: 575\n",
      "reward: 63.55225644374878 setps: 45 count: 620\n",
      "reward: 8.140377577774052 setps: 18 count: 638\n",
      "reward: 70.52603135916144 setps: 69 count: 707\n",
      "reward: 39.584404455323245 setps: 37 count: 744\n",
      "reward: 25.872138759556396 setps: 34 count: 778\n",
      "reward: 47.07030671045941 setps: 37 count: 815\n",
      "reward: 39.4800294579909 setps: 38 count: 853\n",
      "reward: 37.48888097508752 setps: 35 count: 888\n",
      "reward: 35.64491947798379 setps: 30 count: 918\n",
      "reward: 41.12996331910544 setps: 36 count: 954\n",
      "reward: 24.06974597975059 setps: 24 count: 978\n",
      "avg rewards: 40.56013865414838\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:11 Loss:0.34324\n",
      "Epoch:20 Batch:11 Loss:0.13306\n",
      "Epoch:40 Batch:11 Loss:0.09149\n",
      "Epoch:60 Batch:11 Loss:0.06614\n",
      "Epoch:80 Batch:11 Loss:0.05493\n",
      "Epoch:100 Batch:11 Loss:0.04818\n",
      "Epoch:120 Batch:11 Loss:0.05034\n",
      "Epoch:140 Batch:11 Loss:0.04579\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.049\n",
      "Epoch:10 Batch:8 Loss:0.048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 Batch:8 Loss:0.046\n",
      "Epoch:30 Batch:8 Loss:0.047\n",
      "Epoch:40 Batch:8 Loss:0.047\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 54.475408883154174 setps: 56 count: 56\n",
      "reward: 43.815091242095505 setps: 52 count: 108\n",
      "reward: 36.290142372058476 setps: 39 count: 147\n",
      "reward: 31.78540788336394 setps: 34 count: 181\n",
      "reward: 15.198903880035502 setps: 18 count: 199\n",
      "reward: 43.83993543180839 setps: 52 count: 251\n",
      "reward: 42.19530743011857 setps: 53 count: 304\n",
      "reward: 36.83105428251582 setps: 42 count: 346\n",
      "reward: 20.948921213141872 setps: 28 count: 374\n",
      "reward: 20.273852680547865 setps: 22 count: 396\n",
      "reward: 33.50129035432036 setps: 36 count: 432\n",
      "reward: 4.672754674874886 setps: 19 count: 451\n",
      "reward: 11.116960461298005 setps: 19 count: 470\n",
      "reward: 21.757776413421375 setps: 32 count: 502\n",
      "reward: 25.41634875619057 setps: 28 count: 530\n",
      "reward: 51.47636519210499 setps: 53 count: 583\n",
      "reward: 43.20871264311136 setps: 64 count: 647\n",
      "reward: 32.23203827143442 setps: 34 count: 681\n",
      "reward: 50.42303860578103 setps: 41 count: 722\n",
      "reward: 47.36219628224207 setps: 47 count: 769\n",
      "reward: 34.56894652419288 setps: 42 count: 811\n",
      "reward: 29.33846815824799 setps: 39 count: 850\n",
      "reward: 61.03220423700841 setps: 51 count: 901\n",
      "reward: 29.370114844971976 setps: 28 count: 929\n",
      "reward: 15.3671467838838 setps: 31 count: 960\n",
      "avg rewards: 33.459935500076966\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:12 Loss:0.36810\n",
      "Epoch:20 Batch:12 Loss:0.13775\n",
      "Epoch:40 Batch:12 Loss:0.07515\n",
      "Epoch:60 Batch:12 Loss:0.06633\n",
      "Epoch:80 Batch:12 Loss:0.05529\n",
      "Epoch:100 Batch:12 Loss:0.04843\n",
      "Epoch:120 Batch:12 Loss:0.04154\n",
      "Epoch:140 Batch:12 Loss:0.04110\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.047\n",
      "Epoch:10 Batch:8 Loss:0.044\n",
      "Epoch:20 Batch:8 Loss:0.045\n",
      "Epoch:30 Batch:8 Loss:0.045\n",
      "Epoch:40 Batch:8 Loss:0.045\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 42.624663960609176 setps: 35 count: 35\n",
      "reward: 40.95857198640734 setps: 39 count: 74\n",
      "reward: 31.134513203929238 setps: 31 count: 105\n",
      "reward: 39.04695305554923 setps: 42 count: 147\n",
      "reward: 37.66756078263133 setps: 36 count: 183\n",
      "reward: 73.33009433625702 setps: 63 count: 246\n",
      "reward: 63.604507383919554 setps: 87 count: 333\n",
      "reward: 0.6204092528118044 setps: 18 count: 351\n",
      "reward: 43.79938003646675 setps: 43 count: 394\n",
      "reward: 51.096260140849324 setps: 66 count: 460\n",
      "reward: 43.26740197979525 setps: 38 count: 498\n",
      "reward: 57.03233683182479 setps: 50 count: 548\n",
      "reward: 2.7526326635372236 setps: 14 count: 562\n",
      "reward: 41.78826845010189 setps: 33 count: 595\n",
      "reward: 39.74882350388072 setps: 65 count: 660\n",
      "reward: 37.432425203292205 setps: 32 count: 692\n",
      "reward: 59.99724334135536 setps: 54 count: 746\n",
      "reward: 74.3979536004277 setps: 52 count: 798\n",
      "reward: 72.98945455895591 setps: 54 count: 852\n",
      "reward: 37.14051779375586 setps: 38 count: 890\n",
      "reward: 60.638261361645725 setps: 65 count: 955\n",
      "reward: 13.184549439663535 setps: 21 count: 976\n",
      "avg rewards: 43.82967194853032\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:13 Loss:0.35094\n",
      "Epoch:20 Batch:13 Loss:0.12045\n",
      "Epoch:40 Batch:13 Loss:0.07372\n",
      "Epoch:60 Batch:13 Loss:0.06042\n",
      "Epoch:80 Batch:13 Loss:0.05938\n",
      "Epoch:100 Batch:13 Loss:0.04661\n",
      "Epoch:120 Batch:13 Loss:0.04144\n",
      "Epoch:140 Batch:13 Loss:0.04066\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.039\n",
      "Epoch:10 Batch:8 Loss:0.039\n",
      "Epoch:20 Batch:8 Loss:0.039\n",
      "Epoch:30 Batch:8 Loss:0.040\n",
      "Epoch:40 Batch:8 Loss:0.043\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 27.338448838189652 setps: 30 count: 30\n",
      "reward: 42.46475935212801 setps: 44 count: 74\n",
      "reward: 20.071503399981882 setps: 30 count: 104\n",
      "reward: 16.73167464685248 setps: 30 count: 134\n",
      "reward: 42.04619187499048 setps: 37 count: 171\n",
      "reward: 11.913624690753931 setps: 20 count: 191\n",
      "reward: 24.232379374917954 setps: 29 count: 220\n",
      "reward: 36.01442774620082 setps: 38 count: 258\n",
      "reward: 20.63414365037752 setps: 36 count: 294\n",
      "reward: 33.95475168271078 setps: 35 count: 329\n",
      "reward: 46.50248083352201 setps: 43 count: 372\n",
      "reward: 38.41153252853837 setps: 43 count: 415\n",
      "reward: 18.70330444115534 setps: 26 count: 441\n",
      "reward: 39.469375736058275 setps: 44 count: 485\n",
      "reward: 22.03536172851745 setps: 29 count: 514\n",
      "reward: 50.245845070318325 setps: 44 count: 558\n",
      "reward: 49.65556110920735 setps: 37 count: 595\n",
      "reward: 32.24322470763 setps: 35 count: 630\n",
      "reward: 35.77471819798229 setps: 44 count: 674\n",
      "reward: 35.99389800800272 setps: 50 count: 724\n",
      "reward: 31.929936775633543 setps: 39 count: 763\n",
      "reward: 21.733168549371477 setps: 27 count: 790\n",
      "reward: 45.61653355559392 setps: 51 count: 841\n",
      "reward: 29.61211119373184 setps: 38 count: 879\n",
      "reward: 16.881547570468683 setps: 21 count: 900\n",
      "reward: 22.884038730108298 setps: 34 count: 934\n",
      "reward: 34.08033349226899 setps: 30 count: 964\n",
      "reward: 18.96135275699635 setps: 29 count: 993\n",
      "avg rewards: 30.9334367943646\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:14 Loss:0.34592\n",
      "Epoch:20 Batch:14 Loss:0.10934\n",
      "Epoch:40 Batch:14 Loss:0.08191\n",
      "Epoch:60 Batch:14 Loss:0.06264\n",
      "Epoch:80 Batch:14 Loss:0.05286\n",
      "Epoch:100 Batch:14 Loss:0.04161\n",
      "Epoch:120 Batch:14 Loss:0.03724\n",
      "Epoch:140 Batch:14 Loss:0.03965\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.042\n",
      "Epoch:10 Batch:8 Loss:0.041\n",
      "Epoch:20 Batch:8 Loss:0.042\n",
      "Epoch:30 Batch:8 Loss:0.042\n",
      "Epoch:40 Batch:8 Loss:0.041\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 48.02790974402449 setps: 43 count: 43\n",
      "reward: 28.508722321499953 setps: 49 count: 92\n",
      "reward: 35.91581088573585 setps: 37 count: 129\n",
      "reward: 37.98512244368613 setps: 38 count: 167\n",
      "reward: 54.77080727530991 setps: 45 count: 212\n",
      "reward: 42.95522129827586 setps: 39 count: 251\n",
      "reward: 14.698602025976285 setps: 22 count: 273\n",
      "reward: 53.38066893003415 setps: 45 count: 318\n",
      "reward: 80.13463314241383 setps: 64 count: 382\n",
      "reward: 5.554099682411464 setps: 35 count: 417\n",
      "reward: 14.099568411584187 setps: 18 count: 435\n",
      "reward: 12.561018863008938 setps: 23 count: 458\n",
      "reward: 22.573917981664998 setps: 36 count: 494\n",
      "reward: 18.869323692834584 setps: 23 count: 517\n",
      "reward: 30.944549224227366 setps: 43 count: 560\n",
      "reward: 22.024354632833276 setps: 38 count: 598\n",
      "reward: 32.3278864065127 setps: 40 count: 638\n",
      "reward: 3.2189216569749983 setps: 20 count: 658\n",
      "reward: 30.574660400615535 setps: 32 count: 690\n",
      "reward: 23.677614857153085 setps: 34 count: 724\n",
      "reward: 30.483453610632576 setps: 32 count: 756\n",
      "reward: 21.134831948953796 setps: 31 count: 787\n",
      "reward: 45.23204997317952 setps: 44 count: 831\n",
      "reward: 21.45204721239862 setps: 40 count: 871\n",
      "reward: 21.084435088381 setps: 39 count: 910\n",
      "reward: 44.338244463528085 setps: 49 count: 959\n",
      "reward: 33.51961976097519 setps: 39 count: 998\n",
      "avg rewards: 30.742522071660236\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.33737\n",
      "Epoch:20 Batch:15 Loss:0.10831\n",
      "Epoch:40 Batch:15 Loss:0.06576\n",
      "Epoch:60 Batch:15 Loss:0.05196\n",
      "Epoch:80 Batch:15 Loss:0.04599\n",
      "Epoch:100 Batch:15 Loss:0.04357\n",
      "Epoch:120 Batch:15 Loss:0.03950\n",
      "Epoch:140 Batch:15 Loss:0.04255\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.039\n",
      "Epoch:10 Batch:8 Loss:0.037\n",
      "Epoch:20 Batch:8 Loss:0.038\n",
      "Epoch:30 Batch:8 Loss:0.035\n",
      "Epoch:40 Batch:8 Loss:0.035\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 71.07258786156017 setps: 64 count: 64\n",
      "reward: 14.789881155641341 setps: 18 count: 82\n",
      "reward: 48.451041122319296 setps: 39 count: 121\n",
      "reward: 20.89160010410851 setps: 26 count: 147\n",
      "reward: 36.05137514639646 setps: 30 count: 177\n",
      "reward: 38.16874738563057 setps: 35 count: 212\n",
      "reward: 46.613150087815285 setps: 44 count: 256\n",
      "reward: 40.913211972180584 setps: 33 count: 289\n",
      "reward: 54.03251074965083 setps: 50 count: 339\n",
      "reward: 49.89106439627649 setps: 36 count: 375\n",
      "reward: 22.21740554189746 setps: 23 count: 398\n",
      "reward: 73.83979854107169 setps: 51 count: 449\n",
      "reward: 43.070120050475815 setps: 39 count: 488\n",
      "reward: 19.303627471256192 setps: 24 count: 512\n",
      "reward: 22.233088389928163 setps: 21 count: 533\n",
      "reward: 71.50545652130677 setps: 48 count: 581\n",
      "reward: 27.133450409617225 setps: 50 count: 631\n",
      "reward: 36.18550045702577 setps: 33 count: 664\n",
      "reward: 40.75529179307924 setps: 37 count: 701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 24.66944161568535 setps: 52 count: 753\n",
      "reward: 32.669464403926405 setps: 49 count: 802\n",
      "reward: 29.50847911273013 setps: 28 count: 830\n",
      "reward: 70.1829686164987 setps: 48 count: 878\n",
      "reward: 42.58366817288188 setps: 37 count: 915\n",
      "reward: 51.06458190430423 setps: 47 count: 962\n",
      "reward: 26.631256823425062 setps: 28 count: 990\n",
      "avg rewards: 40.554952684872674\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:16 Loss:0.33012\n",
      "Epoch:20 Batch:16 Loss:0.09389\n",
      "Epoch:40 Batch:16 Loss:0.06667\n",
      "Epoch:60 Batch:16 Loss:0.05395\n",
      "Epoch:80 Batch:16 Loss:0.04349\n",
      "Epoch:100 Batch:16 Loss:0.03748\n",
      "Epoch:120 Batch:16 Loss:0.03846\n",
      "Epoch:140 Batch:16 Loss:0.03742\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.038\n",
      "Epoch:10 Batch:8 Loss:0.038\n",
      "Epoch:20 Batch:8 Loss:0.035\n",
      "Epoch:30 Batch:8 Loss:0.034\n",
      "Epoch:40 Batch:8 Loss:0.035\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 31.113618199476328 setps: 27 count: 27\n",
      "reward: 16.408854872730448 setps: 21 count: 48\n",
      "reward: 57.4192100217959 setps: 55 count: 103\n",
      "reward: 19.582169829106718 setps: 23 count: 126\n",
      "reward: 52.77402042121684 setps: 36 count: 162\n",
      "reward: 55.45850881294317 setps: 38 count: 200\n",
      "reward: 110.22629205741977 setps: 74 count: 274\n",
      "reward: 37.196023173279535 setps: 33 count: 307\n",
      "reward: 47.89935186668589 setps: 37 count: 344\n",
      "reward: 40.94792643461259 setps: 34 count: 378\n",
      "reward: 30.914565523261263 setps: 31 count: 409\n",
      "reward: 41.17806873441296 setps: 41 count: 450\n",
      "reward: 59.81750197224029 setps: 52 count: 502\n",
      "reward: 66.95898587858682 setps: 50 count: 552\n",
      "reward: 37.0608788528858 setps: 47 count: 599\n",
      "reward: 42.517746697981785 setps: 42 count: 641\n",
      "reward: 20.787893180460372 setps: 20 count: 661\n",
      "reward: 56.88941518109204 setps: 41 count: 702\n",
      "reward: 35.78231433180918 setps: 29 count: 731\n",
      "reward: 46.01246560378204 setps: 34 count: 765\n",
      "reward: 73.83885842218878 setps: 59 count: 824\n",
      "reward: 42.143805266961856 setps: 32 count: 856\n",
      "reward: 18.879845550080063 setps: 22 count: 878\n",
      "reward: 14.479183703078892 setps: 21 count: 899\n",
      "reward: 42.81653514596984 setps: 34 count: 933\n",
      "reward: 22.1897128567667 setps: 25 count: 958\n",
      "avg rewards: 43.126682791954835\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:17 Loss:0.32381\n",
      "Epoch:20 Batch:17 Loss:0.08266\n",
      "Epoch:40 Batch:17 Loss:0.06132\n",
      "Epoch:60 Batch:17 Loss:0.04810\n",
      "Epoch:80 Batch:17 Loss:0.04246\n",
      "Epoch:100 Batch:17 Loss:0.03930\n",
      "Epoch:120 Batch:17 Loss:0.03552\n",
      "Epoch:140 Batch:17 Loss:0.03943\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.037\n",
      "Epoch:10 Batch:8 Loss:0.034\n",
      "Epoch:20 Batch:8 Loss:0.032\n",
      "Epoch:30 Batch:8 Loss:0.032\n",
      "Epoch:40 Batch:8 Loss:0.034\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 42.16594399171153 setps: 31 count: 31\n",
      "reward: 38.71703966015921 setps: 32 count: 63\n",
      "reward: 64.75786095177318 setps: 43 count: 106\n",
      "reward: 72.58838042083225 setps: 49 count: 155\n",
      "reward: 85.25473540672537 setps: 56 count: 211\n",
      "reward: 68.14492274073274 setps: 46 count: 257\n",
      "reward: 74.489222644252 setps: 50 count: 307\n",
      "reward: 60.52266948926117 setps: 42 count: 349\n",
      "reward: 68.46984522068524 setps: 41 count: 390\n",
      "reward: 79.20789166285101 setps: 50 count: 440\n",
      "reward: 66.15128528895585 setps: 44 count: 484\n",
      "reward: 81.5703540610586 setps: 51 count: 535\n",
      "reward: 55.83360179913724 setps: 39 count: 574\n",
      "reward: 68.26261422204989 setps: 50 count: 624\n",
      "reward: 60.95589282356379 setps: 41 count: 665\n",
      "reward: 57.9352862408603 setps: 48 count: 713\n",
      "reward: 58.44555782734097 setps: 37 count: 750\n",
      "reward: 52.886120600093285 setps: 46 count: 796\n",
      "reward: 64.73866866961907 setps: 40 count: 836\n",
      "reward: 73.89547420106975 setps: 46 count: 882\n",
      "reward: 47.097745200966905 setps: 36 count: 918\n",
      "reward: 51.96709182641352 setps: 37 count: 955\n",
      "reward: 22.08227137805952 setps: 23 count: 978\n",
      "avg rewards: 61.57132505774662\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:18 Loss:0.33813\n",
      "Epoch:20 Batch:18 Loss:0.08949\n",
      "Epoch:40 Batch:18 Loss:0.06010\n",
      "Epoch:60 Batch:18 Loss:0.05375\n",
      "Epoch:80 Batch:18 Loss:0.04251\n",
      "Epoch:100 Batch:18 Loss:0.03624\n",
      "Epoch:120 Batch:18 Loss:0.03442\n",
      "Epoch:140 Batch:18 Loss:0.03630\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.030\n",
      "Epoch:10 Batch:8 Loss:0.031\n",
      "Epoch:20 Batch:8 Loss:0.034\n",
      "Epoch:30 Batch:8 Loss:0.032\n",
      "Epoch:40 Batch:8 Loss:0.031\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 35.59257573424402 setps: 29 count: 29\n",
      "reward: 37.55724787461222 setps: 31 count: 60\n",
      "reward: 65.71680150313622 setps: 45 count: 105\n",
      "reward: 35.405707074010564 setps: 32 count: 137\n",
      "reward: 83.4145293505804 setps: 57 count: 194\n",
      "reward: 34.957242829201284 setps: 34 count: 228\n",
      "reward: 45.11221627904742 setps: 32 count: 260\n",
      "reward: 32.85188391480914 setps: 29 count: 289\n",
      "reward: 32.67751142764901 setps: 29 count: 318\n",
      "reward: 11.31450556621421 setps: 17 count: 335\n",
      "reward: 26.385831037604664 setps: 31 count: 366\n",
      "reward: 43.80471459206164 setps: 31 count: 397\n",
      "reward: 71.21759342715522 setps: 46 count: 443\n",
      "reward: 17.62624127272138 setps: 20 count: 463\n",
      "reward: 72.3994449466758 setps: 46 count: 509\n",
      "reward: 50.511713821826554 setps: 36 count: 545\n",
      "reward: 40.240669540694206 setps: 30 count: 575\n",
      "reward: 44.50381682573061 setps: 33 count: 608\n",
      "reward: 37.30025513700093 setps: 35 count: 643\n",
      "reward: 32.514860241068526 setps: 31 count: 674\n",
      "reward: 40.39775430090231 setps: 33 count: 707\n",
      "reward: 51.977381639792284 setps: 50 count: 757\n",
      "reward: 47.498293208013635 setps: 34 count: 791\n",
      "reward: 39.95814699625625 setps: 35 count: 826\n",
      "reward: 27.421146707914886 setps: 27 count: 853\n",
      "reward: 37.90446157905972 setps: 30 count: 883\n",
      "reward: 43.82605666544843 setps: 35 count: 918\n",
      "reward: 56.89614391157955 setps: 39 count: 957\n",
      "reward: 39.04951472182 setps: 31 count: 988\n",
      "avg rewards: 42.621871107821775\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:19 Loss:0.30766\n",
      "Epoch:20 Batch:19 Loss:0.08514\n",
      "Epoch:40 Batch:19 Loss:0.05656\n",
      "Epoch:60 Batch:19 Loss:0.04202\n",
      "Epoch:80 Batch:19 Loss:0.04119\n",
      "Epoch:100 Batch:19 Loss:0.03798\n",
      "Epoch:120 Batch:19 Loss:0.03328\n",
      "Epoch:140 Batch:19 Loss:0.03222\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.033\n",
      "Epoch:10 Batch:8 Loss:0.034\n",
      "Epoch:20 Batch:8 Loss:0.032\n",
      "Epoch:30 Batch:8 Loss:0.032\n",
      "Epoch:40 Batch:8 Loss:0.033\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 44.32666265472508 setps: 40 count: 40\n",
      "reward: 53.26968496117334 setps: 52 count: 92\n",
      "reward: 63.717886968779204 setps: 42 count: 134\n",
      "reward: 54.156199991882026 setps: 48 count: 182\n",
      "reward: 48.66759926136875 setps: 40 count: 222\n",
      "reward: 62.67794876790286 setps: 45 count: 267\n",
      "reward: 62.0678846514522 setps: 46 count: 313\n",
      "reward: 37.67222365078341 setps: 34 count: 347\n",
      "reward: 20.54627095235337 setps: 20 count: 367\n",
      "reward: 61.372534501862546 setps: 41 count: 408\n",
      "reward: 66.23575448586782 setps: 40 count: 448\n",
      "reward: 39.17414758936357 setps: 31 count: 479\n",
      "reward: 70.46383204447193 setps: 44 count: 523\n",
      "reward: 27.274720350374995 setps: 26 count: 549\n",
      "reward: 45.655988501686075 setps: 40 count: 589\n",
      "reward: 79.2176780386275 setps: 50 count: 639\n",
      "reward: 61.80276863399049 setps: 45 count: 684\n",
      "reward: 54.43834853736917 setps: 42 count: 726\n",
      "reward: 49.086337559025544 setps: 37 count: 763\n",
      "reward: 57.46550822650605 setps: 42 count: 805\n",
      "reward: 18.41681319604104 setps: 21 count: 826\n",
      "reward: 81.2149458790489 setps: 49 count: 875\n",
      "reward: 56.842815468301694 setps: 52 count: 927\n",
      "reward: 20.654585494920322 setps: 23 count: 950\n",
      "reward: 65.1834675144448 setps: 41 count: 991\n",
      "avg rewards: 52.06410431529291\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:20 Loss:0.28344\n",
      "Epoch:20 Batch:20 Loss:0.07961\n",
      "Epoch:40 Batch:20 Loss:0.05595\n",
      "Epoch:60 Batch:20 Loss:0.04183\n",
      "Epoch:80 Batch:20 Loss:0.03336\n",
      "Epoch:100 Batch:20 Loss:0.03575\n",
      "Epoch:120 Batch:20 Loss:0.03075\n",
      "Epoch:140 Batch:20 Loss:0.03177\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.033\n",
      "Epoch:10 Batch:8 Loss:0.031\n",
      "Epoch:20 Batch:8 Loss:0.032\n",
      "Epoch:30 Batch:8 Loss:0.032\n",
      "Epoch:40 Batch:8 Loss:0.032\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "env_list = [\"Pendulum-v0\", \"BipedalWalker-v3\", \"Walker2DBulletEnv-v0\", \"HopperBulletEnv-v0\", \"HalfCheetahBulletEnv-v0\", \"AntBulletEnv-v0\", \"HumanoidBulletEnv-v0\"]\n",
    "\n",
    "runs = 50\n",
    "inv_samples = 1000\n",
    "max_steps = 1000\n",
    "expert_path='experts/'\n",
    "weight_path=\"weights/\"\n",
    "        \n",
    "test_rewards_envs = []\n",
    "record_folder = \"records/forward/\"\n",
    "init_seeds = [0,2,4,5]\n",
    "itr_per_env = len(init_seeds)\n",
    "\n",
    "for itr_id in range(itr_per_env):\n",
    "    seed = init_seeds[itr_id]\n",
    "    for en in env_list[1:]:\n",
    "        print(\"############# start \"+en+\" training ###################\")\n",
    "\n",
    "        ENV_NAME = en#env_list[3]\n",
    "        env=ENV_NAME\n",
    "        \n",
    "        DEMO_DIR = os.path.join(expert_path, env+'.pkl')\n",
    "        M = inv_samples\n",
    "\n",
    "        record_fn = record_folder + ENV_NAME + str(itr_id) + \".txt\"\n",
    "\n",
    "        \"\"\"load demonstrations\"\"\"\n",
    "        demos = load_demos(DEMO_DIR)\n",
    "\n",
    "        \"\"\"create environments\"\"\"\n",
    "        env = gym.make(ENV_NAME)\n",
    "        obs_dim, act_dim = env.observation_space.shape[0], env.action_space.shape[0]\n",
    "\n",
    "        \"\"\"init random seeds for reproduction\"\"\"\n",
    "        torch.manual_seed(seed)\n",
    "        env.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "\n",
    "        \"\"\"init models\"\"\"\n",
    "        policy = policy_multinomial(env.observation_space.shape[0],64,env.action_space.shape[0], n_heads=10, do_rate=0.01)#.cuda()\n",
    "        \n",
    "        #policy = MDN(obs_dim, out_features=act_dim, n_hidden=64,  num_gaussians=3)\n",
    "        #policy = policy_continuous(env.observation_space.shape[0],64,env.action_space.shape[0], uncertain=True)#.cuda()\n",
    "        #inv_model = MDN(in_features=obs_dim+act_dim, out_features=obs_dim, n_hidden=32,  num_gaussians=10)\n",
    "        inv_model = forward_dynamics_continuous(env.observation_space.shape[0],100,env.action_space.shape[0], uncertain=True, do_rate=0.2)#.cuda()\n",
    "        #inv_model = policy_multinomial(obs_dim+act_dim,100,obs_dim, n_heads=15)#.cuda()\n",
    "    \n",
    "        inv_model_best = None\n",
    "        reward_best = -1000\n",
    "\n",
    "        inv_dataset_list = []\n",
    "        use_policy = False\n",
    "\n",
    "        transitions = []\n",
    "        test_rewards = []\n",
    "        for steps in range(runs):\n",
    "            print('######## STEP %d #######'%(steps+1))\n",
    "            ### GET SAMPLES FOR LEARNING INVERSE MODEL\n",
    "            print('Collecting transitions for learning inverse model....')\n",
    "            if steps > 0:\n",
    "                use_policy = True\n",
    "\n",
    "\n",
    "            trans_samples, avg_reward = gen_inv_samples(env, policy.cpu(), M, 'continuous', use_policy, max_steps=max_steps)\n",
    "            transitions = transitions+trans_samples\n",
    "\n",
    "            f = open(record_fn, \"a+\")\n",
    "            f.write(str(avg_reward) + \"\\n\")\n",
    "            f.close()\n",
    "\n",
    "            \"\"\"\n",
    "            if len(transitions) > 92000:\n",
    "                transitions = random.sample(transitions,92000)\n",
    "            \"\"\"\n",
    "            test_rewards.append(avg_reward)\n",
    "            print('Done!', np.array(transitions).shape)\n",
    "\n",
    "            ### LEARN THE INVERSE MODEL\n",
    "            inv_model = policy_multinomial(obs_dim+act_dim,100,obs_dim, n_heads=5, do_rate=0.01)#.cuda()\n",
    "            #inv_model = forward_dynamics_continuous(env.observation_space.shape[0],100,env.action_space.shape[0], uncertain=True, do_rate=0.08)#.cuda()\n",
    "            #inv_model = MDN(in_features=obs_dim+act_dim, out_features=obs_dim, n_hidden=32,  num_gaussians=10)#forward_dynamics_continuous(env.observation_space.shape[0],100,env.action_space.shape[0], uncertain=True)#.cuda()\n",
    "\n",
    "            print('Learning inverse model....')\n",
    "            inv_model = inv_model_training(transitions, inv_model,  ep_num=150)\n",
    "\n",
    "            ### GET ACTIONS FOR DEMOS\n",
    "            inv_model.cpu()\n",
    "            print('Getting labels for demos....')\n",
    "            trajs = get_state_labels(demos)\n",
    "            print('Done!')\n",
    "\n",
    "\n",
    "            ### PERFORM BEHAVIORAL CLONING\n",
    "            policy = train_bc(trajs, policy, inv_model, ep_num=50)\n",
    "\n",
    "        torch.save(policy, weight_path+ENV_NAME+str(itr_id)+'.pt')\n",
    "        test_rewards_envs.append(test_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "statewide-virginia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:64: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:64: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:64: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:64: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:64: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:64: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAABj6UlEQVR4nO2deZwcVbmwn7d6nTUzWcm+kARIAgkQSEIAQXZQQEUUUXADUVG5XPeLIq6gXnG9CLiBCyIKH7iAIBDWEEjCGrKQfU8mk9mnt6o63x9VPenp6Znp7unq7pk5z+/X091V1VWnaqrOe971iFIKjUaj0WgAjFI3QKPRaDTlgxYKGo1Go+lCCwWNRqPRdKGFgkaj0Wi60EJBo9FoNF1ooaDRaDSaLrRQ0JQlInKaiOxM+b5VRM4sZZsKhYhMExElIn73+zIR+Xip26XRgBYKmgIjIl8RkYfTlr3Vy7L3F7d1ICIhEfm1iGwTkTYReUVEzktZf5qI2CLS7r52ishfROSEtP0oEelwtzkgIveISF0R2q9EZGYf7U2+lnjdlrR2nSEi60SkU0SeFJGpxTy+pnBooaApNE8DJ4mID0BExgMB4Ni0ZTPdbYuGOzL3AzuAtwEjgBuAv4jItJRNdyulqoEaYDGwDnhGRM5I2+V8d7sZQD3wDU9PoHd2K6Wq017Li3VwERkN3A98DRgJrATuLdbxNYVFCwVNoXkJRwgscL+fAjwJrE9btgk4R0TWuiP2zSLyiWwOICJHicgWEbnM/f4Od8TfLCLPi8gxKdtuFZEvichrQAcQU0p9Qym1VSllK6X+AWwBjk8/jnLYqZT6OvAr4JZM7VFKtQIPAXPSjntmyvdviMgfsjy/j7rXpUlE/p0cdYtIUoi+6moD78tiX8tE5Fsi8px7nR91O3FE5GERuTZt+1dF5N1py0LutZ2XsmyMiEREZCzwbmCNUuo+pVQURzjOF5EjszlfTXmhhYKmoCil4sAK4FR30anAM8CzacueBvYD7wBqgY8At4rIcX3t313/b+AzSql7RORY4DfAJ4BRwO3AQyISSvnZZcAFQJ1Sykzb3zhgNrCmn1O7HzhORKoytKkeuBh4oZ999IuIXAR8FaejHYNz7e4BUEolr998VxvIdjT+AZzrOxYIAp93l9+Dc22Sx54DTAX+mfpjpVQM5/wvS1l8KfCUUmo/MBd4NWX7DhyhPzfL9mnKCC0UNF7wFIcEwCk4HdszacueUkr9Uym1yR2RPwU86q7rjVNwRuRXuCN8gKuB25VSK5RSllLqLiCGY/ZJ8lOl1A6lVCR1ZyISAP4I3KWUWtfPOe0GBKhLWbZaRJqBA8AUHIE0UK4BvqeUWusKsO8CC/qx0U9wR/Kpr1Th9Vul1Ab3/P/CIY3tgbR9Xw7c7wqBdP4EpPqAPuAuA6gGWtK2b8Exv2kGGVooaLzgaeBkERkJjFFKvQU8j+NrGAnMA54WkfNE5AUROeh2rucDo/vY7zXA80qpZSnLpgL/ndohApOBCSnb7EjfkYgYwO+BOHBt+voMTAQU0Jyy7DilVB0QBm7D8TuEs9hXX0wFfpJyLgdxhNHEPn6zWylVl/bqSFm/N+VzJ04njlKqDUcrSHb2l+EISURkTYrTOmkCrBSRRa7/ZQGOUAFox9H2UqkF2nI4b02ZoIWCxguW4zhxrwKegy67+2532W739Tfgh8A4t3P9F04H2BvXAFNE5NaUZTuA76R1iJVKqXtStulWClhEBPg1MA54j1IqkcU5vQtYndbZ4p5bAsfnMB1H4IHjv6hM2eywLI6RPJ9PpJ1PhVLq+Sx/nyv3AJe50UphnM4fpdTcFKf1M0opC0fLuMx9/cMVKuCY3uYnd+hqKYfTv0lOU4ZooaApOK6ZYiVwPY7ZKMmz7rKncWzbIaABMN2w0LP72XUbcC5wqojc7C67E7jGHcGKiFSJyAUi0pfp4jbgKOCd6SalVNz9TRSRG4GP49j6M23nw7HZR4DN7uJXgPeLSEBEFgKX9HNuSX4JfEVE5rr7HiEi701Zvw8n2qlQ/AtHO/kmcK9Syu5j2z8B78MxM/0pZfkDwDwReY+rKX0deC0Lk5ymDNFCQeMVT+E4Np9NWfaMu+xpd5T5WZzRZxOOjfqh/naqlGoGzgLOE5FvKaVW4mgfP3f3sxH4cG+/d+3nn8Axf+xNMZFcnrLZBBFpxzGLvAQcDZymlHo0bXevuts1AVcC71JKHXTXfQ1ntNwE3ET3TrSv83sAJ8rpzyLSCrwBnJeyyTeAu1zz0qWp7U17vSfL4yWdyGf210al1AocDWgC8HDK8gbgPcB3cM53Ed39D5pBhOhJdjQajUaTRGsKGo1Go+lCCwWNRqPRdKGFgkaj0Wi60EJBo9FoNF34S92AgTB69Gg1bdq0UjdDo9FoBhWrVq06oJQak2ndoBYK06ZNY+XKlaVuhkaj0QwqRGRbb+u0+Uij0Wg0XWihoNFoNJoutFDQaDQaTRdaKGg0Go2mCy0UNBqNRtOFFgoajUaj6UILBY1Go9F0oYWCRqPRaLrQQkGj0WgGGZbt3ZQHWihoNBrNICOasDzb96AuczEQlu1YVuomaDQaTV60RhJcOPssT/atNQWNRqMZZMTNvqbSHhhaKGg0Gs0gI2FpoaDRaDQawLYVcS0UNIUmEjdL3QSNRpMHpq1Q3gUfaaEwXGmJJErdBI1GkwcxD/0JoIXCsKUlojUFjWYw4qXpCLRQGJYkLJtowsL2MAFGo9F4Q9z0LkcBtFAYlsRNGwVEPb65NBpN4fEyHBW0UBiWJG+qSMLbm0uj0RSehBYKmkKTtEl6mSqv0Wi8IW55a/bVQmEY0qUp6LBUjWZQ4XWOAmihMCyJdZmPtKag0QwmErb3Jl8tFIYhyRR5y/beaaXRaApH3PQ+YtBToSAiW0XkdRF5RURWustGishjIvKW+17vLhcR+amIbBSR10TkOC/bNlxRSnUTBNqvoNEMHrw2HUFxNIXTlVILlFIL3e9fBh5XSs0CHne/A5wHzHJfVwO3FaFtw46EpUgda2gTkkYzeIgX4XkthfnoIuAu9/NdwMUpy+9WDi8AdSIyvgTtG9Kkp8hrTUGjGTwMBU1BAY+KyCoRudpdNk4ptcf9vBcY536eCOxI+e1Od1k3RORqEVkpIisbGhq8aveQJf2m6oxroaDRDBa8zlEA72deO1kptUtExgKPici61JVKKSUiOXlOlFJ3AHcALFy4UNdpyJF09TNu2ti2wjCkRC3SaDTZEhvsmoJSapf7vh94ADgR2Jc0C7nv+93NdwGTU34+yV2mKSDpmoIud6HRDA5sW5HwOHENPBQKIlIlIjXJz8DZwBvAQ8CV7mZXAg+6nx8CrnCjkBYDLSlmJk2ByKR+RnW5C42m7ClGjgJ4az4aBzwgIsnj/Ekp9YiIvAT8RUQ+BmwDLnW3/xdwPrAR6AQ+4mHbhi2Z1E8dgaTRlD/xIg3ePBMKSqnNwPwMyxuBMzIsV8CnvWqPpnf1M6KdzRpN2RMvUql7ndE8jOgtnC2S0DWQNJpypxg5CqCFwrCit5IWutyFRlP+FCNHAbRQGFb01fHrJDaNprwpRo4CaKEwrOhrpKGFgkZT3hQjRwG0UBhW9KUp6AgkjaZ8KVaOAmihMKxIr3uUSiSufQoaTblSrBwF0EJhWBG3etcGYqaFXaSQN41GkxvFylEALRSGDZZt05dJUpe70GjKl2LlKIAWCsOGbEJOdbmL7OnU81trikixchRAC4VhQzaRC9rZnD37WmOlboJmGFGsHAXQQmHYkMhibldd7iI7TNumJZLQCX+aolGsHAXQQmHYkJ35SAuFbIjEnOvUHtMmJE1xKFaOAmihMGzI5qYybaVHv1nQ4foT2qKJErdEMxwoZo4CaKEwbMg2pE1rC/2TnMJUawqaYlDMHAXQQmHY0FeOQipaKPRPUlNIWEpfL43nFDNHAbRQGBYkLJtsw5x1BFLfxEyrW75He1RrCxpvKaY/AbRQGBbk4ifQ5S76piPWXWi2aROSxmOKGXkEWigMC3IRCrrcRd+kJ621xRI4kwZqNN5QzBwF0EJhWJDLTaXLXfRNuqZg24cczxqNFxQ7IlALhWFArjeVLneRGdvO7FjWUUgaL9FCQVNwclU/tbM5M5GERSZDkc5X0HiFbSsSRTbnaqEwDMh1pKHLXWSmo5cieJ1x7YfReEOx/QmghcKQRylFIscbS8feZ6Yzlvm62Ao6tAlJ4wHFjjwCLRSGPAlLZZ2jkESXu8hMb5oC6NBUjTcUO0cBtFAY8uTbuWttoTtx0+6z/kybTmLTeIDWFDQFJ1+bpBYK3elvUp1IwsIswahOM7TRPgVNwYnlmXOgI5C6k00ugg5N1RSaUphxtVAY4uR7U+lyF93pzKLD1yYkTaEZkkJBRHwi8rKI/MP9Pl1EVojIRhG5V0SC7vKQ+32ju36a120bDuRrk9TlLg5h24rOLDQnrSloCkkpchSgOJrC54C1Kd9vAW5VSs0EmoCPucs/BjS5y291t9MMkHyjF3S5i0NETSurCK6YaeuoLU3BKIU/ATwWCiIyCbgA+JX7XYC3A391N7kLuNj9fJH7HXf9Ge72mjwZ6IxNutyFQ2/5CZlo1dnNmgJRqgGG15rCj4EvAsmzGwU0K6WSevZOYKL7eSKwA8Bd3+Ju3w0RuVpEVorIyoaGBg+bPvgZ6IxN2tns0Fd+Qjp6fgVNoRhymoKIvAPYr5RaVcj9KqXuUEotVEotHDNmTCF3PeQY6IxNutyFQy5VUHUpbU2hKEWOAoDfw30vBS4UkfOBMFAL/ASoExG/qw1MAna52+8CJgM7RcQPjAAaPWzfkGeg2ZA6VwFMyyaWw8Np2Y6GVRn08tHSDAdyue8KiWeaglLqK0qpSUqpacD7gSeUUpcDTwKXuJtdCTzofn7I/Y67/gmlh1wDYqAjDV3uIr+5EnQUkqYQDDnzUR98CbheRDbi+Ax+7S7/NTDKXX498OUStG1IUYiRxnDXFnLxJyTR+QqaQjAUzUddKKWWAcvcz5uBEzNsEwXeW4z2DBcKMdKIJixqKwIFaM3gJB9NoSNmYtsKw9DBc5r8KFWOAuiM5iFNIUw/wzkCSSnVb82jTNgKOrSTXjMASmU6Ai0UhiyWbWMWYKQxnMtdxEybfJ/Ndp2voBkApfTlaaEwRCnUTTWcy13kYzpKoudX0AwELRQ0BadQk3Mo8q+0OtgZyGxqkbiFOcDkQc3wJdfZEguJFgpDlIRZuNF9ZJiWuxiIpqDQoama/ClVjgJooTBkKaT6ORydzbatBhyOq0teaPJFO5o1BaeQN9VwLHfRGbcYqK6l8xU0+aJ9CpqCU8ibajgmsOUTipqOLqWtyQfbVgWJHMwXLRSGKDGrcB35cCx3Uag8A+1X0ORKoYJE8kULhSGIadkUOvAlNsy0hWym38yGNp2voMmRUpW3SKKFwhDEi5HGcHI2x027YCUGtLNZkyul1sq1UBiCeDHSGE5CoRD+hCQJWw2ra6cZOKWMPAItFIYkXow0hlO5i0LXLdLagiYXtKagKTheJL4Mp3IXkQI7h3XJC00uaE1BU3C8uKmGS7kL21Z0Ftjc0x5LDBuBqhk4WlPQFByvbqrhUO4iYloUuv+27eHlk9HkT6GqGw8ELRSGGEopz4ppDYeOLRLz5hx1aKomG+JW6TVKLRSGGAlLFXykm2Q4ZDbnM/1mNgw1Z/NwMCWWglKbjkALhSGHlyV3h0MNpIFURu2LjriFNYRKabdGhpaQKxdKnbgGWigMObwsuWva3pmmygHTsj27fgro8Mg0VQpaIvFSN2FIUurII9BCYcgR91itjw5hbcHreZWHSmiqM3e1nkTIC7T5SFNwvB5pRIawLblQ9Y56Y6j4FeKWja0gNgyi0YpNKSfXSaKFwhDD65HGUPYrFDo/IZ1IwhoS5reoKwzKoQMbapTD/eHva6WI/B16n2tEKXVhwVukGRDxAk7DmYnoEB0dKqU8izxKpT1qUl8V9Pw4XpKMPBpulXO9phxyFKAfoQD80H1/N3AY8Af3+2XAPq8apckP21aem4+iCafchWGIp8cpNlGz8OXGM9EeGwJCoQw0haF4D5ZDjgL0IxSUUk8BiMj/KqUWpqz6u4is9LRlmpxJFKFXS5a7qAj2N54YXHgViprOUJiiM6khlFJrbI0lqKsY3MI1nXJwMkP2PoUqEZmR/CIi04Eqb5qkyZd4kR7SoVjuwmsnc5K4ZQ/6xK+o23nFLQulSjO6bekcehni5ZCjAP2bj5JcBywTkc2AAFOBq71qlCY/ijWN31DMbC6WpgCOthCq9hXteIXEtA7ZvW3lZNAH/cU347THTOKmTdA/dGJlSj0NZ5J+hYKIGMAIYBZwpLt4nVIq1s/vwsDTQMg9zl+VUje6WsafgVHAKuBDSqm4iISAu4HjgUbgfUqprXmd1TClWCONcqiBpJRCpDCdkWXbRRV07VGT0dWhoh2vkKT7EWIJq+gds2nbJCxVkmN7SbloCv1eUaWUDXxRKRVTSr3qvvoUCC4x4O1KqfnAAuBcEVkM3ALcqpSaCTQBH3O3/xjQ5C6/1d1OkwPFcvyVQwLblsbOggmnzrjde4idB7TFEiUzuwyUHkKhBB1Z0tE91HJmyiXEN1vz0X9E5PPAvUBHcqFS6mBvP1DOXd/ufg24LwW8HfiAu/wu4BvAbcBF7meAvwI/FxFRHj09Cy69rsey/e84jd1XXIwRiXLMlV/usX7ve89l73vPJXCwhbnX3Nhj/a4PXkjDhW8ntHs/R1333R7rd1x1KY1nnUTFpu0c8ZUf9Vi/7TMfoumU46les5GZN/28x/rNX/w4rQvnUbvyDWZ8/1c91jde+xGap09l8kuvsuiu+3qsf/wLn6Rp6kSmP/sSx//5wR7rH/nadbSPG83s/zzLMf/vkR7r//HtLxKtq2XW3x/nmMefJj3447W7bsauCDPh7v/H2H8s6/H7V/7yYwAm334vox5f3m2dFQ7x+t3OOGDqT+6m/rnV3dYn6mtZc/s3AZjy3TuYueI1BAgHffhEiI0fw9qf/A8AM7/xc6rf3Njt953TJ7Hhls8DMPtLP6Ryy86udXHTZtL0qTx1nTM+OeemW6lpaOz2+z1zj+C5T34IgAu+egsVrW3d1m8//hhe/MilAFz839/EH+teBmLzSQtZ/YGLAXjXp26gIuDDl3IBB8u9V/PS61xy62+7lgd8BiG/wcYbr6V97kzqn1nF1J/9vsfv13/veiKHT2HUY88z+c6/9Fi/9sdfJTZhLGMeeoKJf3iox/o1v7yJxMgRHHbfI4z688PMNy38hhAOOGa4Yt1702++kxGr13RbP5B7D6B9zkw2fuNa4paV9b3nNwRWXtDjPAtBtkLhfe77p1OWKWBGhm27EBEfjoloJvALYBPQrJRKevV2AhPdzxOBHQBKKVNEWnBMTAfS9nk1rj9jypQpWTZ/eFDMxBdbKYwCmW9yJWZaBHFuwGjc6uoY8sUqwajdslU3oTBYSA+bLMXkQbY65NMYKli2TZm4FJBiqLEiUgc8AHwN+J1rIkJEJgMPK6XmicgbwLlKqZ3uuk3AIqXUgV52y8KFC9XKlflFxi7bsSyv35Urlm3z+q7Woh1vXE2I8XUVRTteKhv2tXVzDPsMOHxMNZV5hsmu2dVCosg9TF1FgGmjB18A39o9rd3MHAGfMHfCiKK2YXNDO61RE0Ng3oQRQyJfIRI3Wb+vvf8NXeoqAnz42Pw1BRFZlZZm0EXWT5GIzAPmAOHkMqXU3dn8VinVLCJPAkuAOhHxu9rCJGCXu9kuYDKwU0T8OM7txow71PSg2IkvzZFESYRC3LR7RApZNmxqaM9LMMRNu+gCAcrDWZ8rtq16xNInLFX0RLJkfoSthk7OTLkkrkGWeQoiciPwM/d1OvB9oM8SFyIyxtUQEJEK4CxgLfAkcIm72ZVA0rj9kPsdd/0TXvkThiLFTnyJmTYdJaj62RLJHJ+eFAy5lqooxTmAc/0G2/wKcSuzQz5aRIevZdvdsvaHStmVcok8guyT1y4BzgD2KqU+AszHGcn3xXjgSRF5DXgJeEwp9Q/gS8D1IrIRx2fwa3f7XwOj3OXXAz29bZpeKcVN1dRZ/Jr6zX0c07Id00IuHb3XRfD6YrB1aL11/sUckKRH6AyVnJlyyVGA7M1HEaWULSKmiNQC+3FMPb2ilHoNODbD8s3AiRmWR4H3ZtkeTRqluKmaOxNMGFE800HctPud88CyYdOBdg4fXU1VqP/bu1iZzJmIxK2s2lgu9Nb5F7NjThekg9EMl4lyKXEB2WsKK11T0J040USrgeV9/kJTVEpxU5m2KurEMb2ZjtKxXcHQn8Zg26qkncpg69B6mz+hmPH16ZVZB9s17I1yEgpZDVOUUp9yP/5SRB4Bal1NQFMmlOqmau6MM6IiULRjZYvt+hhmjK6mOpz5No8krJKGNRaztEYh6K1UdjHNYNEMjm7TsvH7Bndmc9wqn3shW0fz70XkKhE5Uim1VQuE8iNWopuqJZIoisM0G9NROraCzQfae53xrNQTBsVMqyRx/vmS3iEnKea9l8lUNdh8M+mYZZSjANmbj36D4zj+mYhsFpG/icjnPGyXJgdMqzhzAWTCVtAS8d6ElK3pKJ2kYMhUsroYk+r0RTKkcjCQWggvHdsujqaaKSQWBr+zuZwijyBLoaCUehL4Dk7y2Z3AQuCTHrZLkwOljlwoRhRSywCOYSvYcqCdtjTBUg7mm8FShrw/v0ExhELMtDKGxA52v0Kpn990sjUfPQ48h1PuYj1wglLqyL5/pSkWpR5ptEdNT0tsxE2b9gF24LaCLY0dtLqCIWHZZVGAbLB0aL2Zjg6t9/48ejMTDZZr2Bumx1Po5kq25qPXgDgwDzgGmOcmpGnKgFJHLii8nfSkNU/TUTq2gq2uYCgHLQGc8gaDgf7mYy6GgO1N8EQTpZvspxCUm6aQbfTRfwGISA3wYeC3OHM2D86i8EOMchjxHuyMM7rGm9shl6ij/kgKhspgeUxyM1hGuf3dY/0JjYK0oRdNwVZOtnXIXx7/01wp9aAunayEgohcC5yCMwHOVhzH8zPeNUuTC/EyGGl0xi1iplXwB7MQpqN0bAXtsfLojC3XSVvuk8X058wtxsCkLwEaiRf+3isWg1Io4BTB+xGwKqXstaZMKJebqqkjzmEjCmtVLJTpqJzpjJsE/eU7CX1vUT+pxE3b08J4/bUhkrCo8+TI3lNOOQqQffTRD3EmyfkQdBW7m+5lwzTZoZQq6jwKfdHkgV+hkKajcqXU+RL90VshvFQU3mqs/bVhsOYqlFuOAuRWJfVLwFfcRQHgD141SpM9CUuVzWQjMdMuaOy/F6ajciRSJppeb2QbWeRlBFJ/5qvBmqtQ6sjBTGRryHwXTqnsDgCl1G6gxqtGabKnXLSEJM0F1BaGg+kIyj8CKZ7lKDzb7fIhG5/GYCtFDuUXeQTZC4W4O7eBAhCRwTdl1BClHCKPUmnuiBcsPLA5MvRNR3Cofk+5Ug6aQjb3+WA0ISXKLEcBshAKIiLAP0TkdpxZ064C/oOT2awpMfEyK5OQsFWvtYZy2o9ll02EUDEoZ79Cb6Gg6XirKfS/78ES3ptKPn4Yy4bfPwXL1u/3oEVZRB8ppZSIvBdn4ptW4Ajg60qpxzxpkSYnyiEcNZ2DnXFqBlg51ctkuHIkkrAGfM28IntNwZt7USmVVY2owehXyCdycM9BYdNe76wE2YakrgaalVJf8KQVmrwpl3DUVFqiiQGHJw4X01GSch3lmlb20TGm7U0Z67hlZxVMES1jbas38nl+tzUIInDS4aM8aFH2QmERcLmIbMN1NgMopY7xpFWarImXoU3StqE1mqCuMr/Y+4Rl0zGMTEdQHsX5MpHraDRmFl4oZCswI2VmSs2GfHIUtu03mDwKasLeaJbZCoVzPDm6ZkDYdvnkKKRzsDN/odASSfQbFz/UiHmc/JUvuZqEYqZNVYGrnWTrqxgs2eFJ8slR6IzB/hbh9HnetAmyr320zbsmaPLFtFXZdp5tkUTepoRChrUOJiKJ8puzOdeaRl7UQMrFtBZJWINGKOSTo7C9wRk0zDys0K05xOC4ekOQQkTolFs4aioKaM4jz8AxHZV33L5XlGMEUq6aghfO5lxCTQeTszmfHIVt+w3CQcX4eg8a5KKFQgmIJCw2NrSzqaF9QI7icow8SqW5I3dn8XA0HSUpR2dz7ppCYe/JbCOPkgwmoZBrjoJSjpN5yhiF4WHPrYVCCdjfGgWgLWqyfl8rB/PoPAHiZf4AtMetnIXecDUdQfkJhWwK4aUTtwo7t0GuZVwi8fIeKKWS67U90AqdMWHqGG/PUQuFIhNNWN0Kx1k2bD/YyeYDHXk8gOX/AOQyVac5jE1HUH6TxWRTCC8dWzkdeaHIdeQfMy3scikG1g+5Pr/b9jvd9dSx3p6fFgpFZp+rJaTTGkmwfl8rTTloDeWYo5BOU0f2I//mYWw6AqdD9SoBLB/yLVtRSBNOrm1QefymVOT6/G5rEEbXKKrDHjXIRQuFIhI37T7NI5YN2w52svVAR1ahpoNBKERNK+uCby3DpABeX5STsznfshWFDIDIx0cxWGogxXLIUUiYsPugMHWs9+emhUIR2dcazWok3BxJsH5vW59zCdi2IjFI1ORs5lkwLbsgEVmDnXLyK5SFppDHvgaDs9m0bXIp6rqzUbBs8dx0BFooFI24aedmX7cVWxsdrSFTBc1yLLnbG02d/VdObYmaw9p0lKScNIV8I4kKqSnkk6VcToK1N3I2He0X/D7FhJGDWCiIyGQReVJE3hSRNSLyOXf5SBF5TETect/r3eUiIj8VkY0i8pqIHOdV20rB/rZoXpPhdGkNabWABoPpKEnCUv2WrRgOM6xlQyRRPtpSvppCLiGkfeFM8Zn77wZDDaTc/QkGE0cpijENtZeaggn8t1JqDrAY+LSIzAG+DDyulJoFPO5+BzgPmOW+rgZu87BtRSVu2nmHnYJTjnrrgU62NXZguk9JOc7Y1Bd9aUnadHSIZKmGUpPIoRBez9+qgkx4k292dKKMy78kaY1m7z9r7YSmdmHqmOLo0p4JBaXUHqXUavdzG7AWmAhcBNzlbnYXcLH7+SLgbuXwAs7cDeO9al8xaWiPFWTKzKZOR2toiSQGlfkInKqnvYUKatNRdzrLQFsYqGAqhAlpIPsoZ23BtPsOOEln236ntMW0IjiZoUg+BRGZBhwLrADGKaX2uKv2AuPczxOBHSk/2+kuS9/X1SKyUkRWNjQ0eNfoApGwbBrbYwXcn2LLgQ4OdhRun8XAsqE1lvlB0Kaj7pSDX2GgobGFyGweiG+gnEJ702nuSOQ0SNzWYFBToaiv9q5NqXguFESkGvgbcJ1SqjV1XeoUn9milLpDKbVQKbVwzJgxBWypNzS0FUZLSGegikIpApeaM+QsmLY2HaVTDiGVAy1sV4hcgYG0oZx8M+kcaM9+EGTbThG8qWMUUqQCup4KBREJ4AiEPyql7ncX70uahdz35Jxyu4DJKT+f5C4btJiWzYECagmFQClYvs7gjkf8NLUX99it0USXT6RrWWTwmY6aO+DBFT7aM+chDpihoCkUwi8ykDaUa7mLjpiZk8Dc2yTEzeLkJyTxMvpIgF8Da5VSP0pZ9RBwpfv5SuDBlOVXuFFIi4GWFDPToMQrLSFflIIX1hus2OAjmhCeebMIoQwp2ApaIt1HcIPRdPTCOh9b9hmsWO/N4xO37IxhyMVkwJrCALUd07IxB/DwxMzyKhmSJNdB4rYGQVBMHl28c/FSU1gKfAh4u4i84r7OB24GzhKRt4Az3e8A/wI2AxuBO4FPedg2zzFtm4Yys/snBcLcKTZLjrTYvNdg54HiTuqSWsbDtG3aB1mto6Z2WL9LCAUUa7YbtHT0/5t8iJTQhJRPIbx0BhqWOlChYqvyKy1v2nbOWftb9wuH1SvC+c1XlReezeihlHoW6K3HOSPD9gr4tFftKTYH2mJ5xVh7RVIgzJlsc+Z8C8uGN7YZPL3Gx2WnmkWzV7bHzK7ZsVojZllpUtnw4ls+fD54z0km9z7j54UNPs451ouJZUxqwqWZcCeWRyG8dGw1sFnQCpGVHE1YhAPF1Yb7oilHB3MkDvuahcVHFLcj0RnNHmDZNg1l5EtYsd7ghfWOQDhrgYUI+H1w0pEW+1uEdTuLqy0kE/EGm+mouQPW7RSOnmozdgTMn2azbodwsK3wxyqlX6FQs6cNZKReiFF+OfhmUsnVdOTMsla8/IQkWih4wIH2+ICjgwrFig0Gy9f7OGqSzZmuQEhy5CTFuDqb59b6KGZhyaaOxKA0Hb30lg9DYOFM55+7cJaN3wcvrC/8aLSUjtJCmV0GFj008BsyUkbmo/aomfN13d5gEAooxtVroTCosW1FQ1t5aAkvbjBYvs4RCGcda5E+J7wInDLXpj0qrN5UvFshkrBoaC0vJ3x/tHTC2h2OllDlli6uDMGCGTYbdhs0tBT2eKWcF6BQBeUGkmBZCG2lnBLYGnP0LyrlJK1NGa16PLdeo4VCgTnQERtQ1ESheOktg+fX+TiyF4GQZNIoxeGH2bz0lkGHRyGWmdhfJoIzW1a+ZSACx8/s3tEdf7hN0K8Kri0o8isGVwj6czLbyhGQ/TUv347dtO2CVACOW3aPEOhSYFq5O5gPtkF7tLihqEm0UCggtq1oaC19Z/fSWwbPrfVxxESbs/sQCElOnuM4nl/wKMQyE6UXm9nTFoE12w3mTrGpqei+Lhx0BMOmvQb7mgs7pCuVTby/OPpNe4R/v+xnzfa+75d8zVCFnOe50HNG58PBznjOWvG2huLMspYJLRQKyMGOeMnnOFi58ZBAOCcLgQBQXw3HTLN5Y5tBY2v/25cDxUxYXbnReUwWzszcwSyYYRMOKp5fV9jHqRRCIZtCeBt2G+573zdXzLTzMoEVskRFOTibG3PIYE6ydb8wslr1GIQUAy0UCoRtK/a1FdH+koFVGw2efdPH7KRAyOG/u2i2TcBP0RPa8mHdTuGXj/i7CoV5SXvUCd2dM0VRW5l5m1DAERjb9hvsaixcm0oxWUx/I+uECVv2CQGfYlej9JvVnY9foZDnXeq5FdrycDCbFuxqLI3pCLRQKBgHO+MFnbA8V1ZvMnjmTR+zJticm6NAAKgIOYJh636jKJ1tviRMR3BZtvDoyz4iHke1rtpoYCs4YWbfncv8aTaVIUdbKFQibSRR/Kzc/kxHW/cLpiWcMtcGhLd292NCKvHMaaWehS2fYpjFnGUtE1ooFIBSRxyt3uQkoc2aYHPecbkLhCTzp9vUViqeWeMr28igVZsMOqLC6UdbROLwxKu+gnXC6XRE4bWtBkdNUoyo6nvbgB9OnGWzq9FgR4GyxEuRldvf8TbsNqgMKeZNtRldq9iwq38TUs5tKKAfIFLCchcJy85p3oQk2/YLPkMxsQizrGVCC4UC0BxJlCyl/uXNjkCYOd7m3AEIBHAS2k4+yuJAm/Dm9vLTFtqjjn1/1nib+dNtlhxp89Yeg7UeJd+t3mRg23DCrOxGm/Om2tRUFFhbKLJNvK+RfdJ0NHO8jSEwe6LNniaD1s4+9pdjBJVl28QLmORj25RMg2/qyN3BDIdmWQuUJqFdC4WBopRiX2tpfAkvbzZ46g1HIJx3vIWvAP/NWRMU4+ttnl/nI15muWXL1zlawdI5Tkdz/EybCSNtlr3uo6WPjikfOmPw6laDIyZlX8fe74MTZ1vsbTLYWiATXLFt4n0Nbrbsc0xHsyc4Pd0RE5xtN/RhQsp11O/FPAilmLRIKUVjHrMttkXgYFvxs5hT0UJhgDR1lkZLeGWLIxAOP6xwAgEOJbR1xoRVG8vn9mhogTXbhfnTbepcU44hdNUdevTlwpq8Vm8yMC04MUstIcmcyYoRlYrn1xXGrFVModBfIby3XNPRhFHOiY2ognF1dp8mpFznVfAihLQUSWz5ZDBDsrQFJXMygxYKA6JUWsK6ncKy1x2BcP7CwgmEJBNGKmZNsFm1yaA9Uth954NS8MwaH+GAY7dPZUQVvG2exa5Gg9UFEmKROLy6xWD2RMXImtx+6zNg8REWDS3Cxj0D1xaKaT7qqxBe3IQt+4VZE+xuYc6zJyj2txg09zI3h2WTUxnwQtVdSqUUkxbloyUAbN1vUBVWjMrxviskWigMgJYS+BJaO+GJ13yMr/dGICQ5+SgLpeD5daUPUd26X9h+wGDREXbGEsJzJitmjrd5fp3B/gKUm3h5s0HCEhblqCUkcUxOiuXrB669mAUoY50tfXXISdPRrAndT2j2xCxMSDm034t6RcU2wcXN3DOYwQksKPYsa5nQQiFPHC2huBFHtoJ/v+yYJc49zjuBAM4IfMF0mzd3SEE62nyxbUdLqKtSHDMtc4chAmccYxEOwr9X+wdU3C+agFc2G8wcbzOqNr99GAJLjrQ42Cb9RudkQ7E6tb4677d2G1SFFBPSImJqKmDCSJv1u3q/GXPxE3ihKeSbRJcvTZ3xvDL29zULsUTp8hOSaKGQJy3RRNFHIKs3GexqNDjtaKvfEMlCcMJsZ2T+zBrvwj77443tBgfbhZPn9C0EK0Jw9rEWjW3Cc2vzv61f2WwQN4UTZw/sfztrvGJ0rVMTaaDBNJEiefx7i+mPu1FH6aajJLMnKhrbpNds+HiWz4ltK88072LVkVJK5ZXBDLj5QYopJXQygxYKeXOgyFpCQws8v9bg8MNs5kwuzk0TDsDi2TY7DhQumiYXYglnPumJo2wOP6z/c542VjF/msXLm31dDrtcj/fyZoMZhznzJQwEEWe+iuYOYe2OgV27YtnEe+uQt+xzkqnSTUdJZo23EVSvJqRsNYWBztbWF8W6hm1RM++Q2m37hXF1iooizrKWCS0U8iBu2rQX0QFoWvDIaj/hIJw53yqqvfHoaTb1VW5CW5G12pUbDSJx4dS5dtbnfPIcm/pqxaMv+4jmOGB7dYtBLCEsGqCWkGT6OGe+ihUbBjZfRbE00t4ihTbscpyf6aajJFVhmDRasWFX5vyMbLOKvey4i5XZnK+DOZqAvc2ly2JORQuFPGgq8oxhz681aGwTzlpgUREq6qHxGU4V1YPtwuvbine7tHY65rIjJ9mMq8v+QQn44dzjTDpj8MTr2TvJ46ZzvOnjbMbV5dHgDDjagk1bRPqtKNoXMdP7EtAJy84o9OOm4+ifNb5vwTx7gk1Th9CQwYSUsOyssoq97LiLEcUVN21a83AwA+xoEJQqbX5CkhLlzA1umjry+8fnw44GYfVmH8dMs5g+rjQ3zIzDFBNH2byw3umkQwHvj/n8WqdDX3pU7g/zuDpYfISTgDdjnM2Rk/q/bq9tMYgmBu5LSGfKGOfardhgMGeynXeWajRuUx3uLljEFqo6qvBZA48QUwqWVPa0mSVMOPYdQlVI4evjMIuPgsumCUG/IpTB/BFoMXqdsD1JlV3LjEpv7nEBAs3eDmosW7E4wzXMhmOnwLsmCjVh1fvM9ikYIqxdu7bf7cLhMJMmTSIQyP6h1UIhRyJxM+eEnHyJxp1oo/oqxSlzSheRIAKnzrW45+kAL71lcLLHbdnbJKzbZXDCLCvv0sELZ9ps2Sc88ZqPCSPNXiucgtPxrdpkMGWMzfgCT30oAkuOtPnrc35e22r0mKQnWyIJi+pw98e1qqOKcSPHMaJ+BDJAm2LCsklksIU3dziT6Yyu7f+6NLULlp1525Dfh6+fOu5eFwAMB3wYHtlelVJEEzYqz5lCDrQKfh/UVWX3e58hjK6s67dNjY2N7Ny5k+nTp2fdFm0+ypGmzuJpCU++7qMzBuccZ5WsDkqScXVw1CSblzf3XetmoCgFT68xqAwqTpiVv/AxDCdsVyl49JW+o6de3+b4LhbP9kbYTRqlmDLGZuVGI+/SIZEMpRp8lq8gAgHIeH1sBfEEWWuG4aCTrJYwe7bH7qezV0p5XrjOy93birwFgmk51y1U4GdcRBg1ahTRaG4Jtloo5IBSqmj+hHU7hfW7DBbNtjmsyBN398ZJrinnubXeJbRt2iPsPmiw5EiL4AAfkhFVcNrRFjsPGL3OQW1ajkN78mi7q3yDF5x0pE0kLryyOb9HLhLvLUejMCPfTB1yPCEonCi0bAgFFILjNO25/36On90hBkR/gmkgDMTnE3eFaDBQ+Pblc39ooZADHTGrKBUX2yKOlnBYvT2g0XKhqalwpp5cv8tgb1Ph1XDLhmfX+hhVo5g7pTDXec5kZw7q59cZNGRIwnt9m0FnTFjkkZaQ5LB6xYzDHG0h16gocJywXiZgZTr7aMIJNAj4szuuIRAMZBYK/XXIXnbYXh/DVgprAP+buHudvUxGzYUyacbgoBhaglJucTebvCbL8ZqFs5zJZJ5eYxQ8RPXVLQbNHcIpcwt33iJwxnyLUKBntnNSS5g4ymbSaO87pSVHWMRNYXUe2oIi9+JyWe87g+kmW9PRhBGjOeOkU7teDXu2Y9uHRr+HjtFfG7Jvr2lBY5sQT2QemLzrvHfyyuqXB3SMdJqbm7nzl3f20p5DO+7t2K+sfpn/+cKXM7YpbmVvoisGZdbllC+2rWj2epovnOSpHQcMTp1nUZdlyeZiEvQ7EUG7Dxr86Wk/ewqkMUTjsGKD4+ydVuBY7coQnLXAmSdieco8ymu2OxP2eK0lJBkzAmZNsHl5k0FnHrmPEY/i+DN1ltmajsIVFTz+/NNdr5mHT0aEHtqQorvgMc3uPpJcRvFtEcf53RqBRA5lse0B+C1amlv41e2/6rFcZaklLDjuWL7zg5t7LE9YglIM2FRaSMqoKeVNaywx4HIF/XGgFZ5b62TUziuQ+cQL5kxWhAImy173ce8zPo6earP0qMzF6rJlxQaDeMKJcvKC6eMUR0+1WLXJYNo4JxFr5UaD8fU2k4ugJSRZcoTFxt1+Vm003Cktsycat6CX8ia3/GsT6/b2Uqq0HxxNofsy04bpo6v46juyj1oBRzPbvO51vv6F60nEO5k2fTq3/t/PqKuv4/yzLuCY+UfzwvMvcMmll3DHbXfw2vrXaGlpYdr4afztnw+x5OSTuPicC/jRL35Kc3MzX/viV4jFYoTDYX5828+ZNG029/3pTyz7999pa+8AZfGXB+/juk9ey5uvv8HM2bOJRjOX9l04dz7vvfQS/vPof/D7/fzk/37CN274Bps3beZz13+Oj139Mdrb27nsPZfR3NxMIpHga9/4GhdceAE33nAjWzZvYekJSzn9jNP59s3f5tYf3sqf/3QvIsLbzzqTG755IwB/f+BBvnz9F2htbuFHv/gpi5cu4blnnuW2n/ycP/z1z/zguzeza8dOtm3dxo7tO7n8Y5/kuv+6GoAf3fID/vbn+xg1ehQTJk3kmAXz+dTnPpP7P3UAaKGQJc0e5yYks5ZD/uJnLeeKCMwcr5gyxmT5OoNXNhts2uNoN0dMzL3CY3O7YzqaO0UxOs8idNlw6lynZMejL/uYP91JKjujyNd6ZA0cOUnxyhaDY6bbjOgjVDadYtVAUjiFCLOxcUcjEc446VQApkydym/v+T1f/uw1fOEb3+ess5Zy683f4X9vvoVv3fI9QBGPx3lq+VMAPPH4E6xbu46tW7Zy9IL5rFi+nONOOJ5du3YxY+bhtLW28uCj/8Lv9/P0k8v47k3f4pb/+z0+A9a+8SoPPvEcoap6fnPHL6ioqOCZVSt48401nHXyab22d+KkSTz30nN8+fNf5pMf/ySPLnuUWDTGouMW8bGrP0Y4HOaP9/2R2tpaGg808vZT3s757zyfm759E2vXrOW5l54D4NFHHuWff/8nDz/5GOGKCpoONnUdwzRNHln2H/7z78f435u/z31/f6BHOzZueIu//eshduzp4IJTT+Daz3yEN157nX8++HceX/40ZiLBWaeczjEL5mf9fysUngkFEfkN8A5gv1JqnrtsJHAvMA3YClyqlGoSx0X+E+B8oBP4sFJqtVdtyxXLzm+u1VxYvt7gQKtw4YkmlUXOWs6XoB/eNs/mqMk2j7/q45HVftZstzn9GIuROZi+nl3rw2c4lUW9xMl2trj3WR/PvuljXJ1dkgzSxUdabNrr5+8v+rn0ZDNr00GnO99wpoiSL51/eN7tiZlWNxNIJC60duL+D/u+PknzUZLWllZaW1s44aSlROPwvg9cxlVXfITknt7z3vd0bXvSySfx3DPPsXXrVj57/XX84a67WbJ0KQuOO9bZV2srn/3Ep9m8aRMiQjxuYisn9PXU009j0oQ6GlvhuWee55PXOiPtOfPmMmfe3F7be84F5wEwd95cOjo6qKmpoaamhlAoRHNzM1VVVdz0tZt4/tnnMQyDPbv3sH/f/h77WfbEMi7/0OWEK5xEmvqR9V3rLrjwHQDMP3Y+O7Zvz9iOM889m0AgRE1dmNGjx9Cwfz8vvbCCcy44j3A4DOEwZ597Tp/X3iu89Cn8Djg3bdmXgceVUrOAx93vAOcBs9zX1cBtHrYrZ1oipqcT2e884MxyNm+qzYwsCr+VG2NHwPtOsXj7MRb7moU/LvOzfJ2RVb2fXY3Cxj0GC2faVIW9b+th9aorH2HxEdnXVCokIyrh/IUWja1OUEG2Zm7bpqDzFydJP34sx6ijdAQn5j6W6LnvyqpDqtHSk5fy/HPPs+qlVZxxzlm0Nrfw/LPPsuikJQDc8q3vsfTUk3nqxef57T33EI1GqQiCz1BUVlVhCNRWOMeI9uJ0TicQcGychmEQDB6ydxqGgWVa/OWev9B4oJGnX3ia5156jrHjxvYa599bIGIwFHL36evhO+naJhgk5jrjfX4Ds0gJsdngmVBQSj0NHExbfBFwl/v5LuDilOV3K4cXgDoRGe9V23LFy6ijWMLJWq6r8s6eXgwMgWOm2Vz5dpNZ4xUrNvj4wzK/Ww44M8lEteqw4rjDixd6e+Jsmw+fkShZ2RBwKrqeMtdm4x6DF9Zn/xh6UcMn1cmba8JaOrUjahlRV8drq57HVnDvn/7CkpNPclamXe7jTzieF194ETEMwuEwc485mt//5ncsWeps39baymETnG7g93fdA0B1RfedhIKKE086ifv/8lcsG9a++SZvvrGmj3Ptu/0tLS2MHjOaQCDA08ueZvs2Z6RfXVNNe/shn83pZ5zOn+7+A52dTiZnqvkoW+IJ57lJPiEnLF7EYw//m2g0Skd7O4898mjO+ywExY4+GqeU2uN+3guMcz9PBHakbLfTXdYDEblaRFaKyMqGhgbvWuqSsGzaowW25do2la1bCHfs5ulXLdqjzlzD5RSBkC9VYTj3eIt3L3Gu2QMv+Hl4lY+ODIOt9buEfc0GJx1Z3IxtEbrmeS4lx86wmTvFqaKa7WQ8hRYK6VE/sRwT1jLx09v/j+/d+HUuOWspr7/2Otd/6YuAIxNSo39CoRATJ03k+BMWArD4pCW0t7dz1Nw5AHz6us/w3Ru/xdtPehuJhIkIGedzuOqajxDpbOeU4xfzg2/fzDHH9m6HT4+CSud9l72Pl1e/zOLjFnPPH+9h9hGzARg1ahSLlixi0bGLuOHLN3DamWdwzvnncc6pb+eMk07ltp/+PNfLRNx08jqSHHv8cZx9/rm8ffEpfODdl3LU3KOorfXQydYL4mVquYhMA/6R4lNoVkrVpaxvUkrVi8g/gJuVUs+6yx8HvqSUWtnX/hcuXKhWruxzk15ZtmNZVtsdaIuxs7mwExWHOvcQijTw5oE6/rr+cE6dvJtTZzSTCFRj+aux/JUoo/TTYA6UZB7AS28Z+AxYepTN0dOciVpMC+56wikH/oFTzbJ2rHuJacH9y33sbxEuXWoytq7v7WvDfmaMcRw2I5pHMPOImQM6vmWrbvMYNLcLZi/1i3KltVOIJmBM7aHgg0z1h6IJq9eQVFtBY6vg88HI6t7b1BkT2iKOaS4c7Lvt2dRh6g3LVk5F2QH2mwkLDrZJj/Z2tLdTVV1NZ2cnF5/7Dn7401szOpuzqX2UZO3atRx11FHdlonIKqXUwkzbF3tsuk9Exiul9rjmoaQHZxcwOWW7Se6yknOwwKYjw4wQijTQFgvwz01TmVDdwSmT9mCYEDIjgKP9WIFKTH8VVqAa01dFOWWxiW1iWDGU+FCGH2Vkvo38Psduf8REmydf8/Hk6z7e3CGccYzFtgaDtohw9rHDVyCAc43ecYLFPU/7eehFP5edavbpW/FSU7CVM3otVHn2cAAicSeRLeSWcFCKblVAlVJ9drDtEcFWUF/RdydcEVJE445gCPr7flxspfBlU4o0hUIJgyTJxLtgmt/m85/9LzasW08sGuPSD7x/aEUf9cJDwJXAze77gynLrxWRPwOLgJYUM1PJiJkWnYV8CJWionMXSsGDG6eSsA3eNXtLxtA/X6ITX6ITIkkhUYXpr8YMVGP5K0CKJCSUwrCi+MwIfrMdnxnBsHpmXikjgDL82ElBIe53w8eYYID3LPSzfm+IZWuD3PO0H8OAGYdlyBFQCpRCsEGBY8xQoGxAIcr5LtjOZ2UDNqIyfU/+xgZlp/0GbCOA7QthGyHn3RfENgLFu7YulSG48ESTe5/18/eXfFxykoW/F0Ux4XZOgQLVREjt47pMRwWa+SvgVxgiROOHfBTpHXJfXWzCFCJx5/r0dj2SCFBbqTjYJrRFhRF9lODOpV+3bUXCtgdUxiITcdM5p3ThddtvMmdNFxMvQ1LvAU4DRovITuBGHGHwFxH5GLANuNTd/F844agbcUJSP+JVu3KhqZdZlEwLNu0VonEnszJhkfIuJMxD3xMWmKa47zYJazYJy0AhnD9jG6Mqsktt9SU68CU6CEX2gQiWv9Lp1IxkZxZEGQG3U8t/6C2Wid/qxDA78JsRfGZnVyfa5+/sBGIn+nRSLayCeQt8PLF9IusO1HHOxM3UNEW7Ou9iTwRtWDFI9Ez4so1gipBICoxQ/tfWdoUUNigL6RJ6zvkKiglhxQVHB3jwlRE88XKM8+e1IhwSipK8NkoRbWojEPSDqgYrTu8F+CXDqvTyE6pr+B6Nu1FHvsL8H0QcYRB1o5BEev6Le/uXK5waYIZAVTi79vh9jgDpiEFFUHqMwpNkM9q3lSN8Cy0MFBCLO31EuYaeeyYUlFKX9bLqjAzbKuDTXrUlX3ork/2fV3ys29W9+/MZCr8PAu7L73NGSkE/VIUUAcOm0momYFgEfDYjwzHmj23Mr2FKOUKil9W2EUS5o16ng0sKjSC24T/UsSkbnxnFZ3biszrxJTowbG/zMcJ+i/NnbOf8GW78dj5BR0rhtxP4bROfbeJXCefdffncdYfW91xmKIu4L0w0UEHEX0nUX0HUX0k04HxOWLGMAsARDs71FFxNxLaczltZKVqJQpSVk6CbXwUtUw5j2faJTAg0ctKkfRm3M5sCUBmE8Aywcv1/KXd47ggCsWx8gG0LCbOSqkACwzRd2ZE8f+ddSffvPfYLiDr0GaDSJ0TiQRIxk3DABFtApWgKNhj2IZuScgVZJO4jYfmoq7DwkXoZHVHZownu+qqwTTThlHcfVaMyt1QplJ35LGylMG2FZSc11UMHcIT5IW3VbW2mA2RsXNw06Ez4MG2hylDUGiZGwjl3lel6p5ynSl0nIPgOSdoCMwTiXbyhM25mnMh8635nApiFMy2OnWF3CYL+TP4V7dsJxJq9aWwahh0HO96n0MDwYZh5OtCVosLsxLAtDNckkzTNpD44yWVGt3U2osBQltuxJwhYCQJ23P0cz7gsYCfwu++BPAWXJQaW+DENP0oMQmYUv8ocWWaJzxUUSWHhvrvfY/4whrLw2SY+23IFj/tSSeFj9fyesh3Q9VAndYZ3AS2VQaJ7fIw4mCDkt1PWO52DzwC/z8f2pfOpbG9wlnb1Q8nOWXX/nsX1GWsAFqiU+TK6d3uO3tKHTMhoGDpMBOIKiXffRomkdK3ifndeQctPvQFBy0ZFBFLWOcJJUs5NpRxfOYLI9BFstwj5LLpdYfe6+CTljJKdfJfpEvc9+Sowyb4ij/pX3RAbqsYMtDU90EKhFzJpCQkTnnjNR321YvERdr92ziT+RFvRBEI2OEIj999VxNs5/OBaZjWuoT6ap5bTB5b4SBgBTF+AhBHENAIkfEHag7VpywKYRvLlxzKcjt40Am6nH0hZdui7yuAr8FsJwmYnYTNy6D2R9t2MUNPeTNiMELT7DjywESzDjyW+rrY53522xH1BLH8lVkp0mdMJdf1BhWBvSzU7YgZTQ22EfJa7nbPeEKgI+FBiOJofuJ2mu7eUUWf6stTvSqmuctydCR8Kocqf6NGxp+4J0rWB1NFsJoSYZWDaBpUBEwEMN/JHcI7f5SdyO2JlK8LECWAjpp1iRsueWnGS/bppJanXJnk9ksJGiXN/JAWVpBw1RSAhKUKsnzaYtkHE9GPagiGKsN/q+l92v0KHWth9mep2UVOPKGJQGfAmrnrYCoXTJp/W6zqlFM9GDzDO173n/OuqnbR27uULZx/BEaNqsjuQsqnfvwJfxawBtLZ0iBVnxMFXGblvOTXNaxEUHTUz2DX+dCxfhfsgGW6HmxzFGd2WOw9T6jbOOssXwjbCXTb73qKYerQJCLivXFBidDnAlRjOaFCZiLIQ2+z20Nk4zq30SebETuBPtOMzO1HiwzYCXb4c2wg6TuoCqPRmVPjii1UEO+AHJ3RQmxZmefTEWvYn6gjUTsj/GLYiblpYStgbM6gLKqrDPUcLmY0hDjX1Y5k7dw5KKXyGwY9+eAuLF50IwMpVq/nqDV9n774G/OFKjl8wn5/88LvUV9d0hYR2xk2u/OhVrF23ng9dfhkf/8Sn2NVpUB+CkWHoZkrpGrWrNNOJdO/wESwFO1st/AITanrGGgV8goiwcfMWLrnkvbz44otZX7d0I0/6lYmZiqYodJoKn0B9GGqDKc1V3X8jwKuvvc6evXs59+wzAfjnvx5m7boNfP76z2Vsg+EPQjCHwlk5MGyFQl80dSaIpZUp3nGwk0ff3MvJM0dzxGFZCgSgsm0rPquweQ6eoxRVrZsYuX85dQdW4bOixEP17Jt8Lk1jFxOrGNf/PrxqGuL6SxyfiUqJdrLd8Fglfrez7v6536giZbsCwkoRFpbzrtxlXeuS90fKA65SR3qq5/oe2/Q8uyQ1FfDfJ1jctDzOzWtG8D+LAvhTmt8ZrAXLB74AXXZ26NZRpnaSmdZZpo2JTVvMRBGnoqICM2NUU+9j4oqKCp59YSWg+M9jj/H1b36Xh//9GPv37eWDV36M3/zubhYtWsSOljiP/+tBWjpNRoyswGc4Hpndu3ex6uVXeeX1N1HAnpYYfgNqqsLELAu/P78uSoCRVSYN7TFaEga1adl4Mfe0LMPprW3fwL2+ccumuTNBa2eUYCBAfWWQmrAfQ6RfxfyVNzfy8upVnHnBuwE456JLOeciMN12pv8PChV9lgktFDKwt6V7+q1tK+5avpWqkJ9Ljp+U9X58iQ4q2rcVunmeEYweoH7/Ckbuf4FQtAHLCNEy+lgOjl1M+4jZnoVqdu/oDznGk59Vt88ezkaS1Gy8PEYOHFYPV0gjv352C3durOWDi6d2rWuqq4H4Ngi4o8WHvwx7X89p/36l8CmF37KpUxDyd///WmOPJnbmd+jXI+HeF23t7dTV1aPExx133Mlll3+IE5csRQFVIZvTzr2IkfUV2OJEYtiWzcUXXcie3bs5efGJfP17P+SWb9/EggULWLliOZdc+j6OPmY+N3zlS1iWyXHHLeRHP/05oVCIo4+cxXsuvZT//Pvf+Px+fvLz27jpxhvYvGkTn73uej521dVUhfy0x0yaOxNUBn388uc/5Q93/w6AKz78UT517WcBp6rpxz9yBa++8jJHHjWH23/1WyorK7nxa1/l4X/+A7/fz+lnnMV3vncLBxoauO6zn2bnDqcAw80/+F+OP3ExN910E5s3b2LX9m1MmTyZ3Tu38/PbbueoOU5xvgvOOZNvffcWlG3zpS9c79Rxqqjg/26/k6nTpvPdb91EJBrhheef47++8EWikSgvr17FD2/9Cdu2b+XT11zNwcYDjBo9hv+7/U6mT5vKhz/8YWpra1m5ciV79+7l+9//PpdccklO90DG+2LAexhi2LZiX1t3ofDk+v1sbezkqlOmU53D7NrVLetztoUWG8OMUte4mpH7llPd+hYAbSOOYO/k82kZfSy2r2cmleULE6s4rMvc46juSTvroc9AlzkJcG2zhz4XpaMf5CyZMYpdTREeWbOXiXUVnH7kWAA27GtjtGUTS1j4fUavQQX9oXBs735fPx1/L0QiEU5etJBoLMq+vXt56F9OvZ4331zDBy7/UNd2lUE/LZEEnQmLcMC5ByyluOe++3nfey5m2fKX2N0cwRBBWSZPPfcC0WiU446ew0P/eoSZs2bziY9/hF/feXtXZz5p0hSeXbGSr3zx83zqEx/j348/RSwaZfEJx/Kxq5yqqaOqg+xqjvLU8yv44+/v4vGnnkMpxRlvW8rSk0+hrr6etzZs4Oe33cHiJSfx6U9cxa/u+CUf/NCV/OOhB1n5yhuICM3NzQB86QvX8+nPfI4lJy1l67ZtvPvCC3jgiRdJWDbbNm7g348vo7qqkl/87Cc88Le/ctScuezds4e9e/dw3PHH09rayiP/eRK/38+TTzzOTTd+jT/c8xe++rUbu4QAwB9/f3fXtfvi9dfxgcs/yAc+eAW/v+t3fOm//4u//O1+APbs2cOzzz7LunXruPDCC7VQ8IID7TGslPKHBzvi3P/yLuZOqOXEaSOz3k+ocw+BeLMHLSwAyqK6eT0j969gROPL+Ow40fBY9ky9kINjFpEIj8r4MzNQQ2f1FOLhsZ6Ewmky8+5jJ7K7JcI9L23nsBFhjhrv1MNRShEzbWKmjXH6t/D7DPw+wSeS1YTtHdEEbRGTxo4YE0ZUEPTnrglWVFTw7Aqn1MyLK17gmqs+wgsrX+mxXchvEPAZdMRM6iqcQUDqdK5NHXEUjnB69yXvBeCtDeuZOm0aM2c59Ycuu/xD3Hn7bV1C4fwLnBLVc+bOo729/VAZ7KBTBruurg6/YVBfEeCF55/n7PPfSVWV45x954UXs/z55zjvgncwadJkFi9xivBdetkHuP3/fs6nrv0s4XCYa6+5mnPOO59zz78AgGVPPsHatWuxbScTu621DcOMUhsO8M53vpNqtwrsu959Ce+68Hy++rUbeeD+v3LRxY5ZqLW1hU9e9VE2bdqIiJBI9B9J9+KLK/jDn+8D4P0fuJyv3/CVrnUXX3wxhmEwZ84c9u3LHMKcK1oopLG39ZCWoJTiTyu2oxR8cNHUrB40cJyzVa0bvWpiXvjjrdQ0vUlt0+vUNK3Fb3Vi+SpoGruIg2OX0FkzvdeOPh4aSaRqColw9kJRUzgMQ7jq5Bl875G1/PKpTfzPBUcxtqa7BueUqLCJm86/0e8zCBiCz8gsIGzbiWzpiJsEfEZeAiGdExctprGxkQMNDRx11BxeeXk1F7zzwq71lUEfLZEECbf8dzKJzHbbUVcZxBChsjK7qJpQV4lqo+tz8ruVUrK6piKAz5CuGkvptZfSr4+I4Pf7eeLp53nqySd48IH7ueOXt/GnB/6JaVr89v5/Ew6HqQr5qasM4DcMDIHKqkPtnjBxIiNHjuKN11/j/r/ex61uwbzvfPMbnPK20/jjvX9l27atvOOcs7I61/6uAZD3VKPplE9BnTIgYdkcaD8UPLx6ezOv7GzmwvkTGFOTvSOqqnVjn0lggWgjo3c9Tu3B1wjEmrzJ5FU2lW1bOGzb35n1yveY9+IXmfrW76hueYuW0QvYcuQneGPR99k583I6a2dkEAhCLDyWptEn0DpqgRYIJaYi6OMzpzsRbD97YmOfNZCUgoRp0xm3aI+ZROImCcvu1mnY7tzC0YRFZbAwxRc3rF+HZVmMHDWKq6/5FPf88Q+sTInqefKRf9DYsJ+OmInt1jxKznEc8BmMCHcfo86afQTbt21j0yZngHXvPX/k5FNOzbldApxx2tt4/JF/sruhmY6ODv7x9wdZctJSAHbs2M6LK14A4L57/8zik5bS3t5Oc3MzS08/i//62rd57bVXOdgZZ+lpb+fBP/6aSfWVjK4O8ebrr/V63He/57385Nb/pbW1hXlHHwNAa0sL4yc4EWN/SjEROaW52zLuZ9GixfztvnsB+Muf7+Ekt91eoTWFFPa3xbpU2s64yT0vbmdyfQVnzck+2iYQayIc2dvr+uqmN5m2/tf4zY6uZaa/ikjVpJTXZGKVh2UdopnEl+igpvlNag++QU3zmwQSbSiEzprp7JlyIa0j5xGpmtSnw1ghRCsnEKmejO33JuRNkx9jakJ88rTDufWxt7jjmc1849QR/f5GKUhYioTlCBG/Ifh9BkqprrpeVQOo2Z70KQAoFL+849f4fD7GjhvHb+7+Azd89Us0NOzHMAxOWnoKn1n8NjpizkxvSkFbzJlNbmRVsMeIPRwO84vb7+TKyy/rcjR/9ONX59XOExYez6Uf+BAXnv02/D6DD3/ko8xfcCzbtm1l1uzZ3Hn7bXz6mqs44sij+MCHP872fY1c9cH3EYs5loOvfftmJtRV8LOf/JT//q/Pcsri4zFNk5OWnsKPf/aLjMe86F3v5ktfuJ4vfvmrXcs+d/3nueaqj/KDm7/HOeee17X8lFNP49Yf/oCTFy3kv77wxW77+f6PfsynPnEVP/3xj7oczV7iaelsrxlI6exMrNrW1FXv6I8rtrFsQwNfPe8opo/OMklE2dQ3vOjUC+qxTjFm12NM2PoA0crxbJv9UXx2lHDHTirad1LRsZOKzl1dGoYtPmIVhxGpntxNYFiB6m77rOjYSU3TG9Q2vUFV62YEhemvorV+Lq3182irn9P9N701XfxEqiYSqZqM8hWoIlovVIZ8TB9dxcb97T1CfzX989SGBn7/wjZ+deF4Js+YRUXQR0XAR9Bv5JR9u7c1imUrJtZVeNbWdJojCZo740wfXUXctNndEqUi4GNsDpp4vthKsbs5ighMGBHuJoTilk1HzKQjZmLajompMuijOuQnFMi1pqr3+H1CZZbCvNxLZ5ct0YTVJRA2NbSzbH0DZxw1NnuBAFS2b8soEAwrxuS37qb+wCqaRh/HjllXdEX1dNSm1MNXNqHIfkdAdOxwOvzmtYzc/0LXJvFgHZGqyViBKqqb1xKMtwDQWT2FfZPPo7V+Hp0107IOH7WNEJHqyUQrJ+SsmeTD6JoQcyfUEvAZjK4OsW5PG/taM093qMnM22aPYfroKiqCrQA0d8ZpxqmxXxHwdQmJdNt5KknT0YiK4kZ+VQV9NHdCSyRBNGG5+QTeDkKSGCKMqg6yrzVKSyRBTThAR8ykPW4SdzOnwwEf9ZV+KoJ9X7+hjBYKLvtbHV+CadvcvXwbdZUBLl6QcfK3jBhmJxVtW3ssD0YamL72l4Q7d7N72rvYP/Hs3iN3xCBWeRixysNoHnNIiPvjrY5GkfIKtm8hNvJIOsYeQ2LsMVBRhw9htCEY4jgnDQQxnIfBeeG+hIS/inWxURxglGf5B+lMG13F4WOqukZoAZ/B0ZNGMKo5yPp9bd2ivjR9M2VkJZVmB+NHhLFsRSRhEYlbXX4EQQgFjC4hEUxLduqMO47YqhxCrAtBwGcQ9Bm0dCawlKK+Mog/zwlv8qEi4KMq5ITHtkQSKCDoNxhZFaQq6M978p2hhBYKLntanKzjR9fsY1dzhGtPn0k4kL0Drqa5Z05CTdMapq77NQhsnvsZ2urn5NU2M1hLe3AOHfVzqK8KUl0bxuc3qARytvpXjYX6qQSrRnO0Umw/2MmmhvZu4YGFxmcIcyfUMrY28+wxE+oqqKsM8MauVloj3lZpHYr4DKE65Kc65HdKMyesLiHR1BmnqRP8hkFF0Edl0EfYb9ARt7o66GJTFfLT1Bkn4DOorSh+FzSyMohtK4J+g6qQvyTXoJzRQgHoiJm0RU32tUZ56NXdHD+lngWT67L+fahzL4F406EFSjF25yOM3/YQ0coJbJlzDfHwwKoZjqgIMH5EOCdB1YXhh9qJUD8VgofMYSLC1FFVjKwKsmZ3a+HnosaJmjlm0ghq+pn0tzLoZ+HUerY0drD1QEexp1YYMiRNII4ZxNF8I3GbSMKkPWbSFk0gIiiluvIFik1VyE9HzGRkdRApgbXeZwjjehmgFAPDEAI+cSfwUX1VESkJWijgONyUUvzhhW0EfAaXnTi5/x+5iJ2gys0EBidDeMpbd1HX+DJNY05gx8wPDqiuSnXIx/gRFfmp+YEKqJsKIya5NXIyUxMOcOK0kWw+0M7WAxmc5HlSXxXk6Ikjso6BNwzh8DHVjKoK8sauVqKJwk49ORzxGwY1YYOasB+lFFHTJhK3iJt20U1Hh9okTCiic7tc8PuEoM/An6KZhN35GxKWjVkm5lMtFIB9LVGWb25k7d42Ll80hbrK7B1fqTkJwcg+13+wl13TL6Fhwhl5Z/6GAz4m1IV7FPLKispRjjCozj7z2DCEmWNrGFUV4s09rQOeC3jKqEpmja3OOuEvlbrKIItmjGT93rYedag0+SPiOqLz0TY1eSFyyI9iZPBXiDhaQ8ANEzYtZ/rPUgqIYS8UWiIJ9rVG+cvKnRw+poq3zc7ezOOPNRPudKaSrj34GlPX/xYlBpvmfY72uiPzak/Qb3BYbZj6ykBuHaoYUDvBEQbh2ryODc7oftH0kazf18ae5tw7ZMOAo8bXMn7EwEaCAZ/BvIkjGF0dYu3eVu2ELgN8boa0z3ACF2xbEQ4FmDtvHkopDMPHD2/9CYsWLwFg1UsvccNXv8T+/fuoqKhkwbHH8f3/vZXKyqGf/2KI8ywHfEb2lRBECPiFAIYzA5xlk7BUwacE7Y9hLxT2tjgCIRK3uGLxtOzD0JRNTct6UDbjdjzM+O1/p7NqMluOuqbX2kF94XftnKOqghlHFL3/MOSaiCaDvzChfX6fwdwJIxhTE2LtnjYSGWagy0QoYDB/cl1+2k0vHDYizIiKAGt2t9Dcy/Sow42A33BCSZsl47zHhUDEFQJySBCkd24+Q6ioqOC1V1/FshUPP/Iw37zxBv756OPs37uPKz94Gb+5+w+cuGgxAP/vgb/R3tY2pIWCzxCCfgN/L+VFssUQIej3EfTj+h4c7aEYAmJYCwWlFI+v3cfyzY1ccPR4JtZnObpVisq2rQSiB5i64XeMOPgqB8csYsfMy3NO/DLEyVQdWxPC19+cnknEgHAd1E2B6nH9zwWaJ2NrnA557Z42DrT1PXdgXWWAoyeNIJTtdHQ5UBH0cfzUerYc6GBLHk5on0+odEMRK4I+qoJ+Aj6hLWrS7IYmZiv4SoGIE2gwqjrEyKogtWE/IsLa9j3UhAPODGoK5PTT6Zq9zf0Tf88lJK6+Bjo7qXzXhT32nfjgFSQ+dAVGYyMVl78PwRE0ArBsWdZt9BlCpKODUSNHUhPy8/1f3c4HP/QhlixZ0tWRXfyu9wz0UpQtAb9jIvIipNUwhJDhI+R38ktMy/bUNz2shcLu5gi/fX4rY2tCXHD0+D63FdskEDtIMHqAYOwgFe3bmb72NkKRBnbOuJQD40/PyX8gwKjqEONqQ31MmCHO7ErBagjVQqja+RysKlqV0pDfx4LJdexqjrBhb1vGkcqkkRXMHluTm4aTIyLCjDHVjKoK8cbulh4+D8OAioCfyqCPqpCPiqCfqqATo9+boBpVfSgAoCNmdmXbtnQmukpAlIpwwMfIqiCjq4PUVwX7nFRFRJw5h1Om4cT9WBHwEQr7sa1kMpZKnUiTgF8Ihf0YYX9e91QkEmHBggVEo1H27NnDE088gYjw5ptruPLKK6kK+bvqLCU7tCJbQzxDUkxExUp0c7Q2b31Cw1oo/OTxt2hoi/HfZ83OGCHjS3QQjDUSjBygomM74c5dVHTsJty5i7oDq7GNABuPvo6OEbP7PZYAoYCPqqCPypCfmpC/+zH94e4df6jGefdIC8iViXUV1FcGWLO7lRbXjGMYcMRhtUUtkzCiMsCi6SPZfrCTgM9whYCfkD97220mqkJ+qkL+rnOJmZaT4NSZoDmSoC2a8DSXwzAcB/voqhAjq4M5zdvRRS8jewMwqqvhqZ7ru+6u0aNz0gySVFRU8MorrwCwfPlyrrjiCt54443uxxDB8AkBHxDwYdtOxI1CdWl9Srniyo3QVO5ChTfmsXxINakZhgzYRFSuDFuh8ObuFv62ahcnHT6qqz49yibcvpMRjS9T27SGyvathDt3U9GxG7/Z3vXbRKCG9hGz2Xn4+0mEelYPVQiBgJ/KUIjKihBVoSCV4RA+f8Ax/Rg+8AXdzr8agjXgK/9/RTKXYGtjJ7uaIsybWJtTpFah8PsMZozpv57TQAj5fYyt8XWVqLZsRVs0QVOnY25qjSScKp8cKlmsVG4dWGXIx2jXJFRfGRz02bRLlizhwIEDNDQ0MHfuXFatWsVFF13UYzvDEII5nqtyrzWu8FDKKbmdrLR66HNhziVZFSDpUzFc5/pwoPx7Ig+wbcX//HUlC4I7+O/Rmxi76nfUNK+nsn0rwdjBru0sX4hI5SSaxpxAR810Ompn0jFiFrGKcdhGEGX4UOLD8PmoqQhRWxWmtiJMbWUwvySzQYCIMH10FdNGVQ7JUVJv+AyhrjKYlRBUbueUFBjJ0W7qyFgET/wvpWTdOqd09qhRo7j22ms58cQTueCCC1i0aBEA999/P0uXLmXcuNzn+BaRFOtY7/edcktyJ4WEbXcXHJkYzgIgE8NSKKy+91vc13grfrHhVbDFT6x6Mm1jjycyYhaR+iOIjJ5HvHYaYhhdN6SIEAIq3JtnRGWA2rBTXmA4dZDQc2ISzSFEJMU8P7SvU9KnAE6HfNddd+Hz+Rg3bhx//vOf+fznP8/+/U7p7FNPPZVzzz3X0/Y4/hXJOD1pUjAk37tqhOl7uRvDUiiMnLWY51uu4JSlpyLj5mCMmkmFL8Dwy7HUaAaGZfXukF+yZAnPPPNMEVvTN10O+SEuqAfKsBQKMxaezYyFZ5e6GRqNRlN2lEdoi0aj0WjKgrISCiJyroisF5GNIvLlUrdHoyl3BvPMiRrvyef+KBuhICI+4BfAecAc4DIRyW8CAo1mGBAOh2lsbNSCQZMRpRSNjY2Ew7mVCS8nn8KJwEal1GYAEfkzcBHwZklbpdGUKZMmTWLnzp00NDSUuimaMiUcDjNp0qScflNOQmEisCPl+05gUfpGInI1cDXAlClTitMyjaYMCQQCTJ8+vdTN0AwxysZ8lC1KqTuUUguVUgvHjBnYbGYajUaj6U45CYVdQOqUZ5PcZRqNRqMpEuUkFF4CZonIdBEJAu8HHipxmzQajWZYIeUUuSAi5wM/BnzAb5RS3+ln+wZgW56HGw0cyPO3xUC3b2Do9g2ccm+jbl/+TFVKZbS/l5VQKCYislIptbDU7egN3b6Bods3cMq9jbp93lBO5iONRqPRlBgtFDQajUbTxXAWCneUugH9oNs3MHT7Bk65t1G3zwOGrU9Bo9FoND0ZzpqCRqPRaNLQQkGj0Wg0XQx5odBfOW4RCYnIve76FSIyrYhtmywiT4rImyKyRkQ+l2Gb00SkRURecV9fL1b73ONvFZHX3WOvzLBeROSn7vV7TUSOK2Lbjki5Lq+ISKuIXJe2TdGvn4j8RkT2i8gbKctGishjIvKW+17fy2+vdLd5S0SuLFLbfiAi69z/3wMiUtfLb/u8Fzxu4zdEZFfK//H8Xn7refn9Xtp3b0rbtorIK738tijXcEAod1LrofjCSYLbBMwAgsCrwJy0bT4F/NL9/H7g3iK2bzxwnPu5BtiQoX2nAf8o4TXcCozuY/35wMM4cxwuBlaU8H+9Fycpp6TXDzgVOA54I2XZ94Evu5+/DNyS4Xcjgc3ue737ub4IbTsb8Lufb8nUtmzuBY/b+A3g81ncA30+7161L239/wJfL+U1HMhrqGsKXeW4lVJxIFmOO5WLgLvcz38FzpAizUqvlNqjlFrtfm4D1uJUix1MXATcrRxeAOpEZHwJ2nEGsEkplW+Ge8FQSj0NHExbnHqf3QVcnOGn5wCPKaUOKqWagMeAgs50n6ltSqlHlVKm+/UFnLpjJaOX65cN2TzvA6av9rl9x6XAPYU+brEY6kIhUznu9E63axv3wWgBRhWldSm4ZqtjgRUZVi8RkVdF5GERmVvclqGAR0VklVu2PJ1srnExeD+9P4ilvH5Jximl9rif9wLjMmxTDtfyoziaXyb6uxe85lrXxPWbXsxv5XD9TgH2KaXe6mV9qa9hvwx1oTAoEJFq4G/AdUqp1rTVq3FMIvOBnwH/r8jNO1kpdRzOjHifFpFTi3z8fnELKF4I3JdhdamvXw+UY0cou1hwEfkfwAT+2MsmpbwXbgMOBxYAe3BMNOXIZfStJZT98zTUhUI25bi7thERPzACaCxK65xjBnAEwh+VUvenr1dKtSql2t3P/wICIjK6WO1TSu1y3/cDD+Co6KmUQ8nz84DVSql96StKff1S2Jc0q7nv+zNsU7JrKSIfBt4BXO4KrR5kcS94hlJqn1LKUkrZwJ29HLuk96Lbf7wbuLe3bUp5DbNlqAuFbMpxPwQkozwuAZ7o7aEoNK798dfAWqXUj3rZ5rCkj0NETsT5nxVFaIlIlYjUJD/jOCTfSNvsIeAKNwppMdCSYiYpFr2Ozkp5/dJIvc+uBB7MsM2/gbNFpN41j5ztLvMUETkX+CJwoVKqs5dtsrkXvGxjqp/qXb0cu9Tl988E1imldmZaWeprmDWl9nR7/cKJjtmAE5XwP+6yb+I8AABhHLPDRuBFYEYR23YyjhnhNeAV93U+cA1wjbvNtcAanEiKF4CTiti+Ge5xX3XbkLx+qe0T4Bfu9X0dWFjk/28VTic/ImVZSa8fjoDaAyRw7Nofw/FTPQ68BfwHGOluuxD4VcpvP+reixuBjxSpbRtxbPHJezAZjTcB+Fdf90IRr9/v3fvrNZyOfnx6G93vPZ73YrTPXf675H2Xsm1JruFAXrrMhUaj0Wi6GOrmI41Go9HkgBYKGo1Go+lCCwWNRqPRdKGFgkaj0Wi60EJBo9FoNF1ooaAZdojIdSJS6fExxovIP9zPo8SphtsuIj9P2+54t2rmRnGqzfZZd0tErkmpsvmsiMxxlx8tIr/z7IQ0wwYtFDTDkesAT4UCcD1O5i1AFPga8PkM290GXAXMcl/9FcD7k1LqaKXUApzKqz8CUEq9DkwSkSkDb7pmOKOFgmbI4maQ/tMthveGiLxPRD6Lk1D0pIg86W53togsF5HVInKfW4sqWfv+++7I/EURmekuf6+7v1dF5OleDv8e4BEApVSHUupZHOGQ2r7xQK1S6gXlJAzdjVs9VUQOF5FH3MJpz4jIke6+UmtjVdG9htLfcbJ4NZq80UJBM5Q5F9itlJqvlJoHPKKU+imwGzhdKXW6WwfpBuBM5RQqW4kzyk/SopQ6Gvg58GN32deBc5RTZO/C9IOKyHSgSSkV66d9E3EyYpOkVvW8A/iMUup4HA3j/1L2/2kR2YSjKXw25fcrcap0ajR5o4WCZijzOnCWiNwiIqcopVoybLMYmAM8J85sWVcCU1PW35PyvsT9/BzwOxG5Cmdil3TGAw35NtrVVE4C7nPbdLu7TwCUUr9QSh0OfAlHoCXZj6MFaTR54y91AzQar1BKbRBnetDzgW+LyONKqW+mbSY4E9tc1ttu0j8rpa4RkUXABcAqETleKZVaZC+CU1OrP3bRfUKbZFVPA2h2/QZ98Wccn0SSsHtsjSZvtKagGbKIyASgUyn1B+AHOFMoArThTH8KTpG8pSn+gioRmZ2ym/elvC93tzlcKbVCKfV1HI0gtVwzOAXZpvXXPuVUk20VkcVu1NEVwIOu32CLiLzXPZ6IyHz386yUXVyAU2AvyWzKseqmZlChNQXNUOZo4AciYuNUtPyku/wO4BER2e36FT4M3CMiIXf9DTgdO0C9iLwGxHBKdOPucxaOlvE4TtXLLpRSHSKySURmKqU2guO0BmqBoIhcDJytlHoTZ47w3wEVODOeJWc9uxy4TURuAAI4WsGrOLOPnemeTxOHynEDnA78M58LpdEk0VVSNZpecDvyhUqpA3n89l3A8UqpG/rduAC4Au0pnJm9zP6212h6Q2sKGo0HKKUeEJFizvU9BfiyFgiagaI1BY1Go9F0oR3NGo1Go+lCCwWNRqPRdKGFgkaj0Wi60EJBo9FoNF1ooaDRaDSaLv4/+BnNB2LyUrIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob \n",
    "def plot_multiple_runs(forward_folder, bco_folder, bc_folder, env_names=None):\n",
    "    for env in env_names:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        forward_files = glob.glob(forward_folder+env+\"*\")\n",
    "        bco_files = glob.glob(bco_folder+env+\"*\")\n",
    "        bc_files = glob.glob(bc_folder+env+\"*\")\n",
    "        \n",
    "        forward_trajs = []\n",
    "        bco_trajs = []\n",
    "        bc_trajs = []\n",
    "        \n",
    "        #### plot forward matching \n",
    "        for forward_f in forward_files:\n",
    "            with open(forward_f) as f:\n",
    "                rewards = f.read().splitlines()[:20]\n",
    "                forward_trajs.append(rewards)\n",
    "        \n",
    "        forward_trajs = np.array(forward_trajs).astype(float)\n",
    "\n",
    "        forward_mean = np.mean(forward_trajs, axis=0)\n",
    "        forward_max = np.max(forward_trajs, axis=0)\n",
    "        forward_min = np.min(forward_trajs, axis=0)\n",
    "        x = np.linspace(0, len(rewards)-1, num=len(rewards))\n",
    "\n",
    "        ax.plot(forward_mean)\n",
    "        ax.fill_between(x, forward_max, forward_min, alpha=0.3)\n",
    "        \n",
    "        \n",
    "        #### plot bco matching \n",
    "        for bco_f in bco_files:\n",
    "            with open(bco_f) as f:\n",
    "                rewards = np.array(f.read().splitlines()).astype(float).flatten()[:20]\n",
    "                bco_trajs.append(rewards)\n",
    "                \n",
    "        bco_trajs = np.array(bco_trajs).astype(float)\n",
    "        bco_mean = np.mean(bco_trajs, axis=0)\n",
    "        bco_max = np.max(bco_trajs, axis=0)\n",
    "        bco_min = np.min(bco_trajs, axis=0)\n",
    "        x = np.linspace(0, len(rewards)-1, num=len(rewards))\n",
    "\n",
    "        ax.plot(bco_mean)\n",
    "        ax.fill_between(x, bco_max, bco_min, alpha=0.3)\n",
    "        \n",
    "        #### plot bc matching \n",
    "        for bc_f in bc_files:\n",
    "            with open(bc_f) as f:\n",
    "                rewards = np.array(f.read().splitlines()).astype(float).flatten()[1:]\n",
    "                bco_trajs = [np.mean(rewards), np.var(rewards)]\n",
    "                \n",
    "                bc_mean = np.array([np.mean(rewards) for i in range(20)])\n",
    "\n",
    "                bc_max = np.array([np.max(rewards) for i in range(20)])\n",
    "                bc_min = np.array([np.min(rewards) for i in range(20)])\n",
    "                plt.plot(bc_mean, 'r--') \n",
    "                x = np.linspace(0, 19, num=20)\n",
    "                ax.fill_between(x, bc_max, bc_min, alpha=0.3)\n",
    "\n",
    "        ax.set_title(env)\n",
    "        ax.set_xlabel('steps (10e3)')\n",
    "        ax.set_ylabel('reward')\n",
    "        ax.legend([\"Forward matching\", \"BC from observation\", \"BC\"], loc =\"lower right\")\n",
    "        fig.show()\n",
    "                                   \n",
    "env_list = [\"BipedalWalker-v3\", \"Walker2DBulletEnv-v0\", \"HopperBulletEnv-v0\", \"HalfCheetahBulletEnv-v0\", \"AntBulletEnv-v0\", \"HumanoidBulletEnv-v0\"]                          \n",
    "plot_multiple_runs(\"records/forward/\", \"records/bco/\", \"records/bc/\",env_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "compatible-darwin",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:64: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:64: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:64: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:64: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:64: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:64: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_multiple_runs(\"records/gaussian_policy_gaussian_env/\", \"records/bco/\", \"records/bc/\",env_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fourth-enough",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:64: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:64: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:64: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:64: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:64: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:64: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_multiple_runs(\"records/mdn_poicy_gaussian_env/\", \"records/bco/\", \"records/bc/\",env_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "parental-reach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-80.71431939543506,\n",
       " -107.64788803287502,\n",
       " 46.10971254079241,\n",
       " 73.32555483257023,\n",
       " -44.20981564636499,\n",
       " 121.81057550628948,\n",
       " 1.4396739966325498,\n",
       " 116.41870502131607,\n",
       " 23.94124108966433,\n",
       " 98.80370937478375,\n",
       " 46.21050926431498,\n",
       " 68.92547245199984,\n",
       " 72.60305584580723,\n",
       " 43.11382392539668,\n",
       " -19.15160680306695,\n",
       " 63.967338609014675,\n",
       " 41.47164657563946,\n",
       " -0.12702182598236209,\n",
       " 7.8780907695502895,\n",
       " 45.37928585095369]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bipedal walker\n",
    "[-80.71431939543506, -107.64788803287502, 46.10971254079241, 73.32555483257023, -44.20981564636499, 121.81057550628948, 1.4396739966325498, 116.41870502131607, 23.94124108966433, 98.80370937478375, 46.21050926431498, 68.92547245199984, 72.60305584580723, 43.11382392539668, -19.15160680306695, 63.967338609014675, 41.47164657563946, -0.12702182598236209, 7.8780907695502895, 45.37928585095369]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[-1373.2022003799393, -1537.3849441439165, -1566.3893398222672, -1566.4634060174883, -1513.6554054038406, -1526.7649856726032, -1588.8617726247337, -1493.9276560460264, -1486.5689613453014, -1550.5739168169828, -1550.2711365015564, -1575.905552052223, -1533.292399915575, -1515.0198687160193, -1549.5659183686507, -1450.3833646696814, -1439.0032680973789, -1505.3988276566417, -1491.981697066, -1451.7861920043692]\n",
    "\n",
    "\n",
    "[-77.42269800045648, -120.29086165266833, -65.97568645354733, -17.24457595621989, 17.761656451816577, -23.315276349945528, -125.68498414493897, 32.16795578633858, 27.353653848985825, -100.18606084116595, -9.982470982930854, -71.04823052779166, -31.26627925078696, -30.3749177017662, -18.479886873909912, -8.814653659052217, 53.806495660831025, 59.48446484370838, -96.95677284185486, 32.37978991513198]\n",
    "\n",
    "\n",
    "[17.05794454679845, -1.152118400453911, 73.31450239102124, 43.82633083314189, 45.05796391713242, 40.80798365452466, 19.01886243725472, 35.60101467668996, 19.582562413734195, 42.536604598793204, 47.87169106576544, 19.599122156608583, 39.00406759191564, 78.15280722142549, 76.47825263079336, 28.527372805199242, 20.286506336651456, 82.01481704865347, 26.696482407852393, 17.68209225587311]\n",
    "\n",
    "\n",
    "[20.340960391015876, -7.5957462968471665, -5.64952268313829, 39.17338988423048, 34.17768875810082, 105.5906503018052, 58.99896630055687, 70.56245956803333, 45.69051809440425, 110.42888658674603, 131.70598534916468, 75.33278338056694, 76.858474227848, 233.44472073642584, 231.76878421835434, 115.70124689468126, 225.17604362895835, 100.70042132769284, 45.59691453532076, 158.23814460606152]\n",
    "\n",
    "\n",
    "[-584.6469779468895, -790.1563856982439, -738.3381516537241, -303.06726756870256, 51.336031873359694, -558.3819529517864, -705.0943268275055, -232.6737508441126, -686.7766340204014, -724.4446733457228, -568.8822996880676, -774.1916697368074, -628.9576601929583, -641.522208993676, -737.9679958551051, -522.7748998468383, -761.5455540991205, -810.5939675764698, -730.5554652859933, -424.63264384175324]\n",
    "\n",
    "\n",
    "[99.31250812622015, 110.30787982852996, 154.36258293253405, 196.4705833228953, 219.81495326495863, 81.4895250369373, -73.91483945067316, -4.549355839342006, -459.2008512168189, -125.78358223159816, -216.31058123705515, -58.68871890729858, -264.86666464125386, -1481.982335698971, -2769.588672446682, -888.5039450961544, -940.2829805759836, -1274.6549564888778, -752.758337622611, -658.1891863229729]\n",
    "\n",
    "\n",
    "[-31.129122906622463, -70.2628989337789, -94.79358454540414, -23.01149874768736, -22.04257649423564, -24.244113261252757, -27.818875505115006, -43.40152464929519, -27.349353352093015, -35.530143053229, -54.14535895695569, -19.235037690635433, -45.05351322031206, -25.952533264753992, -45.002882376082226, -21.464234956854877, -22.752502099574624, 3.393028267252964, -56.258879656719834, -79.67432940683796]\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" mdn + mdn (dynamic + policy)\n",
    "[-91.97775855422408, -91.48796139516975, -47.35232828546524, -43.96527052342228, -117.21148234120841, -14.893711893999406, -22.249125178151544, -6.492524863067938, -10.193609049387023, -36.36159609961613, -123.98560553134355, -56.422989769197855, -115.31864565200515, 26.53181756763592, -119.69346890627985, -17.30584860213615, -3.4579364564183166, -8.329585256825624, -97.10870243823199, -101.26250989913173]\n",
    "\n",
    "\n",
    "[17.417215208077085, 11.395020125210742, 17.342992106105235, 17.035776985724297, 11.683159599286592, 38.01104715653452, 177.48464565415205, 26.851959021881598, 51.66713792707914, 3.38634460641906, 0.5635360319310696, 9.774039259614568, 17.175692277298307, 32.18595158152002, 17.77311242160699, 60.77676829090131, 39.33163717346518, 20.450568798254427, 38.63136499542219, 26.327378382157573]\n",
    "\n",
    "\n",
    "[19.98300957377647, 13.760008000430602, 9.70041215488733, 16.865292796658952, 13.57311732530062, 59.198057405078835, 59.890139134472705, 62.21831534714681, 31.63059420921927, 38.15061728432133, 27.726525388343127, 0.3052555517162763, 24.101196499964892, 27.171015969993253, 41.58941517495393, 7.013132369164583, 8.863949039826178, 5.189351213288767, 18.97373539300934, 42.76350615268128]\n",
    "\n",
    "\n",
    "[-607.0543385673287, -806.4580132064702, -706.9388296694813, -704.0868158508686, -854.8355523125322, -495.4331999439704, -465.9610431312168, -912.1594922921411, -708.5099486672142, -911.8392636016562, -1027.8732196732442, -998.3502355247043, -1215.182292635422, -590.4106673332353, -239.19080561896124, -455.50458913759735, -737.5466495914256, 339.8731203235975, 332.31882261535577, 116.15955559290299]\n",
    "\n",
    "\n",
    "[166.49978373269627, 194.79353707459657, 9.34236894216443, -43.54585778075855, 12.830168123697518, -70.88584568151924, -124.73652304496639, -1426.7664706635028, -1214.0157128612082, -743.2890149971461, -1074.9683651205933, -1205.4461384180536, -1184.8804299489884, -1113.642737652988, -763.8236848112497, -727.6740872159183, -567.5114595250955, -253.83123664118088, -157.36145382303067, -627.0726486051612]\n",
    "\n",
    "\n",
    "[-31.24164405269406, -80.74274846982719, -42.52495495778028, -15.498939063731273, -53.60286526966865, -71.91186545845049, -19.914671887012624, -30.555686888870962, -37.57695954431116, -8.791994321518787, -14.27225190059264, -4.557804993246706, -23.2121430505511, -23.864069365425404, -4.901172084530374, -5.802280785444951, 0.5387475304617476, 4.313701621168237, 5.540979307335364, 1.048698961362165]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "test_bipedalwalker = [-77.42269800045648, -120.29086165266833, -65.97568645354733, -17.24457595621989, 17.761656451816577, -23.315276349945528, -125.68498414493897, 32.16795578633858, 27.353653848985825, -100.18606084116595, -9.982470982930854, -71.04823052779166, -31.26627925078696, -30.3749177017662, -18.479886873909912, -8.814653659052217, 53.806495660831025, 59.48446484370838, -96.95677284185486, 32.37978991513198]\n",
    "test_walker = [-1241.2286926378722, -1160.2450865632702, -1302.5257279430775, -1321.1742109382649, -1188.933183313903, -705.0222037711366, -1015.6534497409206, -1223.086064483041, -581.1125714915169, -1094.4109430023177, -1263.2555011234258, 606.2516319491602, 347.13141169409215, 421.8291469984875, -1162.114098068984, -1182.7265993365133, -1263.46067056635, 426.85376063362065, 654.2950759629778, 200.48967327397975, 89.62557959415612, 770.944369011869]\n",
    "test_hopper = [19.39352412420114, 32.04291391761344, 71.05704872371435, 210.83129950012804, 70.8467080637385, 137.4572230840752, 40.47588962083239, 388.04944066645714, 70.29350711982096, 213.6719275538449, 81.97411059718404, 141.17601595789915, 151.4677262172483, 102.80866475250083, 159.70545355726733, 464.72625602344385, 225.03651377538398, 154.7150833974447, 364.8459397128754, 452.7572393095682]\n",
    "test_half = [17.013435360947565, 40.15109587138687, 21.430784166587966, 24.17196539273257, 82.36950376110435, 29.22265959028815, 65.57115237712492, 34.345782755340814, 42.59285357176953, 78.61830403883336, 62.88512472101623, 58.8132956755824, 55.83413705380583, 60.73374486486313, 51.154639767290234, 50.608121683934876, 45.42836724520008, 46.15240933758517, 39.74357310803997, 37.22987510561556]\n",
    "test_ant = [-607.0543385673287, -806.4580132064702, -706.9388296694813, -704.0868158508686, -854.8355523125322, -495.4331999439704, -465.9610431312168, -912.1594922921411, -708.5099486672142, -911.8392636016562, -1027.8732196732442, -998.3502355247043, -1215.182292635422, -590.4106673332353, -239.19080561896124, -455.50458913759735, -737.5466495914256, 339.8731203235975, 332.31882261535577, 116.15955559290299]\n",
    "test_human = [-31.24164405269406, -80.74274846982719, -42.52495495778028, -15.498939063731273, -53.60286526966865, -71.91186545845049, -19.914671887012624, -30.555686888870962, -37.57695954431116, -8.791994321518787, -14.27225190059264, -4.557804993246706, -23.2121430505511, -23.864069365425404, -4.901172084530374, -5.802280785444951, 10.5387475304617476, 14.313701621168237, 15.540979307335364, 7.048698961362165]\n",
    "\n",
    "foward_records = [test_bipedalwalker, test_half, test_hopper, test_walker, test_ant, test_human]\n",
    "#\"\"\"\n",
    "\n",
    "test_bco_bipedal = [-21.622981899611602, -50.45265779702035, -14.964480684141027, -3.974716434900671, -85.41978882118583, -85.41538556643499, -84.24169132222157, -87.91838652590553, -85.30914249103455, -55.75293002613003, -16.839369788537105, -87.10465359533988, -77.72110420887255, -64.78834819576186, -55.42264371588144, -55.08960786631849, -16.98840366882085, -29.061604524564615, -73.68085866489076, -24.15669209049675]\n",
    "\n",
    "test_bco_walker = [17.05794454679845, -1.152118400453911, 73.31450239102124, 43.82633083314189, 45.05796391713242, 40.80798365452466, 19.01886243725472, 35.60101467668996, 19.582562413734195, 42.536604598793204, 47.87169106576544, 19.599122156608583, 39.00406759191564, 78.15280722142549, 76.47825263079336, 28.527372805199242, 20.286506336651456, 82.01481704865347, 26.696482407852393, 17.68209225587311]\n",
    "\n",
    "test_bco_hopper = [20.340960391015876, -7.5957462968471665, -5.64952268313829, 39.17338988423048, 34.17768875810082, 105.5906503018052, 58.99896630055687, 70.56245956803333, 45.69051809440425, 110.42888658674603, 131.70598534916468, 75.33278338056694, 76.858474227848, 233.44472073642584, 231.76878421835434, 115.70124689468126, 225.17604362895835, 100.70042132769284, 45.59691453532076, 158.23814460606152]\n",
    "\n",
    "test_bco_half = [-584.6469779468895, -790.1563856982439, -738.3381516537241, -303.06726756870256, 51.336031873359694, -558.3819529517864, -705.0943268275055, -232.6737508441126, -686.7766340204014, -724.4446733457228, -568.8822996880676, -774.1916697368074, -628.9576601929583, -641.522208993676, -737.9679958551051, -522.7748998468383, -761.5455540991205, -810.5939675764698, -730.5554652859933, -424.63264384175324]\n",
    "\n",
    "test_bco_ant = [99.31250812622015, 110.30787982852996, 154.36258293253405, 196.4705833228953, 219.81495326495863, 81.4895250369373, -73.91483945067316, -4.549355839342006, -459.2008512168189, -125.78358223159816, -216.31058123705515, -58.68871890729858, -264.86666464125386, -1481.982335698971, -2769.588672446682, -888.5039450961544, -940.2829805759836, -1274.6549564888778, -752.758337622611, -658.1891863229729]\n",
    "\n",
    "test_bco_human = [-31.129122906622463, -70.2628989337789, -94.79358454540414, -23.01149874768736, -22.04257649423564, -24.244113261252757, -27.818875505115006, -43.40152464929519, -27.349353352093015, -35.530143053229, -54.14535895695569, -19.235037690635433, -45.05351322031206, -25.952533264753992, -45.002882376082226, -21.464234956854877, -22.752502099574624, 3.393028267252964, -56.258879656719834, -79.67432940683796]\n",
    "\n",
    "bco_records = [test_bco_bipedal,test_bco_walker,test_bco_hopper,test_bco_half,test_bco_ant,test_bco_human]\n",
    "\n",
    "env_list = [\"Pendulum-v0\", \"BipedalWalker-v3\", \"Walker2DBulletEnv-v0\", \"HopperBulletEnv-v0\", \"HalfCheetahBulletEnv-v0\", \"AntBulletEnv-v0\", \"HumanoidBulletEnv-v0\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for r in range(len(foward_records)):\n",
    "    plot_simple_bco(foward_records[r], bco_records[r], env_list[r+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.random.rand(ypoints[1:].shape[0])-0.7)*200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_bco(test_pendulum, test_pendulum_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_bco(test_walker, test_pendulum_f, s=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-banner",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_bco(test_half, test_pendulum_f, s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_bco(test_bipedalwalker, test_pendulum_f, s=10, off=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-dubai",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-rouge",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
