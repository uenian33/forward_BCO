{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "revolutionary-blackberry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pybullet\n",
      "  Downloading pybullet-3.1.7.tar.gz (79.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 79.0 MB 2.1 MB/s eta 0:00:01     |█████████▋                      | 23.8 MB 1.3 MB/s eta 0:00:42\n",
      "\u001b[?25hBuilding wheels for collected packages: pybullet\n",
      "  Building wheel for pybullet (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pybullet: filename=pybullet-3.1.7-cp37-cp37m-macosx_10_9_x86_64.whl size=67488518 sha256=59cb04eac1dd868a40a4a21ff00c5c11eb7e958e8016e5ed1692d505ce8a626f\n",
      "  Stored in directory: /Users/user/Library/Caches/pip/wheels/70/1c/62/86c8b68885c24123d87c5392d6678aa2b68a1796c8113e1aa6\n",
      "Successfully built pybullet\n",
      "Installing collected packages: pybullet\n",
      "  Attempting uninstall: pybullet\n",
      "    Found existing installation: pybullet 3.0.8\n",
      "    Uninstalling pybullet-3.0.8:\n",
      "      Successfully uninstalled pybullet-3.0.8\n",
      "Successfully installed pybullet-3.1.7\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/Users/user/anaconda3/envs/pwil/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pybullet --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "chinese-spencer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch_optimizer as th_optim\n",
    "import pybullet_envs\n",
    "\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch.distributions import Normal\n",
    "from models import *\n",
    "from utils import *\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "based-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_simple_bco(records, records_f, s=100, off=0.8):\n",
    "    ypoints = np.array(records)\n",
    "    plt.plot(ypoints, linestyle = 'dotted')\n",
    "    ypoints[1:] = ypoints[1:] #- (np.random.rand(ypoints[1:].shape[0])-0.25)*2*s + (np.random.rand(ypoints[1:].shape[0])-off)*4*s\n",
    "    plt.plot(ypoints, linestyle = 'dotted')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stuffed-attack",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_model_training(transitions, inv_model, ep_num=100):\n",
    "    inv_dataset = transition_dataset(transitions)\n",
    "    inv_dataset_list.append(inv_dataset)\n",
    "    inv_dataset_final = ConcatDataset(inv_dataset_list)\n",
    "    inv_loader = DataLoader(inv_dataset_final, batch_size=1024, shuffle=True, num_workers=4)\n",
    "\n",
    "    inv_opt = optim.Adam(inv_model.parameters(), lr=1e-3, weight_decay=0.0001)\n",
    "    \"\"\"\n",
    "    inv_opt_yogi = th_optim.Yogi(\n",
    "        inv_model.parameters(),\n",
    "        lr= 1e-2,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-3,\n",
    "        initial_accumulator=1e-6,\n",
    "        weight_decay=0,\n",
    "    )\n",
    "\n",
    "    inv_opt = th_optim.Lookahead(inv_opt_yogi,  alpha=0.5)#k=5\n",
    "    \"\"\"\n",
    "    inv_loss = nn.MSELoss()\n",
    "    #inv_loss = nn.L1Loss()\n",
    "\n",
    "    for epoch in range(ep_num): \n",
    "        running_loss = 0\n",
    "        for i, data in enumerate(inv_loader):\n",
    "            s, a, s_prime = data\n",
    "            inv_opt.zero_grad()\n",
    "            \"\"\"\n",
    "            a_pred = inv_model(s.float(), s_prime.float())\n",
    "            \"\"\"\n",
    "            sprime_m, sprime_v = inv_model(s.float(), a.float()) \n",
    "            pred = Normal(sprime_m, sprime_v).rsample()\n",
    "            loss = inv_loss(pred, s_prime.float())\n",
    "            #loss = inv_loss(a_pred, a.float())\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "            if i%100 == 99:\n",
    "                running_loss = 0\n",
    "            inv_opt.step()\n",
    "        if epoch%20==0:\n",
    "            print('Epoch:%d Batch:%d Loss:%.5f'%(epoch, i+1, loss))\n",
    "    print('Done!')\n",
    "    return inv_model\n",
    "\n",
    "def train_bc(trajs, policy, dynamics,  ep_num=50, sample_itr=500):\n",
    "    bc_dataset = imitation_dataset(trajs)\n",
    "    bc_loader = DataLoader(bc_dataset, batch_size=1024, shuffle=True, num_workers=4)\n",
    "    print('Learning policy....')\n",
    "    #bc_opt = optim.Adam(policy.parameters(), lr=1e-3, weight_decay=0.0001)\n",
    "    #\"\"\"\n",
    "    bc_opt_yogi = th_optim.Yogi(\n",
    "        policy.parameters(),\n",
    "        lr= 1e-2,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-3,\n",
    "        initial_accumulator=1e-6,\n",
    "        weight_decay=0,\n",
    "    )\n",
    "\n",
    "    bc_opt = th_optim.Lookahead(bc_opt_yogi,  alpha=0.5)#k=5,\n",
    "    #\"\"\"\n",
    "    bc_loss = nn.MSELoss()\n",
    "    # bc_loss = nn.L1Loss()\n",
    "\n",
    "    for epoch in range(ep_num):  \n",
    "        running_loss = 0\n",
    "        for i, data in enumerate(bc_loader):\n",
    "            s, s_prime = data\n",
    "            bc_opt.zero_grad()\n",
    "            \n",
    "            \n",
    "            #\"\"\"\n",
    "            a_mu, a_sigma = policy(s.float())\n",
    "            a_pred = Normal(loc=a_mu, scale=a_sigma).rsample()\n",
    "            #\"\"\"\n",
    "            s_m, s_v = dynamics(s, a_pred) \n",
    "            \n",
    "            \n",
    "            preds = Normal(s_m, s_v).rsample([sample_itr])\n",
    "            new_gts = torch.cat(sample_itr*[s_prime]).view(sample_itr,s_prime.shape[0],s_prime.shape[1])\n",
    "            loss = bc_loss(preds, new_gts)\n",
    "            #print(loss, loss.shape, preds.shape, new_gts.shape)\n",
    "            \n",
    "            #for sid in range(sample_itr):\n",
    "                #\"\"\"\n",
    "                #a_mu, a_sigma = policy(s.float())\n",
    "                #a_pred = Normal(loc=a_mu, scale=a_sigma).rsample()\n",
    "                #\"\"\"\n",
    "                #s_m, s_v = dynamics(s, a_pred) \n",
    "                #print(pred.shape, s.shape, s_prime.shape)\n",
    "                #print(pred[0])\n",
    "                #loss = loss + bc_loss(preds[sid], s_prime)\n",
    "                \n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            if i%20 == 19:\n",
    "                running_loss = 0\n",
    "            bc_opt.step()\n",
    "        if epoch%10==0:\n",
    "            print('Epoch:%d Batch:%d Loss:%.3f'%(epoch, i+1, loss))\n",
    "\n",
    "    print('Done!')\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deadly-science",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_list = [\"Pendulum-v0\", \"BipedalWalker-v3\", \"Walker2DBulletEnv-v0\", \"HopperBulletEnv-v0\", \"HalfCheetahBulletEnv-v0\", \"AntBulletEnv-v0\", \"HumanoidBulletEnv-v0\"]\n",
    "ENV_NAME = env_list[-3]\n",
    "env=ENV_NAME\n",
    "runs = 20\n",
    "inv_samples = 1000\n",
    "max_steps = 500\n",
    "expert_path='experts/'\n",
    "DEMO_DIR = os.path.join(expert_path, env+'.pkl')\n",
    "M = inv_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "persistent-johnston",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    demos = np.load(\"experts/states_expert_walker_.npy\")[:10]\n",
    "except:\n",
    "    with open(DEMO_DIR, 'rb') as f:\n",
    "        trajs = pickle.load(f)\n",
    "\n",
    "        \n",
    "env = gym.make(ENV_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "visible-identification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1000, 26)\n"
     ]
    }
   ],
   "source": [
    "demos = []\n",
    "for t_id, traj in enumerate(trajs):\n",
    "    demo =[]\n",
    "    #print(t_id)\n",
    "    for item in traj:    \n",
    "        obs = item['observation']\n",
    "        #obs = list(obs)\n",
    "        #print(obs)\n",
    "        demo.append(obs)\n",
    "    #print(np.array(demo).shape)\n",
    "    demos.append(np.array(demo))\n",
    "\n",
    "print(np.array(demos).shape)\n",
    "demos = demos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dramatic-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "policy = policy_continuous(env.observation_space.shape[0],64,env.action_space.shape[0], uncertain=True)#.cuda()\n",
    "inv_model = forward_dynamics_continuous(env.observation_space.shape[0],100,env.action_space.shape[0], uncertain=True)#.cuda()\n",
    "\n",
    "#get_action_labels(inv_model, demos, 'continuous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lesser-orleans",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# start BipedalWalker-v3 training ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:35: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49,)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -113.42819713090422 setps: 45 count: 45\n",
      "reward: -104.53504114604245 setps: 76 count: 121\n",
      "reward: -98.1807835285524 setps: 58 count: 179\n",
      "reward: -24.84264123198795 setps: 500 count: 679\n",
      "avg rewards: -85.24666575937175\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:1 Loss:1.30817\n",
      "Epoch:20 Batch:1 Loss:0.46234\n",
      "Epoch:40 Batch:1 Loss:0.14068\n",
      "Epoch:60 Batch:1 Loss:0.09420\n",
      "Epoch:80 Batch:1 Loss:0.07061\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.195\n",
      "Epoch:10 Batch:9 Loss:0.115\n",
      "Epoch:20 Batch:9 Loss:0.109\n",
      "Epoch:30 Batch:9 Loss:0.106\n",
      "Epoch:40 Batch:9 Loss:0.101\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -117.11789462951322 setps: 46 count: 46\n",
      "reward: -116.3433254983658 setps: 45 count: 91\n",
      "reward: -116.3801485413167 setps: 45 count: 136\n",
      "reward: -117.44115629367158 setps: 48 count: 184\n",
      "reward: -116.66818331891841 setps: 47 count: 231\n",
      "reward: -116.87212064207779 setps: 47 count: 278\n",
      "reward: -116.38647374093843 setps: 44 count: 322\n",
      "reward: -117.10073666971425 setps: 46 count: 368\n",
      "reward: -116.07901677489406 setps: 42 count: 410\n",
      "reward: -116.26770894675516 setps: 52 count: 462\n",
      "reward: -116.82460280121863 setps: 45 count: 507\n",
      "reward: -116.87125103012349 setps: 46 count: 553\n",
      "reward: -116.51032820627373 setps: 44 count: 597\n",
      "reward: -116.51478959269612 setps: 45 count: 642\n",
      "reward: -116.64826877063327 setps: 46 count: 688\n",
      "reward: -116.52945144690077 setps: 45 count: 733\n",
      "reward: -116.7529470856137 setps: 44 count: 777\n",
      "reward: -117.54309355647862 setps: 46 count: 823\n",
      "reward: -115.755690846242 setps: 42 count: 865\n",
      "reward: -116.54179835917677 setps: 45 count: 910\n",
      "reward: -112.88375145746767 setps: 47 count: 957\n",
      "avg rewards: -116.47774943852333\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.07825\n",
      "Epoch:20 Batch:3 Loss:0.03938\n",
      "Epoch:40 Batch:3 Loss:0.03574\n",
      "Epoch:60 Batch:3 Loss:0.03257\n",
      "Epoch:80 Batch:3 Loss:0.03110\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.069\n",
      "Epoch:10 Batch:9 Loss:0.059\n",
      "Epoch:20 Batch:9 Loss:0.061\n",
      "Epoch:30 Batch:9 Loss:0.057\n",
      "Epoch:40 Batch:9 Loss:0.055\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -10.129519708082395 setps: 500 count: 500\n",
      "reward: -11.803098711596027 setps: 500 count: 1000\n",
      "avg rewards: -10.96630920983921\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.03346\n",
      "Epoch:20 Batch:6 Loss:0.02506\n",
      "Epoch:40 Batch:6 Loss:0.02292\n",
      "Epoch:60 Batch:6 Loss:0.02285\n",
      "Epoch:80 Batch:6 Loss:0.02232\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.056\n",
      "Epoch:10 Batch:9 Loss:0.056\n",
      "Epoch:20 Batch:9 Loss:0.049\n",
      "Epoch:30 Batch:9 Loss:0.053\n",
      "Epoch:40 Batch:9 Loss:0.049\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -6.371614107343998 setps: 500 count: 500\n",
      "reward: 40.979714966740026 setps: 500 count: 1000\n",
      "avg rewards: 17.304050429698016\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.02701\n",
      "Epoch:20 Batch:10 Loss:0.02292\n",
      "Epoch:40 Batch:10 Loss:0.02090\n",
      "Epoch:60 Batch:10 Loss:0.02149\n",
      "Epoch:80 Batch:10 Loss:0.02180\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.049\n",
      "Epoch:10 Batch:9 Loss:0.042\n",
      "Epoch:20 Batch:9 Loss:0.044\n",
      "Epoch:30 Batch:9 Loss:0.043\n",
      "Epoch:40 Batch:9 Loss:0.038\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -117.36099687088964 setps: 70 count: 70\n",
      "reward: -119.24027265507604 setps: 72 count: 142\n",
      "reward: -117.72402301495087 setps: 70 count: 212\n",
      "reward: -115.80557452259275 setps: 67 count: 279\n",
      "reward: -119.90227529212274 setps: 73 count: 352\n",
      "reward: -121.72103672749736 setps: 81 count: 433\n",
      "reward: -117.5485279692101 setps: 69 count: 502\n",
      "reward: -116.91189147688448 setps: 69 count: 571\n",
      "reward: -118.48026793632098 setps: 72 count: 643\n",
      "reward: -119.6481290790954 setps: 80 count: 723\n",
      "reward: -115.15488157068503 setps: 67 count: 790\n",
      "reward: -121.61161408701042 setps: 81 count: 871\n",
      "reward: -118.50091607972979 setps: 77 count: 948\n",
      "avg rewards: -118.43156979092814\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.02340\n",
      "Epoch:20 Batch:15 Loss:0.02100\n",
      "Epoch:40 Batch:15 Loss:0.01784\n",
      "Epoch:60 Batch:15 Loss:0.01944\n",
      "Epoch:80 Batch:15 Loss:0.01896\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.039\n",
      "Epoch:10 Batch:9 Loss:0.040\n",
      "Epoch:20 Batch:9 Loss:0.036\n",
      "Epoch:30 Batch:9 Loss:0.035\n",
      "Epoch:40 Batch:9 Loss:0.038\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 60.13610418952955 setps: 500 count: 500\n",
      "reward: -121.42213726870406 setps: 68 count: 568\n",
      "reward: -112.9612826861106 setps: 59 count: 627\n",
      "reward: -122.33591217593724 setps: 66 count: 693\n",
      "avg rewards: -74.1458069853056\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:21 Loss:0.02059\n",
      "Epoch:20 Batch:21 Loss:0.01792\n",
      "Epoch:40 Batch:21 Loss:0.01793\n",
      "Epoch:60 Batch:21 Loss:0.01744\n",
      "Epoch:80 Batch:21 Loss:0.01812\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.030\n",
      "Epoch:10 Batch:9 Loss:0.032\n",
      "Epoch:20 Batch:9 Loss:0.033\n",
      "Epoch:30 Batch:9 Loss:0.035\n",
      "Epoch:40 Batch:9 Loss:0.031\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 98.35143692002175 setps: 500 count: 500\n",
      "reward: 42.19769728388572 setps: 500 count: 1000\n",
      "avg rewards: 70.27456710195374\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:28 Loss:0.01945\n",
      "Epoch:20 Batch:28 Loss:0.01954\n",
      "Epoch:40 Batch:28 Loss:0.01616\n",
      "Epoch:60 Batch:28 Loss:0.01718\n",
      "Epoch:80 Batch:28 Loss:0.01761\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.032\n",
      "Epoch:10 Batch:9 Loss:0.028\n",
      "Epoch:20 Batch:9 Loss:0.029\n",
      "Epoch:30 Batch:9 Loss:0.030\n",
      "Epoch:40 Batch:9 Loss:0.031\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 106.94415850549257 setps: 500 count: 500\n",
      "reward: 94.67161763315035 setps: 500 count: 1000\n",
      "avg rewards: 100.80788806932145\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:36 Loss:0.01699\n",
      "Epoch:20 Batch:36 Loss:0.01458\n",
      "Epoch:40 Batch:36 Loss:0.01497\n",
      "Epoch:60 Batch:36 Loss:0.01563\n",
      "Epoch:80 Batch:36 Loss:0.01487\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.035\n",
      "Epoch:10 Batch:9 Loss:0.030\n",
      "Epoch:20 Batch:9 Loss:0.028\n",
      "Epoch:30 Batch:9 Loss:0.025\n",
      "Epoch:40 Batch:9 Loss:0.025\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -71.32476481267437 setps: 197 count: 197\n",
      "reward: -41.80247630062338 setps: 350 count: 547\n",
      "avg rewards: -56.56362055664888\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:44 Loss:0.01815\n",
      "Epoch:20 Batch:44 Loss:0.01620\n",
      "Epoch:40 Batch:44 Loss:0.01695\n",
      "Epoch:60 Batch:44 Loss:0.01743\n",
      "Epoch:80 Batch:44 Loss:0.01600\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.030\n",
      "Epoch:10 Batch:9 Loss:0.026\n",
      "Epoch:20 Batch:9 Loss:0.025\n",
      "Epoch:30 Batch:9 Loss:0.026\n",
      "Epoch:40 Batch:9 Loss:0.025\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -26.89335494587317 setps: 379 count: 379\n",
      "reward: -36.732362448012445 setps: 474 count: 853\n",
      "avg rewards: -31.812858696942808\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:54 Loss:0.01721\n",
      "Epoch:20 Batch:54 Loss:0.01738\n",
      "Epoch:40 Batch:54 Loss:0.01766\n",
      "Epoch:60 Batch:54 Loss:0.01707\n",
      "Epoch:80 Batch:54 Loss:0.01752\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.027\n",
      "Epoch:10 Batch:9 Loss:0.026\n",
      "Epoch:20 Batch:9 Loss:0.026\n",
      "Epoch:30 Batch:9 Loss:0.025\n",
      "Epoch:40 Batch:9 Loss:0.025\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 42.62696033967504 setps: 500 count: 500\n",
      "reward: -64.33007871111359 setps: 249 count: 749\n",
      "avg rewards: -10.851559185719275\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:65 Loss:0.01794\n",
      "Epoch:20 Batch:65 Loss:0.01654\n",
      "Epoch:40 Batch:65 Loss:0.01776\n",
      "Epoch:60 Batch:65 Loss:0.01686\n",
      "Epoch:80 Batch:65 Loss:0.01710\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.027\n",
      "Epoch:10 Batch:9 Loss:0.023\n",
      "Epoch:20 Batch:9 Loss:0.025\n",
      "Epoch:30 Batch:9 Loss:0.028\n",
      "Epoch:40 Batch:9 Loss:0.025\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 83.91335099510974 setps: 500 count: 500\n",
      "reward: -55.591821293207204 setps: 311 count: 811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg rewards: 14.160764850951267\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:77 Loss:0.01719\n",
      "Epoch:20 Batch:77 Loss:0.01659\n",
      "Epoch:40 Batch:77 Loss:0.01562\n",
      "Epoch:60 Batch:77 Loss:0.02161\n",
      "Epoch:80 Batch:77 Loss:0.01844\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.026\n",
      "Epoch:10 Batch:9 Loss:0.024\n",
      "Epoch:20 Batch:9 Loss:0.031\n",
      "Epoch:30 Batch:9 Loss:0.026\n",
      "Epoch:40 Batch:9 Loss:0.026\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 88.02722132326564 setps: 500 count: 500\n",
      "reward: -30.939239945817747 setps: 406 count: 906\n",
      "avg rewards: 28.543990688723945\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:89 Loss:0.01689\n",
      "Epoch:20 Batch:89 Loss:0.01688\n",
      "Epoch:40 Batch:89 Loss:0.01714\n",
      "Epoch:60 Batch:89 Loss:0.01641\n",
      "Epoch:80 Batch:89 Loss:0.01717\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.026\n",
      "Epoch:10 Batch:9 Loss:0.029\n",
      "Epoch:20 Batch:9 Loss:0.023\n",
      "Epoch:30 Batch:9 Loss:0.023\n",
      "Epoch:40 Batch:9 Loss:0.028\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 126.7123231789815 setps: 500 count: 500\n",
      "reward: -46.47864158531373 setps: 256 count: 756\n",
      "reward: -97.38565975486456 setps: 107 count: 863\n",
      "avg rewards: -5.717326053732265\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:103 Loss:0.01674\n",
      "Epoch:20 Batch:103 Loss:0.01735\n",
      "Epoch:40 Batch:103 Loss:0.01579\n",
      "Epoch:60 Batch:103 Loss:0.01754\n",
      "Epoch:80 Batch:103 Loss:0.01679\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.022\n",
      "Epoch:10 Batch:9 Loss:0.023\n",
      "Epoch:20 Batch:9 Loss:0.024\n",
      "Epoch:30 Batch:9 Loss:0.023\n",
      "Epoch:40 Batch:9 Loss:0.022\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -63.1875990273494 setps: 207 count: 207\n",
      "reward: -15.467121458870054 setps: 397 count: 604\n",
      "reward: -34.00502134056143 setps: 306 count: 910\n",
      "avg rewards: -37.55324727559363\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:118 Loss:0.01741\n",
      "Epoch:20 Batch:118 Loss:0.01645\n",
      "Epoch:40 Batch:118 Loss:0.01835\n",
      "Epoch:60 Batch:118 Loss:0.01815\n",
      "Epoch:80 Batch:118 Loss:0.01357\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.022\n",
      "Epoch:10 Batch:9 Loss:0.022\n",
      "Epoch:20 Batch:9 Loss:0.024\n",
      "Epoch:30 Batch:9 Loss:0.025\n",
      "Epoch:40 Batch:9 Loss:0.020\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 128.2530052079226 setps: 500 count: 500\n",
      "reward: -10.916067920212939 setps: 468 count: 968\n",
      "avg rewards: 58.66846864385483\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:133 Loss:0.01545\n",
      "Epoch:20 Batch:133 Loss:0.01691\n",
      "Epoch:40 Batch:133 Loss:0.01654\n",
      "Epoch:60 Batch:133 Loss:0.01618\n",
      "Epoch:80 Batch:133 Loss:0.01573\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.022\n",
      "Epoch:10 Batch:9 Loss:0.024\n",
      "Epoch:20 Batch:9 Loss:0.023\n",
      "Epoch:30 Batch:9 Loss:0.023\n",
      "Epoch:40 Batch:9 Loss:0.021\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 133.2859639308553 setps: 500 count: 500\n",
      "reward: 124.18411506385446 setps: 500 count: 1000\n",
      "avg rewards: 128.73503949735488\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:150 Loss:0.01588\n",
      "Epoch:20 Batch:150 Loss:0.01640\n",
      "Epoch:40 Batch:150 Loss:0.01611\n",
      "Epoch:60 Batch:150 Loss:0.01624\n",
      "Epoch:80 Batch:150 Loss:0.01669\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.021\n",
      "Epoch:10 Batch:9 Loss:0.022\n",
      "Epoch:20 Batch:9 Loss:0.019\n",
      "Epoch:30 Batch:9 Loss:0.021\n",
      "Epoch:40 Batch:9 Loss:0.020\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 145.08556034107937 setps: 500 count: 500\n",
      "reward: 134.02579123937997 setps: 500 count: 1000\n",
      "avg rewards: 139.55567579022966\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:167 Loss:0.01570\n",
      "Epoch:20 Batch:167 Loss:0.01639\n",
      "Epoch:40 Batch:167 Loss:0.01740\n",
      "Epoch:60 Batch:167 Loss:0.01529\n",
      "Epoch:80 Batch:167 Loss:0.01656\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.022\n",
      "Epoch:10 Batch:9 Loss:0.021\n",
      "Epoch:20 Batch:9 Loss:0.023\n",
      "Epoch:30 Batch:9 Loss:0.023\n",
      "Epoch:40 Batch:9 Loss:0.025\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -38.4239272349542 setps: 358 count: 358\n",
      "reward: -33.139890604587904 setps: 289 count: 647\n",
      "reward: -87.93624508661838 setps: 116 count: 763\n",
      "avg rewards: -53.166687642053496\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:186 Loss:0.01675\n",
      "Epoch:20 Batch:186 Loss:0.01669\n",
      "Epoch:40 Batch:186 Loss:0.01554\n",
      "Epoch:60 Batch:186 Loss:0.01615\n",
      "Epoch:80 Batch:186 Loss:0.01529\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.019\n",
      "Epoch:10 Batch:9 Loss:0.020\n",
      "Epoch:20 Batch:9 Loss:0.021\n",
      "Epoch:30 Batch:9 Loss:0.021\n",
      "Epoch:40 Batch:9 Loss:0.021\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -46.62566809082782 setps: 287 count: 287\n",
      "reward: 139.79079227906217 setps: 500 count: 787\n",
      "avg rewards: 46.58256209411718\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:206 Loss:0.01608\n",
      "Epoch:20 Batch:206 Loss:0.01606\n",
      "Epoch:40 Batch:206 Loss:0.01782\n",
      "Epoch:60 Batch:206 Loss:0.01580\n",
      "Epoch:80 Batch:206 Loss:0.01510\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:9 Loss:0.023\n",
      "Epoch:10 Batch:9 Loss:0.022\n",
      "Epoch:20 Batch:9 Loss:0.019\n",
      "Epoch:30 Batch:9 Loss:0.018\n",
      "Epoch:40 Batch:9 Loss:0.024\n",
      "Done!\n",
      "############# start Walker2DBulletEnv-v0 training ###################\n",
      "(49, 1000, 22)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 18.943514061845782 setps: 18 count: 18\n",
      "reward: 19.43693314643897 setps: 22 count: 40\n",
      "reward: 13.000269795120403 setps: 8 count: 48\n",
      "reward: 14.481477957284367 setps: 11 count: 59\n",
      "reward: 14.827648740707081 setps: 10 count: 69\n",
      "reward: 14.75082765920233 setps: 11 count: 80\n",
      "reward: 11.81516546372004 setps: 8 count: 88\n",
      "reward: 14.968652242561802 setps: 12 count: 100\n",
      "reward: 16.120262931451723 setps: 11 count: 111\n",
      "reward: 12.891826689818117 setps: 9 count: 120\n",
      "reward: 16.532527403165293 setps: 12 count: 132\n",
      "reward: 18.587778218310266 setps: 18 count: 150\n",
      "reward: 27.28014047792094 setps: 25 count: 175\n",
      "reward: 20.774004744915878 setps: 20 count: 195\n",
      "reward: 19.2228351418089 setps: 17 count: 212\n",
      "reward: 13.358055467596568 setps: 8 count: 220\n",
      "reward: 17.39687905318715 setps: 15 count: 235\n",
      "reward: 25.43084842374083 setps: 27 count: 262\n",
      "reward: 17.02092626058584 setps: 17 count: 279\n",
      "reward: 20.474644402861305 setps: 18 count: 297\n",
      "reward: 12.04452335252281 setps: 8 count: 305\n",
      "reward: 13.273107160227665 setps: 11 count: 316\n",
      "reward: 22.578748472708686 setps: 22 count: 338\n",
      "reward: 18.35240065661928 setps: 16 count: 354\n",
      "reward: 16.115349125461943 setps: 12 count: 366\n",
      "reward: 14.089922879231745 setps: 11 count: 377\n",
      "reward: 19.09183545339183 setps: 17 count: 394\n",
      "reward: 12.369724230206339 setps: 10 count: 404\n",
      "reward: 15.283366575214313 setps: 12 count: 416\n",
      "reward: 12.786252298422914 setps: 7 count: 423\n",
      "reward: 16.797754508838988 setps: 13 count: 436\n",
      "reward: 14.42342033058376 setps: 10 count: 446\n",
      "reward: 17.970651251576783 setps: 17 count: 463\n",
      "reward: 16.292953656686585 setps: 14 count: 477\n",
      "reward: 12.9985870623379 setps: 9 count: 486\n",
      "reward: 19.747148894502605 setps: 24 count: 510\n",
      "reward: 15.092327864548134 setps: 10 count: 520\n",
      "reward: 18.17334392784833 setps: 15 count: 535\n",
      "reward: 16.436399551664362 setps: 14 count: 549\n",
      "reward: 12.969339344212495 setps: 9 count: 558\n",
      "reward: 11.638246180822899 setps: 13 count: 571\n",
      "reward: 18.47609922841511 setps: 21 count: 592\n",
      "reward: 24.44957721126411 setps: 26 count: 618\n",
      "reward: 17.27170911673602 setps: 13 count: 631\n",
      "reward: 17.851913626874737 setps: 15 count: 646\n",
      "reward: 13.936073491195565 setps: 11 count: 657\n",
      "reward: 14.228037275961832 setps: 9 count: 666\n",
      "reward: 14.286996274182455 setps: 9 count: 675\n",
      "reward: 16.020705095060112 setps: 13 count: 688\n",
      "reward: 17.833538190461695 setps: 15 count: 703\n",
      "reward: 16.298859502277626 setps: 12 count: 715\n",
      "reward: 23.59053421471471 setps: 21 count: 736\n",
      "reward: 15.297087484961958 setps: 13 count: 749\n",
      "reward: 17.667659740100504 setps: 13 count: 762\n",
      "reward: 17.566510067855415 setps: 13 count: 775\n",
      "reward: 16.068627424399892 setps: 11 count: 786\n",
      "reward: 13.965625776538218 setps: 11 count: 797\n",
      "reward: 14.417169742070834 setps: 8 count: 805\n",
      "reward: 12.492276494835096 setps: 8 count: 813\n",
      "reward: 13.207443211527425 setps: 9 count: 822\n",
      "reward: 23.068671412089316 setps: 20 count: 842\n",
      "reward: 16.531669122831953 setps: 19 count: 861\n",
      "reward: 14.083490592005546 setps: 9 count: 870\n",
      "reward: 19.79879077446822 setps: 17 count: 887\n",
      "reward: 16.401562073965035 setps: 14 count: 901\n",
      "reward: 12.120780076558004 setps: 17 count: 918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 15.155802202856284 setps: 11 count: 929\n",
      "reward: 16.002085692604304 setps: 12 count: 941\n",
      "reward: 20.760759302791847 setps: 16 count: 957\n",
      "reward: 15.988287944735204 setps: 14 count: 971\n",
      "reward: 22.236783421068687 setps: 24 count: 995\n",
      "avg rewards: 16.660785166834955\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:1.31638\n",
      "Epoch:20 Batch:1 Loss:0.34947\n",
      "Epoch:40 Batch:1 Loss:0.12588\n",
      "Epoch:60 Batch:1 Loss:0.08091\n",
      "Epoch:80 Batch:1 Loss:0.05153\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.143\n",
      "Epoch:10 Batch:10 Loss:0.095\n",
      "Epoch:20 Batch:10 Loss:0.094\n",
      "Epoch:30 Batch:10 Loss:0.094\n",
      "Epoch:40 Batch:10 Loss:0.092\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -2.663067228817092 setps: 48 count: 48\n",
      "reward: -1.9333013157302066 setps: 52 count: 100\n",
      "reward: -0.23313707945344442 setps: 52 count: 152\n",
      "reward: -3.41092196496029 setps: 54 count: 206\n",
      "reward: -4.044247181971151 setps: 50 count: 256\n",
      "reward: -1.4629395313546423 setps: 55 count: 311\n",
      "reward: -0.6800871224259057 setps: 44 count: 355\n",
      "reward: -4.88447630344017 setps: 46 count: 401\n",
      "reward: -4.907203881899478 setps: 41 count: 442\n",
      "reward: -0.7676902895982525 setps: 52 count: 494\n",
      "reward: -3.0114904959395057 setps: 43 count: 537\n",
      "reward: -4.490088743231901 setps: 41 count: 578\n",
      "reward: 1.3662924794087297 setps: 55 count: 633\n",
      "reward: 2.347913145988421 setps: 48 count: 681\n",
      "reward: -5.227943797662739 setps: 43 count: 724\n",
      "reward: -3.470393828771195 setps: 45 count: 769\n",
      "reward: 1.691866223033868 setps: 51 count: 820\n",
      "reward: 0.8130567041473067 setps: 46 count: 866\n",
      "reward: -2.3184874487764318 setps: 50 count: 916\n",
      "reward: -1.6320265358954216 setps: 51 count: 967\n",
      "avg rewards: -1.945918709867475\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.04589\n",
      "Epoch:20 Batch:3 Loss:0.01938\n",
      "Epoch:40 Batch:3 Loss:0.01606\n",
      "Epoch:60 Batch:3 Loss:0.01452\n",
      "Epoch:80 Batch:3 Loss:0.01431\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.042\n",
      "Epoch:10 Batch:10 Loss:0.035\n",
      "Epoch:20 Batch:10 Loss:0.034\n",
      "Epoch:30 Batch:10 Loss:0.033\n",
      "Epoch:40 Batch:10 Loss:0.034\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 23.003858178394147 setps: 63 count: 63\n",
      "reward: 43.75588648729172 setps: 87 count: 150\n",
      "reward: 45.40234557653603 setps: 86 count: 236\n",
      "reward: 41.656773399090156 setps: 92 count: 328\n",
      "reward: 41.67804284999875 setps: 81 count: 409\n",
      "reward: 24.884090005599138 setps: 74 count: 483\n",
      "reward: 54.87570338643127 setps: 99 count: 582\n",
      "reward: 61.09370254937819 setps: 85 count: 667\n",
      "reward: 20.122219769864742 setps: 61 count: 728\n",
      "reward: 28.739747394584974 setps: 64 count: 792\n",
      "reward: 21.53641086892749 setps: 64 count: 856\n",
      "reward: 40.29340348069381 setps: 82 count: 938\n",
      "avg rewards: 37.253515328899205\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.01573\n",
      "Epoch:20 Batch:6 Loss:0.01105\n",
      "Epoch:40 Batch:6 Loss:0.00989\n",
      "Epoch:60 Batch:6 Loss:0.01066\n",
      "Epoch:80 Batch:6 Loss:0.00974\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.031\n",
      "Epoch:10 Batch:10 Loss:0.030\n",
      "Epoch:20 Batch:10 Loss:0.029\n",
      "Epoch:30 Batch:10 Loss:0.029\n",
      "Epoch:40 Batch:10 Loss:0.030\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 91.8295657873663 setps: 74 count: 74\n",
      "reward: 64.28653840273039 setps: 68 count: 142\n",
      "reward: 70.91303191641492 setps: 51 count: 193\n",
      "reward: 52.85084896162153 setps: 59 count: 252\n",
      "reward: 97.2471661826916 setps: 76 count: 328\n",
      "reward: 37.50968066538189 setps: 43 count: 371\n",
      "reward: 28.638022243174785 setps: 38 count: 409\n",
      "reward: 26.09890753510845 setps: 35 count: 444\n",
      "reward: 73.46957008588363 setps: 53 count: 497\n",
      "reward: 83.31621140533679 setps: 67 count: 564\n",
      "reward: 126.71223656823565 setps: 125 count: 689\n",
      "reward: 82.936590095525 setps: 66 count: 755\n",
      "reward: 89.71300911841826 setps: 71 count: 826\n",
      "reward: 82.27755001649929 setps: 70 count: 896\n",
      "reward: 34.62067319256166 setps: 43 count: 939\n",
      "reward: 27.237149104653497 setps: 34 count: 973\n",
      "avg rewards: 66.85354695510023\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.01272\n",
      "Epoch:20 Batch:10 Loss:0.00835\n",
      "Epoch:40 Batch:10 Loss:0.00916\n",
      "Epoch:60 Batch:10 Loss:0.00804\n",
      "Epoch:80 Batch:10 Loss:0.00822\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.026\n",
      "Epoch:10 Batch:10 Loss:0.025\n",
      "Epoch:20 Batch:10 Loss:0.025\n",
      "Epoch:30 Batch:10 Loss:0.024\n",
      "Epoch:40 Batch:10 Loss:0.024\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 27.82874541701894 setps: 29 count: 29\n",
      "reward: 39.40112119354163 setps: 39 count: 68\n",
      "reward: 24.865289460690107 setps: 26 count: 94\n",
      "reward: 28.958212359211757 setps: 29 count: 123\n",
      "reward: 31.710742687190944 setps: 30 count: 153\n",
      "reward: 33.85800047342345 setps: 31 count: 184\n",
      "reward: 24.03058179858781 setps: 25 count: 209\n",
      "reward: 29.6968298024236 setps: 29 count: 238\n",
      "reward: 22.330119192253914 setps: 23 count: 261\n",
      "reward: 34.44125047062116 setps: 34 count: 295\n",
      "reward: 34.23471872317168 setps: 32 count: 327\n",
      "reward: 39.157615582451385 setps: 36 count: 363\n",
      "reward: 35.428976100195726 setps: 33 count: 396\n",
      "reward: 26.52894394408213 setps: 22 count: 418\n",
      "reward: 25.05620677115948 setps: 28 count: 446\n",
      "reward: 29.476263342131276 setps: 31 count: 477\n",
      "reward: 33.00609420592519 setps: 35 count: 512\n",
      "reward: 35.92078501692449 setps: 34 count: 546\n",
      "reward: 31.560936609086646 setps: 31 count: 577\n",
      "reward: 30.923731147988295 setps: 31 count: 608\n",
      "reward: 30.477586468409577 setps: 31 count: 639\n",
      "reward: 29.83884460580448 setps: 30 count: 669\n",
      "reward: 26.245907057606377 setps: 27 count: 696\n",
      "reward: 35.7871936288866 setps: 35 count: 731\n",
      "reward: 32.03319597217195 setps: 31 count: 762\n",
      "reward: 28.66912867452047 setps: 26 count: 788\n",
      "reward: 23.95484362252173 setps: 24 count: 812\n",
      "reward: 29.017499165527983 setps: 31 count: 843\n",
      "reward: 31.558473433172907 setps: 33 count: 876\n",
      "reward: 25.406423181484573 setps: 26 count: 902\n",
      "reward: 24.34321586411534 setps: 27 count: 929\n",
      "reward: 32.549979092834114 setps: 30 count: 959\n",
      "reward: 23.631729428689866 setps: 26 count: 985\n",
      "avg rewards: 30.05846013617653\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.00793\n",
      "Epoch:20 Batch:15 Loss:0.00896\n",
      "Epoch:40 Batch:15 Loss:0.00757\n",
      "Epoch:60 Batch:15 Loss:0.00702\n",
      "Epoch:80 Batch:15 Loss:0.00714\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.025\n",
      "Epoch:10 Batch:10 Loss:0.024\n",
      "Epoch:20 Batch:10 Loss:0.025\n",
      "Epoch:30 Batch:10 Loss:0.024\n",
      "Epoch:40 Batch:10 Loss:0.023\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 23.477988153195476 setps: 23 count: 23\n",
      "reward: 21.696293354933733 setps: 21 count: 44\n",
      "reward: 27.202161122320103 setps: 25 count: 69\n",
      "reward: 38.260648078841044 setps: 47 count: 116\n",
      "reward: 23.446578499287714 setps: 20 count: 136\n",
      "reward: 25.685450775423668 setps: 23 count: 159\n",
      "reward: 26.384006207545458 setps: 24 count: 183\n",
      "reward: 21.789836433915475 setps: 20 count: 203\n",
      "reward: 25.32922213772545 setps: 21 count: 224\n",
      "reward: 28.52164556124335 setps: 25 count: 249\n",
      "reward: 22.408423021141783 setps: 21 count: 270\n",
      "reward: 27.19590552209847 setps: 24 count: 294\n",
      "reward: 22.74415947115922 setps: 21 count: 315\n",
      "reward: 30.118335902928084 setps: 28 count: 343\n",
      "reward: 26.805498608655768 setps: 24 count: 367\n",
      "reward: 20.224107932214977 setps: 19 count: 386\n",
      "reward: 21.437388552940675 setps: 18 count: 404\n",
      "reward: 22.418787338715628 setps: 20 count: 424\n",
      "reward: 27.236598220519955 setps: 23 count: 447\n",
      "reward: 21.296991621528285 setps: 19 count: 466\n",
      "reward: 27.16834578608104 setps: 24 count: 490\n",
      "reward: 27.450813268301133 setps: 25 count: 515\n",
      "reward: 23.960144166795363 setps: 22 count: 537\n",
      "reward: 20.218469151147293 setps: 19 count: 556\n",
      "reward: 24.812554797036864 setps: 22 count: 578\n",
      "reward: 24.446475620653654 setps: 22 count: 600\n",
      "reward: 24.17340977599815 setps: 21 count: 621\n",
      "reward: 28.093990679220582 setps: 27 count: 648\n",
      "reward: 22.950016406214854 setps: 20 count: 668\n",
      "reward: 24.264045024914957 setps: 22 count: 690\n",
      "reward: 25.577970690585786 setps: 23 count: 713\n",
      "reward: 24.903558570522 setps: 22 count: 735\n",
      "reward: 22.86419473267015 setps: 21 count: 756\n",
      "reward: 27.257110530688074 setps: 24 count: 780\n",
      "reward: 23.316831041614932 setps: 19 count: 799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 21.65518491613911 setps: 20 count: 819\n",
      "reward: 26.27284825462411 setps: 23 count: 842\n",
      "reward: 26.048897867061893 setps: 25 count: 867\n",
      "reward: 27.924573522782882 setps: 27 count: 894\n",
      "reward: 65.39305627564318 setps: 57 count: 951\n",
      "reward: 25.467102354652884 setps: 23 count: 974\n",
      "reward: 25.707428262829488 setps: 24 count: 998\n",
      "avg rewards: 26.03826305267888\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:21 Loss:0.00707\n",
      "Epoch:20 Batch:21 Loss:0.00667\n",
      "Epoch:40 Batch:21 Loss:0.00769\n",
      "Epoch:60 Batch:21 Loss:0.00588\n",
      "Epoch:80 Batch:21 Loss:0.00676\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.023\n",
      "Epoch:10 Batch:10 Loss:0.023\n",
      "Epoch:20 Batch:10 Loss:0.024\n",
      "Epoch:30 Batch:10 Loss:0.022\n",
      "Epoch:40 Batch:10 Loss:0.022\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 24.749040722541395 setps: 20 count: 20\n",
      "reward: 20.488565265157376 setps: 17 count: 37\n",
      "reward: 23.25693154416658 setps: 17 count: 54\n",
      "reward: 25.625293775311725 setps: 20 count: 74\n",
      "reward: 22.955914301950543 setps: 19 count: 93\n",
      "reward: 26.346026260493087 setps: 20 count: 113\n",
      "reward: 23.1514368678836 setps: 19 count: 132\n",
      "reward: 24.76018933726882 setps: 19 count: 151\n",
      "reward: 25.126933521174944 setps: 22 count: 173\n",
      "reward: 20.140335371074617 setps: 16 count: 189\n",
      "reward: 24.733374963051755 setps: 19 count: 208\n",
      "reward: 25.018262158121804 setps: 19 count: 227\n",
      "reward: 23.652709642294212 setps: 19 count: 246\n",
      "reward: 24.322188211580215 setps: 19 count: 265\n",
      "reward: 22.841388170285786 setps: 18 count: 283\n",
      "reward: 23.38735871615499 setps: 19 count: 302\n",
      "reward: 24.090494231155024 setps: 18 count: 320\n",
      "reward: 24.93964173957647 setps: 19 count: 339\n",
      "reward: 24.680807387547976 setps: 20 count: 359\n",
      "reward: 23.28430912240292 setps: 18 count: 377\n",
      "reward: 23.773016689522777 setps: 18 count: 395\n",
      "reward: 22.63208432721731 setps: 17 count: 412\n",
      "reward: 23.977313830793722 setps: 19 count: 431\n",
      "reward: 24.23547119341965 setps: 20 count: 451\n",
      "reward: 22.967994694838126 setps: 17 count: 468\n",
      "reward: 26.89596702292183 setps: 21 count: 489\n",
      "reward: 24.10641793699324 setps: 18 count: 507\n",
      "reward: 24.421077646339835 setps: 21 count: 528\n",
      "reward: 24.957152887525446 setps: 19 count: 547\n",
      "reward: 23.1886518365558 setps: 18 count: 565\n",
      "reward: 23.210573376971293 setps: 18 count: 583\n",
      "reward: 22.822688508500875 setps: 17 count: 600\n",
      "reward: 24.507298289750178 setps: 20 count: 620\n",
      "reward: 23.16566289923794 setps: 17 count: 637\n",
      "reward: 21.451769334918936 setps: 16 count: 653\n",
      "reward: 24.5281867630023 setps: 19 count: 672\n",
      "reward: 24.344596290113987 setps: 19 count: 691\n",
      "reward: 24.515515188491552 setps: 19 count: 710\n",
      "reward: 23.277571676767543 setps: 18 count: 728\n",
      "reward: 23.5386058417178 setps: 18 count: 746\n",
      "reward: 23.05931320697564 setps: 18 count: 764\n",
      "reward: 27.1729847599665 setps: 22 count: 786\n",
      "reward: 26.332356159016488 setps: 21 count: 807\n",
      "reward: 24.48203524573765 setps: 19 count: 826\n",
      "reward: 24.688945090581548 setps: 20 count: 846\n",
      "reward: 24.73673993542179 setps: 19 count: 865\n",
      "reward: 24.218289723922496 setps: 18 count: 883\n",
      "reward: 25.18015971165878 setps: 20 count: 903\n",
      "reward: 23.952051446484987 setps: 18 count: 921\n",
      "reward: 23.96238497187296 setps: 18 count: 939\n",
      "reward: 23.14188815460075 setps: 18 count: 957\n",
      "reward: 23.807747867979934 setps: 20 count: 977\n",
      "reward: 25.66397105448268 setps: 20 count: 997\n",
      "avg rewards: 24.046560091952763\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:28 Loss:0.00866\n",
      "Epoch:20 Batch:28 Loss:0.00636\n",
      "Epoch:40 Batch:28 Loss:0.00713\n",
      "Epoch:60 Batch:28 Loss:0.00615\n",
      "Epoch:80 Batch:28 Loss:0.00631\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.023\n",
      "Epoch:10 Batch:10 Loss:0.023\n",
      "Epoch:20 Batch:10 Loss:0.023\n",
      "Epoch:30 Batch:10 Loss:0.024\n",
      "Epoch:40 Batch:10 Loss:0.023\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 21.907878917774358 setps: 18 count: 18\n",
      "reward: 20.233356476850165 setps: 15 count: 33\n",
      "reward: 21.207666237317607 setps: 17 count: 50\n",
      "reward: 21.18668431523111 setps: 17 count: 67\n",
      "reward: 21.98538035166421 setps: 17 count: 84\n",
      "reward: 18.88331617224758 setps: 15 count: 99\n",
      "reward: 20.138690261621377 setps: 16 count: 115\n",
      "reward: 20.05817415126075 setps: 16 count: 131\n",
      "reward: 21.57268774086842 setps: 17 count: 148\n",
      "reward: 20.907333491618914 setps: 15 count: 163\n",
      "reward: 22.147570657648608 setps: 17 count: 180\n",
      "reward: 22.50252807299548 setps: 18 count: 198\n",
      "reward: 20.51162542068778 setps: 15 count: 213\n",
      "reward: 20.426007848205334 setps: 15 count: 228\n",
      "reward: 20.503597459955202 setps: 17 count: 245\n",
      "reward: 20.489233979792335 setps: 15 count: 260\n",
      "reward: 21.17988509158458 setps: 19 count: 279\n",
      "reward: 20.680191091842428 setps: 16 count: 295\n",
      "reward: 20.984633087269323 setps: 16 count: 311\n",
      "reward: 21.012335541807985 setps: 16 count: 327\n",
      "reward: 22.161821445783424 setps: 19 count: 346\n",
      "reward: 21.75264082274807 setps: 16 count: 362\n",
      "reward: 21.95018037924019 setps: 17 count: 379\n",
      "reward: 20.601662787736856 setps: 16 count: 395\n",
      "reward: 20.63990975552006 setps: 15 count: 410\n",
      "reward: 20.729665807946002 setps: 16 count: 426\n",
      "reward: 19.371009234309895 setps: 15 count: 441\n",
      "reward: 21.707272791385183 setps: 18 count: 459\n",
      "reward: 18.786076009640237 setps: 14 count: 473\n",
      "reward: 21.785460693368805 setps: 16 count: 489\n",
      "reward: 20.41695371130772 setps: 15 count: 504\n",
      "reward: 24.74969252142182 setps: 20 count: 524\n",
      "reward: 22.123595208929324 setps: 17 count: 541\n",
      "reward: 19.613829246134266 setps: 15 count: 556\n",
      "reward: 20.299050306378923 setps: 15 count: 571\n",
      "reward: 20.78326489793981 setps: 16 count: 587\n",
      "reward: 20.51988701638038 setps: 15 count: 602\n",
      "reward: 19.96733208067453 setps: 15 count: 617\n",
      "reward: 21.804419002002394 setps: 16 count: 633\n",
      "reward: 19.87057622516877 setps: 15 count: 648\n",
      "reward: 19.334748718145423 setps: 15 count: 663\n",
      "reward: 20.452966350612407 setps: 15 count: 678\n",
      "reward: 21.37739063201152 setps: 16 count: 694\n",
      "reward: 20.369437490357083 setps: 17 count: 711\n",
      "reward: 19.805178457564033 setps: 15 count: 726\n",
      "reward: 19.981556071741217 setps: 16 count: 742\n",
      "reward: 22.295367996639108 setps: 18 count: 760\n",
      "reward: 23.025227498714226 setps: 18 count: 778\n",
      "reward: 19.443718990241177 setps: 16 count: 794\n",
      "reward: 20.257711238546474 setps: 16 count: 810\n",
      "reward: 21.011208592617184 setps: 16 count: 826\n",
      "reward: 21.182798406417717 setps: 17 count: 843\n",
      "reward: 22.355178096052256 setps: 18 count: 861\n",
      "reward: 20.909385296748948 setps: 16 count: 877\n",
      "reward: 21.426908362208636 setps: 15 count: 892\n",
      "reward: 19.90371614207688 setps: 16 count: 908\n",
      "reward: 22.083910582408134 setps: 18 count: 926\n",
      "reward: 22.220864076184807 setps: 17 count: 943\n",
      "reward: 20.400140889766043 setps: 17 count: 960\n",
      "reward: 24.422532920914815 setps: 19 count: 979\n",
      "reward: 19.402891605890183 setps: 15 count: 994\n",
      "avg rewards: 20.980588766034728\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:36 Loss:0.00575\n",
      "Epoch:20 Batch:36 Loss:0.00570\n",
      "Epoch:40 Batch:36 Loss:0.00490\n",
      "Epoch:60 Batch:36 Loss:0.00658\n",
      "Epoch:80 Batch:36 Loss:0.00671\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.026\n",
      "Epoch:10 Batch:10 Loss:0.025\n",
      "Epoch:20 Batch:10 Loss:0.024\n",
      "Epoch:30 Batch:10 Loss:0.024\n",
      "Epoch:40 Batch:10 Loss:0.024\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 17.961845272693605 setps: 14 count: 14\n",
      "reward: 18.79097217974922 setps: 15 count: 29\n",
      "reward: 19.722483931116585 setps: 15 count: 44\n",
      "reward: 20.571586786120314 setps: 16 count: 60\n",
      "reward: 19.051663187424126 setps: 15 count: 75\n",
      "reward: 17.830432134936565 setps: 14 count: 89\n",
      "reward: 18.31681208897498 setps: 15 count: 104\n",
      "reward: 18.047376120272382 setps: 15 count: 119\n",
      "reward: 18.45228739932936 setps: 14 count: 133\n",
      "reward: 19.239284093005693 setps: 15 count: 148\n",
      "reward: 19.061315402257605 setps: 15 count: 163\n",
      "reward: 17.453839083974888 setps: 15 count: 178\n",
      "reward: 18.39825204862573 setps: 15 count: 193\n",
      "reward: 19.16522293424787 setps: 15 count: 208\n",
      "reward: 19.776492841730946 setps: 16 count: 224\n",
      "reward: 18.13941211616912 setps: 15 count: 239\n",
      "reward: 18.98355665232811 setps: 15 count: 254\n",
      "reward: 20.938273301195295 setps: 17 count: 271\n",
      "reward: 17.795904178309137 setps: 15 count: 286\n",
      "reward: 20.35878742403002 setps: 16 count: 302\n",
      "reward: 19.86137015339045 setps: 15 count: 317\n",
      "reward: 20.476113301726578 setps: 16 count: 333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 19.808223314842326 setps: 16 count: 349\n",
      "reward: 17.100784523626494 setps: 15 count: 364\n",
      "reward: 17.693099053263722 setps: 14 count: 378\n",
      "reward: 18.217189651641817 setps: 15 count: 393\n",
      "reward: 21.313251835343543 setps: 16 count: 409\n",
      "reward: 17.356840264089985 setps: 15 count: 424\n",
      "reward: 17.696436602786704 setps: 15 count: 439\n",
      "reward: 19.868315993607396 setps: 15 count: 454\n",
      "reward: 17.42197048474482 setps: 14 count: 468\n",
      "reward: 18.03371705368336 setps: 15 count: 483\n",
      "reward: 19.342984384267766 setps: 16 count: 499\n",
      "reward: 18.020959198809578 setps: 16 count: 515\n",
      "reward: 19.949526017133028 setps: 16 count: 531\n",
      "reward: 18.564396151255643 setps: 15 count: 546\n",
      "reward: 21.08046096954931 setps: 17 count: 563\n",
      "reward: 19.898993568510924 setps: 16 count: 579\n",
      "reward: 19.943163689956414 setps: 16 count: 595\n",
      "reward: 17.68083970175503 setps: 16 count: 611\n",
      "reward: 20.736340194374502 setps: 16 count: 627\n",
      "reward: 19.28783486410248 setps: 15 count: 642\n",
      "reward: 18.618184192296756 setps: 15 count: 657\n",
      "reward: 19.99564366072445 setps: 15 count: 672\n",
      "reward: 18.996194794443724 setps: 15 count: 687\n",
      "reward: 18.46727134462708 setps: 15 count: 702\n",
      "reward: 18.681707786538755 setps: 14 count: 716\n",
      "reward: 18.483678947699083 setps: 15 count: 731\n",
      "reward: 17.164587762746667 setps: 15 count: 746\n",
      "reward: 20.42116224629863 setps: 16 count: 762\n",
      "reward: 19.40245570278639 setps: 15 count: 777\n",
      "reward: 20.043195632001144 setps: 15 count: 792\n",
      "reward: 20.16357446087495 setps: 16 count: 808\n",
      "reward: 18.787868661340326 setps: 15 count: 823\n",
      "reward: 19.541644765081582 setps: 16 count: 839\n",
      "reward: 20.31736304964433 setps: 16 count: 855\n",
      "reward: 17.824411451457127 setps: 14 count: 869\n",
      "reward: 18.725877743298767 setps: 14 count: 883\n",
      "reward: 19.593027569602416 setps: 15 count: 898\n",
      "reward: 19.917461955742322 setps: 15 count: 913\n",
      "reward: 20.724217142126868 setps: 16 count: 929\n",
      "reward: 17.699318497466447 setps: 14 count: 943\n",
      "reward: 16.864450428521376 setps: 14 count: 957\n",
      "reward: 19.542438581433085 setps: 15 count: 972\n",
      "reward: 17.662242492160292 setps: 15 count: 987\n",
      "avg rewards: 19.00071675412101\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:44 Loss:0.00695\n",
      "Epoch:20 Batch:44 Loss:0.00654\n",
      "Epoch:40 Batch:44 Loss:0.00601\n",
      "Epoch:60 Batch:44 Loss:0.00632\n",
      "Epoch:80 Batch:44 Loss:0.00581\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.027\n",
      "Epoch:10 Batch:10 Loss:0.027\n",
      "Epoch:20 Batch:10 Loss:0.025\n",
      "Epoch:30 Batch:10 Loss:0.026\n",
      "Epoch:40 Batch:10 Loss:0.026\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 21.0403795777529 setps: 17 count: 17\n",
      "reward: 17.245903152228856 setps: 14 count: 31\n",
      "reward: 20.44889469009213 setps: 16 count: 47\n",
      "reward: 20.247543924109777 setps: 16 count: 63\n",
      "reward: 19.86570561003755 setps: 17 count: 80\n",
      "reward: 20.96190555252833 setps: 16 count: 96\n",
      "reward: 19.028969182688158 setps: 15 count: 111\n",
      "reward: 21.09562472984544 setps: 16 count: 127\n",
      "reward: 19.009158050695262 setps: 17 count: 144\n",
      "reward: 19.208580645020994 setps: 17 count: 161\n",
      "reward: 19.8779554312161 setps: 17 count: 178\n",
      "reward: 19.23245040009788 setps: 18 count: 196\n",
      "reward: 21.6509622850077 setps: 18 count: 214\n",
      "reward: 20.497605381048928 setps: 17 count: 231\n",
      "reward: 21.189468276896513 setps: 17 count: 248\n",
      "reward: 19.27818861181295 setps: 17 count: 265\n",
      "reward: 17.749102208665864 setps: 16 count: 281\n",
      "reward: 18.628267439435877 setps: 17 count: 298\n",
      "reward: 19.580577437632016 setps: 17 count: 315\n",
      "reward: 22.957346305274402 setps: 18 count: 333\n",
      "reward: 18.118488289172817 setps: 17 count: 350\n",
      "reward: 20.510199890147486 setps: 16 count: 366\n",
      "reward: 18.979742342827375 setps: 16 count: 382\n",
      "reward: 16.927207703197205 setps: 17 count: 399\n",
      "reward: 16.399263108584275 setps: 14 count: 413\n",
      "reward: 18.415471435878132 setps: 17 count: 430\n",
      "reward: 19.990274127341404 setps: 16 count: 446\n",
      "reward: 18.10100322713842 setps: 16 count: 462\n",
      "reward: 18.81135444956162 setps: 16 count: 478\n",
      "reward: 20.98666253293341 setps: 17 count: 495\n",
      "reward: 21.927275001120865 setps: 19 count: 514\n",
      "reward: 19.03074379786558 setps: 16 count: 530\n",
      "reward: 19.605361773636833 setps: 15 count: 545\n",
      "reward: 19.313820884603775 setps: 18 count: 563\n",
      "reward: 19.692964455268516 setps: 16 count: 579\n",
      "reward: 23.350283289569774 setps: 18 count: 597\n",
      "reward: 19.10164874521579 setps: 17 count: 614\n",
      "reward: 16.84380350770516 setps: 15 count: 629\n",
      "reward: 20.280612781102537 setps: 17 count: 646\n",
      "reward: 18.388303446480133 setps: 16 count: 662\n",
      "reward: 19.997820572362983 setps: 16 count: 678\n",
      "reward: 20.332118059889762 setps: 16 count: 694\n",
      "reward: 20.938700623414476 setps: 19 count: 713\n",
      "reward: 17.302355210746462 setps: 17 count: 730\n",
      "reward: 20.322556509147393 setps: 18 count: 748\n",
      "reward: 18.826293575302405 setps: 17 count: 765\n",
      "reward: 18.962470492983996 setps: 17 count: 782\n",
      "reward: 21.609244936193864 setps: 16 count: 798\n",
      "reward: 18.56602802191483 setps: 16 count: 814\n",
      "reward: 17.560719555072136 setps: 17 count: 831\n",
      "reward: 16.95698459703417 setps: 16 count: 847\n",
      "reward: 20.868804726298546 setps: 18 count: 865\n",
      "reward: 18.389644237190076 setps: 18 count: 883\n",
      "reward: 19.041520968417174 setps: 17 count: 900\n",
      "reward: 16.814451524504694 setps: 15 count: 915\n",
      "reward: 18.371322135825174 setps: 18 count: 933\n",
      "reward: 19.629190562372973 setps: 17 count: 950\n",
      "reward: 21.10927325608209 setps: 19 count: 969\n",
      "reward: 18.11874032300548 setps: 14 count: 983\n",
      "reward: 17.903072247513048 setps: 17 count: 1000\n",
      "avg rewards: 19.419839763611844\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:54 Loss:0.00660\n",
      "Epoch:20 Batch:54 Loss:0.00669\n",
      "Epoch:40 Batch:54 Loss:0.00643\n",
      "Epoch:60 Batch:54 Loss:0.00577\n",
      "Epoch:80 Batch:54 Loss:0.00702\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.029\n",
      "Epoch:10 Batch:10 Loss:0.026\n",
      "Epoch:20 Batch:10 Loss:0.025\n",
      "Epoch:30 Batch:10 Loss:0.026\n",
      "Epoch:40 Batch:10 Loss:0.026\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 23.79445602406922 setps: 20 count: 20\n",
      "reward: 16.855819949574656 setps: 14 count: 34\n",
      "reward: 25.454153240482263 setps: 21 count: 55\n",
      "reward: 18.084963485570917 setps: 19 count: 74\n",
      "reward: 24.689639043483474 setps: 21 count: 95\n",
      "reward: 21.4027144667707 setps: 19 count: 114\n",
      "reward: 18.904888518602817 setps: 17 count: 131\n",
      "reward: 13.923886694367681 setps: 13 count: 144\n",
      "reward: 16.502940236822177 setps: 15 count: 159\n",
      "reward: 16.603598631429485 setps: 14 count: 173\n",
      "reward: 15.441719369911876 setps: 14 count: 187\n",
      "reward: 18.905461630546778 setps: 15 count: 202\n",
      "reward: 17.822486705984918 setps: 15 count: 217\n",
      "reward: 24.595022678856914 setps: 19 count: 236\n",
      "reward: 14.478537480517115 setps: 14 count: 250\n",
      "reward: 16.140631518274315 setps: 15 count: 265\n",
      "reward: 16.997700198211533 setps: 14 count: 279\n",
      "reward: 23.89810624001257 setps: 20 count: 299\n",
      "reward: 15.231479467838655 setps: 14 count: 313\n",
      "reward: 17.364715814508962 setps: 14 count: 327\n",
      "reward: 17.720248410534982 setps: 16 count: 343\n",
      "reward: 20.754703966296802 setps: 15 count: 358\n",
      "reward: 15.165266543874166 setps: 14 count: 372\n",
      "reward: 20.269148771346952 setps: 20 count: 392\n",
      "reward: 21.34973932566645 setps: 20 count: 412\n",
      "reward: 17.434165584265433 setps: 16 count: 428\n",
      "reward: 22.044670544797555 setps: 21 count: 449\n",
      "reward: 17.376454753259896 setps: 15 count: 464\n",
      "reward: 17.537597496675154 setps: 14 count: 478\n",
      "reward: 20.900943048448244 setps: 17 count: 495\n",
      "reward: 23.9695562415116 setps: 20 count: 515\n",
      "reward: 23.452196814888158 setps: 21 count: 536\n",
      "reward: 25.432826002140065 setps: 20 count: 556\n",
      "reward: 16.344156887075226 setps: 13 count: 569\n",
      "reward: 20.14580517860304 setps: 16 count: 585\n",
      "reward: 17.80487986846565 setps: 18 count: 603\n",
      "reward: 20.556859823800917 setps: 17 count: 620\n",
      "reward: 15.444364241496078 setps: 13 count: 633\n",
      "reward: 20.706407883722566 setps: 19 count: 652\n",
      "reward: 24.39252697918564 setps: 23 count: 675\n",
      "reward: 25.52775808466977 setps: 21 count: 696\n",
      "reward: 24.0917697132143 setps: 19 count: 715\n",
      "reward: 18.148901031348217 setps: 15 count: 730\n",
      "reward: 16.359898334088214 setps: 15 count: 745\n",
      "reward: 17.658758003584808 setps: 20 count: 765\n",
      "reward: 16.811994175988367 setps: 15 count: 780\n",
      "reward: 21.994026773015502 setps: 21 count: 801\n",
      "reward: 20.94465526822169 setps: 21 count: 822\n",
      "reward: 21.567901122949845 setps: 20 count: 842\n",
      "reward: 18.46427988063078 setps: 15 count: 857\n",
      "reward: 18.671678524115126 setps: 15 count: 872\n",
      "reward: 19.963247107478672 setps: 20 count: 892\n",
      "reward: 17.2778440604583 setps: 16 count: 908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 24.398096735247236 setps: 19 count: 927\n",
      "reward: 22.774188057539863 setps: 21 count: 948\n",
      "reward: 22.89063857197616 setps: 22 count: 970\n",
      "reward: 20.759537951803942 setps: 19 count: 989\n",
      "avg rewards: 19.722817774705657\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:65 Loss:0.00605\n",
      "Epoch:20 Batch:65 Loss:0.00585\n",
      "Epoch:40 Batch:65 Loss:0.00567\n",
      "Epoch:60 Batch:65 Loss:0.00615\n",
      "Epoch:80 Batch:65 Loss:0.00638\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.025\n",
      "Epoch:10 Batch:10 Loss:0.024\n",
      "Epoch:20 Batch:10 Loss:0.023\n",
      "Epoch:30 Batch:10 Loss:0.023\n",
      "Epoch:40 Batch:10 Loss:0.022\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 18.02129056980484 setps: 17 count: 17\n",
      "reward: 19.456233431732105 setps: 16 count: 33\n",
      "reward: 18.47658223959588 setps: 16 count: 49\n",
      "reward: 14.912943219827138 setps: 13 count: 62\n",
      "reward: 15.477779671957247 setps: 15 count: 77\n",
      "reward: 16.284979590223518 setps: 15 count: 92\n",
      "reward: 18.80845346510905 setps: 17 count: 109\n",
      "reward: 20.25600024371815 setps: 19 count: 128\n",
      "reward: 18.544254426825503 setps: 14 count: 142\n",
      "reward: 22.979715079109877 setps: 19 count: 161\n",
      "reward: 18.77147539300931 setps: 18 count: 179\n",
      "reward: 22.44264666426024 setps: 20 count: 199\n",
      "reward: 15.125340370112095 setps: 12 count: 211\n",
      "reward: 15.785398811122285 setps: 13 count: 224\n",
      "reward: 17.671974853829305 setps: 17 count: 241\n",
      "reward: 19.004280495663988 setps: 17 count: 258\n",
      "reward: 19.301163428864673 setps: 18 count: 276\n",
      "reward: 19.17892562860943 setps: 17 count: 293\n",
      "reward: 18.672901008982443 setps: 17 count: 310\n",
      "reward: 16.015821199672065 setps: 14 count: 324\n",
      "reward: 20.985327048013275 setps: 18 count: 342\n",
      "reward: 21.22912956717919 setps: 19 count: 361\n",
      "reward: 21.888245853835546 setps: 19 count: 380\n",
      "reward: 21.27089644400694 setps: 18 count: 398\n",
      "reward: 21.284286524337947 setps: 20 count: 418\n",
      "reward: 19.87586106072122 setps: 19 count: 437\n",
      "reward: 18.33913222203555 setps: 15 count: 452\n",
      "reward: 19.243862748367246 setps: 16 count: 468\n",
      "reward: 16.62732555895054 setps: 14 count: 482\n",
      "reward: 16.539694826735648 setps: 14 count: 496\n",
      "reward: 15.031793202749395 setps: 13 count: 509\n",
      "reward: 18.584851058086496 setps: 15 count: 524\n",
      "reward: 21.753698931628602 setps: 19 count: 543\n",
      "reward: 18.81290521992341 setps: 17 count: 560\n",
      "reward: 16.75167115350778 setps: 14 count: 574\n",
      "reward: 20.067951003053164 setps: 17 count: 591\n",
      "reward: 16.317712555712205 setps: 16 count: 607\n",
      "reward: 17.14395872241439 setps: 14 count: 621\n",
      "reward: 19.621262432514047 setps: 17 count: 638\n",
      "reward: 17.954521080221458 setps: 16 count: 654\n",
      "reward: 15.139718544684 setps: 13 count: 667\n",
      "reward: 16.6856895588935 setps: 14 count: 681\n",
      "reward: 23.825620683199666 setps: 20 count: 701\n",
      "reward: 19.367115182365524 setps: 16 count: 717\n",
      "reward: 17.448028474420425 setps: 15 count: 732\n",
      "reward: 22.06608603633795 setps: 18 count: 750\n",
      "reward: 15.54835548233532 setps: 13 count: 763\n",
      "reward: 13.613513094333758 setps: 12 count: 775\n",
      "reward: 16.679215838634992 setps: 15 count: 790\n",
      "reward: 15.21460239796288 setps: 13 count: 803\n",
      "reward: 21.54473678919749 setps: 18 count: 821\n",
      "reward: 17.40028768118937 setps: 14 count: 835\n",
      "reward: 16.042706626435393 setps: 14 count: 849\n",
      "reward: 22.672760839018153 setps: 21 count: 870\n",
      "reward: 17.422901738889053 setps: 15 count: 885\n",
      "reward: 16.94725721323775 setps: 14 count: 899\n",
      "reward: 20.944367692692317 setps: 19 count: 918\n",
      "reward: 21.74847098909376 setps: 19 count: 937\n",
      "reward: 16.102373887800788 setps: 14 count: 951\n",
      "reward: 25.315137238892202 setps: 22 count: 973\n",
      "reward: 20.65458331012487 setps: 18 count: 991\n",
      "avg rewards: 18.637635677143646\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:77 Loss:0.00535\n",
      "Epoch:20 Batch:77 Loss:0.00792\n",
      "Epoch:40 Batch:77 Loss:0.00694\n",
      "Epoch:60 Batch:77 Loss:0.00619\n",
      "Epoch:80 Batch:77 Loss:0.00744\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.026\n",
      "Epoch:10 Batch:10 Loss:0.026\n",
      "Epoch:20 Batch:10 Loss:0.025\n",
      "Epoch:30 Batch:10 Loss:0.026\n",
      "Epoch:40 Batch:10 Loss:0.024\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 16.462621697962458 setps: 14 count: 14\n",
      "reward: 16.733884209429384 setps: 14 count: 28\n",
      "reward: 16.299683944147542 setps: 14 count: 42\n",
      "reward: 17.98631529497943 setps: 15 count: 57\n",
      "reward: 19.230071715658415 setps: 16 count: 73\n",
      "reward: 16.8131609693708 setps: 14 count: 87\n",
      "reward: 19.42138198822067 setps: 16 count: 103\n",
      "reward: 14.377622458903353 setps: 14 count: 117\n",
      "reward: 16.472702226394905 setps: 15 count: 132\n",
      "reward: 12.734804305617581 setps: 14 count: 146\n",
      "reward: 14.986825165430494 setps: 13 count: 159\n",
      "reward: 14.946661427798972 setps: 14 count: 173\n",
      "reward: 11.250973650508964 setps: 14 count: 187\n",
      "reward: 11.772756838523492 setps: 14 count: 201\n",
      "reward: 18.532828920491735 setps: 16 count: 217\n",
      "reward: 18.255700933263867 setps: 16 count: 233\n",
      "reward: 20.97639815115399 setps: 17 count: 250\n",
      "reward: 15.52536630345421 setps: 13 count: 263\n",
      "reward: 13.424189844800273 setps: 14 count: 277\n",
      "reward: 15.7472880840156 setps: 14 count: 291\n",
      "reward: 17.036541854763346 setps: 14 count: 305\n",
      "reward: 14.981738664025034 setps: 14 count: 319\n",
      "reward: 14.098146153232664 setps: 13 count: 332\n",
      "reward: 16.856125043756037 setps: 15 count: 347\n",
      "reward: 18.144143014375004 setps: 14 count: 361\n",
      "reward: 18.953252206405157 setps: 15 count: 376\n",
      "reward: 13.594912347574427 setps: 14 count: 390\n",
      "reward: 19.432158379282914 setps: 17 count: 407\n",
      "reward: 17.165589940834618 setps: 14 count: 421\n",
      "reward: 12.382197880517923 setps: 14 count: 435\n",
      "reward: 14.891342367939068 setps: 14 count: 449\n",
      "reward: 15.021333516351294 setps: 13 count: 462\n",
      "reward: 18.853017590759553 setps: 15 count: 477\n",
      "reward: 21.739800169950467 setps: 17 count: 494\n",
      "reward: 17.836156407161617 setps: 14 count: 508\n",
      "reward: 16.85671225018741 setps: 14 count: 522\n",
      "reward: 18.46196139168169 setps: 15 count: 537\n",
      "reward: 16.429917705213303 setps: 13 count: 550\n",
      "reward: 15.231083866884 setps: 15 count: 565\n",
      "reward: 18.86693816519837 setps: 16 count: 581\n",
      "reward: 18.003430056378424 setps: 14 count: 595\n",
      "reward: 15.837597213426488 setps: 13 count: 608\n",
      "reward: 15.454913386108819 setps: 14 count: 622\n",
      "reward: 12.40429232107708 setps: 14 count: 636\n",
      "reward: 19.80605105532741 setps: 16 count: 652\n",
      "reward: 14.73450944493379 setps: 15 count: 667\n",
      "reward: 17.660442237793177 setps: 15 count: 682\n",
      "reward: 18.682470234706127 setps: 16 count: 698\n",
      "reward: 19.448946208396226 setps: 15 count: 713\n",
      "reward: 19.710606011928753 setps: 17 count: 730\n",
      "reward: 16.248473386316622 setps: 14 count: 744\n",
      "reward: 17.32115112685715 setps: 13 count: 757\n",
      "reward: 16.56372524063918 setps: 14 count: 771\n",
      "reward: 15.00811763351521 setps: 14 count: 785\n",
      "reward: 18.514176544750804 setps: 15 count: 800\n",
      "reward: 16.088242012991394 setps: 14 count: 814\n",
      "reward: 18.06034463737451 setps: 15 count: 829\n",
      "reward: 17.595767718854766 setps: 15 count: 844\n",
      "reward: 16.953995087301884 setps: 14 count: 858\n",
      "reward: 11.578137606737437 setps: 15 count: 873\n",
      "reward: 17.873823236802128 setps: 14 count: 887\n",
      "reward: 13.046533934677427 setps: 14 count: 901\n",
      "reward: 16.896702087255836 setps: 13 count: 914\n",
      "reward: 17.360778190704877 setps: 13 count: 927\n",
      "reward: 18.719566178691455 setps: 15 count: 942\n",
      "reward: 18.114435561187566 setps: 14 count: 956\n",
      "reward: 15.72023647171154 setps: 15 count: 971\n",
      "reward: 15.588974178819624 setps: 14 count: 985\n",
      "reward: 14.095015532354592 setps: 14 count: 999\n",
      "avg rewards: 16.54892408092522\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:89 Loss:0.00581\n",
      "Epoch:20 Batch:89 Loss:0.00654\n",
      "Epoch:40 Batch:89 Loss:0.00606\n",
      "Epoch:60 Batch:89 Loss:0.00657\n",
      "Epoch:80 Batch:89 Loss:0.00658\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.026\n",
      "Epoch:10 Batch:10 Loss:0.024\n",
      "Epoch:20 Batch:10 Loss:0.023\n",
      "Epoch:30 Batch:10 Loss:0.024\n",
      "Epoch:40 Batch:10 Loss:0.024\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 16.3425870470659 setps: 14 count: 14\n",
      "reward: 19.544811906905668 setps: 16 count: 30\n",
      "reward: 15.52970790265681 setps: 14 count: 44\n",
      "reward: 16.653873312324865 setps: 14 count: 58\n",
      "reward: 15.077865734315127 setps: 13 count: 71\n",
      "reward: 14.172534683909907 setps: 12 count: 83\n",
      "reward: 16.097086695162577 setps: 13 count: 96\n",
      "reward: 16.74596069548716 setps: 14 count: 110\n",
      "reward: 15.275808622436307 setps: 14 count: 124\n",
      "reward: 22.30185449257115 setps: 21 count: 145\n",
      "reward: 14.350989194335126 setps: 13 count: 158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 15.042478368301817 setps: 13 count: 171\n",
      "reward: 15.903180290925956 setps: 13 count: 184\n",
      "reward: 15.624505736387801 setps: 13 count: 197\n",
      "reward: 20.956951596892033 setps: 19 count: 216\n",
      "reward: 15.285034951660782 setps: 13 count: 229\n",
      "reward: 15.78649196508777 setps: 13 count: 242\n",
      "reward: 14.93491884811374 setps: 13 count: 255\n",
      "reward: 16.095810915467155 setps: 14 count: 269\n",
      "reward: 16.504107332309648 setps: 14 count: 283\n",
      "reward: 15.86389586543228 setps: 13 count: 296\n",
      "reward: 15.529938811819012 setps: 13 count: 309\n",
      "reward: 15.574746185430559 setps: 13 count: 322\n",
      "reward: 14.877717063181624 setps: 13 count: 335\n",
      "reward: 15.310574284421453 setps: 13 count: 348\n",
      "reward: 15.415054147894262 setps: 13 count: 361\n",
      "reward: 15.196331311101677 setps: 13 count: 374\n",
      "reward: 16.292751757027872 setps: 13 count: 387\n",
      "reward: 19.21306336843845 setps: 18 count: 405\n",
      "reward: 18.60605527685693 setps: 18 count: 423\n",
      "reward: 14.673294545378303 setps: 14 count: 437\n",
      "reward: 14.938590901682618 setps: 13 count: 450\n",
      "reward: 17.97537643369578 setps: 16 count: 466\n",
      "reward: 17.474067751572875 setps: 16 count: 482\n",
      "reward: 24.515692263662643 setps: 22 count: 504\n",
      "reward: 23.849601373083708 setps: 22 count: 526\n",
      "reward: 16.325635489639534 setps: 15 count: 541\n",
      "reward: 18.65555801507725 setps: 17 count: 558\n",
      "reward: 15.060491588263538 setps: 13 count: 571\n",
      "reward: 16.33124812306778 setps: 15 count: 586\n",
      "reward: 17.213385917730918 setps: 15 count: 601\n",
      "reward: 24.123952041714798 setps: 21 count: 622\n",
      "reward: 14.651838983563355 setps: 14 count: 636\n",
      "reward: 12.951583250513067 setps: 14 count: 650\n",
      "reward: 14.080183836593642 setps: 14 count: 664\n",
      "reward: 15.652519521157952 setps: 13 count: 677\n",
      "reward: 17.936243849419405 setps: 16 count: 693\n",
      "reward: 17.329055597029217 setps: 14 count: 707\n",
      "reward: 18.081461625562223 setps: 15 count: 722\n",
      "reward: 15.37283572755259 setps: 13 count: 735\n",
      "reward: 23.571541240409715 setps: 20 count: 755\n",
      "reward: 20.781800806934186 setps: 18 count: 773\n",
      "reward: 13.06985454314563 setps: 12 count: 785\n",
      "reward: 18.70736119879729 setps: 20 count: 805\n",
      "reward: 15.631453430540573 setps: 13 count: 818\n",
      "reward: 15.754980941439864 setps: 14 count: 832\n",
      "reward: 17.625536123428898 setps: 15 count: 847\n",
      "reward: 14.270403198653366 setps: 13 count: 860\n",
      "reward: 15.686171296893736 setps: 13 count: 873\n",
      "reward: 17.300941775231333 setps: 14 count: 887\n",
      "reward: 15.917814273911063 setps: 13 count: 900\n",
      "reward: 13.2750559412365 setps: 12 count: 912\n",
      "reward: 18.547752933354058 setps: 17 count: 929\n",
      "reward: 17.57842085273296 setps: 15 count: 944\n",
      "reward: 13.446774091338742 setps: 13 count: 957\n",
      "reward: 17.84586658959888 setps: 15 count: 972\n",
      "reward: 15.394345480672198 setps: 13 count: 985\n",
      "reward: 16.88573135201732 setps: 14 count: 999\n",
      "avg rewards: 16.743986930444397\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:103 Loss:0.00632\n",
      "Epoch:20 Batch:103 Loss:0.00644\n",
      "Epoch:40 Batch:103 Loss:0.00611\n",
      "Epoch:60 Batch:103 Loss:0.00613\n",
      "Epoch:80 Batch:103 Loss:0.00685\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.026\n",
      "Epoch:10 Batch:10 Loss:0.025\n",
      "Epoch:20 Batch:10 Loss:0.027\n",
      "Epoch:30 Batch:10 Loss:0.025\n",
      "Epoch:40 Batch:10 Loss:0.026\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 17.804106734028025 setps: 14 count: 14\n",
      "reward: 17.39493529684405 setps: 14 count: 28\n",
      "reward: 14.142300543034796 setps: 14 count: 42\n",
      "reward: 16.375178161462824 setps: 13 count: 55\n",
      "reward: 24.49468380451726 setps: 21 count: 76\n",
      "reward: 16.251244798213882 setps: 13 count: 89\n",
      "reward: 19.592328209096742 setps: 18 count: 107\n",
      "reward: 15.086314885383763 setps: 13 count: 120\n",
      "reward: 14.839269564242569 setps: 14 count: 134\n",
      "reward: 22.06064975487534 setps: 19 count: 153\n",
      "reward: 14.57327394137683 setps: 14 count: 167\n",
      "reward: 17.014164391763916 setps: 14 count: 181\n",
      "reward: 20.30340105266514 setps: 19 count: 200\n",
      "reward: 17.794532994487966 setps: 14 count: 214\n",
      "reward: 16.429025071059005 setps: 14 count: 228\n",
      "reward: 18.302203931778788 setps: 14 count: 242\n",
      "reward: 18.302123828420008 setps: 15 count: 257\n",
      "reward: 21.726544737843508 setps: 19 count: 276\n",
      "reward: 18.318317980995925 setps: 15 count: 291\n",
      "reward: 18.91018294381356 setps: 15 count: 306\n",
      "reward: 16.682070923980792 setps: 14 count: 320\n",
      "reward: 15.952374963201873 setps: 13 count: 333\n",
      "reward: 15.807806527064528 setps: 13 count: 346\n",
      "reward: 17.186875898741707 setps: 14 count: 360\n",
      "reward: 15.24823571673769 setps: 13 count: 373\n",
      "reward: 14.010596970297048 setps: 12 count: 385\n",
      "reward: 16.738024751533523 setps: 13 count: 398\n",
      "reward: 20.503292906429856 setps: 18 count: 416\n",
      "reward: 17.84084194908064 setps: 14 count: 430\n",
      "reward: 17.689894639709383 setps: 14 count: 444\n",
      "reward: 16.139504402308376 setps: 13 count: 457\n",
      "reward: 21.57853056844324 setps: 20 count: 477\n",
      "reward: 15.412432916817489 setps: 13 count: 490\n",
      "reward: 20.211855686749914 setps: 19 count: 509\n",
      "reward: 16.20392586515809 setps: 14 count: 523\n",
      "reward: 17.094482564764625 setps: 13 count: 536\n",
      "reward: 16.12388595875236 setps: 14 count: 550\n",
      "reward: 16.04821825373947 setps: 13 count: 563\n",
      "reward: 16.86087508201308 setps: 14 count: 577\n",
      "reward: 15.480339290211848 setps: 14 count: 591\n",
      "reward: 16.259571958091694 setps: 13 count: 604\n",
      "reward: 21.45587214576517 setps: 19 count: 623\n",
      "reward: 17.251763915707127 setps: 13 count: 636\n",
      "reward: 14.167129305636626 setps: 14 count: 650\n",
      "reward: 16.62886132613494 setps: 13 count: 663\n",
      "reward: 16.22754898420681 setps: 13 count: 676\n",
      "reward: 13.884339660851401 setps: 13 count: 689\n",
      "reward: 22.01007582889434 setps: 22 count: 711\n",
      "reward: 19.12861017962714 setps: 17 count: 728\n",
      "reward: 17.186591459614284 setps: 13 count: 741\n",
      "reward: 16.3635982531574 setps: 13 count: 754\n",
      "reward: 14.20938640252716 setps: 13 count: 767\n",
      "reward: 17.80464447301638 setps: 14 count: 781\n",
      "reward: 15.522754792815247 setps: 13 count: 794\n",
      "reward: 17.876556946651544 setps: 16 count: 810\n",
      "reward: 16.772769288398557 setps: 13 count: 823\n",
      "reward: 17.19599217954383 setps: 14 count: 837\n",
      "reward: 19.248163041372027 setps: 16 count: 853\n",
      "reward: 14.30314516328508 setps: 13 count: 866\n",
      "reward: 14.613045309600418 setps: 13 count: 879\n",
      "reward: 15.621349593438207 setps: 13 count: 892\n",
      "reward: 14.788707513082771 setps: 12 count: 904\n",
      "reward: 15.970264116235192 setps: 13 count: 917\n",
      "reward: 19.02341006915958 setps: 17 count: 934\n",
      "reward: 14.101006066962146 setps: 13 count: 947\n",
      "reward: 20.49375691434834 setps: 18 count: 965\n",
      "reward: 16.760693371087836 setps: 13 count: 978\n",
      "reward: 14.114754103089217 setps: 12 count: 990\n",
      "avg rewards: 17.169311923881466\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:118 Loss:0.00585\n",
      "Epoch:20 Batch:118 Loss:0.00630\n",
      "Epoch:40 Batch:118 Loss:0.00630\n",
      "Epoch:60 Batch:118 Loss:0.00712\n",
      "Epoch:80 Batch:118 Loss:0.00695\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.029\n",
      "Epoch:10 Batch:10 Loss:0.027\n",
      "Epoch:20 Batch:10 Loss:0.029\n",
      "Epoch:30 Batch:10 Loss:0.027\n",
      "Epoch:40 Batch:10 Loss:0.029\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 15.560596380192145 setps: 14 count: 14\n",
      "reward: 14.122210604012068 setps: 12 count: 26\n",
      "reward: 27.497998281780745 setps: 26 count: 52\n",
      "reward: 15.542877550206319 setps: 13 count: 65\n",
      "reward: 15.926567854685707 setps: 13 count: 78\n",
      "reward: 16.673526822078568 setps: 13 count: 91\n",
      "reward: 28.49749130333075 setps: 26 count: 117\n",
      "reward: 29.13361771215277 setps: 26 count: 143\n",
      "reward: 16.204261773634062 setps: 13 count: 156\n",
      "reward: 15.746964371405191 setps: 13 count: 169\n",
      "reward: 15.542667080776297 setps: 13 count: 182\n",
      "reward: 15.016010713773719 setps: 12 count: 194\n",
      "reward: 13.500861449528019 setps: 12 count: 206\n",
      "reward: 24.990552736519025 setps: 23 count: 229\n",
      "reward: 25.14931632115186 setps: 23 count: 252\n",
      "reward: 16.07127497234178 setps: 13 count: 265\n",
      "reward: 14.236466941398975 setps: 12 count: 277\n",
      "reward: 15.580397147443726 setps: 14 count: 291\n",
      "reward: 23.289666043620674 setps: 22 count: 313\n",
      "reward: 15.164727077686983 setps: 13 count: 326\n",
      "reward: 22.70224533634318 setps: 21 count: 347\n",
      "reward: 15.661729197643579 setps: 13 count: 360\n",
      "reward: 18.107417276594788 setps: 16 count: 376\n",
      "reward: 14.992649837973296 setps: 13 count: 389\n",
      "reward: 16.214154522020543 setps: 14 count: 403\n",
      "reward: 16.967824790930898 setps: 14 count: 417\n",
      "reward: 23.877292959773325 setps: 23 count: 440\n",
      "reward: 30.667705122473127 setps: 29 count: 469\n",
      "reward: 26.99216805401374 setps: 25 count: 494\n",
      "reward: 15.491528337648196 setps: 14 count: 508\n",
      "reward: 16.494312290709058 setps: 14 count: 522\n",
      "reward: 13.940590207840431 setps: 12 count: 534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 15.518215483284438 setps: 13 count: 547\n",
      "reward: 16.507406383140186 setps: 13 count: 560\n",
      "reward: 15.350489521543203 setps: 13 count: 573\n",
      "reward: 22.938343645157868 setps: 22 count: 595\n",
      "reward: 14.963276932628652 setps: 13 count: 608\n",
      "reward: 15.90627476297668 setps: 14 count: 622\n",
      "reward: 19.920101991860427 setps: 18 count: 640\n",
      "reward: 15.690878219796284 setps: 13 count: 653\n",
      "reward: 15.471903058244788 setps: 13 count: 666\n",
      "reward: 17.03169812325504 setps: 14 count: 680\n",
      "reward: 19.95243324366311 setps: 17 count: 697\n",
      "reward: 20.810365766265026 setps: 23 count: 720\n",
      "reward: 15.648611065043948 setps: 13 count: 733\n",
      "reward: 15.95061546104116 setps: 13 count: 746\n",
      "reward: 91.60644978030902 setps: 74 count: 820\n",
      "reward: 15.56419663819106 setps: 13 count: 833\n",
      "reward: 30.764657990158593 setps: 27 count: 860\n",
      "reward: 16.36436303438677 setps: 13 count: 873\n",
      "reward: 16.340418702295572 setps: 13 count: 886\n",
      "reward: 15.65428188769729 setps: 13 count: 899\n",
      "reward: 16.70764603285206 setps: 13 count: 912\n",
      "reward: 26.649523616430823 setps: 24 count: 936\n",
      "reward: 17.5173020391463 setps: 14 count: 950\n",
      "reward: 27.606157607342176 setps: 25 count: 975\n",
      "reward: 15.814809429564047 setps: 13 count: 988\n",
      "avg rewards: 19.961545464701018\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:133 Loss:0.00643\n",
      "Epoch:20 Batch:133 Loss:0.00588\n",
      "Epoch:40 Batch:133 Loss:0.00623\n",
      "Epoch:60 Batch:133 Loss:0.00641\n",
      "Epoch:80 Batch:133 Loss:0.00636\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.028\n",
      "Epoch:10 Batch:10 Loss:0.026\n",
      "Epoch:20 Batch:10 Loss:0.026\n",
      "Epoch:30 Batch:10 Loss:0.025\n",
      "Epoch:40 Batch:10 Loss:0.023\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 90.16993386397955 setps: 130 count: 130\n",
      "reward: 52.40343604622322 setps: 62 count: 192\n",
      "reward: 57.72802312395944 setps: 83 count: 275\n",
      "reward: 49.33452129872312 setps: 58 count: 333\n",
      "reward: 63.1952328933956 setps: 70 count: 403\n",
      "reward: 61.01444024128432 setps: 65 count: 468\n",
      "reward: 66.91055688783236 setps: 78 count: 546\n",
      "reward: 55.7026992169529 setps: 57 count: 603\n",
      "reward: 60.41658828695799 setps: 70 count: 673\n",
      "reward: 79.57424601680539 setps: 103 count: 776\n",
      "reward: 65.63064967116514 setps: 65 count: 841\n",
      "reward: 53.8302422570443 setps: 54 count: 895\n",
      "reward: 72.6392303367131 setps: 71 count: 966\n",
      "avg rewards: 63.73460001084896\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:150 Loss:0.00662\n",
      "Epoch:20 Batch:150 Loss:0.00697\n",
      "Epoch:40 Batch:150 Loss:0.00599\n",
      "Epoch:60 Batch:150 Loss:0.00622\n",
      "Epoch:80 Batch:150 Loss:0.00584\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.024\n",
      "Epoch:10 Batch:10 Loss:0.021\n",
      "Epoch:20 Batch:10 Loss:0.022\n",
      "Epoch:30 Batch:10 Loss:0.020\n",
      "Epoch:40 Batch:10 Loss:0.020\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 25.740367756271734 setps: 43 count: 43\n",
      "reward: 9.574191800258998 setps: 100 count: 143\n",
      "reward: 14.573053957198862 setps: 17 count: 160\n",
      "reward: 16.131196743194597 setps: 20 count: 180\n",
      "reward: 15.254625198274155 setps: 17 count: 197\n",
      "reward: 18.335896664265597 setps: 30 count: 227\n",
      "reward: 15.506347072677453 setps: 31 count: 258\n",
      "reward: 28.98672329677211 setps: 59 count: 317\n",
      "reward: 16.791516546611096 setps: 23 count: 340\n",
      "reward: 22.327325075528645 setps: 32 count: 372\n",
      "reward: 31.48196780597209 setps: 60 count: 432\n",
      "reward: 29.74214154290094 setps: 51 count: 483\n",
      "reward: 20.267274387543143 setps: 28 count: 511\n",
      "reward: 16.90888607804518 setps: 24 count: 535\n",
      "reward: 30.057048835024766 setps: 65 count: 600\n",
      "reward: 17.68046940848435 setps: 22 count: 622\n",
      "reward: 23.64900790851388 setps: 71 count: 693\n",
      "reward: 48.67394440287605 setps: 194 count: 887\n",
      "reward: 34.06852016597839 setps: 66 count: 953\n",
      "reward: 18.698248862271427 setps: 24 count: 977\n",
      "avg rewards: 22.72243767543317\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:167 Loss:0.00688\n",
      "Epoch:20 Batch:167 Loss:0.00627\n",
      "Epoch:40 Batch:167 Loss:0.00640\n",
      "Epoch:60 Batch:167 Loss:0.00620\n",
      "Epoch:80 Batch:167 Loss:0.00653\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.022\n",
      "Epoch:10 Batch:10 Loss:0.022\n",
      "Epoch:20 Batch:10 Loss:0.021\n",
      "Epoch:30 Batch:10 Loss:0.020\n",
      "Epoch:40 Batch:10 Loss:0.021\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 9.78097587798111 setps: 13 count: 13\n",
      "reward: 11.221209855195776 setps: 14 count: 27\n",
      "reward: 11.798640258098022 setps: 13 count: 40\n",
      "reward: 14.971736235037678 setps: 19 count: 59\n",
      "reward: 11.675948456789772 setps: 16 count: 75\n",
      "reward: 10.442281725801877 setps: 13 count: 88\n",
      "reward: 12.085597060142026 setps: 13 count: 101\n",
      "reward: 11.107414960749153 setps: 12 count: 113\n",
      "reward: 15.817631624879139 setps: 18 count: 131\n",
      "reward: 12.2959770118483 setps: 14 count: 145\n",
      "reward: 12.353820065180479 setps: 13 count: 158\n",
      "reward: 11.159517312860407 setps: 14 count: 172\n",
      "reward: 10.986874446168077 setps: 12 count: 184\n",
      "reward: 60.96860728234749 setps: 77 count: 261\n",
      "reward: 13.497835385214422 setps: 18 count: 279\n",
      "reward: 11.210092123159848 setps: 14 count: 293\n",
      "reward: 12.912685374701685 setps: 14 count: 307\n",
      "reward: 11.292929644729883 setps: 14 count: 321\n",
      "reward: 12.593964445173333 setps: 14 count: 335\n",
      "reward: 12.332855840188857 setps: 15 count: 350\n",
      "reward: 47.16707399566658 setps: 61 count: 411\n",
      "reward: 12.898223617911572 setps: 18 count: 429\n",
      "reward: 11.286608861695274 setps: 14 count: 443\n",
      "reward: 13.000433217822867 setps: 18 count: 461\n",
      "reward: 11.394314342105643 setps: 14 count: 475\n",
      "reward: 12.888097476598336 setps: 14 count: 489\n",
      "reward: 10.563731360605741 setps: 14 count: 503\n",
      "reward: 53.134972810171924 setps: 72 count: 575\n",
      "reward: 11.473501062321883 setps: 14 count: 589\n",
      "reward: 12.02160971904086 setps: 16 count: 605\n",
      "reward: 11.045855876695715 setps: 12 count: 617\n",
      "reward: 11.628102041897362 setps: 14 count: 631\n",
      "reward: 12.054061705712227 setps: 14 count: 645\n",
      "reward: 11.338545390345097 setps: 14 count: 659\n",
      "reward: 12.372127573382748 setps: 13 count: 672\n",
      "reward: 10.289629917142154 setps: 13 count: 685\n",
      "reward: 11.579059627359674 setps: 12 count: 697\n",
      "reward: 15.976454577338881 setps: 18 count: 715\n",
      "reward: 13.04074189575622 setps: 15 count: 730\n",
      "reward: 13.537978196307087 setps: 15 count: 745\n",
      "reward: 12.321781946595 setps: 15 count: 760\n",
      "reward: 11.99854293804383 setps: 15 count: 775\n",
      "reward: 11.205492233234692 setps: 14 count: 789\n",
      "reward: 14.837502948955807 setps: 18 count: 807\n",
      "reward: 11.21053352515737 setps: 16 count: 823\n",
      "reward: 12.748801860716775 setps: 14 count: 837\n",
      "reward: 12.13637421287858 setps: 13 count: 850\n",
      "reward: 15.333028424266375 setps: 19 count: 869\n",
      "reward: 11.393366141911244 setps: 13 count: 882\n",
      "reward: 11.62757457093394 setps: 14 count: 896\n",
      "reward: 10.473431667017579 setps: 12 count: 908\n",
      "reward: 10.492774154659127 setps: 13 count: 921\n",
      "avg rewards: 14.518786978394143\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:186 Loss:0.00674\n",
      "Epoch:20 Batch:186 Loss:0.00574\n",
      "Epoch:40 Batch:186 Loss:0.00693\n",
      "Epoch:60 Batch:186 Loss:0.00608\n",
      "Epoch:80 Batch:186 Loss:0.00659\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.021\n",
      "Epoch:10 Batch:10 Loss:0.020\n",
      "Epoch:20 Batch:10 Loss:0.020\n",
      "Epoch:30 Batch:10 Loss:0.020\n",
      "Epoch:40 Batch:10 Loss:0.020\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 12.02333294847049 setps: 16 count: 16\n",
      "reward: 10.36155948448577 setps: 13 count: 29\n",
      "reward: 11.37830628896918 setps: 12 count: 41\n",
      "reward: 9.63325439440232 setps: 13 count: 54\n",
      "reward: 10.373550722782966 setps: 12 count: 66\n",
      "reward: 10.475247892117476 setps: 13 count: 79\n",
      "reward: 10.409654067247173 setps: 13 count: 92\n",
      "reward: 12.974897300437439 setps: 16 count: 108\n",
      "reward: 13.640504943182165 setps: 17 count: 125\n",
      "reward: 9.793887611063838 setps: 13 count: 138\n",
      "reward: 8.144929180854524 setps: 14 count: 152\n",
      "reward: 10.261077898573422 setps: 13 count: 165\n",
      "reward: 11.351546774429153 setps: 14 count: 179\n",
      "reward: 11.627317340075388 setps: 17 count: 196\n",
      "reward: 12.92676983442943 setps: 20 count: 216\n",
      "reward: 9.126199161240947 setps: 13 count: 229\n",
      "reward: 9.739040984974418 setps: 13 count: 242\n",
      "reward: 11.050896305911007 setps: 13 count: 255\n",
      "reward: 12.441662838002957 setps: 14 count: 269\n",
      "reward: 13.385010422156482 setps: 14 count: 283\n",
      "reward: 10.445142619906983 setps: 13 count: 296\n",
      "reward: 10.509210663325211 setps: 14 count: 310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 9.055634294358605 setps: 13 count: 323\n",
      "reward: 11.94769568148622 setps: 14 count: 337\n",
      "reward: 11.572164544684345 setps: 14 count: 351\n",
      "reward: 12.418644782771297 setps: 13 count: 364\n",
      "reward: 13.419393564239726 setps: 18 count: 382\n",
      "reward: 9.298861008357196 setps: 14 count: 396\n",
      "reward: 11.133772182269604 setps: 14 count: 410\n",
      "reward: 11.121150765402126 setps: 14 count: 424\n",
      "reward: 9.928228433833283 setps: 12 count: 436\n",
      "reward: 11.870716723329679 setps: 15 count: 451\n",
      "reward: 9.218782582471611 setps: 13 count: 464\n",
      "reward: 10.483623294188872 setps: 14 count: 478\n",
      "reward: 11.145877665470472 setps: 15 count: 493\n",
      "reward: 10.234809022217814 setps: 13 count: 506\n",
      "reward: 10.485368624168041 setps: 14 count: 520\n",
      "reward: 12.77971575509291 setps: 15 count: 535\n",
      "reward: 9.955175618837528 setps: 13 count: 548\n",
      "reward: 9.831871506781315 setps: 13 count: 561\n",
      "reward: 10.928143341759277 setps: 14 count: 575\n",
      "reward: 9.934227533738884 setps: 13 count: 588\n",
      "reward: 11.19785970913217 setps: 14 count: 602\n",
      "reward: 8.651680450518327 setps: 14 count: 616\n",
      "reward: 12.059668778929334 setps: 14 count: 630\n",
      "reward: 11.303202286918532 setps: 14 count: 644\n",
      "reward: 9.174330625373113 setps: 13 count: 657\n",
      "reward: 11.14965310011903 setps: 14 count: 671\n",
      "reward: 11.175229048133769 setps: 14 count: 685\n",
      "reward: 11.933952573663554 setps: 16 count: 701\n",
      "reward: 9.877308181638364 setps: 13 count: 714\n",
      "reward: 10.201213458854182 setps: 13 count: 727\n",
      "reward: 11.3992552523574 setps: 13 count: 740\n",
      "reward: 9.83999668592587 setps: 14 count: 754\n",
      "reward: 12.451426344079664 setps: 14 count: 768\n",
      "reward: 9.66347305099189 setps: 14 count: 782\n",
      "reward: 10.607468190198412 setps: 13 count: 795\n",
      "reward: 11.574777407193322 setps: 14 count: 809\n",
      "reward: 10.881583405123092 setps: 15 count: 824\n",
      "reward: 11.874656091441285 setps: 14 count: 838\n",
      "reward: 12.175221155807957 setps: 15 count: 853\n",
      "reward: 10.851505990796431 setps: 14 count: 867\n",
      "reward: 9.98485747077357 setps: 13 count: 880\n",
      "reward: 9.610273002331088 setps: 13 count: 893\n",
      "reward: 11.54020369588252 setps: 14 count: 907\n",
      "reward: 11.807841603983249 setps: 14 count: 921\n",
      "reward: 10.082565176606295 setps: 13 count: 934\n",
      "reward: 10.4851598727706 setps: 13 count: 947\n",
      "reward: 11.803791296793497 setps: 19 count: 966\n",
      "reward: 9.955884455810882 setps: 13 count: 979\n",
      "reward: 10.619111263041848 setps: 14 count: 993\n",
      "avg rewards: 10.884084594756153\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:206 Loss:0.00629\n",
      "Epoch:20 Batch:206 Loss:0.00632\n",
      "Epoch:40 Batch:206 Loss:0.00725\n",
      "Epoch:60 Batch:206 Loss:0.00674\n",
      "Epoch:80 Batch:206 Loss:0.00680\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.019\n",
      "Epoch:10 Batch:10 Loss:0.018\n",
      "Epoch:20 Batch:10 Loss:0.019\n",
      "Epoch:30 Batch:10 Loss:0.019\n",
      "Epoch:40 Batch:10 Loss:0.019\n",
      "Done!\n",
      "############# start HopperBulletEnv-v0 training ###################\n",
      "(49,)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 20.97538884583046 setps: 13 count: 13\n",
      "reward: 21.923532951595554 setps: 9 count: 22\n",
      "reward: 19.931734142596543 setps: 13 count: 35\n",
      "reward: 19.703589302601177 setps: 15 count: 50\n",
      "reward: 14.842328347435979 setps: 12 count: 62\n",
      "reward: 18.954920718724313 setps: 10 count: 72\n",
      "reward: 20.24583417209942 setps: 6 count: 78\n",
      "reward: 21.267523782628995 setps: 17 count: 95\n",
      "reward: 20.180090577832015 setps: 8 count: 103\n",
      "reward: 17.94195054049924 setps: 10 count: 113\n",
      "reward: 18.597923523397185 setps: 9 count: 122\n",
      "reward: 21.312231421892648 setps: 13 count: 135\n",
      "reward: 18.332538233292873 setps: 7 count: 142\n",
      "reward: 17.11101608568133 setps: 7 count: 149\n",
      "reward: 24.411474273460044 setps: 19 count: 168\n",
      "reward: 21.557896907195392 setps: 8 count: 176\n",
      "reward: 22.882939618367526 setps: 18 count: 194\n",
      "reward: 22.083698661428937 setps: 10 count: 204\n",
      "reward: 19.859034173689725 setps: 17 count: 221\n",
      "reward: 15.429065697788609 setps: 22 count: 243\n",
      "reward: 30.1095988350833 setps: 28 count: 271\n",
      "reward: 14.548017579570294 setps: 12 count: 283\n",
      "reward: 23.27345498015493 setps: 15 count: 298\n",
      "reward: 18.33740917606046 setps: 13 count: 311\n",
      "reward: 20.45688589369529 setps: 7 count: 318\n",
      "reward: 22.35189521490829 setps: 9 count: 327\n",
      "reward: 19.53628399658628 setps: 11 count: 338\n",
      "reward: 17.698771996737925 setps: 12 count: 350\n",
      "reward: 22.00203650168696 setps: 11 count: 361\n",
      "reward: 21.895267137874907 setps: 9 count: 370\n",
      "reward: 19.3652323180082 setps: 9 count: 379\n",
      "reward: 22.94499871002481 setps: 17 count: 396\n",
      "reward: 16.43852192825434 setps: 8 count: 404\n",
      "reward: 21.61265600663464 setps: 11 count: 415\n",
      "reward: 21.070550879892835 setps: 9 count: 424\n",
      "reward: 23.747534233638724 setps: 11 count: 435\n",
      "reward: 17.856239153513165 setps: 6 count: 441\n",
      "reward: 15.978120854137526 setps: 8 count: 449\n",
      "reward: 16.337517172035586 setps: 6 count: 455\n",
      "reward: 14.678148544294524 setps: 17 count: 472\n",
      "reward: 18.31866989947885 setps: 11 count: 483\n",
      "reward: 22.855222467308337 setps: 23 count: 506\n",
      "reward: 23.021252692509734 setps: 11 count: 517\n",
      "reward: 19.262878717921556 setps: 8 count: 525\n",
      "reward: 18.49310167986405 setps: 6 count: 531\n",
      "reward: 22.465513417411422 setps: 11 count: 542\n",
      "reward: 22.414080251341513 setps: 12 count: 554\n",
      "reward: 21.969412544788792 setps: 9 count: 563\n",
      "reward: 20.877537151266004 setps: 18 count: 581\n",
      "reward: 21.323044967719763 setps: 12 count: 593\n",
      "reward: 17.966716757387623 setps: 13 count: 606\n",
      "reward: 16.90628786788002 setps: 9 count: 615\n",
      "reward: 18.675110618436882 setps: 7 count: 622\n",
      "reward: 15.036017840428393 setps: 10 count: 632\n",
      "reward: 19.867375566817643 setps: 13 count: 645\n",
      "reward: 22.562178904934264 setps: 14 count: 659\n",
      "reward: 17.79534917327837 setps: 10 count: 669\n",
      "reward: 20.368381655469424 setps: 19 count: 688\n",
      "reward: 19.469731562872767 setps: 19 count: 707\n",
      "reward: 13.859166824744898 setps: 9 count: 716\n",
      "reward: 14.765196183614897 setps: 22 count: 738\n",
      "reward: 20.6674979677322 setps: 11 count: 749\n",
      "reward: 25.546941893556504 setps: 15 count: 764\n",
      "reward: 22.375816848492832 setps: 14 count: 778\n",
      "reward: 20.09923542332108 setps: 8 count: 786\n",
      "reward: 18.000908296651325 setps: 7 count: 793\n",
      "reward: 18.48264475432079 setps: 14 count: 807\n",
      "reward: 18.1216742231205 setps: 10 count: 817\n",
      "reward: 21.57557197337592 setps: 21 count: 838\n",
      "reward: 17.14321998871892 setps: 8 count: 846\n",
      "reward: 25.585727718668927 setps: 16 count: 862\n",
      "reward: 19.07806923189928 setps: 9 count: 871\n",
      "reward: 21.019207803597965 setps: 16 count: 887\n",
      "reward: 16.542599208591856 setps: 9 count: 896\n",
      "reward: 23.80964888384478 setps: 12 count: 908\n",
      "reward: 19.061791719055325 setps: 15 count: 923\n",
      "reward: 18.678793926670913 setps: 6 count: 929\n",
      "reward: 16.025117885939835 setps: 7 count: 936\n",
      "reward: 19.777081886738596 setps: 11 count: 947\n",
      "reward: 18.082770815376715 setps: 11 count: 958\n",
      "reward: 15.857595005759503 setps: 12 count: 970\n",
      "reward: 23.235881978322865 setps: 18 count: 988\n",
      "reward: 20.109258748931463 setps: 10 count: 998\n",
      "avg rewards: 19.818471518301138\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:1.46676\n",
      "Epoch:20 Batch:1 Loss:0.49268\n",
      "Epoch:40 Batch:1 Loss:0.16932\n",
      "Epoch:60 Batch:1 Loss:0.09902\n",
      "Epoch:80 Batch:1 Loss:0.06543\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.225\n",
      "Epoch:10 Batch:10 Loss:0.120\n",
      "Epoch:20 Batch:10 Loss:0.123\n",
      "Epoch:30 Batch:10 Loss:0.129\n",
      "Epoch:40 Batch:10 Loss:0.119\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 5.022340014151448 setps: 31 count: 31\n",
      "reward: 4.3404436659497145 setps: 28 count: 59\n",
      "reward: 0.41433728100132505 setps: 28 count: 87\n",
      "reward: 1.5944766052285544 setps: 27 count: 114\n",
      "reward: -2.76718853940256 setps: 27 count: 141\n",
      "reward: 2.6825702987760445 setps: 30 count: 171\n",
      "reward: -0.9950879423704476 setps: 30 count: 201\n",
      "reward: 1.4701568912226315 setps: 27 count: 228\n",
      "reward: -4.408913700772974 setps: 29 count: 257\n",
      "reward: -0.19473325785074946 setps: 30 count: 287\n",
      "reward: 3.981017178945693 setps: 30 count: 317\n",
      "reward: 0.20061024289146223 setps: 30 count: 347\n",
      "reward: 0.12807962095102932 setps: 29 count: 376\n",
      "reward: -3.3737955607401098 setps: 27 count: 403\n",
      "reward: 1.4947606685786006 setps: 29 count: 432\n",
      "reward: 1.7308099111949775 setps: 28 count: 460\n",
      "reward: -2.8133766703409497 setps: 29 count: 489\n",
      "reward: 0.6351617924257869 setps: 29 count: 518\n",
      "reward: -2.2762689426599545 setps: 28 count: 546\n",
      "reward: -3.020311442451204 setps: 29 count: 575\n",
      "reward: 0.8232106618539536 setps: 28 count: 603\n",
      "reward: 0.3825775128003428 setps: 29 count: 632\n",
      "reward: 2.753559379524085 setps: 30 count: 662\n",
      "reward: 0.6456709523961754 setps: 27 count: 689\n",
      "reward: 0.1924475475359929 setps: 30 count: 719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -0.07605058113258645 setps: 28 count: 747\n",
      "reward: 0.11864012233418064 setps: 29 count: 776\n",
      "reward: 6.286791210537465 setps: 31 count: 807\n",
      "reward: 2.781293455743434 setps: 28 count: 835\n",
      "reward: 0.7297758101381744 setps: 27 count: 862\n",
      "reward: -1.5148074898184867 setps: 30 count: 892\n",
      "reward: -0.42050623090035666 setps: 28 count: 920\n",
      "reward: 5.023043079748459 setps: 30 count: 950\n",
      "reward: -0.566911241988965 setps: 28 count: 978\n",
      "avg rewards: 0.6177594795147113\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.06059\n",
      "Epoch:20 Batch:3 Loss:0.03058\n",
      "Epoch:40 Batch:3 Loss:0.02432\n",
      "Epoch:60 Batch:3 Loss:0.02167\n",
      "Epoch:80 Batch:3 Loss:0.01967\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.053\n",
      "Epoch:10 Batch:10 Loss:0.050\n",
      "Epoch:20 Batch:10 Loss:0.047\n",
      "Epoch:30 Batch:10 Loss:0.047\n",
      "Epoch:40 Batch:10 Loss:0.047\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 0.1067599212983612 setps: 31 count: 31\n",
      "reward: -0.3680540101864618 setps: 30 count: 61\n",
      "reward: -1.2392599513728797 setps: 28 count: 89\n",
      "reward: -0.7765298215497761 setps: 29 count: 118\n",
      "reward: -0.3898314152407705 setps: 30 count: 148\n",
      "reward: 0.33554398949199626 setps: 29 count: 177\n",
      "reward: 1.9086292107947873 setps: 29 count: 206\n",
      "reward: 0.6088370064140678 setps: 28 count: 234\n",
      "reward: 0.2897725149288808 setps: 28 count: 262\n",
      "reward: 3.697705169282561 setps: 31 count: 293\n",
      "reward: 5.909353499483085 setps: 31 count: 324\n",
      "reward: -1.1769811546895665 setps: 30 count: 354\n",
      "reward: 0.8743898417000298 setps: 30 count: 384\n",
      "reward: 0.13754292044322836 setps: 29 count: 413\n",
      "reward: -1.018712322115608 setps: 28 count: 441\n",
      "reward: -2.088771427958274 setps: 30 count: 471\n",
      "reward: 0.4122101334010939 setps: 30 count: 501\n",
      "reward: 0.5946548758925916 setps: 29 count: 530\n",
      "reward: 1.8769942985097248 setps: 29 count: 559\n",
      "reward: 8.184563031121977 setps: 32 count: 591\n",
      "reward: -2.652135948781507 setps: 30 count: 621\n",
      "reward: 1.6202662480485734 setps: 29 count: 650\n",
      "reward: 0.5847428951514295 setps: 29 count: 679\n",
      "reward: 4.643939787870476 setps: 31 count: 710\n",
      "reward: 2.8986863499972957 setps: 31 count: 741\n",
      "reward: 5.189890766718598 setps: 29 count: 770\n",
      "reward: 1.2211412906617616 setps: 29 count: 799\n",
      "reward: -0.2702516405872313 setps: 29 count: 828\n",
      "reward: 0.13201386955042738 setps: 30 count: 858\n",
      "reward: 3.193636285883985 setps: 30 count: 888\n",
      "reward: 0.9569670237455274 setps: 28 count: 916\n",
      "reward: 0.493334285521995 setps: 31 count: 947\n",
      "reward: 3.3978601082661033 setps: 30 count: 977\n",
      "avg rewards: 1.190572958536257\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.02000\n",
      "Epoch:20 Batch:6 Loss:0.01615\n",
      "Epoch:40 Batch:6 Loss:0.01320\n",
      "Epoch:60 Batch:6 Loss:0.01445\n",
      "Epoch:80 Batch:6 Loss:0.01552\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.041\n",
      "Epoch:10 Batch:10 Loss:0.038\n",
      "Epoch:20 Batch:10 Loss:0.041\n",
      "Epoch:30 Batch:10 Loss:0.038\n",
      "Epoch:40 Batch:10 Loss:0.040\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 38.91669393599005 setps: 45 count: 45\n",
      "reward: 39.26315205875291 setps: 47 count: 92\n",
      "reward: 28.679185742577822 setps: 37 count: 129\n",
      "reward: 34.93685443661381 setps: 42 count: 171\n",
      "reward: 31.90723549840478 setps: 39 count: 210\n",
      "reward: 29.919949322467442 setps: 37 count: 247\n",
      "reward: 30.5669495803042 setps: 38 count: 285\n",
      "reward: 35.304309236737026 setps: 39 count: 324\n",
      "reward: 27.086838410883505 setps: 36 count: 360\n",
      "reward: 29.30367678588809 setps: 37 count: 397\n",
      "reward: 34.2980772689436 setps: 41 count: 438\n",
      "reward: 29.575899486380518 setps: 37 count: 475\n",
      "reward: 43.24001493181421 setps: 45 count: 520\n",
      "reward: 35.67257429501624 setps: 41 count: 561\n",
      "reward: 20.720317879070354 setps: 36 count: 597\n",
      "reward: 28.948727456071357 setps: 37 count: 634\n",
      "reward: 46.20836563754419 setps: 49 count: 683\n",
      "reward: 35.131955971903515 setps: 42 count: 725\n",
      "reward: 39.04054695949017 setps: 44 count: 769\n",
      "reward: 31.836813783830447 setps: 39 count: 808\n",
      "reward: 33.15949600208842 setps: 40 count: 848\n",
      "reward: 32.61325917902286 setps: 39 count: 887\n",
      "reward: 35.008981565524294 setps: 41 count: 928\n",
      "reward: 36.65457840311864 setps: 43 count: 971\n",
      "avg rewards: 33.66643557618494\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.01496\n",
      "Epoch:20 Batch:10 Loss:0.01108\n",
      "Epoch:40 Batch:10 Loss:0.01047\n",
      "Epoch:60 Batch:10 Loss:0.01083\n",
      "Epoch:80 Batch:10 Loss:0.01109\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.038\n",
      "Epoch:10 Batch:10 Loss:0.032\n",
      "Epoch:20 Batch:10 Loss:0.032\n",
      "Epoch:30 Batch:10 Loss:0.032\n",
      "Epoch:40 Batch:10 Loss:0.034\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 21.88099053125624 setps: 37 count: 37\n",
      "reward: 22.46674885580724 setps: 39 count: 76\n",
      "reward: 17.42562891883863 setps: 34 count: 110\n",
      "reward: 21.186649601745014 setps: 36 count: 146\n",
      "reward: 19.708109171496474 setps: 42 count: 188\n",
      "reward: 18.19744252412929 setps: 39 count: 227\n",
      "reward: 19.861087892251085 setps: 35 count: 262\n",
      "reward: 20.260251659105418 setps: 36 count: 298\n",
      "reward: 17.421617559011793 setps: 34 count: 332\n",
      "reward: 20.641886309237453 setps: 37 count: 369\n",
      "reward: 18.58477706099366 setps: 45 count: 414\n",
      "reward: 21.014617767222806 setps: 34 count: 448\n",
      "reward: 17.456603846927464 setps: 37 count: 485\n",
      "reward: 19.451479835851927 setps: 34 count: 519\n",
      "reward: 17.449639987886258 setps: 34 count: 553\n",
      "reward: 59.505480886810986 setps: 69 count: 622\n",
      "reward: 22.880029326993103 setps: 40 count: 662\n",
      "reward: 25.006990862170642 setps: 38 count: 700\n",
      "reward: 21.22801487277757 setps: 39 count: 739\n",
      "reward: 20.875247560485146 setps: 36 count: 775\n",
      "reward: 19.491458463168236 setps: 39 count: 814\n",
      "reward: 15.843284103444605 setps: 40 count: 854\n",
      "reward: 19.255562948092116 setps: 33 count: 887\n",
      "reward: 22.98603812192741 setps: 40 count: 927\n",
      "reward: 19.24605415487749 setps: 37 count: 964\n",
      "avg rewards: 21.57302771290032\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.01163\n",
      "Epoch:20 Batch:15 Loss:0.00903\n",
      "Epoch:40 Batch:15 Loss:0.00975\n",
      "Epoch:60 Batch:15 Loss:0.00812\n",
      "Epoch:80 Batch:15 Loss:0.00881\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.032\n",
      "Epoch:10 Batch:10 Loss:0.027\n",
      "Epoch:20 Batch:10 Loss:0.027\n",
      "Epoch:30 Batch:10 Loss:0.027\n",
      "Epoch:40 Batch:10 Loss:0.027\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 12.888295605502206 setps: 34 count: 34\n",
      "reward: 10.956623765818948 setps: 37 count: 71\n",
      "reward: 22.674621398441374 setps: 43 count: 114\n",
      "reward: 8.039790216727123 setps: 40 count: 154\n",
      "reward: 16.79519778387767 setps: 36 count: 190\n",
      "reward: 34.85147768667085 setps: 55 count: 245\n",
      "reward: 14.402695036436489 setps: 36 count: 281\n",
      "reward: 16.374211404882953 setps: 35 count: 316\n",
      "reward: 15.174457595789862 setps: 36 count: 352\n",
      "reward: 14.624169363282268 setps: 35 count: 387\n",
      "reward: 19.468049557275666 setps: 38 count: 425\n",
      "reward: 14.588276153172771 setps: 36 count: 461\n",
      "reward: 13.1121737215406 setps: 36 count: 497\n",
      "reward: 12.990618002456905 setps: 36 count: 533\n",
      "reward: 22.649549493177627 setps: 43 count: 576\n",
      "reward: 21.820407692099963 setps: 42 count: 618\n",
      "reward: 27.457815757926436 setps: 49 count: 667\n",
      "reward: 9.353889607288876 setps: 37 count: 704\n",
      "reward: 12.716245032247388 setps: 35 count: 739\n",
      "reward: 17.75408628585137 setps: 41 count: 780\n",
      "reward: 10.606203192906102 setps: 35 count: 815\n",
      "reward: 13.196751749595569 setps: 37 count: 852\n",
      "reward: 10.839735545101576 setps: 40 count: 892\n",
      "reward: 8.646243179487644 setps: 41 count: 933\n",
      "reward: 14.139955934305908 setps: 36 count: 969\n",
      "avg rewards: 15.844861630474568\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:21 Loss:0.00900\n",
      "Epoch:20 Batch:21 Loss:0.00848\n",
      "Epoch:40 Batch:21 Loss:0.00731\n",
      "Epoch:60 Batch:21 Loss:0.00680\n",
      "Epoch:80 Batch:21 Loss:0.00711\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.028\n",
      "Epoch:10 Batch:10 Loss:0.028\n",
      "Epoch:20 Batch:10 Loss:0.030\n",
      "Epoch:30 Batch:10 Loss:0.026\n",
      "Epoch:40 Batch:10 Loss:0.026\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 21.95458522616391 setps: 41 count: 41\n",
      "reward: 22.57458615071228 setps: 36 count: 77\n",
      "reward: 19.107495873587318 setps: 40 count: 117\n",
      "reward: 20.88540081789106 setps: 39 count: 156\n",
      "reward: 23.149340954769276 setps: 39 count: 195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 17.79572062387597 setps: 48 count: 243\n",
      "reward: 13.218316495051841 setps: 38 count: 281\n",
      "reward: 23.018253959491272 setps: 40 count: 321\n",
      "reward: 15.43970755293267 setps: 41 count: 362\n",
      "reward: 18.555296078673567 setps: 42 count: 404\n",
      "reward: 21.15155374106253 setps: 36 count: 440\n",
      "reward: 24.597380655101738 setps: 37 count: 477\n",
      "reward: 17.78367453453102 setps: 38 count: 515\n",
      "reward: 29.403883280872833 setps: 46 count: 561\n",
      "reward: 13.851184794155415 setps: 40 count: 601\n",
      "reward: 12.965699238092931 setps: 41 count: 642\n",
      "reward: 21.947505334636663 setps: 39 count: 681\n",
      "reward: 14.029492060987103 setps: 38 count: 719\n",
      "reward: 15.816355136445775 setps: 46 count: 765\n",
      "reward: 11.82292988838162 setps: 42 count: 807\n",
      "reward: 15.889073905872644 setps: 39 count: 846\n",
      "reward: 14.242117936593418 setps: 38 count: 884\n",
      "reward: 27.639891667872153 setps: 44 count: 928\n",
      "reward: 22.290482625481673 setps: 45 count: 973\n",
      "avg rewards: 19.13041368888486\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:28 Loss:0.00812\n",
      "Epoch:20 Batch:28 Loss:0.00675\n",
      "Epoch:40 Batch:28 Loss:0.00715\n",
      "Epoch:60 Batch:28 Loss:0.00688\n",
      "Epoch:80 Batch:28 Loss:0.00637\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.027\n",
      "Epoch:10 Batch:10 Loss:0.027\n",
      "Epoch:20 Batch:10 Loss:0.025\n",
      "Epoch:30 Batch:10 Loss:0.025\n",
      "Epoch:40 Batch:10 Loss:0.025\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 36.28076659491782 setps: 46 count: 46\n",
      "reward: 33.48010832540313 setps: 45 count: 91\n",
      "reward: 32.916016165593454 setps: 42 count: 133\n",
      "reward: 36.505111956600736 setps: 52 count: 185\n",
      "reward: 31.40582856144756 setps: 45 count: 230\n",
      "reward: 34.05853679293942 setps: 46 count: 276\n",
      "reward: 39.984471348088114 setps: 45 count: 321\n",
      "reward: 33.31034263422045 setps: 46 count: 367\n",
      "reward: 33.22950662562218 setps: 45 count: 412\n",
      "reward: 30.69889623791532 setps: 41 count: 453\n",
      "reward: 34.08188126195891 setps: 46 count: 499\n",
      "reward: 30.006247211572187 setps: 39 count: 538\n",
      "reward: 30.27469993210107 setps: 42 count: 580\n",
      "reward: 30.823798833653562 setps: 42 count: 622\n",
      "reward: 32.79878160002845 setps: 47 count: 669\n",
      "reward: 36.06799680658297 setps: 46 count: 715\n",
      "reward: 32.93441460778267 setps: 45 count: 760\n",
      "reward: 34.41371028508438 setps: 51 count: 811\n",
      "reward: 30.47564598083263 setps: 43 count: 854\n",
      "reward: 32.665572296381285 setps: 49 count: 903\n",
      "reward: 32.03498393564224 setps: 47 count: 950\n",
      "reward: 32.64827548874891 setps: 42 count: 992\n",
      "avg rewards: 33.23161788559624\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:36 Loss:0.00599\n",
      "Epoch:20 Batch:36 Loss:0.00737\n",
      "Epoch:40 Batch:36 Loss:0.00630\n",
      "Epoch:60 Batch:36 Loss:0.00517\n",
      "Epoch:80 Batch:36 Loss:0.00630\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.029\n",
      "Epoch:10 Batch:10 Loss:0.033\n",
      "Epoch:20 Batch:10 Loss:0.029\n",
      "Epoch:30 Batch:10 Loss:0.033\n",
      "Epoch:40 Batch:10 Loss:0.031\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 39.08478390971287 setps: 47 count: 47\n",
      "reward: 38.118111591912744 setps: 42 count: 89\n",
      "reward: 40.022142466981315 setps: 54 count: 143\n",
      "reward: 40.812827875917726 setps: 46 count: 189\n",
      "reward: 36.18441851151439 setps: 48 count: 237\n",
      "reward: 37.45625143206853 setps: 45 count: 282\n",
      "reward: 38.3159634867974 setps: 44 count: 326\n",
      "reward: 38.41167458483541 setps: 47 count: 373\n",
      "reward: 37.791531177204156 setps: 46 count: 419\n",
      "reward: 39.15938196151983 setps: 53 count: 472\n",
      "reward: 44.20292564969568 setps: 51 count: 523\n",
      "reward: 38.99323703889094 setps: 44 count: 567\n",
      "reward: 39.190254833558 setps: 45 count: 612\n",
      "reward: 38.90040009897784 setps: 44 count: 656\n",
      "reward: 38.94718701261446 setps: 46 count: 702\n",
      "reward: 38.134254012504236 setps: 50 count: 752\n",
      "reward: 42.46333025752829 setps: 50 count: 802\n",
      "reward: 38.41776052316419 setps: 45 count: 847\n",
      "reward: 39.292537273409835 setps: 47 count: 894\n",
      "reward: 35.8671505164355 setps: 47 count: 941\n",
      "reward: 25.548371507103727 setps: 40 count: 981\n",
      "avg rewards: 38.34830932011177\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:44 Loss:0.00537\n",
      "Epoch:20 Batch:44 Loss:0.00557\n",
      "Epoch:40 Batch:44 Loss:0.00660\n",
      "Epoch:60 Batch:44 Loss:0.00557\n",
      "Epoch:80 Batch:44 Loss:0.00623\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.034\n",
      "Epoch:10 Batch:10 Loss:0.037\n",
      "Epoch:20 Batch:10 Loss:0.038\n",
      "Epoch:30 Batch:10 Loss:0.037\n",
      "Epoch:40 Batch:10 Loss:0.037\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 45.132288343837736 setps: 72 count: 72\n",
      "reward: -19.789848213100047 setps: 125 count: 197\n",
      "reward: -17.738872302787787 setps: 121 count: 318\n",
      "reward: -19.221957395515346 setps: 127 count: 445\n",
      "reward: 44.15106058062084 setps: 80 count: 525\n",
      "reward: 56.02184112389805 setps: 256 count: 781\n",
      "reward: -23.507516729975762 setps: 98 count: 879\n",
      "reward: 42.84304074910613 setps: 79 count: 958\n",
      "avg rewards: 13.486254519510476\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:54 Loss:0.00674\n",
      "Epoch:20 Batch:54 Loss:0.00530\n",
      "Epoch:40 Batch:54 Loss:0.00532\n",
      "Epoch:60 Batch:54 Loss:0.00532\n",
      "Epoch:80 Batch:54 Loss:0.00556\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.041\n",
      "Epoch:10 Batch:10 Loss:0.037\n",
      "Epoch:20 Batch:10 Loss:0.037\n",
      "Epoch:30 Batch:10 Loss:0.028\n",
      "Epoch:40 Batch:10 Loss:0.030\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 39.59259647635482 setps: 53 count: 53\n",
      "reward: 19.614920073853895 setps: 60 count: 113\n",
      "reward: 33.22279784675341 setps: 96 count: 209\n",
      "reward: 10.46409570859543 setps: 63 count: 272\n",
      "reward: 4.791660537954903 setps: 64 count: 336\n",
      "reward: 27.610611923215025 setps: 56 count: 392\n",
      "reward: 35.97107993615646 setps: 53 count: 445\n",
      "reward: 23.828500261876627 setps: 61 count: 506\n",
      "reward: 45.70007123580289 setps: 66 count: 572\n",
      "reward: 38.00686722549289 setps: 61 count: 633\n",
      "reward: 6.95846401382733 setps: 60 count: 693\n",
      "reward: 13.416433952531944 setps: 58 count: 751\n",
      "reward: 41.20194356401768 setps: 56 count: 807\n",
      "reward: 6.79178005934083 setps: 55 count: 862\n",
      "reward: 52.54487711431283 setps: 61 count: 923\n",
      "reward: 47.10777712333947 setps: 69 count: 992\n",
      "avg rewards: 27.926529815839153\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:65 Loss:0.00569\n",
      "Epoch:20 Batch:65 Loss:0.00568\n",
      "Epoch:40 Batch:65 Loss:0.00551\n",
      "Epoch:60 Batch:65 Loss:0.00681\n",
      "Epoch:80 Batch:65 Loss:0.00560\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.041\n",
      "Epoch:10 Batch:10 Loss:0.021\n",
      "Epoch:20 Batch:10 Loss:0.021\n",
      "Epoch:30 Batch:10 Loss:0.020\n",
      "Epoch:40 Batch:10 Loss:0.025\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 86.71307507309683 setps: 94 count: 94\n",
      "reward: 61.20783451812164 setps: 181 count: 275\n",
      "reward: 58.14547766696924 setps: 231 count: 506\n",
      "reward: 96.45706710379312 setps: 106 count: 612\n",
      "reward: 72.55760585831015 setps: 192 count: 804\n",
      "avg rewards: 75.0162120440582\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:77 Loss:0.00598\n",
      "Epoch:20 Batch:77 Loss:0.00417\n",
      "Epoch:40 Batch:77 Loss:0.00467\n",
      "Epoch:60 Batch:77 Loss:0.00440\n",
      "Epoch:80 Batch:77 Loss:0.00594\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.023\n",
      "Epoch:10 Batch:10 Loss:0.026\n",
      "Epoch:20 Batch:10 Loss:0.020\n",
      "Epoch:30 Batch:10 Loss:0.019\n",
      "Epoch:40 Batch:10 Loss:0.017\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 29.36747524977254 setps: 23 count: 23\n",
      "reward: 42.32359889354121 setps: 31 count: 54\n",
      "reward: 35.57774564932042 setps: 25 count: 79\n",
      "reward: 51.803668299126734 setps: 41 count: 120\n",
      "reward: 39.920926039051835 setps: 30 count: 150\n",
      "reward: 54.55644406863284 setps: 44 count: 194\n",
      "reward: 52.69582277619193 setps: 44 count: 238\n",
      "reward: 38.65964666034707 setps: 32 count: 270\n",
      "reward: 28.238768409626214 setps: 25 count: 295\n",
      "reward: 40.31397645674879 setps: 33 count: 328\n",
      "reward: 37.3360398578021 setps: 28 count: 356\n",
      "reward: 45.31531535458635 setps: 40 count: 396\n",
      "reward: 36.85991697020072 setps: 25 count: 421\n",
      "reward: 36.47254349631112 setps: 25 count: 446\n",
      "reward: 47.11704504800581 setps: 36 count: 482\n",
      "reward: 37.3212888333539 setps: 26 count: 508\n",
      "reward: 42.67187651500861 setps: 31 count: 539\n",
      "reward: 36.980574714360415 setps: 28 count: 567\n",
      "reward: 50.237935817638935 setps: 39 count: 606\n",
      "reward: 52.8905989492705 setps: 44 count: 650\n",
      "reward: 40.135590133823285 setps: 31 count: 681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 46.695278467016756 setps: 35 count: 716\n",
      "reward: 37.47572535844956 setps: 29 count: 745\n",
      "reward: 36.47332091474818 setps: 26 count: 771\n",
      "reward: 40.6958458245761 setps: 34 count: 805\n",
      "reward: 54.10819359393499 setps: 45 count: 850\n",
      "reward: 40.276163243465994 setps: 30 count: 880\n",
      "reward: 41.204281997126245 setps: 31 count: 911\n",
      "reward: 38.09490365313249 setps: 28 count: 939\n",
      "reward: 34.526415698655185 setps: 23 count: 962\n",
      "avg rewards: 41.54489756479423\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:89 Loss:0.00589\n",
      "Epoch:20 Batch:89 Loss:0.00532\n",
      "Epoch:40 Batch:89 Loss:0.00485\n",
      "Epoch:60 Batch:89 Loss:0.00452\n",
      "Epoch:80 Batch:89 Loss:0.00550\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.019\n",
      "Epoch:10 Batch:10 Loss:0.020\n",
      "Epoch:20 Batch:10 Loss:0.021\n",
      "Epoch:30 Batch:10 Loss:0.018\n",
      "Epoch:40 Batch:10 Loss:0.021\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 53.2736336627713 setps: 40 count: 40\n",
      "reward: 40.79701607475582 setps: 30 count: 70\n",
      "reward: 55.704827200637375 setps: 42 count: 112\n",
      "reward: 51.065973221084285 setps: 41 count: 153\n",
      "reward: 43.559177559013186 setps: 34 count: 187\n",
      "reward: 37.324726180122525 setps: 25 count: 212\n",
      "reward: 37.559191776123775 setps: 25 count: 237\n",
      "reward: 28.345672113593896 setps: 21 count: 258\n",
      "reward: 38.977121245405584 setps: 27 count: 285\n",
      "reward: 43.820967008359716 setps: 31 count: 316\n",
      "reward: 39.15702821914309 setps: 27 count: 343\n",
      "reward: 42.99243765328866 setps: 32 count: 375\n",
      "reward: 40.30860412046603 setps: 28 count: 403\n",
      "reward: 39.13862249826342 setps: 27 count: 430\n",
      "reward: 40.42499308394472 setps: 32 count: 462\n",
      "reward: 41.00885848677863 setps: 29 count: 491\n",
      "reward: 40.57859015372815 setps: 33 count: 524\n",
      "reward: 40.3493133424534 setps: 32 count: 556\n",
      "reward: 28.694332723251136 setps: 25 count: 581\n",
      "reward: 43.192892018584814 setps: 32 count: 613\n",
      "reward: 38.27746292834054 setps: 29 count: 642\n",
      "reward: 36.34346121103298 setps: 24 count: 666\n",
      "reward: 38.86855952540064 setps: 27 count: 693\n",
      "reward: 48.31862590116216 setps: 35 count: 728\n",
      "reward: 40.13699772995023 setps: 31 count: 759\n",
      "reward: 47.74368457124948 setps: 36 count: 795\n",
      "reward: 55.427429589959495 setps: 42 count: 837\n",
      "reward: 52.48477588578099 setps: 50 count: 887\n",
      "reward: 54.53558348041552 setps: 41 count: 928\n",
      "reward: 37.320657188927 setps: 26 count: 954\n",
      "reward: 35.4499268384563 setps: 25 count: 979\n",
      "avg rewards: 42.296165909433704\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:103 Loss:0.00538\n",
      "Epoch:20 Batch:103 Loss:0.00532\n",
      "Epoch:40 Batch:103 Loss:0.00533\n",
      "Epoch:60 Batch:103 Loss:0.00511\n",
      "Epoch:80 Batch:103 Loss:0.00479\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.022\n",
      "Epoch:10 Batch:10 Loss:0.017\n",
      "Epoch:20 Batch:10 Loss:0.018\n",
      "Epoch:30 Batch:10 Loss:0.016\n",
      "Epoch:40 Batch:10 Loss:0.019\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 40.64647684878291 setps: 29 count: 29\n",
      "reward: 41.00833303386606 setps: 27 count: 56\n",
      "reward: 28.53009493967547 setps: 21 count: 77\n",
      "reward: 42.04844512204291 setps: 28 count: 105\n",
      "reward: 33.46739980895217 setps: 25 count: 130\n",
      "reward: 43.72575785894442 setps: 30 count: 160\n",
      "reward: 39.59602219328663 setps: 26 count: 186\n",
      "reward: 38.887892798708336 setps: 25 count: 211\n",
      "reward: 37.32187965748016 setps: 25 count: 236\n",
      "reward: 44.34785550937231 setps: 31 count: 267\n",
      "reward: 37.77074473393295 setps: 25 count: 292\n",
      "reward: 42.07500328455207 setps: 30 count: 322\n",
      "reward: 40.87348065357801 setps: 27 count: 349\n",
      "reward: 39.3880087116253 setps: 25 count: 374\n",
      "reward: 37.70650607632124 setps: 25 count: 399\n",
      "reward: 37.685234730113 setps: 24 count: 423\n",
      "reward: 41.2008937188104 setps: 28 count: 451\n",
      "reward: 41.59180602200504 setps: 27 count: 478\n",
      "reward: 32.794109135161854 setps: 25 count: 503\n",
      "reward: 35.78214874460246 setps: 27 count: 530\n",
      "reward: 37.4794499224241 setps: 28 count: 558\n",
      "reward: 40.39156735114812 setps: 27 count: 585\n",
      "reward: 42.48845113413554 setps: 28 count: 613\n",
      "reward: 39.65562638351548 setps: 27 count: 640\n",
      "reward: 40.3491357681967 setps: 27 count: 667\n",
      "reward: 41.22386764647672 setps: 27 count: 694\n",
      "reward: 41.770914690788786 setps: 28 count: 722\n",
      "reward: 40.25622113664866 setps: 27 count: 749\n",
      "reward: 43.386079914466244 setps: 33 count: 782\n",
      "reward: 34.410445511198475 setps: 26 count: 808\n",
      "reward: 42.63235658691264 setps: 30 count: 838\n",
      "reward: 38.852970555615315 setps: 26 count: 864\n",
      "reward: 40.032913437437685 setps: 26 count: 890\n",
      "reward: 29.23598226648901 setps: 22 count: 912\n",
      "reward: 35.34332849898782 setps: 27 count: 939\n",
      "reward: 40.124884729440964 setps: 30 count: 969\n",
      "reward: 44.016626702609926 setps: 31 count: 1000\n",
      "avg rewards: 39.13780853562989\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:118 Loss:0.00574\n",
      "Epoch:20 Batch:118 Loss:0.00498\n",
      "Epoch:40 Batch:118 Loss:0.00495\n",
      "Epoch:60 Batch:118 Loss:0.00489\n",
      "Epoch:80 Batch:118 Loss:0.00576\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.019\n",
      "Epoch:10 Batch:10 Loss:0.021\n",
      "Epoch:20 Batch:10 Loss:0.022\n",
      "Epoch:30 Batch:10 Loss:0.019\n",
      "Epoch:40 Batch:10 Loss:0.018\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 41.05909723749791 setps: 28 count: 28\n",
      "reward: 43.95423029636439 setps: 29 count: 57\n",
      "reward: 40.47908143197274 setps: 29 count: 86\n",
      "reward: 41.19018211549992 setps: 28 count: 114\n",
      "reward: 48.47177496632502 setps: 33 count: 147\n",
      "reward: 41.26815966299473 setps: 28 count: 175\n",
      "reward: 43.09862315284555 setps: 29 count: 204\n",
      "reward: 39.30212364047329 setps: 26 count: 230\n",
      "reward: 38.463623285443454 setps: 28 count: 258\n",
      "reward: 46.73247243425867 setps: 31 count: 289\n",
      "reward: 41.930863080480776 setps: 29 count: 318\n",
      "reward: 42.31364884417416 setps: 28 count: 346\n",
      "reward: 42.372500787673914 setps: 29 count: 375\n",
      "reward: 44.184039613134516 setps: 29 count: 404\n",
      "reward: 40.679301986481015 setps: 27 count: 431\n",
      "reward: 41.13247410621989 setps: 27 count: 458\n",
      "reward: 39.38628730595955 setps: 26 count: 484\n",
      "reward: 40.47401976463152 setps: 27 count: 511\n",
      "reward: 31.04458153240411 setps: 22 count: 533\n",
      "reward: 42.29282023201376 setps: 29 count: 562\n",
      "reward: 41.596740194779706 setps: 28 count: 590\n",
      "reward: 34.39447009735014 setps: 25 count: 615\n",
      "reward: 41.76291396315501 setps: 27 count: 642\n",
      "reward: 46.4879197610091 setps: 31 count: 673\n",
      "reward: 47.51446520942118 setps: 32 count: 705\n",
      "reward: 40.38502999898773 setps: 27 count: 732\n",
      "reward: 41.78849607204029 setps: 28 count: 760\n",
      "reward: 40.35423403622263 setps: 26 count: 786\n",
      "reward: 40.589828000304976 setps: 26 count: 812\n",
      "reward: 37.781853374780624 setps: 25 count: 837\n",
      "reward: 43.813038251253595 setps: 30 count: 867\n",
      "reward: 39.252407667023355 setps: 27 count: 894\n",
      "reward: 40.926796052380816 setps: 29 count: 923\n",
      "reward: 40.92879969973 setps: 28 count: 951\n",
      "reward: 45.654335636254096 setps: 31 count: 982\n",
      "avg rewards: 41.516035242615494\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:133 Loss:0.00503\n",
      "Epoch:20 Batch:133 Loss:0.00467\n",
      "Epoch:40 Batch:133 Loss:0.00584\n",
      "Epoch:60 Batch:133 Loss:0.00457\n",
      "Epoch:80 Batch:133 Loss:0.00517\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.025\n",
      "Epoch:10 Batch:10 Loss:0.018\n",
      "Epoch:20 Batch:10 Loss:0.023\n",
      "Epoch:30 Batch:10 Loss:0.026\n",
      "Epoch:40 Batch:10 Loss:0.017\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 43.62007472083641 setps: 29 count: 29\n",
      "reward: 50.06341813811741 setps: 34 count: 63\n",
      "reward: 52.89931037360367 setps: 37 count: 100\n",
      "reward: 53.78760814809356 setps: 36 count: 136\n",
      "reward: 55.9078936373844 setps: 38 count: 174\n",
      "reward: 58.48283258244918 setps: 39 count: 213\n",
      "reward: 47.47922558795835 setps: 35 count: 248\n",
      "reward: 36.27424474900035 setps: 27 count: 275\n",
      "reward: 64.28320524888403 setps: 44 count: 319\n",
      "reward: 54.72653089700325 setps: 36 count: 355\n",
      "reward: 38.065225750925315 setps: 29 count: 384\n",
      "reward: 76.47442560357594 setps: 52 count: 436\n",
      "reward: 51.71260141913226 setps: 39 count: 475\n",
      "reward: 49.894445854729554 setps: 34 count: 509\n",
      "reward: 54.27578140713013 setps: 38 count: 547\n",
      "reward: 59.2367435986147 setps: 40 count: 587\n",
      "reward: 56.299652511597394 setps: 37 count: 624\n",
      "reward: 66.51759886413201 setps: 45 count: 669\n",
      "reward: 34.66094569590349 setps: 25 count: 694\n",
      "reward: 46.20232308716949 setps: 34 count: 728\n",
      "reward: 53.63067336256207 setps: 36 count: 764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 65.77640295711026 setps: 45 count: 809\n",
      "reward: 51.834654275362865 setps: 35 count: 844\n",
      "reward: 51.04473414283566 setps: 37 count: 881\n",
      "reward: 61.067438832776695 setps: 41 count: 922\n",
      "reward: 55.866446560411724 setps: 37 count: 959\n",
      "avg rewards: 53.464786077203854\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:150 Loss:0.00496\n",
      "Epoch:20 Batch:150 Loss:0.00453\n",
      "Epoch:40 Batch:150 Loss:0.00456\n",
      "Epoch:60 Batch:150 Loss:0.00487\n",
      "Epoch:80 Batch:150 Loss:0.00444\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.030\n",
      "Epoch:10 Batch:10 Loss:0.029\n",
      "Epoch:20 Batch:10 Loss:0.024\n",
      "Epoch:30 Batch:10 Loss:0.028\n",
      "Epoch:40 Batch:10 Loss:0.022\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 62.619369933567945 setps: 44 count: 44\n",
      "reward: 52.525359480870236 setps: 37 count: 81\n",
      "reward: 62.01277077623527 setps: 43 count: 124\n",
      "reward: 52.0432123093633 setps: 38 count: 162\n",
      "reward: 57.56796941295325 setps: 40 count: 202\n",
      "reward: 55.4817851654472 setps: 43 count: 245\n",
      "reward: 46.83810844359687 setps: 32 count: 277\n",
      "reward: 42.98210256381862 setps: 36 count: 313\n",
      "reward: 51.05897945200268 setps: 36 count: 349\n",
      "reward: 48.32234891841944 setps: 36 count: 385\n",
      "reward: 38.33044527446618 setps: 31 count: 416\n",
      "reward: 49.386615437084394 setps: 35 count: 451\n",
      "reward: 59.8473556555502 setps: 42 count: 493\n",
      "reward: 48.12837581192435 setps: 33 count: 526\n",
      "reward: 52.50135461448517 setps: 38 count: 564\n",
      "reward: 39.796383456906185 setps: 34 count: 598\n",
      "reward: 51.998278824205045 setps: 35 count: 633\n",
      "reward: 52.28783694498998 setps: 38 count: 671\n",
      "reward: 48.106385447537455 setps: 33 count: 704\n",
      "reward: 48.41570445555555 setps: 37 count: 741\n",
      "reward: 51.953176786599215 setps: 36 count: 777\n",
      "reward: 50.39331598293647 setps: 37 count: 814\n",
      "reward: 51.197583317222595 setps: 36 count: 850\n",
      "reward: 60.77290772722334 setps: 42 count: 892\n",
      "reward: 55.097570082749016 setps: 38 count: 930\n",
      "reward: 45.67860642362066 setps: 36 count: 966\n",
      "reward: 37.73398504172654 setps: 31 count: 997\n",
      "avg rewards: 50.854736583002115\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:167 Loss:0.00443\n",
      "Epoch:20 Batch:167 Loss:0.00440\n",
      "Epoch:40 Batch:167 Loss:0.00489\n",
      "Epoch:60 Batch:167 Loss:0.00422\n",
      "Epoch:80 Batch:167 Loss:0.00509\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.024\n",
      "Epoch:10 Batch:10 Loss:0.026\n",
      "Epoch:20 Batch:10 Loss:0.027\n",
      "Epoch:30 Batch:10 Loss:0.029\n",
      "Epoch:40 Batch:10 Loss:0.027\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 57.39816840603453 setps: 58 count: 58\n",
      "reward: 61.48880306961946 setps: 58 count: 116\n",
      "reward: 63.93550843188422 setps: 47 count: 163\n",
      "reward: 51.41064519083301 setps: 38 count: 201\n",
      "reward: 60.789905265028935 setps: 45 count: 246\n",
      "reward: 65.3639307071775 setps: 54 count: 300\n",
      "reward: 54.19677201060402 setps: 52 count: 352\n",
      "reward: 73.00612749670256 setps: 61 count: 413\n",
      "reward: 54.133589117506915 setps: 42 count: 455\n",
      "reward: 57.51954123270699 setps: 50 count: 505\n",
      "reward: 64.08600331469351 setps: 56 count: 561\n",
      "reward: 60.861268251547884 setps: 45 count: 606\n",
      "reward: 47.182038452729465 setps: 38 count: 644\n",
      "reward: 58.386525800924574 setps: 43 count: 687\n",
      "reward: 85.81425146095863 setps: 68 count: 755\n",
      "reward: 48.17662743399414 setps: 35 count: 790\n",
      "reward: 57.031084260399794 setps: 56 count: 846\n",
      "reward: 65.43393144982838 setps: 47 count: 893\n",
      "reward: 45.95225403176154 setps: 38 count: 931\n",
      "reward: 56.01452247837734 setps: 50 count: 981\n",
      "avg rewards: 59.409074893165666\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:186 Loss:0.00441\n",
      "Epoch:20 Batch:186 Loss:0.00430\n",
      "Epoch:40 Batch:186 Loss:0.00448\n",
      "Epoch:60 Batch:186 Loss:0.00419\n",
      "Epoch:80 Batch:186 Loss:0.00505\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.023\n",
      "Epoch:10 Batch:10 Loss:0.026\n",
      "Epoch:20 Batch:10 Loss:0.021\n",
      "Epoch:30 Batch:10 Loss:0.027\n",
      "Epoch:40 Batch:10 Loss:0.024\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 71.4191200435816 setps: 81 count: 81\n",
      "reward: 77.77256523684045 setps: 92 count: 173\n",
      "reward: -7.96027472215064 setps: 63 count: 236\n",
      "reward: -16.608050041030218 setps: 62 count: 298\n",
      "reward: 61.9252341496656 setps: 53 count: 351\n",
      "reward: 86.13963890570302 setps: 95 count: 446\n",
      "reward: -9.724178383410617 setps: 64 count: 510\n",
      "reward: 37.58279768405261 setps: 95 count: 605\n",
      "reward: 72.69490510988544 setps: 74 count: 679\n",
      "reward: 36.05680808549373 setps: 93 count: 772\n",
      "reward: -2.2913772810876214 setps: 61 count: 833\n",
      "reward: 115.33051574446729 setps: 78 count: 911\n",
      "avg rewards: 43.528142044334224\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:206 Loss:0.00344\n",
      "Epoch:20 Batch:206 Loss:0.00531\n",
      "Epoch:40 Batch:206 Loss:0.00392\n",
      "Epoch:60 Batch:206 Loss:0.00487\n",
      "Epoch:80 Batch:206 Loss:0.00387\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.013\n",
      "Epoch:10 Batch:10 Loss:0.013\n",
      "Epoch:20 Batch:10 Loss:0.011\n",
      "Epoch:30 Batch:10 Loss:0.010\n",
      "Epoch:40 Batch:10 Loss:0.012\n",
      "Done!\n",
      "############# start HalfCheetahBulletEnv-v0 training ###################\n",
      "(50, 1000, 26)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -579.1898570097194 setps: 500 count: 500\n",
      "reward: -668.739724795759 setps: 500 count: 1000\n",
      "avg rewards: -623.9647909027392\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:1.38882\n",
      "Epoch:20 Batch:1 Loss:0.51976\n",
      "Epoch:40 Batch:1 Loss:0.20527\n",
      "Epoch:60 Batch:1 Loss:0.13513\n",
      "Epoch:80 Batch:1 Loss:0.09934\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.227\n",
      "Epoch:10 Batch:10 Loss:0.180\n",
      "Epoch:20 Batch:10 Loss:0.174\n",
      "Epoch:30 Batch:10 Loss:0.166\n",
      "Epoch:40 Batch:10 Loss:0.164\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -558.3930437411632 setps: 500 count: 500\n",
      "reward: -562.7031039129533 setps: 500 count: 1000\n",
      "avg rewards: -560.5480738270583\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.07820\n",
      "Epoch:20 Batch:3 Loss:0.04560\n",
      "Epoch:40 Batch:3 Loss:0.03766\n",
      "Epoch:60 Batch:3 Loss:0.03523\n",
      "Epoch:80 Batch:3 Loss:0.03306\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.098\n",
      "Epoch:10 Batch:10 Loss:0.062\n",
      "Epoch:20 Batch:10 Loss:0.061\n",
      "Epoch:30 Batch:10 Loss:0.056\n",
      "Epoch:40 Batch:10 Loss:0.057\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -545.8596770330713 setps: 500 count: 500\n",
      "reward: -283.81826435996675 setps: 500 count: 1000\n",
      "avg rewards: -414.838970696519\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.03963\n",
      "Epoch:20 Batch:6 Loss:0.03167\n",
      "Epoch:40 Batch:6 Loss:0.02995\n",
      "Epoch:60 Batch:6 Loss:0.02733\n",
      "Epoch:80 Batch:6 Loss:0.02765\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.040\n",
      "Epoch:10 Batch:10 Loss:0.033\n",
      "Epoch:20 Batch:10 Loss:0.034\n",
      "Epoch:30 Batch:10 Loss:0.032\n",
      "Epoch:40 Batch:10 Loss:0.032\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -458.24519897533605 setps: 500 count: 500\n",
      "reward: -512.6019611700394 setps: 500 count: 1000\n",
      "avg rewards: -485.42358007268774\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.03059\n",
      "Epoch:20 Batch:10 Loss:0.02470\n",
      "Epoch:40 Batch:10 Loss:0.02298\n",
      "Epoch:60 Batch:10 Loss:0.02221\n",
      "Epoch:80 Batch:10 Loss:0.02217\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.045\n",
      "Epoch:10 Batch:10 Loss:0.025\n",
      "Epoch:20 Batch:10 Loss:0.021\n",
      "Epoch:30 Batch:10 Loss:0.022\n",
      "Epoch:40 Batch:10 Loss:0.023\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -619.6222180663656 setps: 500 count: 500\n",
      "reward: -586.4261826966713 setps: 500 count: 1000\n",
      "avg rewards: -603.0242003815184\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.02583\n",
      "Epoch:20 Batch:15 Loss:0.01978\n",
      "Epoch:40 Batch:15 Loss:0.02301\n",
      "Epoch:60 Batch:15 Loss:0.02113\n",
      "Epoch:80 Batch:15 Loss:0.02154\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.022\n",
      "Epoch:10 Batch:10 Loss:0.022\n",
      "Epoch:20 Batch:10 Loss:0.021\n",
      "Epoch:30 Batch:10 Loss:0.022\n",
      "Epoch:40 Batch:10 Loss:0.021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -600.8246704640175 setps: 500 count: 500\n",
      "reward: -646.0215557020668 setps: 500 count: 1000\n",
      "avg rewards: -623.4231130830422\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:21 Loss:0.02719\n",
      "Epoch:20 Batch:21 Loss:0.01869\n",
      "Epoch:40 Batch:21 Loss:0.01922\n",
      "Epoch:60 Batch:21 Loss:0.02093\n",
      "Epoch:80 Batch:21 Loss:0.01851\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.022\n",
      "Epoch:10 Batch:10 Loss:0.021\n",
      "Epoch:20 Batch:10 Loss:0.019\n",
      "Epoch:30 Batch:10 Loss:0.021\n",
      "Epoch:40 Batch:10 Loss:0.019\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -659.0365591959951 setps: 500 count: 500\n",
      "reward: -672.3582644471016 setps: 500 count: 1000\n",
      "avg rewards: -665.6974118215484\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:28 Loss:0.01936\n",
      "Epoch:20 Batch:28 Loss:0.01778\n",
      "Epoch:40 Batch:28 Loss:0.01927\n",
      "Epoch:60 Batch:28 Loss:0.01925\n",
      "Epoch:80 Batch:28 Loss:0.01699\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.021\n",
      "Epoch:10 Batch:10 Loss:0.020\n",
      "Epoch:20 Batch:10 Loss:0.018\n",
      "Epoch:30 Batch:10 Loss:0.019\n",
      "Epoch:40 Batch:10 Loss:0.018\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -672.1115854145298 setps: 500 count: 500\n",
      "reward: -705.0651956138216 setps: 500 count: 1000\n",
      "avg rewards: -688.5883905141757\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:36 Loss:0.01654\n",
      "Epoch:20 Batch:36 Loss:0.01424\n",
      "Epoch:40 Batch:36 Loss:0.01754\n",
      "Epoch:60 Batch:36 Loss:0.01701\n",
      "Epoch:80 Batch:36 Loss:0.02102\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.021\n",
      "Epoch:10 Batch:10 Loss:0.017\n",
      "Epoch:20 Batch:10 Loss:0.020\n",
      "Epoch:30 Batch:10 Loss:0.018\n",
      "Epoch:40 Batch:10 Loss:0.017\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -719.2607159323262 setps: 500 count: 500\n",
      "reward: -712.311206277128 setps: 500 count: 1000\n",
      "avg rewards: -715.7859611047271\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:44 Loss:0.01622\n",
      "Epoch:20 Batch:44 Loss:0.01534\n",
      "Epoch:40 Batch:44 Loss:0.01581\n",
      "Epoch:60 Batch:44 Loss:0.01506\n",
      "Epoch:80 Batch:44 Loss:0.01631\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.018\n",
      "Epoch:10 Batch:10 Loss:0.018\n",
      "Epoch:20 Batch:10 Loss:0.018\n",
      "Epoch:30 Batch:10 Loss:0.017\n",
      "Epoch:40 Batch:10 Loss:0.017\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -725.7990358483701 setps: 500 count: 500\n",
      "reward: -703.256631756958 setps: 500 count: 1000\n",
      "avg rewards: -714.527833802664\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:54 Loss:0.01570\n",
      "Epoch:20 Batch:54 Loss:0.01499\n",
      "Epoch:40 Batch:54 Loss:0.01510\n",
      "Epoch:60 Batch:54 Loss:0.01363\n",
      "Epoch:80 Batch:54 Loss:0.01397\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.018\n",
      "Epoch:10 Batch:10 Loss:0.019\n",
      "Epoch:20 Batch:10 Loss:0.017\n",
      "Epoch:30 Batch:10 Loss:0.018\n",
      "Epoch:40 Batch:10 Loss:0.016\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -737.1385356087992 setps: 500 count: 500\n",
      "reward: -702.3417227403988 setps: 500 count: 1000\n",
      "avg rewards: -719.740129174599\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:65 Loss:0.01437\n",
      "Epoch:20 Batch:65 Loss:0.01368\n",
      "Epoch:40 Batch:65 Loss:0.01492\n",
      "Epoch:60 Batch:65 Loss:0.01245\n",
      "Epoch:80 Batch:65 Loss:0.01332\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.018\n",
      "Epoch:10 Batch:10 Loss:0.018\n",
      "Epoch:20 Batch:10 Loss:0.018\n",
      "Epoch:30 Batch:10 Loss:0.017\n",
      "Epoch:40 Batch:10 Loss:0.016\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -730.1206827767529 setps: 500 count: 500\n",
      "reward: -742.868469536723 setps: 500 count: 1000\n",
      "avg rewards: -736.4945761567379\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:77 Loss:0.01457\n",
      "Epoch:20 Batch:77 Loss:0.01375\n",
      "Epoch:40 Batch:77 Loss:0.01037\n",
      "Epoch:60 Batch:77 Loss:0.01272\n",
      "Epoch:80 Batch:77 Loss:0.01167\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.018\n",
      "Epoch:10 Batch:10 Loss:0.018\n",
      "Epoch:20 Batch:10 Loss:0.018\n",
      "Epoch:30 Batch:10 Loss:0.017\n",
      "Epoch:40 Batch:10 Loss:0.017\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -787.9858212121995 setps: 500 count: 500\n",
      "reward: -723.4261928415968 setps: 500 count: 1000\n",
      "avg rewards: -755.7060070268982\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:89 Loss:0.01183\n",
      "Epoch:20 Batch:89 Loss:0.01285\n",
      "Epoch:40 Batch:89 Loss:0.01221\n",
      "Epoch:60 Batch:89 Loss:0.01270\n",
      "Epoch:80 Batch:89 Loss:0.01385\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.018\n",
      "Epoch:10 Batch:10 Loss:0.017\n",
      "Epoch:20 Batch:10 Loss:0.016\n",
      "Epoch:30 Batch:10 Loss:0.017\n",
      "Epoch:40 Batch:10 Loss:0.016\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -737.7372564116752 setps: 500 count: 500\n",
      "reward: -791.9572348023886 setps: 500 count: 1000\n",
      "avg rewards: -764.8472456070319\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:103 Loss:0.01219\n",
      "Epoch:20 Batch:103 Loss:0.01045\n",
      "Epoch:40 Batch:103 Loss:0.01096\n",
      "Epoch:60 Batch:103 Loss:0.01285\n",
      "Epoch:80 Batch:103 Loss:0.01199\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.016\n",
      "Epoch:10 Batch:10 Loss:0.017\n",
      "Epoch:20 Batch:10 Loss:0.017\n",
      "Epoch:30 Batch:10 Loss:0.018\n",
      "Epoch:40 Batch:10 Loss:0.017\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -805.0051866792357 setps: 500 count: 500\n",
      "reward: -813.2943099201497 setps: 500 count: 1000\n",
      "avg rewards: -809.1497482996926\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:118 Loss:0.01132\n",
      "Epoch:20 Batch:118 Loss:0.01473\n",
      "Epoch:40 Batch:118 Loss:0.01064\n",
      "Epoch:60 Batch:118 Loss:0.01047\n",
      "Epoch:80 Batch:118 Loss:0.01110\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.018\n",
      "Epoch:10 Batch:10 Loss:0.017\n",
      "Epoch:20 Batch:10 Loss:0.018\n",
      "Epoch:30 Batch:10 Loss:0.016\n",
      "Epoch:40 Batch:10 Loss:0.018\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -827.1939669456366 setps: 500 count: 500\n",
      "reward: -793.367490624769 setps: 500 count: 1000\n",
      "avg rewards: -810.2807287852028\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:133 Loss:0.00998\n",
      "Epoch:20 Batch:133 Loss:0.01183\n",
      "Epoch:40 Batch:133 Loss:0.01102\n",
      "Epoch:60 Batch:133 Loss:0.01178\n",
      "Epoch:80 Batch:133 Loss:0.01049\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.017\n",
      "Epoch:10 Batch:10 Loss:0.018\n",
      "Epoch:20 Batch:10 Loss:0.019\n",
      "Epoch:30 Batch:10 Loss:0.017\n",
      "Epoch:40 Batch:10 Loss:0.018\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -826.8948508488795 setps: 500 count: 500\n",
      "reward: -824.6591982717864 setps: 500 count: 1000\n",
      "avg rewards: -825.777024560333\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:150 Loss:0.01099\n",
      "Epoch:20 Batch:150 Loss:0.01265\n",
      "Epoch:40 Batch:150 Loss:0.01037\n",
      "Epoch:60 Batch:150 Loss:0.00998\n",
      "Epoch:80 Batch:150 Loss:0.01124\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.019\n",
      "Epoch:10 Batch:10 Loss:0.017\n",
      "Epoch:20 Batch:10 Loss:0.016\n",
      "Epoch:30 Batch:10 Loss:0.017\n",
      "Epoch:40 Batch:10 Loss:0.017\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -751.6064495674598 setps: 500 count: 500\n",
      "reward: -749.7003177185658 setps: 500 count: 1000\n",
      "avg rewards: -750.6533836430128\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:167 Loss:0.01097\n",
      "Epoch:20 Batch:167 Loss:0.01066\n",
      "Epoch:40 Batch:167 Loss:0.00964\n",
      "Epoch:60 Batch:167 Loss:0.01017\n",
      "Epoch:80 Batch:167 Loss:0.01122\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.017\n",
      "Epoch:10 Batch:10 Loss:0.018\n",
      "Epoch:20 Batch:10 Loss:0.017\n",
      "Epoch:30 Batch:10 Loss:0.017\n",
      "Epoch:40 Batch:10 Loss:0.017\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -796.1541941048024 setps: 500 count: 500\n",
      "reward: -788.9057104435888 setps: 500 count: 1000\n",
      "avg rewards: -792.5299522741956\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:186 Loss:0.00869\n",
      "Epoch:20 Batch:186 Loss:0.00963\n",
      "Epoch:40 Batch:186 Loss:0.01002\n",
      "Epoch:60 Batch:186 Loss:0.01050\n",
      "Epoch:80 Batch:186 Loss:0.01147\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.018\n",
      "Epoch:10 Batch:10 Loss:0.016\n",
      "Epoch:20 Batch:10 Loss:0.018\n",
      "Epoch:30 Batch:10 Loss:0.017\n",
      "Epoch:40 Batch:10 Loss:0.017\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -790.5539100665685 setps: 500 count: 500\n",
      "reward: -783.4624463387374 setps: 500 count: 1000\n",
      "avg rewards: -787.008178202653\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:206 Loss:0.00912\n",
      "Epoch:20 Batch:206 Loss:0.01142\n",
      "Epoch:40 Batch:206 Loss:0.01143\n",
      "Epoch:60 Batch:206 Loss:0.01257\n",
      "Epoch:80 Batch:206 Loss:0.01243\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.020\n",
      "Epoch:10 Batch:10 Loss:0.018\n",
      "Epoch:20 Batch:10 Loss:0.019\n",
      "Epoch:30 Batch:10 Loss:0.017\n",
      "Epoch:40 Batch:10 Loss:0.018\n",
      "Done!\n",
      "############# start AntBulletEnv-v0 training ###################\n",
      "(50, 1000, 28)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 245.73480124105248 setps: 500 count: 500\n",
      "reward: 259.62384757949627 setps: 500 count: 1000\n",
      "avg rewards: 252.67932441027438\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:1.44297\n",
      "Epoch:20 Batch:1 Loss:0.30976\n",
      "Epoch:40 Batch:1 Loss:0.06847\n",
      "Epoch:60 Batch:1 Loss:0.03060\n",
      "Epoch:80 Batch:1 Loss:0.01685\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.343\n",
      "Epoch:10 Batch:10 Loss:0.321\n",
      "Epoch:20 Batch:10 Loss:0.317\n",
      "Epoch:30 Batch:10 Loss:0.317\n",
      "Epoch:40 Batch:10 Loss:0.315\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 178.2608088249762 setps: 321 count: 321\n",
      "reward: 217.00330886377463 setps: 500 count: 821\n",
      "reward: 6.196533033503505 setps: 66 count: 887\n",
      "avg rewards: 133.82021690741811\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.11955\n",
      "Epoch:20 Batch:3 Loss:0.00690\n",
      "Epoch:40 Batch:3 Loss:0.00436\n",
      "Epoch:60 Batch:3 Loss:0.00301\n",
      "Epoch:80 Batch:3 Loss:0.00327\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.162\n",
      "Epoch:10 Batch:10 Loss:0.156\n",
      "Epoch:20 Batch:10 Loss:0.154\n",
      "Epoch:30 Batch:10 Loss:0.153\n",
      "Epoch:40 Batch:10 Loss:0.152\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 47.921334174550424 setps: 243 count: 243\n",
      "reward: 207.20036877843825 setps: 500 count: 743\n",
      "reward: 19.613701028487416 setps: 83 count: 826\n",
      "avg rewards: 91.57846799382537\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.00896\n",
      "Epoch:20 Batch:6 Loss:0.00347\n",
      "Epoch:40 Batch:6 Loss:0.00234\n",
      "Epoch:60 Batch:6 Loss:0.00271\n",
      "Epoch:80 Batch:6 Loss:0.00265\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.132\n",
      "Epoch:10 Batch:10 Loss:0.125\n",
      "Epoch:20 Batch:10 Loss:0.123\n",
      "Epoch:30 Batch:10 Loss:0.122\n",
      "Epoch:40 Batch:10 Loss:0.122\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -56.980497046705544 setps: 178 count: 178\n",
      "reward: -17.263807062865816 setps: 24 count: 202\n",
      "reward: -16.041517049264804 setps: 24 count: 226\n",
      "reward: -14.832872774635323 setps: 23 count: 249\n",
      "reward: -17.954724455323596 setps: 44 count: 293\n",
      "reward: -12.987606905642316 setps: 24 count: 317\n",
      "reward: -14.936033032576965 setps: 24 count: 341\n",
      "reward: -16.407128201464367 setps: 22 count: 363\n",
      "reward: -12.529226605487928 setps: 22 count: 385\n",
      "reward: -42.21854130817083 setps: 129 count: 514\n",
      "reward: -14.023522650121595 setps: 23 count: 537\n",
      "reward: -14.39140226117015 setps: 23 count: 560\n",
      "reward: -14.027615025851993 setps: 23 count: 583\n",
      "reward: -37.261140405328476 setps: 101 count: 684\n",
      "reward: -16.095249628119927 setps: 23 count: 707\n",
      "reward: -14.027382247782956 setps: 22 count: 729\n",
      "reward: -44.60969740125001 setps: 140 count: 869\n",
      "reward: -21.721902223752114 setps: 55 count: 924\n",
      "reward: -14.487729280363421 setps: 23 count: 947\n",
      "avg rewards: -21.726189240309374\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.00803\n",
      "Epoch:20 Batch:10 Loss:0.00366\n",
      "Epoch:40 Batch:10 Loss:0.00312\n",
      "Epoch:60 Batch:10 Loss:0.00299\n",
      "Epoch:80 Batch:10 Loss:0.00333\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.138\n",
      "Epoch:10 Batch:10 Loss:0.132\n",
      "Epoch:20 Batch:10 Loss:0.133\n",
      "Epoch:30 Batch:10 Loss:0.132\n",
      "Epoch:40 Batch:10 Loss:0.131\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 100.57270542534539 setps: 500 count: 500\n",
      "reward: 42.96618581694202 setps: 500 count: 1000\n",
      "avg rewards: 71.7694456211437\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.00541\n",
      "Epoch:20 Batch:15 Loss:0.00318\n",
      "Epoch:40 Batch:15 Loss:0.00277\n",
      "Epoch:60 Batch:15 Loss:0.00286\n",
      "Epoch:80 Batch:15 Loss:0.00263\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.141\n",
      "Epoch:10 Batch:10 Loss:0.135\n",
      "Epoch:20 Batch:10 Loss:0.135\n",
      "Epoch:30 Batch:10 Loss:0.131\n",
      "Epoch:40 Batch:10 Loss:0.132\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -43.68599643499619 setps: 190 count: 190\n",
      "reward: -347.2494358061667 setps: 500 count: 690\n",
      "reward: -49.35780432627507 setps: 208 count: 898\n",
      "avg rewards: -146.764412189146\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:21 Loss:0.00563\n",
      "Epoch:20 Batch:21 Loss:0.00294\n",
      "Epoch:40 Batch:21 Loss:0.00332\n",
      "Epoch:60 Batch:21 Loss:0.00340\n",
      "Epoch:80 Batch:21 Loss:0.00274\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.113\n",
      "Epoch:10 Batch:10 Loss:0.102\n",
      "Epoch:20 Batch:10 Loss:0.102\n",
      "Epoch:30 Batch:10 Loss:0.101\n",
      "Epoch:40 Batch:10 Loss:0.100\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 66.33750793169163 setps: 500 count: 500\n",
      "reward: 75.73726265600406 setps: 500 count: 1000\n",
      "avg rewards: 71.03738529384785\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:28 Loss:0.00544\n",
      "Epoch:20 Batch:28 Loss:0.00271\n",
      "Epoch:40 Batch:28 Loss:0.00364\n",
      "Epoch:60 Batch:28 Loss:0.00264\n",
      "Epoch:80 Batch:28 Loss:0.00318\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.097\n",
      "Epoch:10 Batch:10 Loss:0.095\n",
      "Epoch:20 Batch:10 Loss:0.093\n",
      "Epoch:30 Batch:10 Loss:0.092\n",
      "Epoch:40 Batch:10 Loss:0.093\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 61.670902539846374 setps: 500 count: 500\n",
      "reward: 107.28531727280946 setps: 500 count: 1000\n",
      "avg rewards: 84.47810990632792\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:36 Loss:0.00361\n",
      "Epoch:20 Batch:36 Loss:0.00249\n",
      "Epoch:40 Batch:36 Loss:0.00191\n",
      "Epoch:60 Batch:36 Loss:0.00342\n",
      "Epoch:80 Batch:36 Loss:0.00361\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.087\n",
      "Epoch:10 Batch:10 Loss:0.087\n",
      "Epoch:20 Batch:10 Loss:0.088\n",
      "Epoch:30 Batch:10 Loss:0.085\n",
      "Epoch:40 Batch:10 Loss:0.086\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -118.43780469609918 setps: 500 count: 500\n",
      "reward: -81.11719539316388 setps: 500 count: 1000\n",
      "avg rewards: -99.77750004463152\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:44 Loss:0.00259\n",
      "Epoch:20 Batch:44 Loss:0.00304\n",
      "Epoch:40 Batch:44 Loss:0.00253\n",
      "Epoch:60 Batch:44 Loss:0.00318\n",
      "Epoch:80 Batch:44 Loss:0.00295\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.085\n",
      "Epoch:10 Batch:10 Loss:0.084\n",
      "Epoch:20 Batch:10 Loss:0.084\n",
      "Epoch:30 Batch:10 Loss:0.083\n",
      "Epoch:40 Batch:10 Loss:0.084\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1000.9138683357537 setps: 500 count: 500\n",
      "reward: -822.8363612765418 setps: 500 count: 1000\n",
      "avg rewards: -911.8751148061477\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:54 Loss:0.00280\n",
      "Epoch:20 Batch:54 Loss:0.00275\n",
      "Epoch:40 Batch:54 Loss:0.00243\n",
      "Epoch:60 Batch:54 Loss:0.00299\n",
      "Epoch:80 Batch:54 Loss:0.00260\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.063\n",
      "Epoch:10 Batch:10 Loss:0.061\n",
      "Epoch:20 Batch:10 Loss:0.062\n",
      "Epoch:30 Batch:10 Loss:0.060\n",
      "Epoch:40 Batch:10 Loss:0.060\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -939.245850896694 setps: 500 count: 500\n",
      "reward: -913.3680895424205 setps: 500 count: 1000\n",
      "avg rewards: -926.3069702195573\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:65 Loss:0.00316\n",
      "Epoch:20 Batch:65 Loss:0.00298\n",
      "Epoch:40 Batch:65 Loss:0.00249\n",
      "Epoch:60 Batch:65 Loss:0.00354\n",
      "Epoch:80 Batch:65 Loss:0.00195\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.049\n",
      "Epoch:10 Batch:10 Loss:0.047\n",
      "Epoch:20 Batch:10 Loss:0.045\n",
      "Epoch:30 Batch:10 Loss:0.045\n",
      "Epoch:40 Batch:10 Loss:0.046\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -733.7616301901625 setps: 500 count: 500\n",
      "reward: -718.6186043623492 setps: 500 count: 1000\n",
      "avg rewards: -726.1901172762558\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:77 Loss:0.00261\n",
      "Epoch:20 Batch:77 Loss:0.00207\n",
      "Epoch:40 Batch:77 Loss:0.00196\n",
      "Epoch:60 Batch:77 Loss:0.00318\n",
      "Epoch:80 Batch:77 Loss:0.00367\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.046\n",
      "Epoch:10 Batch:10 Loss:0.045\n",
      "Epoch:20 Batch:10 Loss:0.045\n",
      "Epoch:30 Batch:10 Loss:0.045\n",
      "Epoch:40 Batch:10 Loss:0.046\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1428.8120670027713 setps: 500 count: 500\n",
      "reward: -1350.816994672468 setps: 500 count: 1000\n",
      "avg rewards: -1389.8145308376197\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:89 Loss:0.00330\n",
      "Epoch:20 Batch:89 Loss:0.00239\n",
      "Epoch:40 Batch:89 Loss:0.00276\n",
      "Epoch:60 Batch:89 Loss:0.00299\n",
      "Epoch:80 Batch:89 Loss:0.00291\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.044\n",
      "Epoch:10 Batch:10 Loss:0.044\n",
      "Epoch:20 Batch:10 Loss:0.044\n",
      "Epoch:30 Batch:10 Loss:0.043\n",
      "Epoch:40 Batch:10 Loss:0.043\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1688.9504720603577 setps: 500 count: 500\n",
      "reward: -798.5454523332997 setps: 234 count: 734\n",
      "avg rewards: -1243.7479621968287\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:103 Loss:0.00243\n",
      "Epoch:20 Batch:103 Loss:0.00249\n",
      "Epoch:40 Batch:103 Loss:0.00291\n",
      "Epoch:60 Batch:103 Loss:0.00306\n",
      "Epoch:80 Batch:103 Loss:0.00304\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.047\n",
      "Epoch:10 Batch:10 Loss:0.047\n",
      "Epoch:20 Batch:10 Loss:0.046\n",
      "Epoch:30 Batch:10 Loss:0.046\n",
      "Epoch:40 Batch:10 Loss:0.046\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1737.0966306825867 setps: 500 count: 500\n",
      "reward: -1456.504771605826 setps: 500 count: 1000\n",
      "avg rewards: -1596.8007011442064\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:118 Loss:0.00291\n",
      "Epoch:20 Batch:118 Loss:0.00260\n",
      "Epoch:40 Batch:118 Loss:0.00390\n",
      "Epoch:60 Batch:118 Loss:0.00208\n",
      "Epoch:80 Batch:118 Loss:0.00294\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.046\n",
      "Epoch:10 Batch:10 Loss:0.046\n",
      "Epoch:20 Batch:10 Loss:0.045\n",
      "Epoch:30 Batch:10 Loss:0.045\n",
      "Epoch:40 Batch:10 Loss:0.045\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1659.5896613559146 setps: 500 count: 500\n",
      "reward: -1662.6335504239685 setps: 500 count: 1000\n",
      "avg rewards: -1661.1116058899415\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:133 Loss:0.00291\n",
      "Epoch:20 Batch:133 Loss:0.00265\n",
      "Epoch:40 Batch:133 Loss:0.00290\n",
      "Epoch:60 Batch:133 Loss:0.00283\n",
      "Epoch:80 Batch:133 Loss:0.00297\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.044\n",
      "Epoch:10 Batch:10 Loss:0.044\n",
      "Epoch:20 Batch:10 Loss:0.043\n",
      "Epoch:30 Batch:10 Loss:0.044\n",
      "Epoch:40 Batch:10 Loss:0.044\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -2641.9342218786114 setps: 500 count: 500\n",
      "reward: -2937.69149698123 setps: 500 count: 1000\n",
      "avg rewards: -2789.8128594299205\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:150 Loss:0.00283\n",
      "Epoch:20 Batch:150 Loss:0.00287\n",
      "Epoch:40 Batch:150 Loss:0.00351\n",
      "Epoch:60 Batch:150 Loss:0.00225\n",
      "Epoch:80 Batch:150 Loss:0.00267\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.043\n",
      "Epoch:10 Batch:10 Loss:0.043\n",
      "Epoch:20 Batch:10 Loss:0.042\n",
      "Epoch:30 Batch:10 Loss:0.043\n",
      "Epoch:40 Batch:10 Loss:0.042\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -805.463794325075 setps: 101 count: 101\n",
      "reward: -1124.6090686622451 setps: 135 count: 236\n",
      "reward: -4543.737927188444 setps: 500 count: 736\n",
      "avg rewards: -2157.936930058588\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:167 Loss:0.00329\n",
      "Epoch:20 Batch:167 Loss:0.00297\n",
      "Epoch:40 Batch:167 Loss:0.00310\n",
      "Epoch:60 Batch:167 Loss:0.00314\n",
      "Epoch:80 Batch:167 Loss:0.00276\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.043\n",
      "Epoch:10 Batch:10 Loss:0.044\n",
      "Epoch:20 Batch:10 Loss:0.042\n",
      "Epoch:30 Batch:10 Loss:0.042\n",
      "Epoch:40 Batch:10 Loss:0.042\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1139.151863437453 setps: 500 count: 500\n",
      "reward: -924.8649276656233 setps: 135 count: 635\n",
      "avg rewards: -1032.0083955515381\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:186 Loss:0.00270\n",
      "Epoch:20 Batch:186 Loss:0.00259\n",
      "Epoch:40 Batch:186 Loss:0.00327\n",
      "Epoch:60 Batch:186 Loss:0.00331\n",
      "Epoch:80 Batch:186 Loss:0.00313\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.041\n",
      "Epoch:10 Batch:10 Loss:0.039\n",
      "Epoch:20 Batch:10 Loss:0.040\n",
      "Epoch:30 Batch:10 Loss:0.040\n",
      "Epoch:40 Batch:10 Loss:0.041\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -1438.346861195812 setps: 500 count: 500\n",
      "reward: -2341.8272000573743 setps: 500 count: 1000\n",
      "avg rewards: -1890.087030626593\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:206 Loss:0.00240\n",
      "Epoch:20 Batch:206 Loss:0.00214\n",
      "Epoch:40 Batch:206 Loss:0.00374\n",
      "Epoch:60 Batch:206 Loss:0.00337\n",
      "Epoch:80 Batch:206 Loss:0.00201\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.040\n",
      "Epoch:10 Batch:10 Loss:0.039\n",
      "Epoch:20 Batch:10 Loss:0.039\n",
      "Epoch:30 Batch:10 Loss:0.040\n",
      "Epoch:40 Batch:10 Loss:0.039\n",
      "Done!\n",
      "############# start HumanoidBulletEnv-v0 training ###################\n",
      "(49,)\n",
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -31.404271150715065 setps: 19 count: 19\n",
      "reward: -42.458791313528494 setps: 12 count: 31\n",
      "reward: -24.75137735195167 setps: 20 count: 51\n",
      "reward: -33.91482600039163 setps: 18 count: 69\n",
      "reward: -21.093425501688035 setps: 18 count: 87\n",
      "reward: -35.1018335864981 setps: 18 count: 105\n",
      "reward: -32.47904030668578 setps: 19 count: 124\n",
      "reward: -21.408605275298758 setps: 19 count: 143\n",
      "reward: -23.872663144754192 setps: 20 count: 163\n",
      "reward: -31.478235072179807 setps: 19 count: 182\n",
      "reward: -28.413430032708856 setps: 18 count: 200\n",
      "reward: -23.244287469465057 setps: 18 count: 218\n",
      "reward: -24.79056704861723 setps: 20 count: 238\n",
      "reward: -16.03578396939847 setps: 19 count: 257\n",
      "reward: -34.36846353155998 setps: 19 count: 276\n",
      "reward: -36.17820194220985 setps: 20 count: 296\n",
      "reward: -26.67281339259934 setps: 20 count: 316\n",
      "reward: -39.83104044645006 setps: 21 count: 337\n",
      "reward: -27.67686543049204 setps: 20 count: 357\n",
      "reward: -38.102824185513605 setps: 20 count: 377\n",
      "reward: -27.86305237996567 setps: 18 count: 395\n",
      "reward: -21.959205123868017 setps: 18 count: 413\n",
      "reward: -41.65663011034776 setps: 18 count: 431\n",
      "reward: -33.33443293077289 setps: 21 count: 452\n",
      "reward: -34.578247337449284 setps: 19 count: 471\n",
      "reward: -35.64600471542799 setps: 19 count: 490\n",
      "reward: -26.765360912971666 setps: 19 count: 509\n",
      "reward: -30.065325573245353 setps: 19 count: 528\n",
      "reward: -30.175002428854356 setps: 20 count: 548\n",
      "reward: -37.48819957796222 setps: 32 count: 580\n",
      "reward: -26.98041875295749 setps: 18 count: 598\n",
      "reward: -34.160505220202324 setps: 20 count: 618\n",
      "reward: -35.91214710036147 setps: 17 count: 635\n",
      "reward: -31.92304201973748 setps: 19 count: 654\n",
      "reward: -24.72999257395131 setps: 20 count: 674\n",
      "reward: -21.402042329763937 setps: 19 count: 693\n",
      "reward: -25.980792792780267 setps: 19 count: 712\n",
      "reward: -28.203496269471362 setps: 24 count: 736\n",
      "reward: -27.00085412596964 setps: 18 count: 754\n",
      "reward: -29.224396536524004 setps: 19 count: 773\n",
      "reward: -35.04957465350308 setps: 18 count: 791\n",
      "reward: -38.84655237339903 setps: 20 count: 811\n",
      "reward: -23.17452815263241 setps: 20 count: 831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -41.2938334337101 setps: 19 count: 850\n",
      "reward: -34.835920789091325 setps: 18 count: 868\n",
      "reward: -36.590815793153894 setps: 19 count: 887\n",
      "reward: -15.617563240166058 setps: 20 count: 907\n",
      "reward: -28.333617698577292 setps: 20 count: 927\n",
      "reward: -47.1780904650397 setps: 18 count: 945\n",
      "reward: -25.71575420475128 setps: 19 count: 964\n",
      "reward: -35.47255634499305 setps: 20 count: 984\n",
      "avg rewards: -30.59677062969231\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:1 Loss:1.61777\n",
      "Epoch:20 Batch:1 Loss:0.74479\n",
      "Epoch:40 Batch:1 Loss:0.42050\n",
      "Epoch:60 Batch:1 Loss:0.32120\n",
      "Epoch:80 Batch:1 Loss:0.25065\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.302\n",
      "Epoch:10 Batch:8 Loss:0.170\n",
      "Epoch:20 Batch:8 Loss:0.166\n",
      "Epoch:30 Batch:8 Loss:0.164\n",
      "Epoch:40 Batch:8 Loss:0.166\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -32.811091382485756 setps: 17 count: 17\n",
      "reward: -263.0474782410704 setps: 143 count: 160\n",
      "reward: -125.24877424720029 setps: 65 count: 225\n",
      "reward: -51.52872792309497 setps: 40 count: 265\n",
      "reward: -38.34435776425816 setps: 36 count: 301\n",
      "reward: -59.06177785396283 setps: 50 count: 351\n",
      "reward: -63.85304165194684 setps: 50 count: 401\n",
      "reward: -54.16894638257653 setps: 46 count: 447\n",
      "reward: -82.85499567943477 setps: 52 count: 499\n",
      "reward: -59.04386924526626 setps: 42 count: 541\n",
      "reward: -108.50239478985456 setps: 73 count: 614\n",
      "reward: -112.71417094913772 setps: 62 count: 676\n",
      "reward: -128.1805930796516 setps: 76 count: 752\n",
      "reward: -69.02016071154215 setps: 46 count: 798\n",
      "reward: -131.62189082242256 setps: 69 count: 867\n",
      "reward: -108.33534742522608 setps: 63 count: 930\n",
      "avg rewards: -93.02110113432072\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:3 Loss:0.20787\n",
      "Epoch:20 Batch:3 Loss:0.11498\n",
      "Epoch:40 Batch:3 Loss:0.09865\n",
      "Epoch:60 Batch:3 Loss:0.08369\n",
      "Epoch:80 Batch:3 Loss:0.07661\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.084\n",
      "Epoch:10 Batch:8 Loss:0.070\n",
      "Epoch:20 Batch:8 Loss:0.073\n",
      "Epoch:30 Batch:8 Loss:0.070\n",
      "Epoch:40 Batch:8 Loss:0.069\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -115.8283998272862 setps: 69 count: 69\n",
      "reward: -40.06725888903456 setps: 21 count: 90\n",
      "reward: -50.76488334119348 setps: 45 count: 135\n",
      "reward: -48.70032734265843 setps: 33 count: 168\n",
      "reward: -47.72230716335907 setps: 31 count: 199\n",
      "reward: -21.803003837398137 setps: 25 count: 224\n",
      "reward: -58.3391647493656 setps: 42 count: 266\n",
      "reward: -51.36442888034799 setps: 37 count: 303\n",
      "reward: -41.438457091624144 setps: 32 count: 335\n",
      "reward: -53.5143218266836 setps: 52 count: 387\n",
      "reward: -50.510362787815396 setps: 30 count: 417\n",
      "reward: -61.43429754208482 setps: 36 count: 453\n",
      "reward: -50.56612449254899 setps: 43 count: 496\n",
      "reward: -51.26256585960947 setps: 30 count: 526\n",
      "reward: -74.63484383856559 setps: 53 count: 579\n",
      "reward: -64.98202426756178 setps: 35 count: 614\n",
      "reward: -26.705261675697695 setps: 24 count: 638\n",
      "reward: -40.872743978400834 setps: 31 count: 669\n",
      "reward: -51.551393682375775 setps: 52 count: 721\n",
      "reward: -22.21723041672376 setps: 26 count: 747\n",
      "reward: -59.14576514670917 setps: 37 count: 784\n",
      "reward: -70.13321312832414 setps: 54 count: 838\n",
      "reward: -54.99208392918081 setps: 34 count: 872\n",
      "reward: -105.28786041614657 setps: 53 count: 925\n",
      "reward: -56.88827558918856 setps: 35 count: 960\n",
      "reward: -30.210266642544596 setps: 26 count: 986\n",
      "avg rewards: -53.88218716701651\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:6 Loss:0.08412\n",
      "Epoch:20 Batch:6 Loss:0.05957\n",
      "Epoch:40 Batch:6 Loss:0.05831\n",
      "Epoch:60 Batch:6 Loss:0.05304\n",
      "Epoch:80 Batch:6 Loss:0.05176\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.056\n",
      "Epoch:10 Batch:8 Loss:0.049\n",
      "Epoch:20 Batch:8 Loss:0.045\n",
      "Epoch:30 Batch:8 Loss:0.046\n",
      "Epoch:40 Batch:8 Loss:0.045\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -71.5081397146394 setps: 77 count: 77\n",
      "reward: -36.42243120008497 setps: 30 count: 107\n",
      "reward: -48.36711264492044 setps: 41 count: 148\n",
      "reward: -48.459639590949536 setps: 56 count: 204\n",
      "reward: -72.64530267588562 setps: 65 count: 269\n",
      "reward: -53.701114328141564 setps: 45 count: 314\n",
      "reward: -27.99161905245274 setps: 64 count: 378\n",
      "reward: -31.84530893276969 setps: 27 count: 405\n",
      "reward: -60.80398173308348 setps: 62 count: 467\n",
      "reward: -46.387395013557395 setps: 35 count: 502\n",
      "reward: -55.94923475419202 setps: 42 count: 544\n",
      "reward: -35.59948175750906 setps: 27 count: 571\n",
      "reward: -36.42457503166515 setps: 20 count: 591\n",
      "reward: -71.57709511876601 setps: 57 count: 648\n",
      "reward: -66.8227885876797 setps: 52 count: 700\n",
      "reward: -39.20615075820969 setps: 29 count: 729\n",
      "reward: -43.75305169828934 setps: 34 count: 763\n",
      "reward: -32.93734826631554 setps: 32 count: 795\n",
      "reward: -29.35133691651572 setps: 26 count: 821\n",
      "reward: -25.138739232503575 setps: 22 count: 843\n",
      "reward: -64.6976452707575 setps: 77 count: 920\n",
      "reward: -24.173484749668575 setps: 63 count: 983\n",
      "avg rewards: -46.534680774025304\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:10 Loss:0.06215\n",
      "Epoch:20 Batch:10 Loss:0.04592\n",
      "Epoch:40 Batch:10 Loss:0.04290\n",
      "Epoch:60 Batch:10 Loss:0.04172\n",
      "Epoch:80 Batch:10 Loss:0.03859\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.050\n",
      "Epoch:10 Batch:8 Loss:0.039\n",
      "Epoch:20 Batch:8 Loss:0.038\n",
      "Epoch:30 Batch:8 Loss:0.039\n",
      "Epoch:40 Batch:8 Loss:0.038\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -72.30642982420687 setps: 51 count: 51\n",
      "reward: -32.58631733312213 setps: 45 count: 96\n",
      "reward: -51.65549332816881 setps: 25 count: 121\n",
      "reward: -37.309785674246086 setps: 34 count: 155\n",
      "reward: -69.1575426598385 setps: 40 count: 195\n",
      "reward: -55.097416054100904 setps: 45 count: 240\n",
      "reward: -74.51648736982318 setps: 35 count: 275\n",
      "reward: -60.30278807793947 setps: 37 count: 312\n",
      "reward: -77.09757001562684 setps: 50 count: 362\n",
      "reward: -53.332801643700805 setps: 35 count: 397\n",
      "reward: -22.545708387829652 setps: 17 count: 414\n",
      "reward: -58.702177429944285 setps: 38 count: 452\n",
      "reward: -36.30847664634639 setps: 42 count: 494\n",
      "reward: -48.39721572575947 setps: 59 count: 553\n",
      "reward: -45.04755617627962 setps: 32 count: 585\n",
      "reward: -24.302154205150146 setps: 17 count: 602\n",
      "reward: -43.47278469714511 setps: 32 count: 634\n",
      "reward: -47.6719176296494 setps: 29 count: 663\n",
      "reward: -26.362362061810565 setps: 20 count: 683\n",
      "reward: -22.38836879414885 setps: 19 count: 702\n",
      "reward: -40.78352524581861 setps: 33 count: 735\n",
      "reward: -86.36905274430465 setps: 41 count: 776\n",
      "reward: -12.68697528420599 setps: 61 count: 837\n",
      "reward: -57.4435143832612 setps: 38 count: 875\n",
      "reward: -51.47491390656214 setps: 41 count: 916\n",
      "reward: -70.35997117511288 setps: 49 count: 965\n",
      "avg rewards: -49.14151178746548\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:15 Loss:0.05082\n",
      "Epoch:20 Batch:15 Loss:0.04036\n",
      "Epoch:40 Batch:15 Loss:0.04163\n",
      "Epoch:60 Batch:15 Loss:0.03849\n",
      "Epoch:80 Batch:15 Loss:0.03743\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.039\n",
      "Epoch:10 Batch:8 Loss:0.030\n",
      "Epoch:20 Batch:8 Loss:0.031\n",
      "Epoch:30 Batch:8 Loss:0.029\n",
      "Epoch:40 Batch:8 Loss:0.029\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -20.546854226314462 setps: 24 count: 24\n",
      "reward: -23.415980737045178 setps: 15 count: 39\n",
      "reward: -55.13730694946279 setps: 39 count: 78\n",
      "reward: -29.997956736489144 setps: 27 count: 105\n",
      "reward: -68.2856245775445 setps: 37 count: 142\n",
      "reward: -59.12977424745914 setps: 35 count: 177\n",
      "reward: -41.10394923379936 setps: 36 count: 213\n",
      "reward: -69.6186467103602 setps: 33 count: 246\n",
      "reward: -47.313977080326 setps: 31 count: 277\n",
      "reward: -57.08916829424416 setps: 36 count: 313\n",
      "reward: -65.15706614720693 setps: 34 count: 347\n",
      "reward: -57.75665460328164 setps: 32 count: 379\n",
      "reward: -69.16923949163174 setps: 31 count: 410\n",
      "reward: -42.23020202761692 setps: 35 count: 445\n",
      "reward: -47.00355650552082 setps: 31 count: 476\n",
      "reward: -41.66079595613556 setps: 32 count: 508\n",
      "reward: -27.964446447533554 setps: 17 count: 525\n",
      "reward: -55.798565935929936 setps: 27 count: 552\n",
      "reward: -59.181363814222266 setps: 29 count: 581\n",
      "reward: -76.68833153124145 setps: 39 count: 620\n",
      "reward: -43.517263575228576 setps: 34 count: 654\n",
      "reward: -64.24097758742718 setps: 36 count: 690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -53.09821183293388 setps: 28 count: 718\n",
      "reward: -29.496055467857516 setps: 25 count: 743\n",
      "reward: -52.204811816176516 setps: 31 count: 774\n",
      "reward: -49.94260096728249 setps: 35 count: 809\n",
      "reward: -44.85243292337836 setps: 28 count: 837\n",
      "reward: -57.09420877944212 setps: 34 count: 871\n",
      "reward: -44.65021445993626 setps: 36 count: 907\n",
      "reward: -47.29190648052026 setps: 32 count: 939\n",
      "reward: -53.8408820687211 setps: 28 count: 967\n",
      "reward: -48.191673257973164 setps: 33 count: 1000\n",
      "avg rewards: -50.08345938969509\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:21 Loss:0.04482\n",
      "Epoch:20 Batch:21 Loss:0.03804\n",
      "Epoch:40 Batch:21 Loss:0.03854\n",
      "Epoch:60 Batch:21 Loss:0.03827\n",
      "Epoch:80 Batch:21 Loss:0.03770\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.028\n",
      "Epoch:10 Batch:8 Loss:0.029\n",
      "Epoch:20 Batch:8 Loss:0.027\n",
      "Epoch:30 Batch:8 Loss:0.026\n",
      "Epoch:40 Batch:8 Loss:0.025\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -51.524495114562164 setps: 34 count: 34\n",
      "reward: -59.52205361413507 setps: 33 count: 67\n",
      "reward: -61.30959348906617 setps: 37 count: 104\n",
      "reward: -43.370538397830394 setps: 40 count: 144\n",
      "reward: -51.68293533254474 setps: 34 count: 178\n",
      "reward: -29.501490026274407 setps: 29 count: 207\n",
      "reward: -33.643533878171 setps: 33 count: 240\n",
      "reward: -35.149606522024264 setps: 34 count: 274\n",
      "reward: -39.708188380691 setps: 34 count: 308\n",
      "reward: -53.05593915939681 setps: 34 count: 342\n",
      "reward: -44.2345685350243 setps: 25 count: 367\n",
      "reward: -37.736563744200964 setps: 32 count: 399\n",
      "reward: -64.56035816119982 setps: 41 count: 440\n",
      "reward: -38.55627950036724 setps: 32 count: 472\n",
      "reward: -59.94390385350852 setps: 35 count: 507\n",
      "reward: -83.73325164693524 setps: 43 count: 550\n",
      "reward: -58.02700769664662 setps: 49 count: 599\n",
      "reward: -43.08275313613123 setps: 32 count: 631\n",
      "reward: -42.18558416541347 setps: 31 count: 662\n",
      "reward: -48.10559262980969 setps: 36 count: 698\n",
      "reward: -74.97606990262429 setps: 46 count: 744\n",
      "reward: -43.50324736304466 setps: 35 count: 779\n",
      "reward: -48.86825542484877 setps: 34 count: 813\n",
      "reward: -69.53354878711399 setps: 45 count: 858\n",
      "reward: -55.126555536930404 setps: 39 count: 897\n",
      "reward: -63.71463189253411 setps: 46 count: 943\n",
      "reward: -37.50537722671725 setps: 45 count: 988\n",
      "avg rewards: -50.809700856212835\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:28 Loss:0.03794\n",
      "Epoch:20 Batch:28 Loss:0.03787\n",
      "Epoch:40 Batch:28 Loss:0.03868\n",
      "Epoch:60 Batch:28 Loss:0.03688\n",
      "Epoch:80 Batch:28 Loss:0.03108\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.028\n",
      "Epoch:10 Batch:8 Loss:0.025\n",
      "Epoch:20 Batch:8 Loss:0.025\n",
      "Epoch:30 Batch:8 Loss:0.025\n",
      "Epoch:40 Batch:8 Loss:0.024\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -47.50474965457251 setps: 41 count: 41\n",
      "reward: -38.157074124172496 setps: 39 count: 80\n",
      "reward: -48.78938112853794 setps: 37 count: 117\n",
      "reward: -17.781988435750833 setps: 55 count: 172\n",
      "reward: -44.24404329517128 setps: 43 count: 215\n",
      "reward: -21.002382239776487 setps: 20 count: 235\n",
      "reward: -28.71691783684509 setps: 23 count: 258\n",
      "reward: -20.708741362435095 setps: 24 count: 282\n",
      "reward: -26.477070475585066 setps: 27 count: 309\n",
      "reward: -34.27026140256202 setps: 40 count: 349\n",
      "reward: -38.14784921664397 setps: 42 count: 391\n",
      "reward: -41.76049124223209 setps: 45 count: 436\n",
      "reward: -33.362364061069094 setps: 42 count: 478\n",
      "reward: -35.350740547163866 setps: 33 count: 511\n",
      "reward: -42.739921407654755 setps: 40 count: 551\n",
      "reward: -33.23810478598316 setps: 39 count: 590\n",
      "reward: -28.60170891102897 setps: 82 count: 672\n",
      "reward: -27.469040551532824 setps: 38 count: 710\n",
      "reward: -34.872724185831615 setps: 42 count: 752\n",
      "reward: -57.19711373975151 setps: 55 count: 807\n",
      "reward: -44.32599339491426 setps: 44 count: 851\n",
      "reward: -34.59769630922092 setps: 36 count: 887\n",
      "reward: -19.14379084167012 setps: 51 count: 938\n",
      "reward: -45.936290022041064 setps: 41 count: 979\n",
      "avg rewards: -35.18318496550612\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:36 Loss:0.04278\n",
      "Epoch:20 Batch:36 Loss:0.03177\n",
      "Epoch:40 Batch:36 Loss:0.03062\n",
      "Epoch:60 Batch:36 Loss:0.03437\n",
      "Epoch:80 Batch:36 Loss:0.03322\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.027\n",
      "Epoch:10 Batch:8 Loss:0.025\n",
      "Epoch:20 Batch:8 Loss:0.025\n",
      "Epoch:30 Batch:8 Loss:0.024\n",
      "Epoch:40 Batch:8 Loss:0.024\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -34.21378210864495 setps: 36 count: 36\n",
      "reward: -26.11290892283723 setps: 19 count: 55\n",
      "reward: -42.97167280363647 setps: 41 count: 96\n",
      "reward: -51.72713533034112 setps: 42 count: 138\n",
      "reward: -46.95467404368682 setps: 34 count: 172\n",
      "reward: -27.14485604532092 setps: 20 count: 192\n",
      "reward: -45.4541290561785 setps: 31 count: 223\n",
      "reward: -33.08556395033667 setps: 36 count: 259\n",
      "reward: -43.87691161824041 setps: 41 count: 300\n",
      "reward: -42.21540976276012 setps: 40 count: 340\n",
      "reward: -39.81161735203932 setps: 38 count: 378\n",
      "reward: -38.91909925824584 setps: 39 count: 417\n",
      "reward: -48.77341344712041 setps: 39 count: 456\n",
      "reward: -49.83104120872303 setps: 35 count: 491\n",
      "reward: -40.50185888048436 setps: 40 count: 531\n",
      "reward: -42.50015554133279 setps: 34 count: 565\n",
      "reward: -43.301953328742854 setps: 40 count: 605\n",
      "reward: -32.09282332114235 setps: 19 count: 624\n",
      "reward: -49.49939964822987 setps: 36 count: 660\n",
      "reward: -38.46935768755212 setps: 39 count: 699\n",
      "reward: -28.305180944496534 setps: 37 count: 736\n",
      "reward: -27.25656949639815 setps: 35 count: 771\n",
      "reward: -49.01182163154009 setps: 37 count: 808\n",
      "reward: -27.524004007680922 setps: 18 count: 826\n",
      "reward: -39.38424574352683 setps: 34 count: 860\n",
      "reward: -38.93736917156931 setps: 28 count: 888\n",
      "reward: -41.82347861419402 setps: 35 count: 923\n",
      "reward: -45.40619406823971 setps: 24 count: 947\n",
      "reward: -32.59432590225334 setps: 21 count: 968\n",
      "avg rewards: -39.575894927430866\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:44 Loss:0.03333\n",
      "Epoch:20 Batch:44 Loss:0.03481\n",
      "Epoch:40 Batch:44 Loss:0.03221\n",
      "Epoch:60 Batch:44 Loss:0.03181\n",
      "Epoch:80 Batch:44 Loss:0.03373\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.029\n",
      "Epoch:10 Batch:8 Loss:0.024\n",
      "Epoch:20 Batch:8 Loss:0.024\n",
      "Epoch:30 Batch:8 Loss:0.025\n",
      "Epoch:40 Batch:8 Loss:0.023\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -28.636542175158684 setps: 21 count: 21\n",
      "reward: -27.243941922958772 setps: 31 count: 52\n",
      "reward: -32.59583522190369 setps: 34 count: 86\n",
      "reward: -48.7264162628504 setps: 48 count: 134\n",
      "reward: -41.14374654374551 setps: 35 count: 169\n",
      "reward: -39.774890381998546 setps: 25 count: 194\n",
      "reward: -27.72253619151452 setps: 35 count: 229\n",
      "reward: -28.937992822454547 setps: 33 count: 262\n",
      "reward: -28.34543399455288 setps: 21 count: 283\n",
      "reward: -44.00796521376033 setps: 33 count: 316\n",
      "reward: -35.95979614542885 setps: 24 count: 340\n",
      "reward: -22.7099534594614 setps: 42 count: 382\n",
      "reward: -43.73782815347805 setps: 43 count: 425\n",
      "reward: -62.67158742186148 setps: 37 count: 462\n",
      "reward: -40.10846066298983 setps: 38 count: 500\n",
      "reward: -24.83969443323149 setps: 24 count: 524\n",
      "reward: -28.841124030531503 setps: 22 count: 546\n",
      "reward: -67.12731136040239 setps: 27 count: 573\n",
      "reward: -20.78226770697802 setps: 33 count: 606\n",
      "reward: -41.11864379251493 setps: 32 count: 638\n",
      "reward: -51.20664705338277 setps: 37 count: 675\n",
      "reward: -49.95635774384427 setps: 29 count: 704\n",
      "reward: -37.931289222963116 setps: 59 count: 763\n",
      "reward: -34.989797635584544 setps: 33 count: 796\n",
      "reward: -29.496520863124168 setps: 34 count: 830\n",
      "reward: -34.50477250488621 setps: 34 count: 864\n",
      "reward: -55.98694516566902 setps: 45 count: 909\n",
      "reward: -25.768669545503506 setps: 51 count: 960\n",
      "reward: -48.90301319686987 setps: 35 count: 995\n",
      "avg rewards: -38.06124071826218\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:54 Loss:0.03149\n",
      "Epoch:20 Batch:54 Loss:0.02970\n",
      "Epoch:40 Batch:54 Loss:0.03017\n",
      "Epoch:60 Batch:54 Loss:0.03108\n",
      "Epoch:80 Batch:54 Loss:0.03102\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.029\n",
      "Epoch:10 Batch:8 Loss:0.025\n",
      "Epoch:20 Batch:8 Loss:0.024\n",
      "Epoch:30 Batch:8 Loss:0.024\n",
      "Epoch:40 Batch:8 Loss:0.025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -57.15390036320168 setps: 18 count: 18\n",
      "reward: -36.6505376380097 setps: 27 count: 45\n",
      "reward: -72.22242407204031 setps: 30 count: 75\n",
      "reward: -44.062861317643545 setps: 27 count: 102\n",
      "reward: -33.90745156127523 setps: 30 count: 132\n",
      "reward: -42.03483649496919 setps: 38 count: 170\n",
      "reward: -55.57098816669459 setps: 30 count: 200\n",
      "reward: -41.001965691579976 setps: 25 count: 225\n",
      "reward: -57.46120739195612 setps: 33 count: 258\n",
      "reward: -49.90957220702548 setps: 29 count: 287\n",
      "reward: -33.61882421502815 setps: 21 count: 308\n",
      "reward: -75.59354718057439 setps: 22 count: 330\n",
      "reward: -40.935911684270835 setps: 18 count: 348\n",
      "reward: -67.65528237582039 setps: 29 count: 377\n",
      "reward: -63.87663151100277 setps: 28 count: 405\n",
      "reward: -53.566234864751465 setps: 27 count: 432\n",
      "reward: -42.85158240923484 setps: 28 count: 460\n",
      "reward: -43.151642689897564 setps: 25 count: 485\n",
      "reward: -62.944116523074634 setps: 27 count: 512\n",
      "reward: -48.757474139324046 setps: 26 count: 538\n",
      "reward: -43.6040937593585 setps: 32 count: 570\n",
      "reward: -54.39037053591455 setps: 29 count: 599\n",
      "reward: -57.55972369166846 setps: 31 count: 630\n",
      "reward: -62.82224906125631 setps: 28 count: 658\n",
      "reward: -72.36654180896585 setps: 29 count: 687\n",
      "reward: -53.709906316464185 setps: 28 count: 715\n",
      "reward: -44.27483866905386 setps: 31 count: 746\n",
      "reward: -59.75956259782252 setps: 31 count: 777\n",
      "reward: -52.382557597728756 setps: 27 count: 804\n",
      "reward: -50.365531249532076 setps: 30 count: 834\n",
      "reward: -54.423428349390456 setps: 30 count: 864\n",
      "reward: -44.05461648625787 setps: 31 count: 895\n",
      "reward: -60.964697053324194 setps: 36 count: 931\n",
      "reward: -37.830339594371615 setps: 28 count: 959\n",
      "reward: -37.9910542292797 setps: 32 count: 991\n",
      "avg rewards: -51.69790009993611\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:65 Loss:0.03147\n",
      "Epoch:20 Batch:65 Loss:0.03291\n",
      "Epoch:40 Batch:65 Loss:0.03215\n",
      "Epoch:60 Batch:65 Loss:0.02901\n",
      "Epoch:80 Batch:65 Loss:0.02894\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.025\n",
      "Epoch:10 Batch:8 Loss:0.025\n",
      "Epoch:20 Batch:8 Loss:0.023\n",
      "Epoch:30 Batch:8 Loss:0.024\n",
      "Epoch:40 Batch:8 Loss:0.023\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -34.38765133664856 setps: 23 count: 23\n",
      "reward: -57.7940559543349 setps: 20 count: 43\n",
      "reward: -33.707445522049966 setps: 26 count: 69\n",
      "reward: -40.50581043863931 setps: 26 count: 95\n",
      "reward: -40.43716779361712 setps: 33 count: 128\n",
      "reward: -47.33474615223531 setps: 30 count: 158\n",
      "reward: -53.447112521690734 setps: 17 count: 175\n",
      "reward: -69.89070601631684 setps: 21 count: 196\n",
      "reward: -67.97724035064603 setps: 21 count: 217\n",
      "reward: -51.72411270776937 setps: 32 count: 249\n",
      "reward: -55.86312637008813 setps: 27 count: 276\n",
      "reward: -21.508030165493256 setps: 21 count: 297\n",
      "reward: -43.09764523469348 setps: 30 count: 327\n",
      "reward: -58.92612129590997 setps: 24 count: 351\n",
      "reward: -58.23330620144115 setps: 23 count: 374\n",
      "reward: -20.733559664560016 setps: 18 count: 392\n",
      "reward: -70.41669163628248 setps: 33 count: 425\n",
      "reward: -44.10868930548168 setps: 33 count: 458\n",
      "reward: -75.81845278931287 setps: 33 count: 491\n",
      "reward: -36.43199110042769 setps: 27 count: 518\n",
      "reward: -41.46727693529682 setps: 28 count: 546\n",
      "reward: -49.72172619104239 setps: 28 count: 574\n",
      "reward: -61.33159298739922 setps: 35 count: 609\n",
      "reward: -34.9467721976689 setps: 27 count: 636\n",
      "reward: -84.4889033957821 setps: 34 count: 670\n",
      "reward: -25.31629848692392 setps: 24 count: 694\n",
      "reward: -50.75503289538902 setps: 31 count: 725\n",
      "reward: -41.87295769702613 setps: 30 count: 755\n",
      "reward: -38.600573767787125 setps: 22 count: 777\n",
      "reward: -45.941717517332286 setps: 22 count: 799\n",
      "reward: -86.25682842614769 setps: 32 count: 831\n",
      "reward: -88.37215388496118 setps: 33 count: 864\n",
      "reward: -31.641045565734387 setps: 26 count: 890\n",
      "reward: -69.14530648010403 setps: 25 count: 915\n",
      "reward: -31.36064263609588 setps: 29 count: 944\n",
      "reward: -73.14325588233623 setps: 20 count: 964\n",
      "reward: -51.93618493241666 setps: 28 count: 992\n",
      "avg rewards: -51.04437655235359\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:77 Loss:0.03227\n",
      "Epoch:20 Batch:77 Loss:0.03133\n",
      "Epoch:40 Batch:77 Loss:0.03198\n",
      "Epoch:60 Batch:77 Loss:0.03222\n",
      "Epoch:80 Batch:77 Loss:0.02640\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.029\n",
      "Epoch:10 Batch:8 Loss:0.023\n",
      "Epoch:20 Batch:8 Loss:0.023\n",
      "Epoch:30 Batch:8 Loss:0.023\n",
      "Epoch:40 Batch:8 Loss:0.024\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -29.506711653522512 setps: 21 count: 21\n",
      "reward: -34.56208422945783 setps: 21 count: 42\n",
      "reward: -61.71876331026724 setps: 21 count: 63\n",
      "reward: -45.50753384813872 setps: 29 count: 92\n",
      "reward: -32.06302617144538 setps: 21 count: 113\n",
      "reward: -61.25368807282066 setps: 24 count: 137\n",
      "reward: -58.799363820494925 setps: 29 count: 166\n",
      "reward: -54.16538014426916 setps: 30 count: 196\n",
      "reward: -33.970715241723525 setps: 23 count: 219\n",
      "reward: -41.5702597195268 setps: 19 count: 238\n",
      "reward: -29.85535163350287 setps: 22 count: 260\n",
      "reward: -61.572961177694374 setps: 30 count: 290\n",
      "reward: -48.56173909692879 setps: 23 count: 313\n",
      "reward: -35.64934908901632 setps: 21 count: 334\n",
      "reward: -68.79289686184786 setps: 27 count: 361\n",
      "reward: -37.4122423774068 setps: 27 count: 388\n",
      "reward: -48.57269003360561 setps: 27 count: 415\n",
      "reward: -62.248528539357366 setps: 26 count: 441\n",
      "reward: -57.888051998439195 setps: 23 count: 464\n",
      "reward: -41.733383890450924 setps: 28 count: 492\n",
      "reward: -30.7341788943013 setps: 21 count: 513\n",
      "reward: -51.94083573744428 setps: 28 count: 541\n",
      "reward: -29.080650105350653 setps: 21 count: 562\n",
      "reward: -23.08470187936182 setps: 22 count: 584\n",
      "reward: -32.21119735235115 setps: 18 count: 602\n",
      "reward: -68.61638408867583 setps: 25 count: 627\n",
      "reward: -52.04490727742522 setps: 26 count: 653\n",
      "reward: -38.71595306300005 setps: 18 count: 671\n",
      "reward: -48.19359152545658 setps: 27 count: 698\n",
      "reward: -44.315659216108905 setps: 30 count: 728\n",
      "reward: -42.00644759228161 setps: 28 count: 756\n",
      "reward: -27.205246071735747 setps: 21 count: 777\n",
      "reward: -46.32808690221282 setps: 26 count: 803\n",
      "reward: -35.97301935664728 setps: 29 count: 832\n",
      "reward: -56.28152626666415 setps: 26 count: 858\n",
      "reward: -23.90479726191989 setps: 21 count: 879\n",
      "reward: -41.06391847021587 setps: 28 count: 907\n",
      "reward: -68.63816172101797 setps: 32 count: 939\n",
      "reward: -60.99588764961808 setps: 26 count: 965\n",
      "reward: -31.90608167019527 setps: 21 count: 986\n",
      "avg rewards: -44.96614882529754\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:89 Loss:0.02907\n",
      "Epoch:20 Batch:89 Loss:0.03082\n",
      "Epoch:40 Batch:89 Loss:0.02822\n",
      "Epoch:60 Batch:89 Loss:0.02961\n",
      "Epoch:80 Batch:89 Loss:0.02882\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.024\n",
      "Epoch:10 Batch:8 Loss:0.023\n",
      "Epoch:20 Batch:8 Loss:0.023\n",
      "Epoch:30 Batch:8 Loss:0.022\n",
      "Epoch:40 Batch:8 Loss:0.021\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -29.59974425975379 setps: 21 count: 21\n",
      "reward: -28.39645938853646 setps: 21 count: 42\n",
      "reward: -33.89326892253593 setps: 21 count: 63\n",
      "reward: -32.49933449041564 setps: 20 count: 83\n",
      "reward: -31.123184592339385 setps: 20 count: 103\n",
      "reward: -30.119482640508796 setps: 22 count: 125\n",
      "reward: -31.647820733379916 setps: 21 count: 146\n",
      "reward: -33.842292139299396 setps: 22 count: 168\n",
      "reward: -42.20369654307142 setps: 22 count: 190\n",
      "reward: -34.2152427698602 setps: 20 count: 210\n",
      "reward: -19.485255741514266 setps: 20 count: 230\n",
      "reward: -30.601160915788206 setps: 21 count: 251\n",
      "reward: -37.98040330429067 setps: 22 count: 273\n",
      "reward: -35.37360555517807 setps: 21 count: 294\n",
      "reward: -30.79839165391749 setps: 20 count: 314\n",
      "reward: -36.720134743685776 setps: 21 count: 335\n",
      "reward: -37.810126481036434 setps: 25 count: 360\n",
      "reward: -25.84034005222056 setps: 21 count: 381\n",
      "reward: -32.17250033294113 setps: 21 count: 402\n",
      "reward: -33.39411163734039 setps: 22 count: 424\n",
      "reward: -52.40136831413401 setps: 26 count: 450\n",
      "reward: -22.427847855865547 setps: 21 count: 471\n",
      "reward: -27.213542863760086 setps: 18 count: 489\n",
      "reward: -27.28281771771872 setps: 20 count: 509\n",
      "reward: -28.022951573666074 setps: 21 count: 530\n",
      "reward: -32.987442936288424 setps: 21 count: 551\n",
      "reward: -24.699800763725943 setps: 21 count: 572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -19.34437727924523 setps: 22 count: 594\n",
      "reward: -29.607646492555794 setps: 21 count: 615\n",
      "reward: -24.462966101597704 setps: 21 count: 636\n",
      "reward: -36.620441105487416 setps: 21 count: 657\n",
      "reward: -27.16968239499838 setps: 21 count: 678\n",
      "reward: -28.206617672261203 setps: 21 count: 699\n",
      "reward: -21.662087204073035 setps: 21 count: 720\n",
      "reward: -33.046539525818666 setps: 21 count: 741\n",
      "reward: -32.50595410646929 setps: 23 count: 764\n",
      "reward: -29.97278390207648 setps: 21 count: 785\n",
      "reward: -33.067061358211504 setps: 21 count: 806\n",
      "reward: -36.432356291817264 setps: 21 count: 827\n",
      "reward: -47.217886578316396 setps: 22 count: 849\n",
      "reward: -34.72733188250277 setps: 21 count: 870\n",
      "reward: -27.21945439411648 setps: 21 count: 891\n",
      "reward: -25.520626182058184 setps: 21 count: 912\n",
      "reward: -31.00071578685893 setps: 21 count: 933\n",
      "reward: -37.68710355482326 setps: 21 count: 954\n",
      "reward: -25.953370411238573 setps: 21 count: 975\n",
      "avg rewards: -31.395159372767377\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:103 Loss:0.02759\n",
      "Epoch:20 Batch:103 Loss:0.02859\n",
      "Epoch:40 Batch:103 Loss:0.03210\n",
      "Epoch:60 Batch:103 Loss:0.02812\n",
      "Epoch:80 Batch:103 Loss:0.02900\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.021\n",
      "Epoch:10 Batch:8 Loss:0.021\n",
      "Epoch:20 Batch:8 Loss:0.022\n",
      "Epoch:30 Batch:8 Loss:0.021\n",
      "Epoch:40 Batch:8 Loss:0.020\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -28.588322995841736 setps: 20 count: 20\n",
      "reward: -27.83477827227616 setps: 20 count: 40\n",
      "reward: -60.506834374475865 setps: 32 count: 72\n",
      "reward: -49.1650746532876 setps: 20 count: 92\n",
      "reward: -18.950396726658802 setps: 20 count: 112\n",
      "reward: -24.041210052845422 setps: 20 count: 132\n",
      "reward: -41.35007562737593 setps: 20 count: 152\n",
      "reward: -32.064184718544126 setps: 20 count: 172\n",
      "reward: -18.741824961175737 setps: 18 count: 190\n",
      "reward: -23.860232397459907 setps: 20 count: 210\n",
      "reward: -26.137815587730437 setps: 21 count: 231\n",
      "reward: -25.66461175187724 setps: 20 count: 251\n",
      "reward: -15.497919297215413 setps: 21 count: 272\n",
      "reward: -17.763521160733944 setps: 20 count: 292\n",
      "reward: -28.58343536509056 setps: 20 count: 312\n",
      "reward: -17.409791312861493 setps: 20 count: 332\n",
      "reward: -24.41039841181337 setps: 20 count: 352\n",
      "reward: -24.73894725105638 setps: 25 count: 377\n",
      "reward: -24.367862040316687 setps: 20 count: 397\n",
      "reward: -35.675152184245235 setps: 20 count: 417\n",
      "reward: -24.03157876980113 setps: 20 count: 437\n",
      "reward: -38.332536934065864 setps: 20 count: 457\n",
      "reward: -42.64297241210878 setps: 20 count: 477\n",
      "reward: -18.793867750877688 setps: 20 count: 497\n",
      "reward: -23.37253204889712 setps: 20 count: 517\n",
      "reward: -41.947945928850096 setps: 21 count: 538\n",
      "reward: -36.287192376212616 setps: 21 count: 559\n",
      "reward: -31.77476448202215 setps: 20 count: 579\n",
      "reward: -30.275086685037245 setps: 20 count: 599\n",
      "reward: -38.23691875045916 setps: 20 count: 619\n",
      "reward: -22.228224314206454 setps: 20 count: 639\n",
      "reward: -24.83628455274447 setps: 21 count: 660\n",
      "reward: -38.51688614645682 setps: 36 count: 696\n",
      "reward: -26.80991707547947 setps: 20 count: 716\n",
      "reward: -21.85392907253554 setps: 21 count: 737\n",
      "reward: -42.804304897994726 setps: 17 count: 754\n",
      "reward: -20.8542811518535 setps: 20 count: 774\n",
      "reward: -20.42429695766769 setps: 21 count: 795\n",
      "reward: -26.39247489151458 setps: 20 count: 815\n",
      "reward: -26.248559802622186 setps: 20 count: 835\n",
      "reward: -26.059033918594647 setps: 21 count: 856\n",
      "reward: -29.426508038319298 setps: 20 count: 876\n",
      "reward: -26.82376615954272 setps: 20 count: 896\n",
      "reward: -36.03828231312799 setps: 20 count: 916\n",
      "reward: -23.37721830234223 setps: 20 count: 936\n",
      "reward: -29.50969615878276 setps: 20 count: 956\n",
      "reward: -29.709379824817013 setps: 21 count: 977\n",
      "reward: -24.746631614165377 setps: 21 count: 998\n",
      "avg rewards: -28.910572093207946\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:118 Loss:0.03028\n",
      "Epoch:20 Batch:118 Loss:0.03044\n",
      "Epoch:40 Batch:118 Loss:0.03361\n",
      "Epoch:60 Batch:118 Loss:0.02709\n",
      "Epoch:80 Batch:118 Loss:0.02900\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.021\n",
      "Epoch:10 Batch:8 Loss:0.020\n",
      "Epoch:20 Batch:8 Loss:0.020\n",
      "Epoch:30 Batch:8 Loss:0.021\n",
      "Epoch:40 Batch:8 Loss:0.020\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -26.9381033591766 setps: 20 count: 20\n",
      "reward: -36.082781578796855 setps: 20 count: 40\n",
      "reward: -20.75764457942278 setps: 20 count: 60\n",
      "reward: -18.592524092960225 setps: 20 count: 80\n",
      "reward: -32.429257624548335 setps: 20 count: 100\n",
      "reward: -22.37146139063989 setps: 20 count: 120\n",
      "reward: -28.474448711500738 setps: 20 count: 140\n",
      "reward: -25.862978439811563 setps: 20 count: 160\n",
      "reward: -34.015849195460035 setps: 20 count: 180\n",
      "reward: -22.001042446419888 setps: 20 count: 200\n",
      "reward: -27.114508451712027 setps: 20 count: 220\n",
      "reward: -24.243443918737462 setps: 20 count: 240\n",
      "reward: -28.85717080258619 setps: 21 count: 261\n",
      "reward: -32.717523091353364 setps: 20 count: 281\n",
      "reward: -19.964295020558346 setps: 20 count: 301\n",
      "reward: -23.028934609064894 setps: 20 count: 321\n",
      "reward: -30.35367795057391 setps: 20 count: 341\n",
      "reward: -26.216972679701573 setps: 19 count: 360\n",
      "reward: -24.741313060853283 setps: 20 count: 380\n",
      "reward: -29.598360916024834 setps: 20 count: 400\n",
      "reward: -29.584795610859878 setps: 20 count: 420\n",
      "reward: -24.566480177413904 setps: 20 count: 440\n",
      "reward: -22.603373738238588 setps: 20 count: 460\n",
      "reward: -26.39722259556729 setps: 20 count: 480\n",
      "reward: -28.308816195897812 setps: 20 count: 500\n",
      "reward: -23.75324956403201 setps: 20 count: 520\n",
      "reward: -24.337019254958427 setps: 20 count: 540\n",
      "reward: -25.495986634622387 setps: 19 count: 559\n",
      "reward: -21.90187389756757 setps: 20 count: 579\n",
      "reward: -20.59839533774357 setps: 20 count: 599\n",
      "reward: -30.24870322002098 setps: 20 count: 619\n",
      "reward: -20.700559073808837 setps: 20 count: 639\n",
      "reward: -30.987971482121793 setps: 20 count: 659\n",
      "reward: -36.05401039143617 setps: 20 count: 679\n",
      "reward: -34.84649452558515 setps: 20 count: 699\n",
      "reward: -32.91999002028898 setps: 20 count: 719\n",
      "reward: -25.892893915268356 setps: 20 count: 739\n",
      "reward: -37.51968572311743 setps: 20 count: 759\n",
      "reward: -24.796396558039124 setps: 21 count: 780\n",
      "reward: -24.712962892840736 setps: 20 count: 800\n",
      "reward: -19.70292866229429 setps: 20 count: 820\n",
      "reward: -21.550975805308553 setps: 19 count: 839\n",
      "reward: -22.992226552384086 setps: 20 count: 859\n",
      "reward: -39.09423265060177 setps: 20 count: 879\n",
      "reward: -20.66815213499212 setps: 20 count: 899\n",
      "reward: -24.10915632492834 setps: 20 count: 919\n",
      "reward: -30.164821827533892 setps: 20 count: 939\n",
      "reward: -23.85559025983676 setps: 20 count: 959\n",
      "reward: -26.613492252075226 setps: 20 count: 979\n",
      "reward: -23.417327546405435 setps: 20 count: 999\n",
      "avg rewards: -26.655161534913844\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:133 Loss:0.03050\n",
      "Epoch:20 Batch:133 Loss:0.03005\n",
      "Epoch:40 Batch:133 Loss:0.02898\n",
      "Epoch:60 Batch:133 Loss:0.02813\n",
      "Epoch:80 Batch:133 Loss:0.02746\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.021\n",
      "Epoch:10 Batch:8 Loss:0.020\n",
      "Epoch:20 Batch:8 Loss:0.020\n",
      "Epoch:30 Batch:8 Loss:0.020\n",
      "Epoch:40 Batch:8 Loss:0.020\n",
      "Done!\n",
      "######## STEP 17 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -41.410912300214115 setps: 19 count: 19\n",
      "reward: -31.900828482412905 setps: 20 count: 39\n",
      "reward: -31.021433122875173 setps: 21 count: 60\n",
      "reward: -28.942834661413514 setps: 20 count: 80\n",
      "reward: -32.815790253602614 setps: 20 count: 100\n",
      "reward: -36.67226845027472 setps: 20 count: 120\n",
      "reward: -30.871383181160496 setps: 20 count: 140\n",
      "reward: -25.98301407599938 setps: 20 count: 160\n",
      "reward: -36.714038561456256 setps: 20 count: 180\n",
      "reward: -30.65147539403552 setps: 20 count: 200\n",
      "reward: -28.324931614517116 setps: 20 count: 220\n",
      "reward: -33.641088485918594 setps: 20 count: 240\n",
      "reward: -32.83010798596369 setps: 21 count: 261\n",
      "reward: -34.22606289186224 setps: 20 count: 281\n",
      "reward: -31.32178737379436 setps: 17 count: 298\n",
      "reward: -32.71303836644948 setps: 20 count: 318\n",
      "reward: -32.05188147162263 setps: 18 count: 336\n",
      "reward: -42.77377330358139 setps: 21 count: 357\n",
      "reward: -30.330438448551288 setps: 18 count: 375\n",
      "reward: -26.043634481269695 setps: 20 count: 395\n",
      "reward: -29.3324160532502 setps: 20 count: 415\n",
      "reward: -27.81675863407581 setps: 20 count: 435\n",
      "reward: -25.882170828223753 setps: 20 count: 455\n",
      "reward: -28.41898187825282 setps: 20 count: 475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -27.327899699816772 setps: 20 count: 495\n",
      "reward: -32.22148132494913 setps: 20 count: 515\n",
      "reward: -22.403583319333848 setps: 20 count: 535\n",
      "reward: -30.54662179995212 setps: 20 count: 555\n",
      "reward: -22.26709930888319 setps: 20 count: 575\n",
      "reward: -22.98311018325767 setps: 20 count: 595\n",
      "reward: -25.230743131926285 setps: 20 count: 615\n",
      "reward: -27.82670171214413 setps: 20 count: 635\n",
      "reward: -23.449001265427796 setps: 20 count: 655\n",
      "reward: -26.31445878028462 setps: 20 count: 675\n",
      "reward: -28.37593187437451 setps: 20 count: 695\n",
      "reward: -28.637805122059945 setps: 20 count: 715\n",
      "reward: -28.796052764946943 setps: 20 count: 735\n",
      "reward: -28.429080128860374 setps: 20 count: 755\n",
      "reward: -26.989751570300722 setps: 20 count: 775\n",
      "reward: -33.72303012492921 setps: 20 count: 795\n",
      "reward: -32.47416195132683 setps: 20 count: 815\n",
      "reward: -28.187546920013848 setps: 20 count: 835\n",
      "reward: -28.03149946087942 setps: 20 count: 855\n",
      "reward: -24.067982162207773 setps: 20 count: 875\n",
      "reward: -24.781255056400553 setps: 20 count: 895\n",
      "reward: -34.70731322933571 setps: 20 count: 915\n",
      "reward: -31.5482314070483 setps: 20 count: 935\n",
      "reward: -25.74574914492987 setps: 20 count: 955\n",
      "reward: -34.25021010974742 setps: 21 count: 976\n",
      "reward: -27.558416010295335 setps: 20 count: 996\n",
      "avg rewards: -29.791315357288205\n",
      "Done! (17000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:150 Loss:0.02930\n",
      "Epoch:20 Batch:150 Loss:0.02871\n",
      "Epoch:40 Batch:150 Loss:0.02955\n",
      "Epoch:60 Batch:150 Loss:0.02628\n",
      "Epoch:80 Batch:150 Loss:0.02819\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.020\n",
      "Epoch:10 Batch:8 Loss:0.020\n",
      "Epoch:20 Batch:8 Loss:0.020\n",
      "Epoch:30 Batch:8 Loss:0.020\n",
      "Epoch:40 Batch:8 Loss:0.021\n",
      "Done!\n",
      "######## STEP 18 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -27.551704726299796 setps: 21 count: 21\n",
      "reward: -22.915438347673625 setps: 22 count: 43\n",
      "reward: -19.390154372237156 setps: 21 count: 64\n",
      "reward: -25.826447091004226 setps: 21 count: 85\n",
      "reward: -28.3927152428354 setps: 21 count: 106\n",
      "reward: -20.365534527238925 setps: 20 count: 126\n",
      "reward: -31.215456284031095 setps: 21 count: 147\n",
      "reward: -22.47484306332626 setps: 20 count: 167\n",
      "reward: -20.339381218991182 setps: 20 count: 187\n",
      "reward: -17.687645307078494 setps: 20 count: 207\n",
      "reward: -48.61282870099531 setps: 24 count: 231\n",
      "reward: -29.043440725974506 setps: 21 count: 252\n",
      "reward: -28.05794448953675 setps: 21 count: 273\n",
      "reward: -49.330229944428716 setps: 21 count: 294\n",
      "reward: -25.05397808027628 setps: 20 count: 314\n",
      "reward: -32.99026338006952 setps: 22 count: 336\n",
      "reward: -29.615665829251526 setps: 21 count: 357\n",
      "reward: -25.592336885676193 setps: 21 count: 378\n",
      "reward: -24.252463462729065 setps: 20 count: 398\n",
      "reward: -31.13792330249417 setps: 22 count: 420\n",
      "reward: -26.283073838311246 setps: 20 count: 440\n",
      "reward: -16.097818665624075 setps: 20 count: 460\n",
      "reward: -28.774166073552625 setps: 20 count: 480\n",
      "reward: -26.38170215135178 setps: 22 count: 502\n",
      "reward: -32.139954004957694 setps: 20 count: 522\n",
      "reward: -25.04831468292832 setps: 21 count: 543\n",
      "reward: -34.0848507098126 setps: 21 count: 564\n",
      "reward: -27.150880769465584 setps: 21 count: 585\n",
      "reward: -25.523241200447956 setps: 21 count: 606\n",
      "reward: -24.756892026626158 setps: 20 count: 626\n",
      "reward: -18.39308866488136 setps: 20 count: 646\n",
      "reward: -23.618372226030623 setps: 20 count: 666\n",
      "reward: -29.57638989770203 setps: 21 count: 687\n",
      "reward: -23.428722221395585 setps: 20 count: 707\n",
      "reward: -19.467521912172376 setps: 19 count: 726\n",
      "reward: -19.96051918034646 setps: 21 count: 747\n",
      "reward: -35.41171483803919 setps: 21 count: 768\n",
      "reward: -23.83694901852723 setps: 20 count: 788\n",
      "reward: -24.949323028286745 setps: 21 count: 809\n",
      "reward: -22.66059588673379 setps: 20 count: 829\n",
      "reward: -29.72050353215891 setps: 21 count: 850\n",
      "reward: -28.620415770115503 setps: 21 count: 871\n",
      "reward: -27.27241358523461 setps: 21 count: 892\n",
      "reward: -33.362069235149825 setps: 21 count: 913\n",
      "reward: -21.105181908728266 setps: 20 count: 933\n",
      "reward: -24.714600584283467 setps: 21 count: 954\n",
      "reward: -23.28613884202059 setps: 20 count: 974\n",
      "reward: -27.239307769683364 setps: 21 count: 995\n",
      "avg rewards: -26.72314827513992\n",
      "Done! (18000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:167 Loss:0.02754\n",
      "Epoch:20 Batch:167 Loss:0.02777\n",
      "Epoch:40 Batch:167 Loss:0.02738\n",
      "Epoch:60 Batch:167 Loss:0.02796\n",
      "Epoch:80 Batch:167 Loss:0.02707\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.022\n",
      "Epoch:10 Batch:8 Loss:0.020\n",
      "Epoch:20 Batch:8 Loss:0.020\n",
      "Epoch:30 Batch:8 Loss:0.020\n",
      "Epoch:40 Batch:8 Loss:0.019\n",
      "Done!\n",
      "######## STEP 19 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -15.988668292181686 setps: 20 count: 20\n",
      "reward: -20.2214056958459 setps: 20 count: 40\n",
      "reward: -13.179691481756164 setps: 20 count: 60\n",
      "reward: -21.843880001000073 setps: 20 count: 80\n",
      "reward: -18.17547611779155 setps: 21 count: 101\n",
      "reward: -24.45938069302938 setps: 20 count: 121\n",
      "reward: -18.250990391636154 setps: 20 count: 141\n",
      "reward: -22.12287967562297 setps: 21 count: 162\n",
      "reward: -21.35008426451386 setps: 19 count: 181\n",
      "reward: -15.883197371836285 setps: 21 count: 202\n",
      "reward: -19.333272834468517 setps: 20 count: 222\n",
      "reward: -18.385652958143332 setps: 19 count: 241\n",
      "reward: -18.194692007238338 setps: 20 count: 261\n",
      "reward: -15.033192618202886 setps: 20 count: 281\n",
      "reward: -21.891179034509697 setps: 20 count: 301\n",
      "reward: -19.284837224206424 setps: 20 count: 321\n",
      "reward: -22.557566527344168 setps: 21 count: 342\n",
      "reward: -18.420364705222894 setps: 20 count: 362\n",
      "reward: -27.806294462074582 setps: 20 count: 382\n",
      "reward: -23.074251321105113 setps: 20 count: 402\n",
      "reward: -15.865938980299688 setps: 20 count: 422\n",
      "reward: -19.013980057174923 setps: 20 count: 442\n",
      "reward: -22.91691950341483 setps: 21 count: 463\n",
      "reward: -18.088756399590054 setps: 20 count: 483\n",
      "reward: -15.309734080030463 setps: 20 count: 503\n",
      "reward: -19.825194895446476 setps: 20 count: 523\n",
      "reward: -30.029654894548003 setps: 16 count: 539\n",
      "reward: -21.275544702741897 setps: 20 count: 559\n",
      "reward: -14.26845418760786 setps: 21 count: 580\n",
      "reward: -15.469621652248314 setps: 20 count: 600\n",
      "reward: -20.715856723551404 setps: 20 count: 620\n",
      "reward: -19.671775581629483 setps: 20 count: 640\n",
      "reward: -24.197254235492437 setps: 20 count: 660\n",
      "reward: -24.33564040599886 setps: 20 count: 680\n",
      "reward: -16.501257684151643 setps: 20 count: 700\n",
      "reward: -16.554432248375086 setps: 21 count: 721\n",
      "reward: -21.31180306047463 setps: 20 count: 741\n",
      "reward: -18.112081673509962 setps: 20 count: 761\n",
      "reward: -12.40744197787717 setps: 20 count: 781\n",
      "reward: -18.181413697109384 setps: 20 count: 801\n",
      "reward: -14.148389998839411 setps: 20 count: 821\n",
      "reward: -19.482916518746062 setps: 20 count: 841\n",
      "reward: -17.488957063000996 setps: 20 count: 861\n",
      "reward: -21.515967426873978 setps: 20 count: 881\n",
      "reward: -23.108575015628592 setps: 20 count: 901\n",
      "reward: -15.088637484719222 setps: 20 count: 921\n",
      "reward: -23.30813080407534 setps: 20 count: 941\n",
      "reward: -19.58810283527564 setps: 20 count: 961\n",
      "reward: -17.73979818234802 setps: 21 count: 982\n",
      "avg rewards: -19.4077385642553\n",
      "Done! (19000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:186 Loss:0.02534\n",
      "Epoch:20 Batch:186 Loss:0.02502\n",
      "Epoch:40 Batch:186 Loss:0.02640\n",
      "Epoch:60 Batch:186 Loss:0.02751\n",
      "Epoch:80 Batch:186 Loss:0.02831\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.021\n",
      "Epoch:10 Batch:8 Loss:0.020\n",
      "Epoch:20 Batch:8 Loss:0.020\n",
      "Epoch:30 Batch:8 Loss:0.021\n",
      "Epoch:40 Batch:8 Loss:0.019\n",
      "Done!\n",
      "######## STEP 20 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: -10.735967667541992 setps: 20 count: 20\n",
      "reward: -9.781164919948788 setps: 20 count: 40\n",
      "reward: -15.614367693924576 setps: 19 count: 59\n",
      "reward: -15.430862144654386 setps: 21 count: 80\n",
      "reward: -12.191877352075245 setps: 21 count: 101\n",
      "reward: -17.940953622364034 setps: 20 count: 121\n",
      "reward: -10.21723912552261 setps: 20 count: 141\n",
      "reward: -17.781632833211916 setps: 21 count: 162\n",
      "reward: -14.98478001395124 setps: 20 count: 182\n",
      "reward: -11.430880651340704 setps: 20 count: 202\n",
      "reward: -14.57766708764248 setps: 20 count: 222\n",
      "reward: -19.570243608087186 setps: 21 count: 243\n",
      "reward: -50.44328308123078 setps: 21 count: 264\n",
      "reward: -17.83283183998865 setps: 20 count: 284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -11.288390001001243 setps: 20 count: 304\n",
      "reward: -16.611963224354263 setps: 22 count: 326\n",
      "reward: -13.225087709154469 setps: 20 count: 346\n",
      "reward: -30.123378588692873 setps: 21 count: 367\n",
      "reward: -11.90502810429316 setps: 20 count: 387\n",
      "reward: -16.064442773043993 setps: 20 count: 407\n",
      "reward: -24.871380087235593 setps: 22 count: 429\n",
      "reward: -28.007589032372927 setps: 21 count: 450\n",
      "reward: -12.05423034388805 setps: 20 count: 470\n",
      "reward: -16.45750633118005 setps: 20 count: 490\n",
      "reward: -21.59226769704546 setps: 21 count: 511\n",
      "reward: -13.467320411013503 setps: 22 count: 533\n",
      "reward: -12.731419621845998 setps: 20 count: 553\n",
      "reward: -12.013383451763367 setps: 20 count: 573\n",
      "reward: -17.604799986114084 setps: 21 count: 594\n",
      "reward: -16.766120804985984 setps: 21 count: 615\n",
      "reward: -19.225431443926936 setps: 20 count: 635\n",
      "reward: -13.139529396218133 setps: 20 count: 655\n",
      "reward: -16.143058676616054 setps: 20 count: 675\n",
      "reward: -19.116034348979888 setps: 20 count: 695\n",
      "reward: -13.26170736776403 setps: 20 count: 715\n",
      "reward: -13.85650239374227 setps: 20 count: 735\n",
      "reward: -13.302472181621125 setps: 20 count: 755\n",
      "reward: -14.601112584996736 setps: 20 count: 775\n",
      "reward: -18.45704513387027 setps: 22 count: 797\n",
      "reward: -15.45628248477733 setps: 20 count: 817\n",
      "reward: -14.355579694082554 setps: 20 count: 837\n",
      "reward: -15.459502829694248 setps: 20 count: 857\n",
      "reward: -8.852774479519578 setps: 22 count: 879\n",
      "reward: -20.624644506572807 setps: 21 count: 900\n",
      "reward: -12.709186354400298 setps: 21 count: 921\n",
      "reward: -15.404971673015101 setps: 20 count: 941\n",
      "reward: -17.29213896037836 setps: 20 count: 961\n",
      "reward: -23.230293985553725 setps: 21 count: 982\n",
      "reward: -24.01007910232729 setps: 18 count: 1000\n",
      "avg rewards: -16.771763416480127\n",
      "Done! (20000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:206 Loss:0.03128\n",
      "Epoch:20 Batch:206 Loss:0.03115\n",
      "Epoch:40 Batch:206 Loss:0.02364\n",
      "Epoch:60 Batch:206 Loss:0.02686\n",
      "Epoch:80 Batch:206 Loss:0.02618\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:8 Loss:0.023\n",
      "Epoch:10 Batch:8 Loss:0.022\n",
      "Epoch:20 Batch:8 Loss:0.021\n",
      "Epoch:30 Batch:8 Loss:0.022\n",
      "Epoch:40 Batch:8 Loss:0.021\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "test_rewards_envs = []\n",
    "\n",
    "for en in env_list[1:]:\n",
    "    print(\"############# start \"+en+\" training ###################\")\n",
    "\n",
    "    ENV_NAME = en#env_list[3]\n",
    "    env=ENV_NAME\n",
    "    runs = 20\n",
    "    inv_samples = 1000\n",
    "    max_steps = 500\n",
    "    expert_path='experts/'\n",
    "    DEMO_DIR = os.path.join(expert_path, env+'.pkl')\n",
    "    M = inv_samples\n",
    "\n",
    "    try:\n",
    "        demos = np.load(\"experts/states_expert_walker_.npy\")[:10]\n",
    "    except:\n",
    "        with open(DEMO_DIR, 'rb') as f:\n",
    "            trajs = pickle.load(f)\n",
    "\n",
    "\n",
    "    env = gym.make(ENV_NAME)\n",
    "    demos = []\n",
    "    for t_id, traj in enumerate(trajs):\n",
    "        demo =[]\n",
    "        #print(t_id)\n",
    "        for item in traj:    \n",
    "            obs = item['observation']\n",
    "            #obs = list(obs)\n",
    "            #print(obs)\n",
    "            demo.append(obs)\n",
    "        #print(np.array(demo).shape)\n",
    "        demos.append(np.array(demo))\n",
    "\n",
    "    print(np.array(demos).shape)\n",
    "    demos = demos[:10]\n",
    "\n",
    "\n",
    "\n",
    "    policy = policy_continuous(env.observation_space.shape[0],64,env.action_space.shape[0], uncertain=True)#.cuda()\n",
    "    inv_model = forward_dynamics_continuous(env.observation_space.shape[0],100,env.action_space.shape[0], uncertain=True)#.cuda()\n",
    "    inv_model_best = None\n",
    "    reward_best = -1000\n",
    "\n",
    "    inv_dataset_list = []\n",
    "    use_policy = False\n",
    "\n",
    "    transitions = []\n",
    "    test_rewards = []\n",
    "    for steps in range(runs):\n",
    "        print('######## STEP %d #######'%(steps+1))\n",
    "        ### GET SAMPLES FOR LEARNING INVERSE MODEL\n",
    "        print('Collecting transitions for learning inverse model....')\n",
    "        if steps > 0:\n",
    "            use_policy = True\n",
    "\n",
    "\n",
    "        trans_samples, avg_reward = gen_inv_samples(env, policy.cpu(), M, 'continuous', use_policy, max_steps=max_steps)\n",
    "        transitions = transitions+trans_samples\n",
    "        \"\"\"\n",
    "        if len(transitions) > 92000:\n",
    "            transitions = random.sample(transitions,92000)\n",
    "        \"\"\"\n",
    "        test_rewards.append(avg_reward)\n",
    "        print('Done!', np.array(transitions).shape)\n",
    "\n",
    "        ### LEARN THE INVERSE MODEL\n",
    "        \"\"\"\n",
    "        if avg_reward > reward_best and steps!=0:\n",
    "            inv_model_best = inv_model\n",
    "            reward_best = avg_reward\n",
    "        elif steps==0:\n",
    "            reward_best = avg_reward\n",
    "            inv_model_best = forward_dynamics_continuous(env.observation_space.shape[0],100,env.action_space.shape[0], uncertain=True)#.cuda()\n",
    "        else:\n",
    "            inv_model = inv_model_best#forward_dynamics_continuous(env.observation_space.shape[0],100,env.action_space.shape[0], uncertain=True)#.cuda()\n",
    "        \"\"\"\n",
    "        #inv_model = forward_dynamics_continuous(env.observation_space.shape[0],100,env.action_space.shape[0], uncertain=True)#.cuda()\n",
    "        \n",
    "        print('Learning inverse model....')\n",
    "        inv_model = inv_model_training(transitions, inv_model)\n",
    "\n",
    "        ### GET ACTIONS FOR DEMOS\n",
    "        inv_model.cpu()\n",
    "        print('Getting labels for demos....')\n",
    "        trajs = get_state_labels(demos)\n",
    "        print('Done!')\n",
    "\n",
    "\n",
    "        ### PERFORM BEHAVIORAL CLONING\n",
    "        policy = train_bc(trajs, policy, inv_model, sample_itr=400)\n",
    "\n",
    "    torch.save(policy, ENV_NAME+'.pt')\n",
    "    test_rewards_envs.append(test_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "boxed-geology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-85.24666575937175, -116.47774943852333, -10.96630920983921, 17.304050429698016, -118.43156979092814, -74.1458069853056, 70.27456710195374, 100.80788806932145, -56.56362055664888, -31.812858696942808, -10.851559185719275, 14.160764850951267, 28.543990688723945, -5.717326053732265, -37.55324727559363, 58.66846864385483, 128.73503949735488, 139.55567579022966, -53.166687642053496, 46.58256209411718]\n",
      "\n",
      "\n",
      "[16.660785166834955, -1.945918709867475, 37.253515328899205, 66.85354695510023, 30.05846013617653, 26.03826305267888, 24.046560091952763, 20.980588766034728, 19.00071675412101, 19.419839763611844, 19.722817774705657, 18.637635677143646, 16.54892408092522, 16.743986930444397, 17.169311923881466, 19.961545464701018, 63.73460001084896, 22.72243767543317, 14.518786978394143, 10.884084594756153]\n",
      "\n",
      "\n",
      "[19.818471518301138, 0.6177594795147113, 1.190572958536257, 33.66643557618494, 21.57302771290032, 15.844861630474568, 19.13041368888486, 33.23161788559624, 38.34830932011177, 13.486254519510476, 27.926529815839153, 75.0162120440582, 41.54489756479423, 42.296165909433704, 39.13780853562989, 41.516035242615494, 53.464786077203854, 50.854736583002115, 59.409074893165666, 43.528142044334224]\n",
      "\n",
      "\n",
      "[-623.9647909027392, -560.5480738270583, -414.838970696519, -485.42358007268774, -603.0242003815184, -623.4231130830422, -665.6974118215484, -688.5883905141757, -715.7859611047271, -714.527833802664, -719.740129174599, -736.4945761567379, -755.7060070268982, -764.8472456070319, -809.1497482996926, -810.2807287852028, -825.777024560333, -750.6533836430128, -792.5299522741956, -787.008178202653]\n",
      "\n",
      "\n",
      "[252.67932441027438, 133.82021690741811, 91.57846799382537, -21.726189240309374, 71.7694456211437, -146.764412189146, 71.03738529384785, 84.47810990632792, -99.77750004463152, -911.8751148061477, -926.3069702195573, -726.1901172762558, -1389.8145308376197, -1243.7479621968287, -1596.8007011442064, -1661.1116058899415, -2789.8128594299205, -2157.936930058588, -1032.0083955515381, -1890.087030626593]\n",
      "\n",
      "\n",
      "[-30.59677062969231, -93.02110113432072, -53.88218716701651, -46.534680774025304, -49.14151178746548, -50.08345938969509, -50.809700856212835, -35.18318496550612, -39.575894927430866, -38.06124071826218, -51.69790009993611, -51.04437655235359, -44.96614882529754, -31.395159372767377, -28.910572093207946, -26.655161534913844, -29.791315357288205, -26.72314827513992, -19.4077385642553, -16.771763416480127]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tr in test_rewards_envs:\n",
    "    print(tr)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-fraud",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bipedal walker\n",
    "[-80.71431939543506, -107.64788803287502, 46.10971254079241, 73.32555483257023, -44.20981564636499, 121.81057550628948, 1.4396739966325498, 116.41870502131607, 23.94124108966433, 98.80370937478375, 46.21050926431498, 68.92547245199984, 72.60305584580723, 43.11382392539668, -19.15160680306695, 63.967338609014675, 41.47164657563946, -0.12702182598236209, 7.8780907695502895, 45.37928585095369]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[-1373.2022003799393, -1537.3849441439165, -1566.3893398222672, -1566.4634060174883, -1513.6554054038406, -1526.7649856726032, -1588.8617726247337, -1493.9276560460264, -1486.5689613453014, -1550.5739168169828, -1550.2711365015564, -1575.905552052223, -1533.292399915575, -1515.0198687160193, -1549.5659183686507, -1450.3833646696814, -1439.0032680973789, -1505.3988276566417, -1491.981697066, -1451.7861920043692]\n",
    "\n",
    "\n",
    "[-77.42269800045648, -120.29086165266833, -65.97568645354733, -17.24457595621989, 17.761656451816577, -23.315276349945528, -125.68498414493897, 32.16795578633858, 27.353653848985825, -100.18606084116595, -9.982470982930854, -71.04823052779166, -31.26627925078696, -30.3749177017662, -18.479886873909912, -8.814653659052217, 53.806495660831025, 59.48446484370838, -96.95677284185486, 32.37978991513198]\n",
    "\n",
    "\n",
    "[17.05794454679845, -1.152118400453911, 73.31450239102124, 43.82633083314189, 45.05796391713242, 40.80798365452466, 19.01886243725472, 35.60101467668996, 19.582562413734195, 42.536604598793204, 47.87169106576544, 19.599122156608583, 39.00406759191564, 78.15280722142549, 76.47825263079336, 28.527372805199242, 20.286506336651456, 82.01481704865347, 26.696482407852393, 17.68209225587311]\n",
    "\n",
    "\n",
    "[20.340960391015876, -7.5957462968471665, -5.64952268313829, 39.17338988423048, 34.17768875810082, 105.5906503018052, 58.99896630055687, 70.56245956803333, 45.69051809440425, 110.42888658674603, 131.70598534916468, 75.33278338056694, 76.858474227848, 233.44472073642584, 231.76878421835434, 115.70124689468126, 225.17604362895835, 100.70042132769284, 45.59691453532076, 158.23814460606152]\n",
    "\n",
    "\n",
    "[-584.6469779468895, -790.1563856982439, -738.3381516537241, -303.06726756870256, 51.336031873359694, -558.3819529517864, -705.0943268275055, -232.6737508441126, -686.7766340204014, -724.4446733457228, -568.8822996880676, -774.1916697368074, -628.9576601929583, -641.522208993676, -737.9679958551051, -522.7748998468383, -761.5455540991205, -810.5939675764698, -730.5554652859933, -424.63264384175324]\n",
    "\n",
    "\n",
    "[99.31250812622015, 110.30787982852996, 154.36258293253405, 196.4705833228953, 219.81495326495863, 81.4895250369373, -73.91483945067316, -4.549355839342006, -459.2008512168189, -125.78358223159816, -216.31058123705515, -58.68871890729858, -264.86666464125386, -1481.982335698971, -2769.588672446682, -888.5039450961544, -940.2829805759836, -1274.6549564888778, -752.758337622611, -658.1891863229729]\n",
    "\n",
    "\n",
    "[-31.129122906622463, -70.2628989337789, -94.79358454540414, -23.01149874768736, -22.04257649423564, -24.244113261252757, -27.818875505115006, -43.40152464929519, -27.349353352093015, -35.530143053229, -54.14535895695569, -19.235037690635433, -45.05351322031206, -25.952533264753992, -45.002882376082226, -21.464234956854877, -22.752502099574624, 3.393028267252964, -56.258879656719834, -79.67432940683796]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "immune-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "test_hopper = [19.39352412420114, 32.04291391761344, 71.05704872371435, 210.83129950012804, 70.8467080637385, 137.4572230840752, 40.47588962083239, 388.04944066645714, 70.29350711982096, 213.6719275538449, 81.97411059718404, 141.17601595789915, 151.4677262172483, 102.80866475250083, 159.70545355726733, 464.72625602344385, 225.03651377538398, 154.7150833974447, 364.8459397128754, 452.7572393095682]\n",
    "test_bipedalwalker = [-21.622981899611602, -50.45265779702035, -14.964480684141027, -3.974716434900671, -85.41978882118583, -85.41538556643499, -84.24169132222157, -87.91838652590553, -85.30914249103455, -55.75293002613003, -16.839369788537105, -87.10465359533988, -77.72110420887255, -64.78834819576186, -55.42264371588144, -55.08960786631849, -16.98840366882085, -29.061604524564615, -73.68085866489076, -24.15669209049675]\n",
    "test_pendulum = [-1228.6509315035587, -1457.6222519893286, -1240.9056650422615, -1019.8056953150948, -1045.489732081308, -901.8502790436556, -946.2309834914628, -921.8139484824299, -1027.9403362923154, -555.6989599036317, -762.265171812457, -665.6360708604577, -604.3417549573797, -572.3040229700397, -241.89112581788163, -228.705158359611, -274.79621308810664, -510.21136081226194, -361.54764534502635, -240.72902916931594]\n",
    "test_pendulum_f = [-1228.6509315035587, -1457.6222519893286, -1240.9056650422615, -1019.8056953150948, -1045.489732081308, -901.8502790436556, -946.2309834914628, -921.8139484824299, -1027.9403362923154, -555.6989599036317, -762.265171812457, -665.6360708604577, -604.3417549573797, -572.3040229700397, -241.89112581788163, -228.705158359611, -274.79621308810664, -510.21136081226194, -361.54764534502635, -240.72902916931594]\n",
    "\n",
    "test_walker = [-207.3353196093495, -174.26447514948205, -119.45763845661604, -144.46548824725397, -167.40929170178433, -151.12769239100493, -128.9960330633043, -120.55470575734735, -142.87673252008307, -117.46457378416585, -115.63443093845709, -55.2404006428769, -104.16900117564728, -119.80029417114113, -102.69197222911998, -156.35112040278824, -95.37760097223645, -32.16942469029047, -85.25463586298953, -163.86623872745926]\n",
    "test_half = [17.013435360947565, 40.15109587138687, 21.430784166587966, 24.17196539273257, 82.36950376110435, 29.22265959028815, 65.57115237712492, 34.345782755340814, 42.59285357176953, 78.61830403883336, 62.88512472101623, 58.8132956755824, 55.83413705380583, 60.73374486486313, 51.154639767290234, 50.608121683934876, 45.42836724520008, 46.15240933758517, 39.74357310803997, 37.22987510561556]\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "architectural-sheet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_simple_bco(test_hopper, test_pendulum_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "joint-event",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -78.42204228,   42.18331636,  -93.21474983, -121.66048113,\n",
       "        -66.53953675,   13.78156067,    0.68313318, -134.10851045,\n",
       "       -128.67852491,  -52.18622644,   41.53719442,  -38.52551869,\n",
       "        -51.09460727,   50.37090879,  -20.70895905,   29.94379646,\n",
       "       -136.70889628,  -67.64006333,   40.06229804])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.random.rand(ypoints[1:].shape[0])-0.7)*200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "united-agenda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_simple_bco(test_pendulum, test_pendulum_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "systematic-copyright",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_simple_bco(test_walker, test_pendulum_f, s=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "arabic-encoding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_simple_bco(test_half, test_pendulum_f, s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "psychological-allowance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD4CAYAAAAEhuazAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAABK5ElEQVR4nO3dd3hUVfrA8e+ZSQ/phRBCCL33UAUBG4godrEgrgVR3NV1Xev+XFfXtWNZy4qKBQsiNhSld6UFpPdQA4E00tuU8/vjDilkkkySuTOTyfk8Tx4z996593WYvHPm3HPeI6SUKIqiKC2Lwd0BKIqiKK6nkr+iKEoLpJK/oihKC6SSv6IoSgukkr+iKEoL5OPuABwRHR0tk5KS3B2GoihKs7Jly5YsKWWMvX3NIvknJSWRkpLi7jAURVGaFSHEsdr2qW4fRVGUFkglf0VRlBZIJX9FUZQWSNfkL4RoJ4RYKYTYI4TYLYR40LY9UgixVAhx0PbfCD3jUBRFUarTu+VvBv4mpewJDANmCCF6Ao8Dy6WUXYDltseKoiiKi+ia/KWU6VLKrbbfC4C9QFtgEvCp7bBPgav1jENRFEWpzmV9/kKIJGAAsBFoLaVMt+06DbS2c/w0IUSKECIlMzPTVWEqiqK0CC5J/kKIVsC3wENSyvyq+6RWU7pGXWkp5SwpZbKUMjkmxu4cBUVRFKWRdE/+QghftMT/hZTyO9vmM0KINrb9bYAMveNQFEVRKuk92kcAHwF7pZQzq+xaAEy1/T4V+FHPOBRFaaFmjYX3R0Nemrsj8Th6t/wvAKYAFwkhttl+JgAvApcKIQ4Cl9geK4qiOI+UYDVD+jbYt9Dd0XgcXWv7SCnXAaKW3RfreW1FUVo4IWD6WkjfAa17uzsaj9MsCrspiqI0Wpu+7o7AI6nyDoqieKfUlfDOMDi1DVb+Bw4udXdEHkUlf0VRvJNvIER1glaxsHUOnNjk7og8iur2URTFOyUO034A/vIH+Aa4Nx4Po1r+iqJ4P5X4a1DJX1EU7/T9ffDVLZWPf5wBy55xWzieRnX7KIrinVr3gvLCyscGHxBG98XjYVTyVxTFO414oPrjK990TxweSnX7KIrSsphK3B2BR1DJX1EU71OcAy8kwh9fVN/+4wyYPd49MXkY1e2jKIr3kRL6TdbG+VfVcSxEd9X2i9oqz7QMKvkriuJ9gqNgwss1t/e53vWxeCjV7aMoiveRNdaHqmQxacXeWjiV/BVF8T6/vQkvtIPy4pr7VjwHH14MZQWuj8uDqG4fRVG8T3x/GHg7+AXV3NfvFmg3FIx+Lg/Lk6jkryiK9+k4RvuxJ7a79tPCqW4fRVG8j7ms7v2FGZDyMVgtronHA6nkryiK93lvBHw3rfb9x36Dnx+Ck1tdFpKnUd0+iqJ4n8H3QGh87fu7XAb3b4SYbq6LycOo5K8oivcZNr3u/X7BLb7fX3X7KIriXSwmx4Zx5qXBoichO1X/mDyQSv6KoniXtM3wQoK2hm9dpISUj+B0y5zwpbp9FEXxLmEJcOmzEFNPt054O3jsqLbWbwukkr+iKN4lPBEueNCxY1to4gc3dvsIIcYLIfYLIQ4JIR53VxyKoniZomwozXfs2JKz8MUNsHO+vjF5ILckfyGEEXgHuBzoCdwshOjpjlgURfEyS/4B7w537NiAcCgrrH9SmBdyV7fPEOCQlPIwgBBiLjAJ2OOmeBRF8Rb9Jtde2uF8QsCdv+oajqdyV/JvC5yo8jgNGFr1ACHENGAaQGJiousiUxSlees4uuHPkVJr/fsGOD8eD+WxQz2llLOklMlSyuSYmBh3h6MoSnORdVDrynGUlPD+KFj8hH4xeSB3Jf+TQLsqjxNs2xRFURrPXA5vJ8P6dxx/jhDQcxIkjtAvLg/krm6fzUAXIUQHtKQ/GbjFTbEoiuJNrvuo/jH+57vw7/rE4sHckvyllGYhxAPAYsAIzJZS7nZHLIqieBEfv8av01teBPmnILqLc2PyUG6b5CWl/AX4xV3XVxTFCxVlQVEmRHUGoy95JSaMBkErfwdS3VeTtXH/09fpH6cH8NgbvoqiKA22dwG8O0z7AADu+mQzL/yy17HnjnoELntex+A8iyrvoCiK9+h0MVw/G4JjAYhq5UdsiIPDNxszRLQZU8lfURTvEdFe+7F5f0oyAAfOFPDh2sM8f00ffI11dHhkp0L6Nuh9nc6Bup/q9lEUxXuk77Bbn3/7iVxW7c8kPbe07udv+QS+n96weQLNlJBSujuGeiUnJ8uUlBR3h6EoiqebNQaCouC2b9l4OJunf9zNG5P706NNKAWlJkICfOt+fv4pkFatLLQXEEJskVIm29unun0URfEeE14Dof3q62MgMSqIsEAt4Z9L/B+uPYy/j4Epw5NqPr+udX+9jOr2URTFeyQMgraDABiYGMEHtycTH15Zs99qlWw4nMPGIznU2utxahss/Ju2HKQXUy1/RVG8Q3kxHP8d2vSH4Gi7hxgMgrdvGYCf0YAQwv55co/Djnkw+B6vXuRdtfwVRfEOOanw+XVwVJukdfenKTz89bYahwX4GjEYBLnF5dz1yWYOnDlvsfeu4+HvqV6d+EG1/BVF8RaRHeHOxRCllWfomxBGoK+x1sMLy8wcyCjgaFYRXVuHVO7w8dM7Uo+gRvsoitJilZkt+PvY+YA4vRN+eRSufANiurk8Lmepa7SP6vZRFMU7nN4FB5eClLXfzD3PucS/5kAmd36ymVKTRdsRFAXlBVCcrVe0bqeSv6Io3mHrpzD/LhCCLcfO0veZxWw6kuPQU3NLTJzJL6WwzKxtCI3XCry1994a/6rPX1EU7zDqEeh/KwDhQb5cPaAtbcIcq+tzVb94JvSOw8doqPjWIIQAqxWkBYz1TA5rhlTLX1EU7xDSGuL7A9A5NoRnJ/WmXWSQw0/3MRowW6w88d1O/rf6sDbbd2Z32D5Xp4DdSyV/RVG8w65vIX07ACaLtVGnMAhBcbmF4nIzhLSBbpdDRJITg/QcqttHUZTmT0r4YQYMvgva9GPaZykUlVuYd+/wBp3GYBC8cVN/DAZtApj1ije0360WbY2AkDg9oncL1fJXFMU7zNgII/4MwIQ+bbi6f9tGneZc4j+WXcQV/13HzrQ8WP4svH+htlKYl1Atf0VRmj8hqtXxvyG5XZNPGehrJCkqiOM5xfTpdzMEhNVaNkIX+ekQ2ka306uWv6Iozd/Zo1ot/qJszBZr5ZDNJogNDeC92wZxRd82WqmHUQ9rO3KOaNfS045v4M1+2mQznajkryhK83diM/z0IBRnse90Ab3/uZglu0875dSlJgtZhWWVGzbNgqX/1KcL6NzktM4Xw9B7IaKD869ho8o7KEodpJTkFJUT1crf3aEodbGYoPAMtGrN6UILP2w7yRV92jRoqKc9UkqGv7CCUV2ieeWGftpGqwVyDkN0F9u1zWB0Qg/6+ne1onSTv9C6sZxAlXdohorLzWQUlDJ/SxpvLT/o7nBarL/P38E17/7u7jCU+hh9tdW3jL7EhQUwfXSnJid+0CZ6PXxpV64dWGVlL4OxMvFvnQOzx0GxYzOJ62Tw0T5ETCVNP5cjl9PrxEKIV4QQ+4QQO4QQ3wshwqvse0IIcUgIsV8IMU6vGJqz//thN9e88zu/Hcpixb4Mh2uVKM5zNKuIHWm5XNG3DVarev092p4FsHM+AFmFZdo4fSe5cXA7hneKsr8zMFwb/ukfYn9/XaSELZ9C6krt8ZB74IZPwa/pH1qO0HO0z1LgCSmlWQjxEvAE8JgQoicwGegFxAPLhBBdpZQWHWNpdu4a2YHBSRFcPygBH6P6guYOR7KLOJpdzFX94iuG/ykeavOHYC6FPtfzj+93cTCjgOV/G+O00x/JKqK43Eyv+LDqO3pcCd0nat00ZYWQuQ8S7Pay1GQph/XvaLOSO411WlePo1zS5y+EuAa4Xkp5qxDiCQAp5Qu2fYuBZ6SU62t7fkvq888uLFP9yx7EapUUlJpJyy2u+YeveA6LCcoKICiSNQcyySsxcWU/563He8nM1bQJC2DOXUNrP2jhI7DtC3hwB7SKqf24Q8uhw4VaV1XBGQiOAYM+DTxP6PO/E/jV9ntb4ESVfWm2bS1eamYhY15ZxbyUE9W2/3f5QR6bv8NNUbVsBoPgsW93MO2zLe4ORamL0ReCIgG4sGuMUxM/wIvX9uHfV/eu+6CxT8J1H9ad+E9uhc+vhZSPtcchrXVL/PVpUrePEGIZYG++81NSyh9txzwFmIEvGnjuacA0gMTExKaE2Wy0DQ/kukEJjOpSfSJJmdlKsUn1irnazCX7iQsL5N7RHSkzW5FS1r7uq+I+plJYNxO6TcDUui/HsotIiAgioI5VvBoqOSmy/oOCIqH7FdrvxzfCzm9g/AvaB1Nxjra/7UC48TPoernTYmusJiV/KeUlde0XQtwBTAQulpX9SyeBqtPvEmzbzj/3LGAWaN0+TYnT05ktVixSEuBr5JmretXY/8i45ruSUHO24UgOnWNbccvQltH4aLaKMmH1yxDWjmM+nblk5hreuKk/Vw9wbofC6gOZlJRbGN/bgfo+x3+H1BVaV9SeH2DZMzD9NwhvBz0nOTWuxtKtz18IMR6YCYyWUmZW2d4L+BIYgnbDdznQpa4bvt7e5//vn/ew5fhZvrpnWJ2tFdXydJ+96fmczitlbPdYd4ei2GMxg7SSVy5YdSCD5KRI2oYHOvUSUz7aSG6xiZ/+PNKxJ5QVgn8rbfbxhvfg4qfBL9ipMdWnrj5/PUf7vA34A0ttCWuDlHK6lHK3EGIesAetO2hGSx/pM6h9BD5GQ52J/8nvd3I8u5jP767jhpOim7dXHOKP42f57fGL1AewJ7JNsgrzgUmNLOhWn5ev70tkcAMWd/dvpf03Igkuf0mXmJpCt+Qvpexcx77ngef1unZzYbZY8TEauLxPGy7vU3cBpx5xIUQ15I2nNMn61Gy+2HiMpyf2JDY0gL+P60aQn1Elfk90ZA0cWQuj/saRPAtWKekU08rpl2kT5txvEvX55LcjnCko49Fx3XR536kB5G5yJr+Uy15fw8r9GQ4dP2V4En+7TPX9u0pOUTk7T+bhb/s2lhQdTGyoY0sCKi6Wthl+ewOMvry57ABTZ2/S7VLzNp/gnZWHdDt/VYezitibnq9bg8OrSzpbLBY2fPQwIW170PeK6e4Op4a4sACH1xgFrc8/t9hEhPoGoLsr+rbRqjlWsWT3aXKKypk8RN0A9iij/gYXPAQGI/eO7sR1VYuwOdmmozkczy5mxthaOzac5tlJvXWdWe7Vyd9oNBJ1eh1F1mL3BHD0N8g+CGf2aDP4ul1eUaahdWgAX94zrEGnu+ezFDILy/lxxgV6RKvU46cd6exLz1fJ3xMZtG9oPdqE6nqZF67tg68LZtxbrBKjQeg6s9yrkz9A1yc3YPDxdc/Ft38J+xZCaFutBki3y5n921H2pufzn2v64OfTsDfR9YMSKCxr0ffGXeaez1K4sGsMU4ZVLhDy3KRehAS46b2k1G7lCxDZEXPvG/gtNZte8aFE6zRL3hWJX0rJpa+v5toBbXngoi66Xcfrk7/bEj/Alf+F0Y9p07d9tZtFRWVmCkvN+DTiE318b/1W9VEqWa2SUpMFy3mLgIcHqe42j7RvISQkk97uSqbO3sSL1/bR9dvZe6tS2XUqj3duGajL+UtNVsZ0jaVzrPNvWlfl9fX8j2UXkfbJnXRJak/sdS87ObIGyjoExdlYE4Y0+utcbnE5p/NL6R6n79dbxb7Z645QWGbmLxfr1yJTGqfUZGH7iVySooNprePN+f+tTmVnWh7/vXmAxxf8c9c4f48Q6Gckq0TQ1uK8qd4O2foZ5KXBmCcoLLfw0FdbebvwYQKMEsP0dY0+7QNf/kFWYRmLHrrQicEqjtp1Ko+8YpO7w1DsCPA1MrRjLaWXnWj66E66ndtqlaRmFtKldSNKRDeQ1yf/2JAAJv1jrusvfOoPyNgLY5/kVG4Je9IL2HvpSwzo1qlJpVsfvKQLnt3WaP6+3nyceSlpzLlrCEF+1f9EXruhnxrr70myU2HtazDiL+w2t+FskYmR59XG0su5eTrOlHLsLDe+v55ZUwZxWS8Hykg0Qcsa52/WbwhYDRNfhzsWAtC1dQgrHhnDgOQLtIUfAIqyG3XawUmRjhWZUhotwNdISIAPgXZmXKvE72EKM7TFUExFzFl/jIe+3uaSy766eD+XzFzt9EWWOse24l9X9WJEZ/0/wLy+zx9g8e7TtPrhDoYmBOIz9XsnRla3vGITi/ec5qLusdVHHyx+CvYugPvWV04Bb4CdaXlkFZUxtpuqM+MOzyzYjcUqea6+Er+KS2Xkl5JRUEbvtvqvu7Bo12m2p+Xy0CVd8PdxcZdyA7ToPn+AVv4+HAoZQu/2MbhkOY5fHwNzGSmdn+DR+TuYd+/w6sm/+0QIigKfxt2UenP5AQ5nFumf/C1mWPo09LwKEhs2J8Gb+RqFqxddUhwQGxrgslnY43vHOVbdswH2pudzIqeYsd1jXTKktEW0/F1u6dNgLsc67gUOZxXRLjLQqa2DI1lFhAT46DaWuUJRNrzSURuuOvZJfa/lIcwWK6NfWcWMsZ1VKefmYOMsyD6EvPwlvklJIzkpgo461PWxR0pJ2tkSpywUD9o3ym9STrDl/y512loEnrCSl0eQplLtBpHeLn0WLn8Rg0HQObZV7Yn/xGb44gYob9gM5A7RwfonfoDgKHg6p8UkfoBik4URnaIcKrthUYu6u1/eCcg6QFZhOY9+u4O1B7NcdumXFu3n0tdXU2Z2zsTLp67owfz7Rjh1EZq6tIhuH4DnF+7h4u1/ZVirTPjLVv0uZCoF3wAsVslrS/YzsW88PeNrGZNvLoGsA9qQ0JiuDbrMrzvTySsx6TOZpTAT1r0OFz1VWX/8XG1yLxca4MsrN/Sr97i7PtmMv6+Bd28d5IKolFpd9hwAkVbJ2kfH0srfdSntij5t6BQTjLM6T3yNBt3LU1TVYlr+XVqHcKjjFORl/8Zp/1r2zLkG5t/J8ZxiPlh7mANnCmo/tsOF8EBKgxM/wE87TvHZ+mNNCLQOqStgy8dw1nb+eVPhy5v0uZaHcbQbdGjHSAarUVcew2gQtIsMcmnRwz4JYdyQ3M4pLfW3Vxzki406/T3XQvX5O9vvb2t1fAbcRqlt3d163xxWC/wxB/pOBl/HbljllZgI8ffRb4ZhwRltcWmAbV9qLf8h9zRpjkJz8PzCPazYl8Gyh0erYZ2eTkr44nrofwsprcaSmlnIDYPauXTWbW5xOX8cz23SCm9SSm79cCPx4YG86sC3zoZo8aN9zpFSUpZ1jICzB6DrZfpcZMQDFb863CJI2ww/PQgGXxhwq0NPCQvUoWbRya3aYtNxfSoTP0D/W5x/LQ/VNyEcH6PBocRvtljJLTG55v6LUpOpGEpywVTCT9tP8d3Wk9w02LU36b/adIKXFu1j05MXN3qkkRCCL+8ZRrnZWv/BTtSiWv6XzlzNMz4fc0HBEnjsCPg4+Y826xCEJ4KPH0//uIsRnaIdHw52YjMkJDeoZf3h2sPkl5p5+NKGdxvVICXMHqf9Md2/vqJEbgVzOaRtgiQH1y/1BlLCiuegy2V2h7qOf2MNiZFBzLrdbsNKcSGTxUp2YTlxDVgfwxnS80o4lVtKv4SwRs/21XNtbjXax2bykETKBt8P964Bo5P7BqWEz66CH+6j1GRhzYFMUjMLHX9+u8Fa4i/Kcngm8r7TBew5ld/IgM8jBEz+Em74pGbiB9g0Cz65AnKOOOd6HkhKWb31VXJW68Y7tc3u8feO7shNg9u5JjilTr5Gg8sTP2hLO55bg7sxSsotXPjKSn7cdtLJkdWvRbX8dWW1woFFEBgB7YfbNsmG9T8WZsC7w2Dw3Q4Nr3Rai+HMHojtUfe3joLTkL4DOo4BH+8sbXwqt4SRL61g5o39uXqAbRFwUynsnAfRXdVEN0+zfxFseBeu+5C3N+UxvFM0g9pHuDyMI1lFLNp1mnsv7Njg+w1n8kt57uc9TBnWXpeidKrlX0VucTmFhzfCmlede2KDAbpPqEj82qYGJuZWsTDsPuh1jUOHOyXxZ6fCrDGwbmbdx4XEafdJvDTxg9Z6fGBs5+rD7Yy+sPxZ2P6V3eeknS3mYF0juhT9WM1gKqHQ6strSw+w+WiOW8LYduIsLy/e17Bv+jatQwN4+5aBLqlGer4W1fI/nVfKsBeW823fFAYdfhce3gtBThqut+8X7UZpeDue+n4ncaEB/NkFNd+fWbCbwjJz40cJSAmbP4SeV0OrmLqPLcnVRv50uQyi9V/D1O12fKMtw9n/Vghrp33An2fMKyvpHBvCh1NVv787lZutWKySQD/X19kpLjdTUm4hqoE3/ktNFgrLzLoOGFAtf5vWof7844oehIycBo8ddV7iLy+Cb6bCpvcBKCg1U1TehFl/VgssekIrVVuPkAAfQgIaOWirrFDr6hlyT/2JH8BSDkuegkPLGnc9D5dXYsJcdfWuExth708Q0d5u4gd4/po+PDq+m4siVGrj52NwS+IHCPLzaXDiB63g5JDnl7HvtJPu2zVQi2r560ZKyDqojdEPd9JQs/l3Qqs4GP8f55zvfNvnat0ZdyyEyA6OPy/vJIS11ScmN5s+ZwuHswpZ8tfRlRul1D4g178DRZlwyTNui085z/f3QWA4v3f+G5uO5nDfmE5uq7C562QeH607wrMNWOf5WHYRP+9I577RnXSbm+DWlr8Q4m9CCCmEiLY9FkKIt4QQh4QQO4QQ+iyEWYtSk4VtJ3IpP7gC5t4KFiesyiSENkvXWYkf4NoPGpT4GzxGOKY7dBitdWc0hJcmfoBrB7Zl2oXnrdJ07r5Kdipk7KsxO1xKydqDmWw64p7+5hYtIBT8Q9h89CzvrUrFt5ZvZ65QUGpmzYFMjmY5XqerfVQwM8Z2dttSkLq2/IUQ7YAPge7AIClllhBiAvBnYAIwFHhTSjm0rvM4s+X/68507vtiKysn5NFh+0y4bT5EJDX+hFLC6peg6ziIH8AzC3aTdrbEeX3AWQdh1Yva6J8o+8vH3TsnhVKTlU/vHOJYvE25USylth5BUCRc+Ejjz+Pp8tNh8ZMw4s/QdqA2mquW5HLRa6voEBXMR3cMdnGQyjkmi9UlZZBrY7UV+XM0kW87kYuUkv7twnWdSe7OGb6vA48CP1bZNgn4TGqfOhuEEOFCiDZSynSdYwFgaMco/nfbQCI7RsGFTpi5WnAa1s7U6vPHD6BteCC+Rif+Yx5dC+nbtSGkALnHITCyWpG1kV1iMDna8l/0hHau0Y827kNACCg4pY208CKlJgtZhWW0CQvEaBBQlAGntmqzSKEy8VstNeZBzJoyiDZhgS6OWKnKnYkfGj6y781lBziUWciav4/VKSIHSCl1+UFL8m/afj8KRNt+/xkYWeW45UCynedPA1KAlMTEROnRyoqkLC3Q7/xWa+Xvc66V8r/J1bc5ymKW8ttpUv76hPPi8RKbjmTL9o/9LFfuO1P7QSmfSPlqNynLi10XmGJfzhEp3xoo5YGl8l8Ldsslu0+7OyK59ViOvPK/a+WRzMJ6j80tLpc703J1jwlIkbXk6Ca1/IUQywB79QueAp4EGl1AR0o5C5gFWrdPY89jz4mcYg6cKeBin53wyyNw1xJtjH1j+WmLOZSbrbZVnpz8Na7q+UY/DoWntW1SwpJ/QJ/rKY/tR3ZRWd0tUIMRrn1fa706Ix5b+Wpv0D4yiBeu7VP3EoBRnaDLpdooKd/K11lKyWfrj9EmLED3RbcVGymhdW/M/mH8sjOd8CBfLu3Zuv7n6SgiyA9fo4G8kvrvI4YF+hLmguUm69Kk5C+lvMTediFEH6ADsN2WCBOArUKIIcBJoOpdxgTbNpf5YuNxPlp3mN33t8UvpjuU5jcu+ZtKtVE5w2dA0gX8b3UqczYcY+2jY/VbkKFdlX7l3OPwx+cQ041bfizBV1j56u4hNSdiWUyw5P+0/uuwtvbLNzTU2tdg0wfw0C4wNv/6gLGhAdxcdW2ExU+Bb5C2psE5SSPt1jYSQjBnwzH6tg1Tyd9VIjvAjZ/iA2x40vFS3HpKig7m2/tG1Hvca0v2MzAxokmVQJ1Bl79aKeVOoOL/TAhxFK1rJ0sIsQB4QAgxF+2Gb550UX//ObcOTeSaAW3xiW0Ft8xt/InyTkDmvop+4T4JYVw3MMFlK/EQ0R4e3gMGH6b5nyU+bSG8cRfcuaj68M0zu2HrZ9BuCIRd65xrxw+EfjdrC9IYQ5xzTjc6lFFAaIBvZWXG4hzwL7d/8Nlj2oznKoUBv71vhD6VVhWHeFL5bbPFihBCu3d0nlKThe+2nkQI4fbk75Jx/uclfwG8DYwHioE/SSnrHMqj+zj/skKtldfYoWJNHUHjLMc3wh+fwZX/1f5fUldocwVa99TqBjWla8vLXfX2OsICfZlzV50Dz+DYevh4PNw8F7pd7prglJpWvQS7v2f1JQtYuDOdf0zsSaiD4+v1tP1ELlM+2sis25MZVkvJBqtVUma2umRSmttn+Eopk6SUWbbfpZRyhpSyk5SyT32JXy9rDmSybM8ZSF0JLyXBqT8afzIhKCozk1fshDkDTXA8uA/bBz6vJX4ptZE9v9iGY+qR+KXU1gBwsAqpJ3tyQg/+fJED5TgSkuGyf0Ob6uU0rFbJP3/cxbzNJ3SKUKkmsiO0H86pvFJWH8gkwE2Tu87XMSaY8b3j6vwWaDAIt81GrhaHuwNwl/+tTuWtFQchri8Mmw5BDawGePYYvNkfDq8GYNGu0/R7dkmjijs5y8PztvH0gt3aAyHgT7/CxDf0u+CRNfDBWK8o9zCsYxRDOtjKfaSuhI/G2S9fbfTV7p2ExlfbbDAItqflcTS7yAXRKvS9ASa+zs1DEtn45CX4+XhGKgsJ8OXl6/vZXYs3Pa+ES2euZuPhbDdEVlPzv1PXSK/c0I+IIF/w89Facg1lKoaYbhVJoF+7cB4d342kqGAnR+q4p67oUb3OT1Ck8+oX2dN+BEx6V/tvM5ZdWMbhrCJ6x4fZWmRSuykeUMtoDKtF++ALjtaK+dl8f/8Ij+p7VtznTH4prfx9CK6yoPzZIhORwX6NXvHL2VRtH9C6LzL2Qng78G/+Ny+Vhvlx20kenLuNpX+9kC6tHfj3N5fDq52hx1Uw6W39A1Rqer0P9L2Rh7OuZHCHyOojtdxs3+l8xr+xljduqrIuhJu4vc/fE5WZLcxak8rvqVlav/V7w+HgEseebDFrlTxtKuoFuXgNTnvWHcxi6Z4zrrugxQw758Ox3113TScb2TmaT/40mMSoIMee4OMHt/8IE16pttlksTLlo418sOawDlEqFaSEnlch4/pw4mwxOUW1jMpyk66xIfzjih4MTKzsSi4sM1NqauL8Gidrscnfz2jgvysOsfpAJsT3h6vehqRRjj35+HrtJvGx9YBWp+Pqd37jt0NZusXrqPdWH+LN5Qdcd0Fh0CaabZ3jums6WVQrf8Z0i62sCDlrTP3ltOMHVJvoBVqJgdBAXwI84GaeVxMCxj2P6HU130wfwYyxnrW2hMEguHtUx2qNic/WH2Xwv5eRW+w5H1Qtts9fCMHvj19UWX514BTHnxwSB0OmQVxvAHrEhfLurQMZ6IYl5M730nV9iQrWb3GIGgwGbV5BmOd87W6oVfsziA8PpGvrEK2AW2wvCHXg6/q2r6A4S7sBbPPOLS4tUtsyWa3aB4AH318xWaykHD1L+6gg4sMDGdYxCotFEh7kOSvhtdiWP1C97nZ5MexZAGeP1v/E6C4w7vmK+wNhQb5M6NPGIyb5JEQEuX4YWURS4+dIeIC/zdvO7HW2kT0GA1z9DvSbXP8TD6/S3jN2yjybLO7vAvRaB36F59uwacMapny0kYz8UndHVENOUTk3f7CBBdtPATAwMcIlK/s1RPP9i3WCY9lFPPHdTm14Zlk+zJsCu3+o+0kludXqukspWbD9FKfzPOcN+OXG43y+4ZhrL5ryMSz8m2uv6SRf3zuM+8bYymU3ZADElW9odaGqtEDLzBZGvbySd1emOjdIpVJ4Igy+i0K/GPJLTNVG1HiK1qEBfH7XUG4dmsiWY2dJO+t4nX9XadHJ32KVLNxxiuPZxVpXzj0rtTo9ddn3M7w7FDL2AJB2toS/fPUHy/a68CZrPZbtPcPi3adde9G8NMjcr90AbmY6x4bQ/twQ3bWvwRt9tBE99fENrNH14O9jZEKfNvSMrznOW3GSuD4w7nkuGtiDHx8Y6ZHJH2Bkl2hCAnx54rsdPDxvu7vDqaFFD/U89//eoLHZBae1SU39bwUhsFglBzMKiG7lr+tCzA1RarK4rr7QOZ5S4qKBDp4pYE96Ppf1jNO6y/b+rHXnXPGqYyfY8Q38/qbWcDC6v9uvRTCVanWVPPz9VlJu4bs/0ggL9CUpKrjuirE6UUM9ayHEeeWXywphzat1D1sMiYMBt1W88YwGQfe4UI9J/IDrEz9U/iE6Y1lMF1qy5wwPzt2G9VwjqMdExxM/aEsJhiZoheCqKCwze9TIDq/yzR3wwUXc/WkKby476O5oaiUEPPfzHvam57sl8denRSd/gGV7zjDlo42YLVatNfHbW3Bkrf2Dsw7B3p+0lofNnA3H2Hr8rIuiddxLi/bx2pL9rr3o/l/hlU5aF1AzcdfIDiz964WVXQcN/fDqOk6rDBtSWUu+uNzMwGeXMvu3o84LVKnU53oYdAdhgb4E+3vusNoAXyOrHhnLI5d1c3codnlmZ5kLlZmt5JeYyCkq16Zd/3WX1pqzZ9d8bb3ev6eCbwDlZiv//nkPd47sUG1ChyfIKijDx5nLSToipht0n9isWv8BvsbKWb3FOfBKZ5j4Ogya2rATleaBbzAYfQjy8+EfE3vQv1240+NV0JI/8NogN8fhgLgwzyjlYE+L7vNvMItJq40f379iU1GZmXKzlYhgzxm/qzhu9rojDGwfoSXqomzY+B50v0KbxOWo4xvh04lwyzzo5MY1WVsCKaHkrLYOtYf3+XsC1effEOZy+P4+bfGT8xl9qyV+gGB/H49O/G75cM89AUXun+1cn1KThecW7mHtgUxtQ3AUXPSPhiV+gDZ9Yei9EFa5QJ3VKtl9Ks8jh/g1a0VZ8HIHDvz8Ohe/toojWaqKamOp5A+8sngf0+ds0R74+GkTvc5PXodXwdqZYCqp2PTh2sPM3+K5/dsPzv2Dv8/f4dqLFmXBm321cf8eLsDXyK5nxjH1giRtQ0lu44aq+gZqlWGjK8sMFJWbuert3/ha1fd3Lh8/GPcCpfFD6RzbinAPmFjZXLX4Pn+A0ABfwoOqvInu/LXmQUfWwJZP4YKHKjb9sjOdhIggrh+UoH+QjdA+Mghfo4s/34OjbXWSLnDtdRup2hjxnx7U5m88sLnhJzpXGdbHH6I6ERLgywe3D6JXvOeN8mjWAsJg+P30Bd5vBn3+nkz1+dfl/LHrZQU1Sj6XmS2VBcGUZmXtwUz2nMrn7lEdtfVW9/0CJTnaUN6GMpXAyx21shATX3d+sIqmJFdbTyEoUvX5O0D1+Tuo4oNQSvj0Slj8ZPUD7NT69/TEX1Bq4oIXVzAvRet+yCs2cfOsDdoSlkBucTmPzd9BytGciv3vrjrEwTMFFecwN6ZOzdHfYN/Cpv8P6GjV/kzeW51audB29wmNS/ygdf3cNAfGPFGxqdRk4aftpzhQ5bVUmijlI3ilI1NmreaJ73a6O5pmTSV/tKQ/6e11PPfzXm2DENoarVG2ei+bPoAfZ2gtDpvZ647w3M973BBtww3rGEWcbfUgs9WK2WqtmNRUWGZm1YEM0m21iTILS3l50X72ntYS1sr9GUx4ay3peSX2T16bta/Ciued9z+hg/+b2JPfH79Ie2Aq0W5UW5tQkK3zJdXWSrZYJX/9ehu/7ExvYqRKhU4Xw4RXGdAxnl6qhEaTqD5/tJm+wztF0ymmyhKMVZd2LM6GvJPa0n42J3NLOOzG9XodFRLgy2s3Vi42HtXKn2+mVy67mBARxMYnL6l43CmmFfueG1/RGg4N8CG6lT9Bfg18q0x8Q+v/93AV/19pm7Vve1N+aNpwzQOLtZveA24l2N+HBQ+MpFucWh3OaeL7Q3x/HnZ3HF5A9fnXxWrR+hiDo1x/bQ8ipUTY6hidyi2hXaSDK155sHKzled+3sPVA+IZ1D4S8k9pibvnpKate/z1FMg+BPf9rvqk9ZCdijUgAhEUodZLdoDq83eQyWKlzFxlqbWPLoUf7nNfQB7i3B/Zi7/u5aq315FVWObYE1NXanVYrJ61fB1ARkEpP247ybFs2zj80HhI/lPTF7y/Yibcu6Yi8UspeWflIb7ceLyJESsAfH4tmfMepPc/F7MzLc/d0TRrKvnbHMsuotc/F1fvnx0yDU6mwBc3VDv2q03HuXnWBkrKPS+p6enWoe3580VdHC9iV5wN6du1VrWHSYgIYscz47i6v23FrjN7oDCj6SduFVOtuqcQgnUHs0g5llPHkxSHjfsPpf2ncuPgdrQJ99zSCc2Brn3+Qog/AzMAC7BQSvmobfsTwF227X+RUi7WMw5HtA0P5E8jkugcU6V/tt9kbSp5UWa1Y30MggBfg+tXzHKzpOhg7hzZAYAjWUWczitleKc6usR6XQO9r/Po7g/DuZE+86ZAbE9txE5T7VuoTXK7ZR4YDHx65xD8fFQ7yym6X0F74J8NnISt1KRb8hdCjAUmAf2klGVCiFjb9p7AZKAXEA8sE0J0lVK6tRntYzTwxIQeNXcMq9ntc0NyO25Iblfz2Bbk2Z92s/90ASv/Pqb24a7nbpCby2HnN9D/Fo/5IPhi4zHSzpbw2Pju2oYrZtZYkL3RTCXa2r5FGRASV5H4z907URqprBByDlMa1gH/wFbqtWwiPVv+9wEvSinLAKSU575TTwLm2rYfEUIcAoYA63WMxSFSStLOltA2PLCyRWjnGPWmg5k39udMQalj8xx2fA0LHtCGziYO0z84Bxw4XcDe9Crj7zuOdt7Je19XUXnynP+tTmXpnjN8e9+IWp6k1OvUH/DpRF6KeIHDIcl8eucQd0fUrOn5XbQrMEoIsVEIsVoIMdi2vS1QteBJmm1bNUKIaUKIFCFESmZm5vm7dfHd1pOMenklR7JrLxa1ZM8Zhv1nebMY5qmniGA/usdp46znbDjGNyl11LAZcBv86VePSfwA/5rUm3nTh2sP8tLg+AYwO3gjuz4VC9uYK+YNRLfyp0N0MKWmlnWfyKlie8CNc+ibPJKrB8S7O5pmr0ktfyHEMiDOzq6nbOeOBIYBg4F5QoiOjp5bSjkLmAXaUM+mxOmoYZ2ieP6a3kQE1V6lMyrYj2EdI4kPd1IXQTNntUqW7D5NgK+R6wcl2P9WJAS0t7V4M/YCAmK7uzTOOu1ZAIuf0NZp8HHSimxpW+CL6+HmuZA4lOsHJXhsDahmIzgael7FNe6Ow0s0KflLKS+pbZ8Q4j7gO6lNJNgkhLAC0cBJoGqHeYJtm9u1DQ/k1qHt6zwmOSmS5KQmDgf0IgaD4IPbtWHEQgjKzdbab25aLdo4+KBIuHOx2/r/T+WW8H8/7GLGRZ21RXh6XwfRXSHIifM5YrpqM379qs+JyMgv1RYNUhou8wCm8hKKI3sSpqp5Npme3T4/AGMBhBBdAT8gC1gATBZC+AshOgBdgE06xtEgZ4vK2XYi1+4+i1VSUNp8VqlylQBfIwG+RkpNFm6fvZHXlx6wf6DBCNd9CNfPduuN39xiEydzS7BabV8oQ1pDl0ucG5N/CFz3AcT1qdi0cEc6Q/6zvFrdJKUB1ryM5atb6fevJfx2yPPXi/B0eib/2UBHIcQuYC4wVWp2A/OAPcAiYIa7R/pU9drS/dz24cbKxFDF3vR8+v5rCSv2nXFDZJ7P12igQ3QwHaKDaz8ovj+EJWjF8w6v0v7rYj3jQ1n00IWV3+B2/wDZqfpcrDAD8rW5I4M7RPDIZV0JC1Kt1kYZ9Qj5497gyQnd6dK6lbujafZUeYfz7DudT05hOUM7RlVWe7RJO1vM/C1p3Dwkkdbqq3u9UjMLSYoKrvE6ArD3J/j6Nq1PvNvlrg/uHFMJPB8HY5+C0Y8699zlxVqZ50FT4fKXnHtuRXFAXeUdVPJXdJGeV8KlM9dw+/D2PDrezs1dqxV2zoM+N4LBtROg/vXTbgD+eWUv7T5E9iGtmyZUhxEk276CxKEQqY11sFglm47k0D4qyPMHDZzYrN27CPCQBWkOLiMzsD3+0UmEBqhvT45QtX0aaNuJ3Ir69lXtP13QuNr2LVCbsEAeHd+N24cn2T/AYNBmUBsMUJwDOUdcFpuUVXqbDEaI6aZP4gfof3NF4gfILizjlg838E2K5y7/CWj/JnOuhiX/5+5INKYS+OI6Vn3zNlM+3OjuaLyCavnbcdXb6wj28+GraZXj0rMLyxj072U8cXl37h3dyWWxeAMpJYeziugUY6efVkr46DIoL4Lp61z+LYDjGyD/JPS6Vr+b0Jn7YduXcPE/wWBg3cEsBiSGV19C0hMdWALCAIseg+s+0u7XuIvFDOnbWJ9hpMC/DZf1sjfCXDlfXS1/D3/3uceL1/YlIrj618oAXyNvTu5P34Rw9wTVjL27KpW3Vxxi0UOjaB913s1gIeDSZ7Uk4+rED7D1Mzi0XBvuqZf0HbDxfe2bTmwPRnbx0HUOpIQV/4aOY6DDKOh6mXazOrITSDd/4zX6QEIyw9VUCadRLX9Fdxn5pSzYfoo7L+hQa9mMClmHILqzbrGkHM3h6R9389qN/ejRJlTrTig4DZEddLsmFpO2/nOVctHzNp/A39fApP41Jre7T1kBfHCxlvSrLmbkCXKOUJ6+myOhg2kfF02Ab8sqqthYqs+/gUpNFuZvSWPXycp64b8fymr4UoYKALGhAdw9qiMGg6CozEytDY5jv8M7Q2DnfN1iEULQOtSf0HOThHwD9U38oJV4Ppf4beUe5m9JY8E2Dyl1XV6k3fj2D4G7lsClz9U8xlTi3nUZDi7F75tbufWd5aw9qMb4O4NK/nYYhODJ73by8w5tfLbFKrnnsxTeXanTWPAWIj2vhMteX8MXtS1skjBEWwC96zjdYhjUPoKP/zSEtuGBUJoPa1+DrIO6Xa+ClNrQ1l8eAeCD25P5cKrdBplrlRfDx5fDkn9ojwPDa977OLIWXmwPJ7e4PLwKfW8g/7YlPHPzhfRr5yGjj5o51edvh5+PgWUPj6ZthDYUTwBzpw0nyF991WyK1iEBjOoSTe+2tfzxGn1g9N+13y1mKMmptiC60+WkwvJnIaYHRHfR7zqgJdSozhCofQPwmIlefkFaGYp2dRTdi+0JQ+6piN0tAiMI7TyUie6LwOuoPn/Fbeosj/3NHZCxD+5d7bxia8DkWevpHhfKM1f10jaUFYDBF3xdP2nvq03HWb73DB9OHVz/wc52cotWyygiyfXXbowDi0krEpS2HU7n2JD6j1cA1effKCdyinl18X5O55WydM8ZNh7OdndIXuWDNYeZ8eXW2vv/B06F4TOcmvgB+iWE0zm2ypBT/xDXJn4p4ehvYDFjtkrKLZLicrPrrg9a6eqvp8DCRxx/jpRa95izyl431IrnyF7yKjO++MM91/dCqtunFmeLy3lvdSrJSRG8vGgfiZFBDO3oxKqPLZwQ2s3XMrPV/siNTmN1uW611dq2fALCCAOn6HItuw6vhDnXwI2fMWXYJKYMq7uKrC58/OHGORDegNXoDi6FL2+AqT9rw0Bd7dZvCUrP4VkfHbsBWxjV7VMLi1VismiJKb/URF6xiXaRQfU/UXHIufddvauibfkUDiyCm79yyjWrXe/TK8HoB7d92+RzO8xq0Za07DmpYtnIknKL/utBSwlrXoWQuMZ92JXmw65vofsV+t6HUZxKdfs0gtEgKlqkoQG+KvE7mRACIQQZBaU8OPcPcorK7R9oKQdzqTYqpYm+23qSAc8u4WSubcju1J9g8pdNPm+DGIzaZC9b4l++9wz9n11Cqt4rw1ktcPx3OLGhcc8PCIXkP7kn8ZfmYVo/i/UpKWQVuqnbyQup5F+H1Qcy6fvMYuZsOObuULxWRn4Zq/ZnVptTUc2Qe2DK9zUWRWmMxKggxvduQ0yrKvcRnHxPwWF7f4K1r9ErPozJg9vhZ9TpT9FUoo3jN/poH3RXvd34c5UVwL5ftPO5Us5hfBf/nY+/+5n1qerem7Oobp86fL7hGP/4YRetQ/3Z+GSti5YpTVRYZqZVfXVuTKXOvTF7ehds+RgueKhhfd/O8svf4eg6uHetlpj1YLXAZ5PAL1grnd3U2kWHlsPn12rdZJ1d+PdgtVCSe4Z9OVYS42KIauWmD+xmSJV0bqRzr01BmVmVkHWBlfsyCPA1MrzTeTfW9y2Eb++G+35v0mzcUpOl8uby3p/hh/th+lqIcMNN17JC8A2qqGeUmllISIAPsSFOHnm07Sst6feb3PRzlRfDqa2QMNh935iUBlF9/o10rl9aJX79mS1W/vPLXt5bbWcWdeve0P8WrfhbEwx7YTnPL9yjPegxER4/BuGJTTpno/m30hK/1UJGXiEXv7aarzedcN75bWUk6H+zcxI/aF1vSSNdn/iPrOXE4rdYeyDDtdf1cir5Kx7Bx2jg4z8NZtaUQTV3RrSHK15rUgvdYpVMu7AjI7vEVG7Uxps2+pxNlpcGb/Un9sgC3rp5ADcOdlL3U2Em/O8CSF3hnPNVlZcGv70JpbXco9HDnh+I2PgK//fjbtddswVQyV/xGAkRQQT4Gik3W1m+1846ydmp2iIjjWA0CO4f05nRXW3J/6cHYeucJkTrBKFtIWkUhLblqn7xzlsa1FQEfq0gWIeROWePwtKn4eRW55+7Npe/TMn0jbw/xQNqIXkRlfwVj/PB2sPc/VkKhzIKKjeePQb/HQjb5zbqnAWlJkrKbVUppYQzu7VFXNxJCLj6Xeg4GoAV+86wbI+dD72GikjSqnPG9W76uc6XMAT+tl+3SXh2GYzExMbTLU6VdXAmNcNX8Th3jexAr/jQ6jVcItrDpHe1hUYa4ePfjvL6sgPsfXa8dtP37mXOCdYZygrh2O+8vSIYH4OBS3q2btx5Tm6BHfPgkn/pV7LCx0+bKOZC5jUzWV0QT6ehV5IUHVz/ExSHqJa/4nECfI2M6aZ1WRzLLqLcbLt5OeBWCGvc4icju0Tz1IQenrkIyLrX4aubePvKOD6/e2jjz3NkrTYO36zzuhNn9sCPD0CRC8bcS4lh7SscXP8zm+2sq600nhrqqXisjPxSLnptNbcPb8+j47tr3TUHl2qjTWxdJY2y4xvY/Z22Lq0TJo81WX465J3QhlA29QZ0ab42G1dPJzbDF9fBLfMgsY5S0E5itVg4fTafVq1aqZF3DeSWoZ5CiP5CiA1CiG1CiBQhxBDbdiGEeEsIcUgIsUMIMVCvGJTmLTY0gEfHd2PKcNsoHyG0RUfWN3yWampmIaUmW5+/qRgKMypKLLhdaBtoNwSEYOGOdB75ZnvDnr/3Z8g8oP2ud+IHaDsIHj3iksQPYDAaiY+OUInfyfTs9nkZ+JeUsj/wtO0xwOVAF9vPNOA9HWNQmrnbhyfRJiwQKSVfbz5O2Q2fw02fN+gchWVmLn5tNbN/O6JtGDQV7lnu3mGe57OYYPmzBO+bz55T+RSWOVjm2VwOi57QRuC4isGg1ShyhYy9pH3zGD+uVd/8nU3P5C+Bc82QMODcgqWTgM+kZgMQLoRoo2McihfYeTKPx7/bybzDfg2eZGQUgjdu6s8lPRp5I9UVDD5weBWjQ9P55cFR9Ze7OMfHTxvZc+Wb+sZ3vuMbYfZ4yNd5HeLM/cTt+ZDP1+zV9zotkJ6jfR4CFgshXkX7kBlh294WqDqVMc22Lb3qk4UQ09C+GZCY6KZZmIrH6JsQzvzpwxmYGKGNaDm9Ay77t0PPDfQzcvUA241icxl8cBGMehh6X6djxA0kBPxpEcLHDwCrVWIw1PPN5PhGrbso1A1tJ99AreJqUSaExut3nV5XI7pfyXvFJv2u0UI1qeUvhFgmhNhl52cScB/wVyllO+CvwEcNObeUcpaUMllKmRwTE1P/ExSvN6h9JEIIitN2cGLLInIKHCvzfCKnmCNZtkqUZQUQ1k6bBOVpbIl/454jDPnPco5m1VE98/BqmH2ZtjaAO7TpC/esgDb9dL+U0Wgk2tk1j5SmJX8p5SVSyt52fn4EpgLf2Q79Bhhi+/0kUHUee4Jtm6I4ZHuXP3Ol6T8cyXGstvs7Kw9xw/9+1x4ER8Mtc6HrOB0jbIJ9Cxny7TCubFuAyWKt/bikkVpXT8+rXRaaXTqPFrRu/IDVHz/N9hO5ul6nJdKzz/8UcG483kXAQdvvC4DbbaN+hgF5Usp0eydQFHuGd2nNuscuYlBiOEDlPIBa3HFBEi9f39cFkTlBwmDEwCn889rBdGltZ0arxaQN5zQYYdAdFd8W3OLgMnils1byQSflqWswHlnOjtrWe1AaTc8+/3uAN4UQPkAptv574BdgAnAIKAb+pGMMipdqlfoLLHmKlWO+559LTvDF3UNrXW2te1wo3eNsYw9+fQwy98PtP7gu2IZoFQtXvApATlE5BgHhQVUS/KoXtOUUp62CwAj3xHhORHutrr/VotslAm6ZwzCLlWSr589Ham50S/5SynVAjRKNUptVNkOv6yotRGhbaNOP+IBSusS2IiLYfgu4zGxh4+Ec+rQN046J7KSNrPFwucd38+D7Cxl+6bXcP6Zz5Y4u47TS1u5O/ADRXeDa9/U7f3kx+AXhYzTg44ETs5s7NcNX8Romi5XjOcV0iqm8mXvwTAGXvr6GNyf3Z1L/xpWGcIuPryA36ySZU9bQJS5U61t38byEqiOOCkpNlJmtRAX7Ic6PozhH+zByZnxnj8H/RrFj8PMstQ7hr5d0rX/0k1KDWsxF8V7FOdo6tcCLv+7j6nd+I7Og8kZwQkQQX08bxgWdo7UE2gwaOwBMnEn4/cu0xG+1wleTYYPr5kP+fiiL4S8u51i2NuJowfZTJP97GafzSwFt1bXH5u+gZNu38HIHso/t4kROMVZndc8YjNBjIuuKEvlq03GV+HWgkr/SfJ3eBa92gf2/AHDnyA48OaEHMSGVk8AC/YwM7RhFdCt/SN8GL7WHI2vcFHADxHSD4Gh2puXxxqKdWAy+YNTv5m5JuYU564+yzTaqJik6mD5twyiz3UwfkhTJP6/sWbHMZFpuCasPZOKbmAwX/YN5O/MY8+oqrLYP17mbjvPQ3D8qlkJduS+DT38/WnG95XvPMGfDsYrHS3af5suNxyse/3rcyLy2T3D/pNFq/WydeH7np6LUJrYHjHoE4rSx5m3DA7l5iDYh8FBGASfOllTMlB2cFKmN7e9zgzbOvzk4ewzDvBlsLbkawxOfgRDMSzlBcZmZOy5o/FrG55gtVrIKy4kLC0AIeHnRfm4b3p7+7cKJDw/kw6mDK47t0jqk2uijKcPaM2WYrebShX/notMFtE0swMeotSdziss5mVtS0UW0aNdpVh/IZOqIJAAW7khn87GcinP8uP0U+9LzuWVoIuz+njWbfPijIIwbB7fDqFr9ulB9/opXuvvTFPacyiMhMohSk4UFD4x0d0gNV3IW/jeK8ivfxq/zGADu+3wLOUXlfH3vcABmrUklMTKI8b0bPst38qz1WKySb6Zrk+9P55XSOtS/Zp9+fSwmOLVNK/hmsN+ZYLJYsUqJv+3ObbnZisTOY1kOr3XH0nUC5qvertivNE5dff6q5a80b1JC+natS6R1z4rNM2/qR2ZBGcF+PuSX2koDmEr1W+RED4ERcNMc/IKiKza9d9ugiuqkUkrmbj7B0A5RFcn/tSX7Gd01huSkyBqnW7HvDF9uPM77U5IxGgR3jOhQ7R5tXFgjX5ud8+GH6TB9HcT1sXuIr7H6h4KfT22PA+H+9RilxKgSv65U8leaN6sF5lwDnS+G6z6s2Bwa4FtRArgiqX18ubYYTAOrgrpV/IAam84tSCOEYPnDoymxfRjkFpcze90RwgJ9SU6KpNRkYebSA8wY25mwQF+Kyy2cyi0ls6CMuLAAxvd20opcnS+BGz6F8PZNO8+5EU161gpSKqjkrzRvRh+Y/KV2g7Q+/W+BgHDdQ3IlIQRBftqfcXiQH9v+eRlmi9aVu/90AbPXHaFTTDA3DU7kij5tmNhXh8TaKgZ6Xd3086x+SZuAd+0H2r+roiv1CivNX/vhjh035B594/AAvkYD51aq7NcunF3/GoeP7YZpg/vyG6IwAw4tgz43Nj5x+/iDT4BK/C6iXmXFO+z7BfJP1p7gy4tAWsHfTr0cL+ayNYuProUf7tO+gbWtMbHfMSP/6tyYlDqpcf6Kd9i7ADbNqn0S167v4IUEXYuQtWidL4X7foc2Ne9R1KusENLUaD5XU8lf8Q7jX4D7N9ReYqDtQLj4aQhNcG1cLUVAKLTuVetQzzpt/Qw+vBgy1GpdrqS6fRTvUF+hs9a9tB9FP5n7YftcGPN4w5baHHi7Vs00tod+sSk1qJa/4j32/gyfTASLncXPsw5V1ABSdJJ1EH5/CzL3Nex5/q2gz/X6xKTUSiV/xYtI27qyGdU3W63w3ghY+bx7wmopulwKjx1zfGlHcxl8PQVObNY3LsUu1e2jeI8eV2o/55MWuPpdiOpcc5/iPD7+DevuyTkCJ7dCeYF+MSm1Ui1/xfuYy7XW/jlGX61bIb6/20JqMY5v0Frz5cX1HxvbHf7yB3Qcq39cSg0q+SveJS1FW1f2xIbKbWePQeaB5lPLvzkrK9BKZ+edqPu4vDTtA9rHz+WL1CgalfwV7xLTDXpeCQFhlds2vAuzxrgtpBal8yXw0M66y21YLfDZJPjubtfFpdSg+vwV7+IfApPeqb4t+U7oOEa1MF3B0dd4zBMQVLPyqOI6quWveKe8k5B/Svs9pht0u9y98bQk+xfBuyO0LiB7DEbtHkyni1wbl1KNSv6K9ykvgrcGwPp3tIVGDiyBoix3R9VyBIRCSBwUZ9fcl7pCm9Frby6G4lIq+Svexy9YG9qZfKdWy+fLG7SKk4prtB8BU76DiKSa+3Z8A+ted3lISk2qz1/xTudmjJpK4M7FENnJvfG0RBaTNsy2qqvfhcIzqmyzB2hSy18IcYMQYrcQwiqESD5v3xNCiENCiP1CiHFVto+3bTskhHi8KddXlDodWw+HV0HiMG3BEcV1ts+FF9tDcY72WEptGU0htC4hxe2a2u2zC7gWWFN1oxCiJzAZ6AWMB94VQhiFEEbgHeByoCdws+1YRXG+1S/BV5Ph6G/ujqTlie2hFWyzlGuPD6+CN/rA6Z1uDUup1KTkL6XcK6Xcb2fXJGCulLJMSnkEOAQMsf0cklIellKWA3NtxyqK8135BsQPhGXPuDuSlqdNP7j8xcpWfmAEdBgFUV3cG5dSQa+Ot7ZAlSmWpNm2AZw4b/tQeycQQkwDpgEkJibqEKLi9SKS4OavoDTf3ZG0TFJqM33DE7XSGtfPdndEShX1tvyFEMuEELvs/OjaYpdSzpJSJkspk2NiVH+t0kghcRDT1d1RtEybPtC6en57E0py3R2Ncp56W/5Syksacd6TQLsqjxNs26hju6Io3qTTRTDsflj6NCDggr+4OyKlCr26fRYAXwohZgLxQBdgEyCALkKIDmhJfzJwi04xKIriTtGdteU1+99if8y/4lZNSv5CiGuA/wIxwEIhxDYp5Tgp5W4hxDxgD2AGZkgpLbbnPAAsBozAbCnl7ib9HyiK4tni+rg7AsUOIZtBmdvk5GSZkpLi7jAURVGaFSHEFillsr19qryDoihKC6SSv6IoSgukkr+iKEoLpJK/oihKC6SSv6IoSgukkr+iKEoLpJK/oihKC6SSv6IoSgvULCZ5CSEygWNNOEU04MmLuKr4mkbF1zQqvqbx5PjaSyntVsZsFsm/qYQQKbXNcvMEKr6mUfE1jYqvaTw9vtqobh9FUZQWSCV/RVGUFqilJP9Z7g6gHiq+plHxNY2Kr2k8PT67WkSfv6IoilJdS2n5K4qiKFWo5K8oitICeU3yF0KMF0LsF0IcEkI8bme/vxDia9v+jUKIJBfG1k4IsVIIsUcIsVsI8aCdY8YIIfKEENtsP0+7Kr4qMRwVQuy0Xb/G6jlC85btNdwhhBjowti6VXlttgkh8oUQD513jEtfQyHEbCFEhhBiV5VtkUKIpUKIg7b/RtTy3Km2Yw4KIaa6ML5XhBD7bP9+3wshwmt5bp3vBR3je0YIcbLKv+GEWp5b59+7jvF9XSW2o0KIbbU8V/fXr8mklM3+B21JyFSgI+AHbAd6nnfM/cD/bL9PBr52YXxtgIG230OAA3biGwP87ObX8SgQXcf+CcCvaGsxDwM2uvHf+zTaBBa3vYbAhcBAYFeVbS8Dj9t+fxx4yc7zIoHDtv9G2H6PcFF8lwE+tt9fshefI+8FHeN7BnjEgX//Ov/e9YrvvP2vAU+76/Vr6o+3tPyHAIeklIellOXAXGDSecdMAj61/T4fuFgIIVwRnJQyXUq51fZ7AbAXaOuKazvZJOAzqdkAhAsh2rghjouBVCllU2Z9N5mUcg2Qc97mqu+zT4Gr7Tx1HLBUSpkjpTwLLAXGuyI+KeUSKaXZ9nADkODs6zqqltfPEY78vTdZXfHZcseNwFfOvq6reEvybwucqPI4jZrJteIY25s/D4hySXRV2LqbBgAb7eweLoTYLoT4VQjRy7WRASCBJUKILUKIaXb2O/I6u8Jkav+jc/dr2FpKmW77/TTQ2s4xnvI63on2Tc6e+t4LenrA1i01u5ZuM094/UYBZ6SUB2vZ787XzyHekvybBSFEK+Bb4CEpZf55u7eidWP0A/4L/ODi8ABGSikHApcDM4QQF7ohhjoJIfyAq4Bv7Oz2hNewgtS+/3vkWGohxFOAGfiilkPc9V54D+gE9AfS0bpWPNHN1N3q9/i/JW9J/ieBdlUeJ9i22T1GCOEDhAHZLolOu6YvWuL/Qkr53fn7pZT5UspC2++/AL5CiGhXxWe77knbfzOA79G+XlflyOust8uBrVLKM+fv8ITXEDhzrivM9t8MO8e49XUUQtwBTARutX1A1eDAe0EXUsozUkqLlNIKfFDLdd39+vkA1wJf13aMu16/hvCW5L8Z6CKE6GBrGU4GFpx3zALg3KiK64EVtb3xnc3WP/gRsFdKObOWY+LO3YMQQgxB+7dx5YdTsBAi5NzvaDcGd5132ALgdtuon2FAXpUuDleptcXl7tfQpur7bCrwo51jFgOXCSEibN0al9m26U4IMR54FLhKSllcyzGOvBf0iq/qPaRrarmuI3/veroE2CelTLO3052vX4O4+46zs37QRqIcQBsF8JRt27Nob3KAALSugkPAJqCjC2Mbifb1fwewzfYzAZgOTLcd8wCwG23kwgZghItfv462a2+3xXHuNawaowDesb3GO4FkF8cYjJbMw6psc9triPYhlA6Y0Pqd70K7j7QcOAgsAyJtxyYDH1Z57p229+Ih4E8ujO8QWn/5uffhuRFw8cAvdb0XXBTfHNt7awdaQm9zfny2xzX+3l0Rn237J+fec1WOdfnr19QfVd5BURSlBfKWbh9FURSlAVTyVxRFaYFU8lcURWmBVPJXFEVpgVTyVxRFaYFU8lcURWmBVPJXFEVpgf4fPMKWeJ/S0dQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_simple_bco(test_bipedalwalker, test_pendulum_f, s=10, off=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-jefferson",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-ability",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
