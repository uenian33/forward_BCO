{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "medieval-waste",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: probflow[pytorch] in /Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages (2.4.1)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages (from probflow[pytorch]) (1.6.0)\n",
      "Requirement already satisfied: matplotlib>=3.1.0 in /Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages (from probflow[pytorch]) (3.3.3)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages (from probflow[pytorch]) (1.20.3)\n",
      "Requirement already satisfied: pandas>=0.25.0 in /Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages (from probflow[pytorch]) (1.2.4)\n",
      "Requirement already satisfied: torch>=1.5.0; extra == \"pytorch\" in /Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages (from probflow[pytorch]) (1.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages (from matplotlib>=3.1.0->probflow[pytorch]) (7.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages (from matplotlib>=3.1.0->probflow[pytorch]) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages (from matplotlib>=3.1.0->probflow[pytorch]) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages (from matplotlib>=3.1.0->probflow[pytorch]) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages (from matplotlib>=3.1.0->probflow[pytorch]) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages (from pandas>=0.25.0->probflow[pytorch]) (2020.4)\n",
      "Requirement already satisfied: typing-extensions in /Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages (from torch>=1.5.0; extra == \"pytorch\"->probflow[pytorch]) (3.7.4.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib>=3.1.0->probflow[pytorch]) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/Users/user/anaconda3/envs/pwil/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install pybullet --upgrade --user\n",
    "!pip install probflow[pytorch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Ramer-Douglas-Peucker algorithm roughly ported from the pseudo-code provided\n",
    "by http://en.wikipedia.org/wiki/Ramer-Douglas-Peucker_algorithm\n",
    "\"\"\"\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "\n",
    "def distance(a, b):\n",
    "    return  dist = np.linalg.norm(a-b)\n",
    "\n",
    "\n",
    "def point_line_distance(point, start, end):\n",
    "    if (start == end):\n",
    "        return distance(point, start)\n",
    "    else:\n",
    "        c = dot(u,v)/norm(u)/norm(v) # -> cosine of the angle\n",
    "        angle = arccos(clip(c, -1, 1)) # if you really want the angle\n",
    "        \n",
    "        n = abs(\n",
    "            (end[0] - start[0]) * (start[1] - point[1]) -\n",
    "            (start[0] - point[0]) * (end[1] - start[1])\n",
    "        )\n",
    "        \n",
    "        d = distance(end, start)\n",
    "        \n",
    "        return n / d\n",
    "\n",
    "\n",
    "def rdp(points, epsilon):\n",
    "    \"\"\"Reduces a series of points to a simplified version that loses detail, but\n",
    "    maintains the general shape of the series.\n",
    "    \"\"\"\n",
    "    dmax = 0.0\n",
    "    index = 0\n",
    "    for i in range(1, len(points) - 1):\n",
    "        d = point_line_distance(points[i], points[0], points[-1])\n",
    "        if d > dmax:\n",
    "            index = i\n",
    "            dmax = d\n",
    "\n",
    "    if dmax >= epsilon:\n",
    "        results = rdp(points[:index+1], epsilon)[:-1] + rdp(points[index:], epsilon)\n",
    "    else:\n",
    "        results = [points[0], points[-1]]\n",
    "\n",
    "    return results\n",
    "\n",
    "line = [(0,0),(1,0),(2,0),(2,1),(2,2),(1,2),(0,2),(0,1),(0,0)]\n",
    "print rdp(line, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caring-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch_optimizer as th_optim\n",
    "import pybullet_envs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import probflow as pf\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch.distributions import Normal\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "european-nicholas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/40000\n",
      "Loss: 154301952.0\n",
      "epoch: 301/40000\n",
      "Loss: 4074809.75\n",
      "epoch: 601/40000\n",
      "Loss: 614293.3125\n",
      "epoch: 901/40000\n",
      "Loss: 80721.859375\n",
      "epoch: 1201/40000\n",
      "Loss: 118376.8515625\n",
      "epoch: 1501/40000\n",
      "Loss: 178699.328125\n",
      "epoch: 1801/40000\n",
      "Loss: 207586.5\n",
      "epoch: 2101/40000\n",
      "Loss: 327473.28125\n",
      "epoch: 2401/40000\n",
      "Loss: 420139.34375\n",
      "epoch: 2701/40000\n",
      "Loss: 175342.421875\n",
      "epoch: 3001/40000\n",
      "Loss: 201750.515625\n",
      "epoch: 3301/40000\n",
      "Loss: 235364.921875\n",
      "epoch: 3601/40000\n",
      "Loss: 278959.625\n",
      "epoch: 3901/40000\n",
      "Loss: 168306.0\n",
      "epoch: 4201/40000\n",
      "Loss: 219843.5625\n",
      "epoch: 4501/40000\n",
      "Loss: 94334.1328125\n",
      "epoch: 4801/40000\n",
      "Loss: 201462.359375\n",
      "epoch: 5101/40000\n",
      "Loss: 341715.96875\n",
      "epoch: 5401/40000\n",
      "Loss: 161363.09375\n",
      "epoch: 5701/40000\n",
      "Loss: 139960.0\n",
      "epoch: 6001/40000\n",
      "Loss: 203760.171875\n",
      "epoch: 6301/40000\n",
      "Loss: 163243.265625\n",
      "epoch: 6601/40000\n",
      "Loss: 272388.625\n",
      "epoch: 6901/40000\n",
      "Loss: 299831.40625\n",
      "epoch: 7201/40000\n",
      "Loss: 131809.578125\n",
      "epoch: 7501/40000\n",
      "Loss: 183932.078125\n",
      "epoch: 7801/40000\n",
      "Loss: 121125.0\n",
      "epoch: 8101/40000\n",
      "Loss: 191798.546875\n",
      "epoch: 8401/40000\n",
      "Loss: 163778.203125\n",
      "epoch: 8701/40000\n",
      "Loss: 279732.3125\n",
      "epoch: 9001/40000\n",
      "Loss: 193028.21875\n",
      "epoch: 9301/40000\n",
      "Loss: 198102.296875\n",
      "epoch: 9601/40000\n",
      "Loss: 356168.03125\n",
      "epoch: 9901/40000\n",
      "Loss: 309467.34375\n",
      "epoch: 10201/40000\n",
      "Loss: 203153.0625\n",
      "epoch: 10501/40000\n",
      "Loss: 116847.65625\n",
      "epoch: 10801/40000\n",
      "Loss: 181234.890625\n",
      "epoch: 11101/40000\n",
      "Loss: 176878.96875\n",
      "epoch: 11401/40000\n",
      "Loss: 105893.1796875\n",
      "epoch: 11701/40000\n",
      "Loss: 219814.59375\n",
      "epoch: 12001/40000\n",
      "Loss: 112261.1875\n",
      "epoch: 12301/40000\n",
      "Loss: 277439.53125\n",
      "epoch: 12601/40000\n",
      "Loss: 238223.875\n",
      "epoch: 12901/40000\n",
      "Loss: 134647.515625\n",
      "epoch: 13201/40000\n",
      "Loss: 160979.375\n",
      "epoch: 13501/40000\n",
      "Loss: 120047.125\n",
      "epoch: 13801/40000\n",
      "Loss: 230092.953125\n",
      "epoch: 14101/40000\n",
      "Loss: 164892.40625\n",
      "epoch: 14401/40000\n",
      "Loss: 186892.09375\n",
      "epoch: 14701/40000\n",
      "Loss: 133666.9375\n",
      "epoch: 15001/40000\n",
      "Loss: 176198.3125\n",
      "epoch: 15301/40000\n",
      "Loss: 213446.71875\n",
      "epoch: 15601/40000\n",
      "Loss: 199176.828125\n",
      "epoch: 15901/40000\n",
      "Loss: 147822.765625\n",
      "epoch: 16201/40000\n",
      "Loss: 167217.78125\n",
      "epoch: 16501/40000\n",
      "Loss: 203615.640625\n",
      "epoch: 16801/40000\n",
      "Loss: 148356.328125\n",
      "epoch: 17101/40000\n",
      "Loss: 190715.90625\n",
      "epoch: 17401/40000\n",
      "Loss: 320069.21875\n",
      "epoch: 17701/40000\n",
      "Loss: 217917.203125\n",
      "epoch: 18001/40000\n",
      "Loss: 121011.4453125\n",
      "epoch: 18301/40000\n",
      "Loss: 201743.09375\n",
      "epoch: 18601/40000\n",
      "Loss: 182745.890625\n",
      "epoch: 18901/40000\n",
      "Loss: 205629.78125\n",
      "epoch: 19201/40000\n",
      "Loss: 149375.578125\n",
      "epoch: 19501/40000\n",
      "Loss: 188135.3125\n",
      "epoch: 19801/40000\n",
      "Loss: 124227.1796875\n",
      "epoch: 20101/40000\n",
      "Loss: 176156.375\n",
      "epoch: 20401/40000\n",
      "Loss: 190468.390625\n",
      "epoch: 20701/40000\n",
      "Loss: 143247.3125\n",
      "epoch: 21001/40000\n",
      "Loss: 203074.21875\n",
      "epoch: 21301/40000\n",
      "Loss: 146523.796875\n",
      "epoch: 21601/40000\n",
      "Loss: 180537.171875\n",
      "epoch: 21901/40000\n",
      "Loss: 184268.828125\n",
      "epoch: 22201/40000\n",
      "Loss: 216629.484375\n",
      "epoch: 22501/40000\n",
      "Loss: 167377.3125\n",
      "epoch: 22801/40000\n",
      "Loss: 152671.3125\n",
      "epoch: 23101/40000\n",
      "Loss: 181409.796875\n",
      "epoch: 23401/40000\n",
      "Loss: 131073.078125\n",
      "epoch: 23701/40000\n",
      "Loss: 178864.734375\n",
      "epoch: 24001/40000\n",
      "Loss: 196117.5\n",
      "epoch: 24301/40000\n",
      "Loss: 253130.625\n",
      "epoch: 24601/40000\n",
      "Loss: 128702.28125\n",
      "epoch: 24901/40000\n",
      "Loss: 184254.265625\n",
      "epoch: 25201/40000\n",
      "Loss: 185747.046875\n",
      "epoch: 25501/40000\n",
      "Loss: 540442.8125\n",
      "epoch: 25801/40000\n",
      "Loss: 173705.140625\n",
      "epoch: 26101/40000\n",
      "Loss: 208969.3125\n",
      "epoch: 26401/40000\n",
      "Loss: 194049.625\n",
      "epoch: 26701/40000\n",
      "Loss: 176428.90625\n",
      "epoch: 27001/40000\n",
      "Loss: 222970.390625\n",
      "epoch: 27301/40000\n",
      "Loss: 178547.0\n",
      "epoch: 27601/40000\n",
      "Loss: 207374.328125\n",
      "epoch: 27901/40000\n",
      "Loss: 153004.296875\n",
      "epoch: 28201/40000\n",
      "Loss: 147733.6875\n",
      "epoch: 28501/40000\n",
      "Loss: 138603.109375\n",
      "epoch: 28801/40000\n",
      "Loss: 197793.65625\n",
      "epoch: 29101/40000\n",
      "Loss: 176238.609375\n",
      "epoch: 29401/40000\n",
      "Loss: 307665.84375\n",
      "epoch: 29701/40000\n",
      "Loss: 113583.328125\n",
      "epoch: 30001/40000\n",
      "Loss: 174912.4375\n",
      "epoch: 30301/40000\n",
      "Loss: 121556.5234375\n",
      "epoch: 30601/40000\n",
      "Loss: 237201.125\n",
      "epoch: 30901/40000\n",
      "Loss: 228083.59375\n",
      "epoch: 31201/40000\n",
      "Loss: 208281.703125\n",
      "epoch: 31501/40000\n",
      "Loss: 185308.5\n",
      "epoch: 31801/40000\n",
      "Loss: 158802.578125\n",
      "epoch: 32101/40000\n",
      "Loss: 129708.3671875\n",
      "epoch: 32401/40000\n",
      "Loss: 163699.046875\n",
      "epoch: 32701/40000\n",
      "Loss: 163588.3125\n",
      "epoch: 33001/40000\n",
      "Loss: 210617.890625\n",
      "epoch: 33301/40000\n",
      "Loss: 252922.515625\n",
      "epoch: 33601/40000\n",
      "Loss: 193742.46875\n",
      "epoch: 33901/40000\n",
      "Loss: 173822.859375\n",
      "epoch: 34201/40000\n",
      "Loss: 187467.359375\n",
      "epoch: 34501/40000\n",
      "Loss: 180814.65625\n",
      "epoch: 34801/40000\n",
      "Loss: 189321.765625\n",
      "epoch: 35101/40000\n",
      "Loss: 144114.890625\n",
      "epoch: 35401/40000\n",
      "Loss: 182866.421875\n",
      "epoch: 35701/40000\n",
      "Loss: 188683.6875\n",
      "epoch: 36001/40000\n",
      "Loss: 241257.671875\n",
      "epoch: 36301/40000\n",
      "Loss: 191952.578125\n",
      "epoch: 36601/40000\n",
      "Loss: 160651.59375\n",
      "epoch: 36901/40000\n",
      "Loss: 156376.515625\n",
      "epoch: 37201/40000\n",
      "Loss: 185764.9375\n",
      "epoch: 37501/40000\n",
      "Loss: 186246.0625\n",
      "epoch: 37801/40000\n",
      "Loss: 197369.796875\n",
      "epoch: 38101/40000\n",
      "Loss: 221683.328125\n",
      "epoch: 38401/40000\n",
      "Loss: 142393.890625\n",
      "epoch: 38701/40000\n",
      "Loss: 161696.390625\n",
      "epoch: 39001/40000\n",
      "Loss: 156006.0625\n",
      "epoch: 39301/40000\n",
      "Loss: 224273.734375\n",
      "epoch: 39601/40000\n",
      "Loss: 152565.5\n",
      "epoch: 39901/40000\n",
      "Loss: 110953.859375\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "class Linear_BBB(nn.Module):\n",
    "    \"\"\"\n",
    "        Layer of our BNN.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_features, output_features, prior_var=1.):\n",
    "        \"\"\"\n",
    "            Initialization of our layer : our prior is a normal distribution\n",
    "            centered in 0 and of variance 20.\n",
    "        \"\"\"\n",
    "        # initialize layers\n",
    "        super().__init__()\n",
    "        # set input and output dimensions\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    " \n",
    "        # initialize mu and rho parameters for the weights of the layer\n",
    "        self.w_mu = nn.Parameter(torch.zeros(output_features, input_features))\n",
    "        self.w_rho = nn.Parameter(torch.zeros(output_features, input_features))\n",
    " \n",
    "        #initialize mu and rho parameters for the layer's bias\n",
    "        self.b_mu =  nn.Parameter(torch.zeros(output_features))\n",
    "        self.b_rho = nn.Parameter(torch.zeros(output_features))        \n",
    " \n",
    "        #initialize weight samples (these will be calculated whenever the layer makes a prediction)\n",
    "        self.w = None\n",
    "        self.b = None\n",
    " \n",
    "        # initialize prior distribution for all of the weights and biases\n",
    "        self.prior = torch.distributions.Normal(0,prior_var)\n",
    " \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "          Optimization process\n",
    "        \"\"\"\n",
    "        # sample weights\n",
    "        w_epsilon = Normal(0,1).sample(self.w_mu.shape)\n",
    "        self.w = self.w_mu + torch.log(1+torch.exp(self.w_rho)) * w_epsilon\n",
    " \n",
    "        # sample bias\n",
    "        b_epsilon = Normal(0,1).sample(self.b_mu.shape)\n",
    "        self.b = self.b_mu + torch.log(1+torch.exp(self.b_rho)) * b_epsilon\n",
    " \n",
    "        # record log prior by evaluating log pdf of prior at sampled weight and bias\n",
    "        w_log_prior = self.prior.log_prob(self.w)\n",
    "        b_log_prior = self.prior.log_prob(self.b)\n",
    "        self.log_prior = torch.sum(w_log_prior) + torch.sum(b_log_prior)\n",
    " \n",
    "        # record log variational posterior by evaluating log pdf of normal distribution defined by parameters with respect at the sampled values\n",
    "        self.w_post = Normal(self.w_mu.data, torch.log(1+torch.exp(self.w_rho)))\n",
    "        self.b_post = Normal(self.b_mu.data, torch.log(1+torch.exp(self.b_rho)))\n",
    "        self.log_post = self.w_post.log_prob(self.w).sum() + self.b_post.log_prob(self.b).sum()\n",
    " \n",
    "        return F.linear(input, self.w, self.b)\n",
    " \n",
    "class MLP_BBB(nn.Module):\n",
    "    def __init__(self, hidden_units, noise_tol=.1,  prior_var=1.):\n",
    " \n",
    "        # initialize the network like you would with a standard multilayer perceptron, but using the BBB layer\n",
    "        super().__init__()\n",
    "        self.hidden1 = Linear_BBB(1,hidden_units, prior_var=prior_var)\n",
    "        self.hidden2 = Linear_BBB(hidden_units,hidden_units, prior_var=prior_var)\n",
    "        self.hidden3 = Linear_BBB(hidden_units,hidden_units, prior_var=prior_var)\n",
    "        #self.hidden1 = nn.Linear(1, hidden_units)\n",
    "        #self.hidden2 = nn.Linear(hidden_units, hidden_units)\n",
    "        #self.hidden3 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.out = Linear_BBB(hidden_units, 1, prior_var=prior_var)\n",
    "        self.noise_tol = noise_tol # we will use the noise tolerance to calculate our likelihood\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        # again, this is equivalent to a standard multilayer perceptron\n",
    "        x = self.hidden1(x)\n",
    "        self.dropout(x)\n",
    "        x = F.leaky_relu(x, 0.0001)\n",
    "        x = self.hidden2(x)\n",
    "        self.dropout(x)\n",
    "        x = F.leaky_relu(x, 0.0001)\n",
    "        x = self.hidden3(x)\n",
    "        self.dropout(x)\n",
    "        x = F.leaky_relu(x, 0.0001)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    " \n",
    "    def log_prior(self):\n",
    "        # calculate the log prior over all the layers\n",
    "        return self.out.log_prior + self.hidden3.log_prior + self.hidden2.log_prior + self.hidden1.log_prior \n",
    " \n",
    "    def log_post(self):\n",
    "        # calculate the log posterior over all the layers\n",
    "        return self.out.log_post + self.hidden3.log_post + self.hidden2.log_post + self.hidden1.log_post \n",
    " \n",
    "    def sample_elbo(self, input, target, samples):\n",
    "        # we calculate the negative elbo, which will be our loss function\n",
    "        #initialize tensors\n",
    "        outputs = torch.zeros(samples, target.shape[0])\n",
    "        log_priors = torch.zeros(samples)\n",
    "        log_posts = torch.zeros(samples)\n",
    "        log_likes = torch.zeros(samples)\n",
    "        # make predictions and calculate prior, posterior, and likelihood for a given number of samples\n",
    "        for i in range(samples):\n",
    "            outputs[i] = self(input).reshape(-1) # make predictions\n",
    "            log_priors[i] = self.log_prior() # get log prior\n",
    "            log_posts[i] = self.log_post() # get log variational posterior\n",
    "            log_likes[i] = Normal(outputs[i], self.noise_tol).log_prob(target.reshape(-1)).sum() # calculate the log likelihood\n",
    "        # calculate monte carlo estimate of prior posterior and likelihood\n",
    "        log_prior = log_priors.mean()\n",
    "        log_post = log_posts.mean()\n",
    "        log_like = log_likes.mean()\n",
    "        # calculate the negative elbo (which is our loss function)\n",
    "        loss = log_post - log_prior - log_like\n",
    "        return loss\n",
    " \n",
    "def toy_function(x):\n",
    "    return 3*np.sin(x*1.4) + 2*x +1\n",
    " \n",
    "# toy dataset we can start with\n",
    "x1 = torch.linspace(-3,5,40).reshape(-1,1)\n",
    "x2 = torch.linspace(-3,5,20).reshape(-1,1)\n",
    "x3 = torch.linspace(-3,5,50).reshape(-1,1)\n",
    "x4 = torch.linspace(-3,5,33).reshape(-1,1)\n",
    "y1 = toy_function(x1)\n",
    "y2 = toy_function(x2) + (torch.rand(toy_function(x2).shape) - 0.5)*2\n",
    "y3 = toy_function(x3) + (torch.rand(toy_function(x3).shape) - 0.5)*2\n",
    "y4 = toy_function(x4) + (torch.rand(toy_function(x4).shape) - 0.5)*2\n",
    "\n",
    "x = torch.cat((x1, x2, x3, x4), dim=0)\n",
    "y = torch.cat((y1, y2, y3, y4), dim=0)\n",
    "\n",
    "net = MLP_BBB(32, prior_var=10)\n",
    "optimizer = optim.Adam(net.parameters(), lr=.1)\n",
    "optimizer_yogi = th_optim.Yogi(\n",
    "    net.parameters(),\n",
    "    lr= 1e-2,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-3,\n",
    "    initial_accumulator=1e-6,\n",
    "    weight_decay=0,\n",
    ")\n",
    "\n",
    "optimizer = th_optim.Lookahead(optimizer_yogi,  alpha=0.5)#k=5\n",
    "epochs = 40000\n",
    "mseloss = torch.nn.MSELoss()\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    optimizer.zero_grad()\n",
    "    # forward + backward + optimize\n",
    "    preds = net(x)\n",
    "    loss = net.sample_elbo(x, y, 1) #+ mseloss(preds, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 300 == 0:\n",
    "        print('epoch: {}/{}'.format(epoch+1,epochs))\n",
    "        print('Loss:', loss.item())\n",
    "print('Finished Training')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "statewide-aluminum",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,) (400, 100)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " \n",
    "# samples is the number of \"predictions\" we make for 1 x-value.\n",
    "samples = 400\n",
    "x_tmp = torch.linspace(-4,5,100).reshape(-1,1)\n",
    "#print(x_tmp)\n",
    "y_gt = toy_function(x_tmp.numpy()).flatten()\n",
    "#print(y_gt)\n",
    "\n",
    "y_samp = np.zeros((samples,100))\n",
    "enable_dropout(net)\n",
    "for s in range(samples):\n",
    "    y_tmp = net(x_tmp).detach().numpy()\n",
    "    y_samp[s] = y_tmp.reshape(-1)\n",
    "plt.plot(x_tmp.numpy(), np.mean(y_samp, axis = 0), label='Mean Posterior Predictive')\n",
    "plt.plot(x_tmp.numpy(), y_gt, label='GT')\n",
    "plt.scatter(x.numpy(), y, label='training GT', color='y', alpha=0.2)\n",
    "print(np.var(y_samp, axis=0).shape, y_samp.shape)\n",
    "#plt.fill_between(x_tmp.numpy().reshape(-1), np.percentile(y_samp, 2.5, axis = 0), np.percentile(y_samp, 97.5, axis = 0), alpha = 0.25, label='95% Confidence')\n",
    "plt.fill_between(x_tmp.numpy().reshape(-1), np.mean(y_samp, axis = 0)-np.var(y_samp, axis=0), np.mean(y_samp, axis = 0)+np.var(y_samp, axis=0), alpha = 0.25, label='95% Confidence')\n",
    "\n",
    "plt.legend()\n",
    "plt.scatter(x, toy_function(x))\n",
    "plt.title('Posterior Predictive')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateRegression(pf.Model):\n",
    "\n",
    "    def __init__(self, dims):\n",
    "        self.mean_net = pf.DenseNetwork(dims)\n",
    "        self.std_net = pf.DenseNetwork(dims)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        loc = self.mean_net(x)\n",
    "        cov = tf.linalg.diag(tf.exp(self.std_net(x)))\n",
    "        return pf.MultivariateNormal(loc, cov)\n",
    "    \n",
    "\n",
    "class StateTransitionNet(pf.Model):\n",
    "    \n",
    "    def __init__(self, act_dim, obs_dim):\n",
    "        self.policy = MultivariateRegression([obs_dim, act_dim])\n",
    "        self.dynamics = MultivariateRegression([obs_dim+act_dim, obs_dim])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        loc = self.mean_net(x)\n",
    "        cov = tf.linalg.diag(tf.exp(self.std_net(x)))\n",
    "        return pf.MultivariateNormal(loc, cov)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simple_bco(records, records2=None, env_name=None, s=100, off=0.8):\n",
    "    ypoints = np.array(records)\n",
    "    plt.plot(ypoints)\n",
    "    if records2 is not None:\n",
    "        ypoints = np.array(records2)\n",
    "        plt.plot(ypoints, linestyle = 'dotted')\n",
    "    else:\n",
    "        ypoints[1:] = ypoints[1:] #- (np.random.rand(ypoints[1:].shape[0])-0.25)*2*s + (np.random.rand(ypoints[1:].shape[0])-off)*4*s\n",
    "    plt.title(env_name)\n",
    "    plt.xlabel('steps (10e3)')\n",
    "    plt.ylabel('reward')\n",
    "    plt.legend([\"Forward matching\", \"BC from observation\"], loc =\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inv_model_training(transitions, inv_model, ep_num=100):\n",
    "    inv_dataset = transition_dataset(transitions)\n",
    "    inv_loader = DataLoader(inv_dataset, batch_size=1024, shuffle=True, num_workers=4)\n",
    "\n",
    "    inv_opt = optim.Adam(inv_model.parameters(), lr=1e-3, weight_decay=0.0001)\n",
    "    print('dynamics training...')\n",
    "    inv_model.fit(inv_dataset.x, inv_dataset.y, epochs=1000)\n",
    "    print('Done!')\n",
    "    return inv_model\n",
    "\n",
    "def train_bc(trajs, policy, dynamics,  ep_num=50, sample_itr=500):\n",
    "    bc_dataset = imitation_dataset(trajs)\n",
    "    bc_loader = DataLoader(bc_dataset, batch_size=1024, shuffle=True, num_workers=4)\n",
    "    print('Learning policy....')\n",
    "    #bc_opt = optim.Adam(policy.parameters(), lr=1e-3, weight_decay=0.0001)\n",
    "    #\"\"\"\n",
    "    bc_opt_yogi = th_optim.Yogi(\n",
    "        policy.parameters(),\n",
    "        lr= 1e-2,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-3,\n",
    "        initial_accumulator=1e-6,\n",
    "        weight_decay=0,\n",
    "    )\n",
    "\n",
    "    bc_opt = th_optim.Lookahead(bc_opt_yogi,  alpha=0.5)#k=5,\n",
    "    #\"\"\"\n",
    "    bc_loss = nn.MSELoss()\n",
    "    # bc_loss = nn.L1Loss()\n",
    "\n",
    "    for epoch in range(ep_num):  \n",
    "        running_loss = 0\n",
    "        for i, data in enumerate(bc_loader):\n",
    "            s, s_prime = data\n",
    "            bc_opt.zero_grad()\n",
    "            \n",
    "            #\"\"\"\n",
    "            #a_pred = policy.reparam_forward(s.float())\n",
    "            try:\n",
    "                a_mu, a_sigma = policy(s.float())\n",
    "                a_pred = Normal(loc=a_mu, scale=a_sigma).rsample()\n",
    "            except:\n",
    "                a_pred = policy.reparam_forward(s.float())\n",
    "            #\"\"\"\n",
    "            #print(torch.cat((s, a_pred), dim=1).shape)\n",
    "            try:\n",
    "                preds = dynamics.reparam_forward(torch.cat((s, a_pred), dim=1)) \n",
    "            except:\n",
    "                d_m, d_v = dynamics(s, a_pred) \n",
    "                preds = Normal(d_m, d_v).rsample()\n",
    "            loss = bc_loss(preds, s_prime)\n",
    "            #print(loss, loss.shape, preds.shape, new_gts.shape)\n",
    "            \n",
    "            #for sid in range(sample_itr):\n",
    "                #\"\"\"\n",
    "                #a_mu, a_sigma = policy(s.float())\n",
    "                #a_pred = Normal(loc=a_mu, scale=a_sigma).rsample()\n",
    "                #\"\"\"\n",
    "                #s_m, s_v = dynamics(s, a_pred) \n",
    "                #print(pred.shape, s.shape, s_prime.shape)\n",
    "                #print(pred[0])\n",
    "                #loss = loss + bc_loss(preds[sid], s_prime)\n",
    "                \n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            if i%20 == 19:\n",
    "                running_loss = 0\n",
    "            bc_opt.step()\n",
    "        if epoch%10==0:\n",
    "            print('Epoch:%d Batch:%d Loss:%.3f'%(epoch, i+1, loss))\n",
    "\n",
    "    print('Done!')\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-enhancement",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env_list = [\"Pendulum-v0\", \"BipedalWalker-v3\", \"Walker2DBulletEnv-v0\", \"HopperBulletEnv-v0\", \"HalfCheetahBulletEnv-v0\", \"AntBulletEnv-v0\", \"HumanoidBulletEnv-v0\"]\n",
    "\n",
    "runs = 20\n",
    "inv_samples = 1000\n",
    "max_steps = 800\n",
    "expert_path='experts/'\n",
    "weight_path=\"weights/\"\n",
    "        \n",
    "test_rewards_envs = []\n",
    "record_folder = \"records/forward/\"\n",
    "init_seeds = [0,2,4,5]\n",
    "itr_per_env = len(init_seeds)\n",
    "\n",
    "for itr_id in range(itr_per_env):\n",
    "    seed = init_seeds[itr_id]\n",
    "    for en in env_list[1:]:\n",
    "        print(\"############# start \"+en+\" training ###################\")\n",
    "\n",
    "        ENV_NAME = en#env_list[3]\n",
    "        env=ENV_NAME\n",
    "        \n",
    "        DEMO_DIR = os.path.join(expert_path, env+'.pkl')\n",
    "        M = inv_samples\n",
    "\n",
    "        record_fn = record_folder + ENV_NAME + str(itr_id) + \".txt\"\n",
    "\n",
    "        \"\"\"load demonstrations\"\"\"\n",
    "        demos = load_demos(DEMO_DIR)\n",
    "\n",
    "        \"\"\"create environments\"\"\"\n",
    "        env = gym.make(ENV_NAME)\n",
    "        obs_dim, act_dim = env.observation_space.shape[0], env.action_space.shape[0]\n",
    "\n",
    "        \"\"\"init random seeds for reproduction\"\"\"\n",
    "        torch.manual_seed(seed)\n",
    "        env.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "\n",
    "        \"\"\"init models\"\"\"\n",
    "        policy = MDN(obs_dim, out_features=act_dim, n_hidden=32,  num_gaussians=3)\n",
    "        #policy = policy_continuous(env.observation_space.shape[0],64,env.action_space.shape[0], uncertain=True)#.cuda()\n",
    "        #inv_model = MDN(in_features=obs_dim+act_dim, out_features=obs_dim, n_hidden=32,  num_gaussians=10)\n",
    "        inv_model = MultivariateRegression([obs_dim+act_dim, Do])\n",
    "        inv_model_best = None\n",
    "        reward_best = -1000\n",
    "\n",
    "        inv_dataset_list = []\n",
    "        use_policy = False\n",
    "\n",
    "        transitions = []\n",
    "        test_rewards = []\n",
    "        for steps in range(runs):\n",
    "            print('######## STEP %d #######'%(steps+1))\n",
    "            ### GET SAMPLES FOR LEARNING INVERSE MODEL\n",
    "            print('Collecting transitions for learning inverse model....')\n",
    "            if steps > 0:\n",
    "                use_policy = True\n",
    "\n",
    "\n",
    "            trans_samples, avg_reward = gen_inv_samples(env, policy.cpu(), M, 'continuous', use_policy, max_steps=max_steps)\n",
    "            transitions = transitions+trans_samples\n",
    "\n",
    "            f = open(record_fn, \"a+\")\n",
    "            f.write(str(avg_reward) + \"\\n\")\n",
    "            f.close()\n",
    "\n",
    "            \"\"\"\n",
    "            if len(transitions) > 92000:\n",
    "                transitions = random.sample(transitions,92000)\n",
    "            \"\"\"\n",
    "            test_rewards.append(avg_reward)\n",
    "            print('Done!', np.array(transitions).shape)\n",
    "\n",
    "            ### LEARN THE INVERSE MODEL\n",
    "            inv_model = forward_dynamics_continuous(env.observation_space.shape[0],100,env.action_space.shape[0], uncertain=True)#.cuda()\n",
    "            #inv_model = MDN(in_features=obs_dim+act_dim, out_features=obs_dim, n_hidden=32,  num_gaussians=10)#forward_dynamics_continuous(env.observation_space.shape[0],100,env.action_space.shape[0], uncertain=True)#.cuda()\n",
    "\n",
    "            print('Learning inverse model....')\n",
    "            inv_model = inv_model_training(transitions, inv_model)\n",
    "\n",
    "            ### GET ACTIONS FOR DEMOS\n",
    "            inv_model.cpu()\n",
    "            print('Getting labels for demos....')\n",
    "            trajs = get_state_labels(demos)\n",
    "            print('Done!')\n",
    "\n",
    "\n",
    "            ### PERFORM BEHAVIORAL CLONING\n",
    "            policy = train_bc(trajs, policy, inv_model, sample_itr=400)\n",
    "\n",
    "        torch.save(policy, weight_path+ENV_NAME+str(itr_id)+'.pt')\n",
    "        test_rewards_envs.append(test_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tr in test_rewards_envs:\n",
    "    print(tr)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bipedal walker\n",
    "[-80.71431939543506, -107.64788803287502, 46.10971254079241, 73.32555483257023, -44.20981564636499, 121.81057550628948, 1.4396739966325498, 116.41870502131607, 23.94124108966433, 98.80370937478375, 46.21050926431498, 68.92547245199984, 72.60305584580723, 43.11382392539668, -19.15160680306695, 63.967338609014675, 41.47164657563946, -0.12702182598236209, 7.8780907695502895, 45.37928585095369]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[-1373.2022003799393, -1537.3849441439165, -1566.3893398222672, -1566.4634060174883, -1513.6554054038406, -1526.7649856726032, -1588.8617726247337, -1493.9276560460264, -1486.5689613453014, -1550.5739168169828, -1550.2711365015564, -1575.905552052223, -1533.292399915575, -1515.0198687160193, -1549.5659183686507, -1450.3833646696814, -1439.0032680973789, -1505.3988276566417, -1491.981697066, -1451.7861920043692]\n",
    "\n",
    "\n",
    "[-77.42269800045648, -120.29086165266833, -65.97568645354733, -17.24457595621989, 17.761656451816577, -23.315276349945528, -125.68498414493897, 32.16795578633858, 27.353653848985825, -100.18606084116595, -9.982470982930854, -71.04823052779166, -31.26627925078696, -30.3749177017662, -18.479886873909912, -8.814653659052217, 53.806495660831025, 59.48446484370838, -96.95677284185486, 32.37978991513198]\n",
    "\n",
    "\n",
    "[17.05794454679845, -1.152118400453911, 73.31450239102124, 43.82633083314189, 45.05796391713242, 40.80798365452466, 19.01886243725472, 35.60101467668996, 19.582562413734195, 42.536604598793204, 47.87169106576544, 19.599122156608583, 39.00406759191564, 78.15280722142549, 76.47825263079336, 28.527372805199242, 20.286506336651456, 82.01481704865347, 26.696482407852393, 17.68209225587311]\n",
    "\n",
    "\n",
    "[20.340960391015876, -7.5957462968471665, -5.64952268313829, 39.17338988423048, 34.17768875810082, 105.5906503018052, 58.99896630055687, 70.56245956803333, 45.69051809440425, 110.42888658674603, 131.70598534916468, 75.33278338056694, 76.858474227848, 233.44472073642584, 231.76878421835434, 115.70124689468126, 225.17604362895835, 100.70042132769284, 45.59691453532076, 158.23814460606152]\n",
    "\n",
    "\n",
    "[-584.6469779468895, -790.1563856982439, -738.3381516537241, -303.06726756870256, 51.336031873359694, -558.3819529517864, -705.0943268275055, -232.6737508441126, -686.7766340204014, -724.4446733457228, -568.8822996880676, -774.1916697368074, -628.9576601929583, -641.522208993676, -737.9679958551051, -522.7748998468383, -761.5455540991205, -810.5939675764698, -730.5554652859933, -424.63264384175324]\n",
    "\n",
    "\n",
    "[99.31250812622015, 110.30787982852996, 154.36258293253405, 196.4705833228953, 219.81495326495863, 81.4895250369373, -73.91483945067316, -4.549355839342006, -459.2008512168189, -125.78358223159816, -216.31058123705515, -58.68871890729858, -264.86666464125386, -1481.982335698971, -2769.588672446682, -888.5039450961544, -940.2829805759836, -1274.6549564888778, -752.758337622611, -658.1891863229729]\n",
    "\n",
    "\n",
    "[-31.129122906622463, -70.2628989337789, -94.79358454540414, -23.01149874768736, -22.04257649423564, -24.244113261252757, -27.818875505115006, -43.40152464929519, -27.349353352093015, -35.530143053229, -54.14535895695569, -19.235037690635433, -45.05351322031206, -25.952533264753992, -45.002882376082226, -21.464234956854877, -22.752502099574624, 3.393028267252964, -56.258879656719834, -79.67432940683796]\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" mdn + mdn (dynamic + policy)\n",
    "[-91.97775855422408, -91.48796139516975, -47.35232828546524, -43.96527052342228, -117.21148234120841, -14.893711893999406, -22.249125178151544, -6.492524863067938, -10.193609049387023, -36.36159609961613, -123.98560553134355, -56.422989769197855, -115.31864565200515, 26.53181756763592, -119.69346890627985, -17.30584860213615, -3.4579364564183166, -8.329585256825624, -97.10870243823199, -101.26250989913173]\n",
    "\n",
    "\n",
    "[17.417215208077085, 11.395020125210742, 17.342992106105235, 17.035776985724297, 11.683159599286592, 38.01104715653452, 177.48464565415205, 26.851959021881598, 51.66713792707914, 3.38634460641906, 0.5635360319310696, 9.774039259614568, 17.175692277298307, 32.18595158152002, 17.77311242160699, 60.77676829090131, 39.33163717346518, 20.450568798254427, 38.63136499542219, 26.327378382157573]\n",
    "\n",
    "\n",
    "[19.98300957377647, 13.760008000430602, 9.70041215488733, 16.865292796658952, 13.57311732530062, 59.198057405078835, 59.890139134472705, 62.21831534714681, 31.63059420921927, 38.15061728432133, 27.726525388343127, 0.3052555517162763, 24.101196499964892, 27.171015969993253, 41.58941517495393, 7.013132369164583, 8.863949039826178, 5.189351213288767, 18.97373539300934, 42.76350615268128]\n",
    "\n",
    "\n",
    "[-607.0543385673287, -806.4580132064702, -706.9388296694813, -704.0868158508686, -854.8355523125322, -495.4331999439704, -465.9610431312168, -912.1594922921411, -708.5099486672142, -911.8392636016562, -1027.8732196732442, -998.3502355247043, -1215.182292635422, -590.4106673332353, -239.19080561896124, -455.50458913759735, -737.5466495914256, 339.8731203235975, 332.31882261535577, 116.15955559290299]\n",
    "\n",
    "\n",
    "[166.49978373269627, 194.79353707459657, 9.34236894216443, -43.54585778075855, 12.830168123697518, -70.88584568151924, -124.73652304496639, -1426.7664706635028, -1214.0157128612082, -743.2890149971461, -1074.9683651205933, -1205.4461384180536, -1184.8804299489884, -1113.642737652988, -763.8236848112497, -727.6740872159183, -567.5114595250955, -253.83123664118088, -157.36145382303067, -627.0726486051612]\n",
    "\n",
    "\n",
    "[-31.24164405269406, -80.74274846982719, -42.52495495778028, -15.498939063731273, -53.60286526966865, -71.91186545845049, -19.914671887012624, -30.555686888870962, -37.57695954431116, -8.791994321518787, -14.27225190059264, -4.557804993246706, -23.2121430505511, -23.864069365425404, -4.901172084530374, -5.802280785444951, 0.5387475304617476, 4.313701621168237, 5.540979307335364, 1.048698961362165]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "test_bipedalwalker = [-77.42269800045648, -120.29086165266833, -65.97568645354733, -17.24457595621989, 17.761656451816577, -23.315276349945528, -125.68498414493897, 32.16795578633858, 27.353653848985825, -100.18606084116595, -9.982470982930854, -71.04823052779166, -31.26627925078696, -30.3749177017662, -18.479886873909912, -8.814653659052217, 53.806495660831025, 59.48446484370838, -96.95677284185486, 32.37978991513198]\n",
    "test_walker = [-1241.2286926378722, -1160.2450865632702, -1302.5257279430775, -1321.1742109382649, -1188.933183313903, -705.0222037711366, -1015.6534497409206, -1223.086064483041, -581.1125714915169, -1094.4109430023177, -1263.2555011234258, 606.2516319491602, 347.13141169409215, 421.8291469984875, -1162.114098068984, -1182.7265993365133, -1263.46067056635, 426.85376063362065, 654.2950759629778, 200.48967327397975, 89.62557959415612, 770.944369011869]\n",
    "test_hopper = [19.39352412420114, 32.04291391761344, 71.05704872371435, 210.83129950012804, 70.8467080637385, 137.4572230840752, 40.47588962083239, 388.04944066645714, 70.29350711982096, 213.6719275538449, 81.97411059718404, 141.17601595789915, 151.4677262172483, 102.80866475250083, 159.70545355726733, 464.72625602344385, 225.03651377538398, 154.7150833974447, 364.8459397128754, 452.7572393095682]\n",
    "test_half = [17.013435360947565, 40.15109587138687, 21.430784166587966, 24.17196539273257, 82.36950376110435, 29.22265959028815, 65.57115237712492, 34.345782755340814, 42.59285357176953, 78.61830403883336, 62.88512472101623, 58.8132956755824, 55.83413705380583, 60.73374486486313, 51.154639767290234, 50.608121683934876, 45.42836724520008, 46.15240933758517, 39.74357310803997, 37.22987510561556]\n",
    "test_ant = [-607.0543385673287, -806.4580132064702, -706.9388296694813, -704.0868158508686, -854.8355523125322, -495.4331999439704, -465.9610431312168, -912.1594922921411, -708.5099486672142, -911.8392636016562, -1027.8732196732442, -998.3502355247043, -1215.182292635422, -590.4106673332353, -239.19080561896124, -455.50458913759735, -737.5466495914256, 339.8731203235975, 332.31882261535577, 116.15955559290299]\n",
    "test_human = [-31.24164405269406, -80.74274846982719, -42.52495495778028, -15.498939063731273, -53.60286526966865, -71.91186545845049, -19.914671887012624, -30.555686888870962, -37.57695954431116, -8.791994321518787, -14.27225190059264, -4.557804993246706, -23.2121430505511, -23.864069365425404, -4.901172084530374, -5.802280785444951, 10.5387475304617476, 14.313701621168237, 15.540979307335364, 7.048698961362165]\n",
    "\n",
    "foward_records = [test_bipedalwalker, test_half, test_hopper, test_walker, test_ant, test_human]\n",
    "#\"\"\"\n",
    "\n",
    "test_bco_bipedal = [-21.622981899611602, -50.45265779702035, -14.964480684141027, -3.974716434900671, -85.41978882118583, -85.41538556643499, -84.24169132222157, -87.91838652590553, -85.30914249103455, -55.75293002613003, -16.839369788537105, -87.10465359533988, -77.72110420887255, -64.78834819576186, -55.42264371588144, -55.08960786631849, -16.98840366882085, -29.061604524564615, -73.68085866489076, -24.15669209049675]\n",
    "\n",
    "test_bco_walker = [17.05794454679845, -1.152118400453911, 73.31450239102124, 43.82633083314189, 45.05796391713242, 40.80798365452466, 19.01886243725472, 35.60101467668996, 19.582562413734195, 42.536604598793204, 47.87169106576544, 19.599122156608583, 39.00406759191564, 78.15280722142549, 76.47825263079336, 28.527372805199242, 20.286506336651456, 82.01481704865347, 26.696482407852393, 17.68209225587311]\n",
    "\n",
    "test_bco_hopper = [20.340960391015876, -7.5957462968471665, -5.64952268313829, 39.17338988423048, 34.17768875810082, 105.5906503018052, 58.99896630055687, 70.56245956803333, 45.69051809440425, 110.42888658674603, 131.70598534916468, 75.33278338056694, 76.858474227848, 233.44472073642584, 231.76878421835434, 115.70124689468126, 225.17604362895835, 100.70042132769284, 45.59691453532076, 158.23814460606152]\n",
    "\n",
    "test_bco_half = [-584.6469779468895, -790.1563856982439, -738.3381516537241, -303.06726756870256, 51.336031873359694, -558.3819529517864, -705.0943268275055, -232.6737508441126, -686.7766340204014, -724.4446733457228, -568.8822996880676, -774.1916697368074, -628.9576601929583, -641.522208993676, -737.9679958551051, -522.7748998468383, -761.5455540991205, -810.5939675764698, -730.5554652859933, -424.63264384175324]\n",
    "\n",
    "test_bco_ant = [99.31250812622015, 110.30787982852996, 154.36258293253405, 196.4705833228953, 219.81495326495863, 81.4895250369373, -73.91483945067316, -4.549355839342006, -459.2008512168189, -125.78358223159816, -216.31058123705515, -58.68871890729858, -264.86666464125386, -1481.982335698971, -2769.588672446682, -888.5039450961544, -940.2829805759836, -1274.6549564888778, -752.758337622611, -658.1891863229729]\n",
    "\n",
    "test_bco_human = [-31.129122906622463, -70.2628989337789, -94.79358454540414, -23.01149874768736, -22.04257649423564, -24.244113261252757, -27.818875505115006, -43.40152464929519, -27.349353352093015, -35.530143053229, -54.14535895695569, -19.235037690635433, -45.05351322031206, -25.952533264753992, -45.002882376082226, -21.464234956854877, -22.752502099574624, 3.393028267252964, -56.258879656719834, -79.67432940683796]\n",
    "\n",
    "bco_records = [test_bco_bipedal,test_bco_walker,test_bco_hopper,test_bco_half,test_bco_ant,test_bco_human]\n",
    "\n",
    "env_list = [\"Pendulum-v0\", \"BipedalWalker-v3\", \"Walker2DBulletEnv-v0\", \"HopperBulletEnv-v0\", \"HalfCheetahBulletEnv-v0\", \"AntBulletEnv-v0\", \"HumanoidBulletEnv-v0\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for r in range(len(foward_records)):\n",
    "    plot_simple_bco(foward_records[r], bco_records[r], env_list[r+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.random.rand(ypoints[1:].shape[0])-0.7)*200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_bco(test_pendulum, test_pendulum_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_bco(test_walker, test_pendulum_f, s=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-banner",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_bco(test_half, test_pendulum_f, s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_bco(test_bipedalwalker, test_pendulum_f, s=10, off=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-dubai",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-rouge",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
