{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "attended-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch.distributions import Normal\n",
    "from models import *\n",
    "from utils import *\n",
    "from dataset import *\n",
    "import pybullet_envs\n",
    "import random\n",
    "\n",
    "\n",
    "def load_demos(DEMO_DIR):\n",
    "    try:\n",
    "        trajs = np.load(\"experts/states_expert_walker_0.npy\")[:10]\n",
    "    except:\n",
    "        with open(DEMO_DIR, 'rb') as f:\n",
    "            trajs = pickle.load(f)\n",
    "\n",
    "    demos = []\n",
    "    for t_id, traj in enumerate(trajs):\n",
    "        demo =[]\n",
    "        #print(t_id)\n",
    "        for item in traj:    \n",
    "            obs = item['observation']\n",
    "            #obs = list(obs)\n",
    "            #print(obs)\n",
    "            demo.append(obs)\n",
    "        #print(np.array(demo).shape)\n",
    "        demos.append(np.array(demo))\n",
    "\n",
    "    print(np.array(demos).shape)\n",
    "    demos = demos[:10]\n",
    "    return demos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "inside-aviation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# start Walker2DBulletEnv-v0 training ###################\n",
      "(49, 1000, 22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## STEP 1 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 13.612366641615518 setps: 9 count: 9\n",
      "reward: 15.333661909894728 setps: 11 count: 20\n",
      "reward: 14.06518672312959 setps: 10 count: 30\n",
      "reward: 22.825537883289506 setps: 26 count: 56\n",
      "reward: 20.030772349816107 setps: 20 count: 76\n",
      "reward: 16.999050246212573 setps: 16 count: 92\n",
      "reward: 15.545413191551052 setps: 14 count: 106\n",
      "reward: 16.226392472333103 setps: 13 count: 119\n",
      "reward: 14.878472657156816 setps: 18 count: 137\n",
      "reward: 17.093303792305232 setps: 15 count: 152\n",
      "reward: 25.38579114184395 setps: 28 count: 180\n",
      "reward: 15.62311114539043 setps: 12 count: 192\n",
      "reward: 25.67762851235602 setps: 21 count: 213\n",
      "reward: 27.122905671574703 setps: 28 count: 241\n",
      "reward: 16.43561664125446 setps: 12 count: 253\n",
      "reward: 19.666378375614293 setps: 19 count: 272\n",
      "reward: 13.895775478085852 setps: 9 count: 281\n",
      "reward: 15.099087875016266 setps: 10 count: 291\n",
      "reward: 17.484739339732915 setps: 15 count: 306\n",
      "reward: 16.458311988427884 setps: 13 count: 319\n",
      "reward: 24.631510851769416 setps: 23 count: 342\n",
      "reward: 17.27214320818748 setps: 13 count: 355\n",
      "reward: 14.835133212765506 setps: 11 count: 366\n",
      "reward: 15.280416195635917 setps: 11 count: 377\n",
      "reward: 24.255114293211953 setps: 25 count: 402\n",
      "reward: 16.724412600457438 setps: 12 count: 414\n",
      "reward: 17.03025952757744 setps: 13 count: 427\n",
      "reward: 14.916645828922626 setps: 11 count: 438\n",
      "reward: 14.978043961331423 setps: 10 count: 448\n",
      "reward: 16.548853618519203 setps: 17 count: 465\n",
      "reward: 15.808945510665946 setps: 12 count: 477\n",
      "reward: 19.61333122198557 setps: 19 count: 496\n",
      "reward: 17.831681481759002 setps: 15 count: 511\n",
      "reward: 19.90174298195634 setps: 19 count: 530\n",
      "reward: 18.793621826905294 setps: 15 count: 545\n",
      "reward: 13.583168764971196 setps: 9 count: 554\n",
      "reward: 15.982339450970178 setps: 12 count: 566\n",
      "reward: 13.074903366122454 setps: 8 count: 574\n",
      "reward: 17.861683710719806 setps: 13 count: 587\n",
      "reward: 14.566726174604263 setps: 13 count: 600\n",
      "reward: 15.696694286954878 setps: 13 count: 613\n",
      "reward: 19.669800819159715 setps: 15 count: 628\n",
      "reward: 16.03928580185311 setps: 11 count: 639\n",
      "reward: 15.914378718014632 setps: 11 count: 650\n",
      "reward: 13.102440853048757 setps: 7 count: 657\n",
      "reward: 17.618380022801286 setps: 16 count: 673\n",
      "reward: 19.715804762375775 setps: 15 count: 688\n",
      "reward: 17.94201492472348 setps: 13 count: 701\n",
      "reward: 12.085757447723882 setps: 8 count: 709\n",
      "reward: 15.273487867224322 setps: 11 count: 720\n",
      "reward: 17.834171069120927 setps: 18 count: 738\n",
      "reward: 18.715817689345567 setps: 17 count: 755\n",
      "reward: 14.797437266143971 setps: 12 count: 767\n",
      "reward: 12.104907780767828 setps: 8 count: 775\n",
      "reward: 16.28500750227686 setps: 12 count: 787\n",
      "reward: 26.443078579950086 setps: 33 count: 820\n",
      "reward: 21.864974904285916 setps: 21 count: 841\n",
      "reward: 16.940205392897767 setps: 13 count: 854\n",
      "reward: 15.434112157528578 setps: 12 count: 866\n",
      "reward: 12.747652808242128 setps: 8 count: 874\n",
      "reward: 24.63804462880071 setps: 39 count: 913\n",
      "reward: 13.530255509416747 setps: 8 count: 921\n",
      "reward: 19.585226617201993 setps: 22 count: 943\n",
      "reward: 21.90765792617749 setps: 24 count: 967\n",
      "reward: 17.693504198580918 setps: 15 count: 982\n",
      "reward: 14.033769243680581 setps: 8 count: 990\n",
      "reward: 13.813665551440499 setps: 10 count: 1000\n",
      "avg rewards: 17.378771882916087\n",
      "Done! (1000, 3)\n",
      "Learning inverse model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/anaconda3/envs/pwil/lib/python3.7/site-packages/ipykernel_launcher.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.070\n",
      "Epoch:10 Batch:10 Loss:0.043\n",
      "Epoch:20 Batch:10 Loss:0.034\n",
      "Epoch:30 Batch:10 Loss:0.027\n",
      "Epoch:40 Batch:10 Loss:0.023\n",
      "Done!\n",
      "######## STEP 2 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 27.70992305003019 setps: 23 count: 23\n",
      "reward: 24.30870364998409 setps: 20 count: 43\n",
      "reward: 28.14831889427005 setps: 23 count: 66\n",
      "reward: 31.856974145905404 setps: 27 count: 93\n",
      "reward: 25.19174199342378 setps: 21 count: 114\n",
      "reward: 24.67039610021747 setps: 21 count: 135\n",
      "reward: 23.50047305035987 setps: 20 count: 155\n",
      "reward: 29.111291222188456 setps: 25 count: 180\n",
      "reward: 24.745713377342323 setps: 21 count: 201\n",
      "reward: 25.20011719787435 setps: 21 count: 222\n",
      "reward: 22.684262043521446 setps: 19 count: 241\n",
      "reward: 22.75751888943487 setps: 19 count: 260\n",
      "reward: 24.91400860860449 setps: 21 count: 281\n",
      "reward: 24.381351894572433 setps: 21 count: 302\n",
      "reward: 23.57206429193902 setps: 21 count: 323\n",
      "reward: 25.006039029307434 setps: 21 count: 344\n",
      "reward: 26.54945244047704 setps: 23 count: 367\n",
      "reward: 26.031051512676644 setps: 22 count: 389\n",
      "reward: 31.779724556287693 setps: 27 count: 416\n",
      "reward: 27.782821625426003 setps: 24 count: 440\n",
      "reward: 23.361973490800302 setps: 20 count: 460\n",
      "reward: 23.534593170970037 setps: 20 count: 480\n",
      "reward: 24.03407165490789 setps: 21 count: 501\n",
      "reward: 24.438631368764618 setps: 21 count: 522\n",
      "reward: 24.796845615832712 setps: 22 count: 544\n",
      "reward: 30.02112214183581 setps: 25 count: 569\n",
      "reward: 21.856072256651533 setps: 20 count: 589\n",
      "reward: 24.740976578481785 setps: 21 count: 610\n",
      "reward: 22.68407570734125 setps: 19 count: 629\n",
      "reward: 25.6571710172182 setps: 22 count: 651\n",
      "reward: 28.104565425183687 setps: 24 count: 675\n",
      "reward: 23.3125688634158 setps: 20 count: 695\n",
      "reward: 29.863237670894883 setps: 25 count: 720\n",
      "reward: 29.218399621648135 setps: 25 count: 745\n",
      "reward: 26.860204992059156 setps: 23 count: 768\n",
      "reward: 24.929751626981304 setps: 21 count: 789\n",
      "reward: 25.334733738117215 setps: 22 count: 811\n",
      "reward: 26.874858723446955 setps: 23 count: 834\n",
      "reward: 25.37393132333964 setps: 21 count: 855\n",
      "reward: 24.311717157885145 setps: 21 count: 876\n",
      "reward: 27.81303319266444 setps: 24 count: 900\n",
      "reward: 23.747660597148933 setps: 20 count: 920\n",
      "reward: 26.64181460242835 setps: 23 count: 943\n",
      "reward: 26.865330188960066 setps: 23 count: 966\n",
      "reward: 26.093708215703376 setps: 22 count: 988\n",
      "avg rewards: 25.7860665892561\n",
      "Done! (2000, 3)\n",
      "Learning inverse model....\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.023\n",
      "Epoch:10 Batch:10 Loss:0.018\n",
      "Epoch:20 Batch:10 Loss:0.014\n",
      "Epoch:30 Batch:10 Loss:0.016\n",
      "Epoch:40 Batch:10 Loss:0.015\n",
      "Done!\n",
      "######## STEP 3 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 112.25850623035078 setps: 98 count: 98\n",
      "reward: 102.5379237434478 setps: 91 count: 189\n",
      "reward: 94.75314685488993 setps: 83 count: 272\n",
      "reward: 95.28742438232729 setps: 82 count: 354\n",
      "reward: 100.02151767620522 setps: 88 count: 442\n",
      "reward: 105.94793312651744 setps: 93 count: 535\n",
      "reward: 103.80066507795122 setps: 96 count: 631\n",
      "reward: 102.02861370721367 setps: 92 count: 723\n",
      "reward: 82.39360385134641 setps: 69 count: 792\n",
      "reward: 80.763298274923 setps: 69 count: 861\n",
      "reward: 67.53285353691462 setps: 56 count: 917\n",
      "reward: 79.54318611329653 setps: 67 count: 984\n",
      "avg rewards: 93.90572271461532\n",
      "Done! (3000, 3)\n",
      "Learning inverse model....\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.025\n",
      "Epoch:10 Batch:10 Loss:0.018\n",
      "Epoch:20 Batch:10 Loss:0.018\n",
      "Epoch:30 Batch:10 Loss:0.015\n",
      "Epoch:40 Batch:10 Loss:0.018\n",
      "Done!\n",
      "######## STEP 4 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 36.42617385197082 setps: 30 count: 30\n",
      "reward: 32.08894853488163 setps: 25 count: 55\n",
      "reward: 28.502445092344715 setps: 23 count: 78\n",
      "reward: 34.698813357022296 setps: 30 count: 108\n",
      "reward: 32.205656883580374 setps: 26 count: 134\n",
      "reward: 34.62489926851995 setps: 29 count: 163\n",
      "reward: 34.57783420198538 setps: 28 count: 191\n",
      "reward: 35.342323073577425 setps: 28 count: 219\n",
      "reward: 37.939878165496346 setps: 31 count: 250\n",
      "reward: 38.473847825745175 setps: 32 count: 282\n",
      "reward: 36.30743978634272 setps: 30 count: 312\n",
      "reward: 27.218726602349488 setps: 21 count: 333\n",
      "reward: 39.31870171055343 setps: 33 count: 366\n",
      "reward: 34.49028773588361 setps: 28 count: 394\n",
      "reward: 33.50488512445445 setps: 28 count: 422\n",
      "reward: 34.92150871522754 setps: 29 count: 451\n",
      "reward: 34.22798846088698 setps: 28 count: 479\n",
      "reward: 36.60408561582736 setps: 30 count: 509\n",
      "reward: 35.71596567491943 setps: 29 count: 538\n",
      "reward: 36.22322394045477 setps: 31 count: 569\n",
      "reward: 29.230729572410922 setps: 24 count: 593\n",
      "reward: 31.872396092941926 setps: 26 count: 619\n",
      "reward: 28.348196862751504 setps: 23 count: 642\n",
      "reward: 35.69622192023816 setps: 30 count: 672\n",
      "reward: 30.14268816292606 setps: 24 count: 696\n",
      "reward: 37.40120451742259 setps: 31 count: 727\n",
      "reward: 32.851744951633734 setps: 27 count: 754\n",
      "reward: 35.20896472074527 setps: 29 count: 783\n",
      "reward: 35.88206560790132 setps: 29 count: 812\n",
      "reward: 35.66170112085237 setps: 30 count: 842\n",
      "reward: 31.43768526729109 setps: 25 count: 867\n",
      "reward: 37.975586105525146 setps: 32 count: 899\n",
      "reward: 27.976116064650704 setps: 22 count: 921\n",
      "reward: 38.906585625969456 setps: 33 count: 954\n",
      "reward: 30.79962464896816 setps: 25 count: 979\n",
      "avg rewards: 34.0801469961215\n",
      "Done! (4000, 3)\n",
      "Learning inverse model....\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.023\n",
      "Epoch:10 Batch:10 Loss:0.018\n",
      "Epoch:20 Batch:10 Loss:0.018\n",
      "Epoch:30 Batch:10 Loss:0.015\n",
      "Epoch:40 Batch:10 Loss:0.015\n",
      "Done!\n",
      "######## STEP 5 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 97.64466027048911 setps: 77 count: 77\n",
      "reward: 77.84538273861105 setps: 62 count: 139\n",
      "reward: 41.09457749791472 setps: 32 count: 171\n",
      "reward: 31.785373136910486 setps: 25 count: 196\n",
      "reward: 96.09461644615804 setps: 76 count: 272\n",
      "reward: 27.66443279620144 setps: 21 count: 293\n",
      "reward: 39.74576941815029 setps: 31 count: 324\n",
      "reward: 32.564882458987995 setps: 25 count: 349\n",
      "reward: 104.0595619570653 setps: 87 count: 436\n",
      "reward: 26.805015069626098 setps: 20 count: 456\n",
      "reward: 88.44953559552522 setps: 69 count: 525\n",
      "reward: 41.302154654369225 setps: 33 count: 558\n",
      "reward: 27.403556106143515 setps: 21 count: 579\n",
      "reward: 86.59388011726695 setps: 68 count: 647\n",
      "reward: 84.63766215023934 setps: 67 count: 714\n",
      "reward: 36.682059232980826 setps: 29 count: 743\n",
      "reward: 36.87844965172698 setps: 28 count: 771\n",
      "reward: 38.75386916231802 setps: 31 count: 802\n",
      "reward: 92.61775155713696 setps: 73 count: 875\n",
      "reward: 29.319613219563323 setps: 22 count: 897\n",
      "reward: 84.67003774076876 setps: 67 count: 964\n",
      "reward: 40.75176128946186 setps: 33 count: 997\n",
      "avg rewards: 57.42566373943707\n",
      "Done! (5000, 3)\n",
      "Learning inverse model....\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.021\n",
      "Epoch:10 Batch:10 Loss:0.016\n",
      "Epoch:20 Batch:10 Loss:0.016\n",
      "Epoch:30 Batch:10 Loss:0.016\n",
      "Epoch:40 Batch:10 Loss:0.013\n",
      "Done!\n",
      "######## STEP 6 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 66.88916037726887 setps: 54 count: 54\n",
      "reward: 83.78674751212819 setps: 67 count: 121\n",
      "reward: 77.14008500677153 setps: 62 count: 183\n",
      "reward: 76.35225186661845 setps: 60 count: 243\n",
      "reward: 71.6176755040622 setps: 56 count: 299\n",
      "reward: 88.94547634225457 setps: 69 count: 368\n",
      "reward: 76.05788464051295 setps: 59 count: 427\n",
      "reward: 81.99876956265359 setps: 69 count: 496\n",
      "reward: 96.90026128858443 setps: 75 count: 571\n",
      "reward: 86.77225888250396 setps: 69 count: 640\n",
      "reward: 66.36014201037032 setps: 54 count: 694\n",
      "reward: 75.3903890929854 setps: 59 count: 753\n",
      "reward: 84.6770342121119 setps: 65 count: 818\n",
      "reward: 84.59677865698465 setps: 66 count: 884\n",
      "reward: 87.73820614840514 setps: 72 count: 956\n",
      "avg rewards: 80.34820807361442\n",
      "Done! (6000, 3)\n",
      "Learning inverse model....\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.015\n",
      "Epoch:10 Batch:10 Loss:0.014\n",
      "Epoch:20 Batch:10 Loss:0.013\n",
      "Epoch:30 Batch:10 Loss:0.012\n",
      "Epoch:40 Batch:10 Loss:0.013\n",
      "Done!\n",
      "######## STEP 7 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 30.895840349007635 setps: 23 count: 23\n",
      "reward: 33.116106883499015 setps: 25 count: 48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 83.21225123064798 setps: 64 count: 112\n",
      "reward: 87.34980113455528 setps: 69 count: 181\n",
      "reward: 95.48670055959231 setps: 72 count: 253\n",
      "reward: 94.84628457068175 setps: 78 count: 331\n",
      "reward: 75.3364926222508 setps: 60 count: 391\n",
      "reward: 76.83131012528024 setps: 63 count: 454\n",
      "reward: 29.172565433582342 setps: 23 count: 477\n",
      "reward: 88.4841854680373 setps: 72 count: 549\n",
      "reward: 89.33303137749607 setps: 70 count: 619\n",
      "reward: 81.04748217390004 setps: 63 count: 682\n",
      "reward: 103.54271847527998 setps: 84 count: 766\n",
      "reward: 95.38515274180531 setps: 76 count: 842\n",
      "reward: 99.31917582450987 setps: 79 count: 921\n",
      "reward: 98.23324892977396 setps: 79 count: 1000\n",
      "avg rewards: 78.84952174374374\n",
      "Done! (7000, 3)\n",
      "Learning inverse model....\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.018\n",
      "Epoch:10 Batch:10 Loss:0.017\n",
      "Epoch:20 Batch:10 Loss:0.012\n",
      "Epoch:30 Batch:10 Loss:0.015\n",
      "Epoch:40 Batch:10 Loss:0.015\n",
      "Done!\n",
      "######## STEP 8 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 72.11943069977134 setps: 56 count: 56\n",
      "reward: 85.98785263007736 setps: 65 count: 121\n",
      "reward: 84.3249785061373 setps: 66 count: 187\n",
      "reward: 86.22265529808642 setps: 67 count: 254\n",
      "reward: 81.26976677186613 setps: 62 count: 316\n",
      "reward: 86.05956564362715 setps: 65 count: 381\n",
      "reward: 100.07456289977709 setps: 79 count: 460\n",
      "reward: 80.69579372651495 setps: 63 count: 523\n",
      "reward: 87.1080315912201 setps: 70 count: 593\n",
      "reward: 77.36219791540499 setps: 60 count: 653\n",
      "reward: 81.27059066596267 setps: 64 count: 717\n",
      "reward: 76.1429862967692 setps: 59 count: 776\n",
      "reward: 104.04257224854373 setps: 79 count: 855\n",
      "reward: 85.22123776569208 setps: 66 count: 921\n",
      "reward: 69.57858579749154 setps: 52 count: 973\n",
      "avg rewards: 83.83205389712948\n",
      "Done! (8000, 3)\n",
      "Learning inverse model....\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.016\n",
      "Epoch:10 Batch:10 Loss:0.014\n",
      "Epoch:20 Batch:10 Loss:0.015\n",
      "Epoch:30 Batch:10 Loss:0.013\n",
      "Epoch:40 Batch:10 Loss:0.017\n",
      "Done!\n",
      "######## STEP 9 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 97.57391382517966 setps: 81 count: 81\n",
      "reward: 99.75158775837916 setps: 79 count: 160\n",
      "reward: 97.02216290791552 setps: 75 count: 235\n",
      "reward: 94.65507100075628 setps: 70 count: 305\n",
      "reward: 77.73572374291108 setps: 59 count: 364\n",
      "reward: 25.047997208614827 setps: 18 count: 382\n",
      "reward: 77.51130231374844 setps: 57 count: 439\n",
      "reward: 93.53361686417345 setps: 71 count: 510\n",
      "reward: 69.62488344162848 setps: 54 count: 564\n",
      "reward: 78.46439784242394 setps: 63 count: 627\n",
      "reward: 84.15710747261762 setps: 62 count: 689\n",
      "reward: 89.47351281864245 setps: 68 count: 757\n",
      "reward: 90.16216526475473 setps: 68 count: 825\n",
      "reward: 77.90621790158765 setps: 60 count: 885\n",
      "reward: 26.11592227899091 setps: 19 count: 904\n",
      "reward: 80.44697639429796 setps: 62 count: 966\n",
      "avg rewards: 78.69890993978888\n",
      "Done! (9000, 3)\n",
      "Learning inverse model....\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.014\n",
      "Epoch:10 Batch:10 Loss:0.014\n",
      "Epoch:20 Batch:10 Loss:0.016\n",
      "Epoch:30 Batch:10 Loss:0.014\n",
      "Epoch:40 Batch:10 Loss:0.013\n",
      "Done!\n",
      "######## STEP 10 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 78.27216267851036 setps: 63 count: 63\n",
      "reward: 78.7400758400341 setps: 63 count: 126\n",
      "reward: 89.78215266450133 setps: 73 count: 199\n",
      "reward: 76.07512128452362 setps: 57 count: 256\n",
      "reward: 77.41565828548917 setps: 58 count: 314\n",
      "reward: 88.71762427478535 setps: 66 count: 380\n",
      "reward: 76.89094665946465 setps: 59 count: 439\n",
      "reward: 77.01666452412319 setps: 57 count: 496\n",
      "reward: 79.43946174243176 setps: 60 count: 556\n",
      "reward: 73.84390044372269 setps: 56 count: 612\n",
      "reward: 88.81920281926288 setps: 67 count: 679\n",
      "reward: 93.8608152108631 setps: 71 count: 750\n",
      "reward: 95.44165549170867 setps: 76 count: 826\n",
      "reward: 85.00487814000516 setps: 65 count: 891\n",
      "reward: 84.78741427288624 setps: 64 count: 955\n",
      "avg rewards: 82.94051562215414\n",
      "Done! (10000, 3)\n",
      "Learning inverse model....\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.017\n",
      "Epoch:10 Batch:10 Loss:0.013\n",
      "Epoch:20 Batch:10 Loss:0.013\n",
      "Epoch:30 Batch:10 Loss:0.014\n",
      "Epoch:40 Batch:10 Loss:0.014\n",
      "Done!\n",
      "######## STEP 11 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 68.5926588882663 setps: 53 count: 53\n",
      "reward: 94.52873719090714 setps: 77 count: 130\n",
      "reward: 102.77555838123227 setps: 81 count: 211\n",
      "reward: 90.78767803844387 setps: 69 count: 280\n",
      "reward: 79.6467936342786 setps: 65 count: 345\n",
      "reward: 84.65986321682722 setps: 66 count: 411\n",
      "reward: 76.35751882495967 setps: 59 count: 470\n",
      "reward: 88.39842285501364 setps: 74 count: 544\n",
      "reward: 90.38069535744725 setps: 77 count: 621\n",
      "reward: 75.9778284288608 setps: 60 count: 681\n",
      "reward: 91.70235530195059 setps: 69 count: 750\n",
      "reward: 82.11158546005025 setps: 61 count: 811\n",
      "reward: 70.2243895372405 setps: 55 count: 866\n",
      "reward: 75.70391264811624 setps: 60 count: 926\n",
      "reward: 79.42442141561766 setps: 60 count: 986\n",
      "avg rewards: 83.41816127861415\n",
      "Done! (11000, 3)\n",
      "Learning inverse model....\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.015\n",
      "Epoch:10 Batch:10 Loss:0.015\n",
      "Epoch:20 Batch:10 Loss:0.013\n",
      "Epoch:30 Batch:10 Loss:0.013\n",
      "Epoch:40 Batch:10 Loss:0.014\n",
      "Done!\n",
      "######## STEP 12 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 73.22951318133127 setps: 58 count: 58\n",
      "reward: 82.10680783016143 setps: 68 count: 126\n",
      "reward: 77.80437166408956 setps: 63 count: 189\n",
      "reward: 82.93949616886967 setps: 63 count: 252\n",
      "reward: 78.01148498488035 setps: 61 count: 313\n",
      "reward: 78.99253785259789 setps: 61 count: 374\n",
      "reward: 99.52333257121792 setps: 81 count: 455\n",
      "reward: 76.20278010807522 setps: 59 count: 514\n",
      "reward: 87.36965351169202 setps: 71 count: 585\n",
      "reward: 80.2081944557722 setps: 63 count: 648\n",
      "reward: 94.43546680836155 setps: 81 count: 729\n",
      "reward: 71.8835880339233 setps: 57 count: 786\n",
      "reward: 98.9724145583721 setps: 77 count: 863\n",
      "reward: 86.83369125140305 setps: 68 count: 931\n",
      "reward: 84.1581431434053 setps: 67 count: 998\n",
      "avg rewards: 83.5114317416102\n",
      "Done! (12000, 3)\n",
      "Learning inverse model....\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.016\n",
      "Epoch:10 Batch:10 Loss:0.015\n",
      "Epoch:20 Batch:10 Loss:0.014\n",
      "Epoch:30 Batch:10 Loss:0.015\n",
      "Epoch:40 Batch:10 Loss:0.015\n",
      "Done!\n",
      "######## STEP 13 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 87.22006522515584 setps: 69 count: 69\n",
      "reward: 92.63895171133483 setps: 70 count: 139\n",
      "reward: 88.52207338307925 setps: 67 count: 206\n",
      "reward: 78.98883029496935 setps: 59 count: 265\n",
      "reward: 77.83983447685314 setps: 61 count: 326\n",
      "reward: 93.83602009389583 setps: 81 count: 407\n",
      "reward: 81.25584405281259 setps: 64 count: 471\n",
      "reward: 86.49047963876366 setps: 66 count: 537\n",
      "reward: 73.03291896556618 setps: 56 count: 593\n",
      "reward: 87.15756327375419 setps: 68 count: 661\n",
      "reward: 89.8368081413166 setps: 71 count: 732\n",
      "reward: 75.77107060105627 setps: 59 count: 791\n",
      "reward: 95.47997911197018 setps: 74 count: 865\n",
      "reward: 79.13060372305773 setps: 59 count: 924\n",
      "reward: 93.65268575421652 setps: 75 count: 999\n",
      "avg rewards: 85.3902485631868\n",
      "Done! (13000, 3)\n",
      "Learning inverse model....\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.016\n",
      "Epoch:10 Batch:10 Loss:0.015\n",
      "Epoch:20 Batch:10 Loss:0.014\n",
      "Epoch:30 Batch:10 Loss:0.014\n",
      "Epoch:40 Batch:10 Loss:0.014\n",
      "Done!\n",
      "######## STEP 14 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 83.08417042998624 setps: 64 count: 64\n",
      "reward: 98.01833166352883 setps: 76 count: 140\n",
      "reward: 78.91810602685726 setps: 61 count: 201\n",
      "reward: 90.75970436474888 setps: 66 count: 267\n",
      "reward: 80.45357318716853 setps: 62 count: 329\n",
      "reward: 85.72554938800313 setps: 63 count: 392\n",
      "reward: 82.4692102108849 setps: 63 count: 455\n",
      "reward: 89.68454926623525 setps: 66 count: 521\n",
      "reward: 107.41289041589481 setps: 88 count: 609\n",
      "reward: 89.30087535188359 setps: 68 count: 677\n",
      "reward: 90.20398253542804 setps: 71 count: 748\n",
      "reward: 71.01597093870659 setps: 56 count: 804\n",
      "reward: 91.16222203853685 setps: 71 count: 875\n",
      "reward: 83.00646325514828 setps: 63 count: 938\n",
      "reward: 75.98361150598792 setps: 58 count: 996\n",
      "avg rewards: 86.47994737193328\n",
      "Done! (14000, 3)\n",
      "Learning inverse model....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:100 Loss:0.01254\n",
      "Epoch:1 Batch:100 Loss:0.00847\n",
      "Epoch:2 Batch:100 Loss:0.00850\n",
      "Epoch:3 Batch:100 Loss:0.00856\n",
      "Epoch:4 Batch:100 Loss:0.00868\n",
      "Epoch:5 Batch:100 Loss:0.00863\n",
      "Epoch:6 Batch:100 Loss:0.00867\n",
      "Epoch:7 Batch:100 Loss:0.00866\n",
      "Epoch:8 Batch:100 Loss:0.00861\n",
      "Epoch:9 Batch:100 Loss:0.00874\n",
      "Epoch:10 Batch:100 Loss:0.00875\n",
      "Epoch:11 Batch:100 Loss:0.00878\n",
      "Epoch:12 Batch:100 Loss:0.00877\n",
      "Epoch:13 Batch:100 Loss:0.00875\n",
      "Epoch:14 Batch:100 Loss:0.00872\n",
      "Epoch:15 Batch:100 Loss:0.00880\n",
      "Epoch:16 Batch:100 Loss:0.00880\n",
      "Epoch:17 Batch:100 Loss:0.00870\n",
      "Epoch:18 Batch:100 Loss:0.00877\n",
      "Epoch:19 Batch:100 Loss:0.00873\n",
      "Epoch:20 Batch:100 Loss:0.00878\n",
      "Epoch:21 Batch:100 Loss:0.00875\n",
      "Epoch:22 Batch:100 Loss:0.00878\n",
      "Epoch:23 Batch:100 Loss:0.00882\n",
      "Epoch:24 Batch:100 Loss:0.00874\n",
      "Epoch:25 Batch:100 Loss:0.00884\n",
      "Epoch:26 Batch:100 Loss:0.00871\n",
      "Epoch:27 Batch:100 Loss:0.00870\n",
      "Epoch:28 Batch:100 Loss:0.00887\n",
      "Epoch:29 Batch:100 Loss:0.00877\n",
      "Epoch:30 Batch:100 Loss:0.00868\n",
      "Epoch:31 Batch:100 Loss:0.00874\n",
      "Epoch:32 Batch:100 Loss:0.00893\n",
      "Epoch:33 Batch:100 Loss:0.00872\n",
      "Epoch:34 Batch:100 Loss:0.00886\n",
      "Epoch:35 Batch:100 Loss:0.00870\n",
      "Epoch:36 Batch:100 Loss:0.00881\n",
      "Epoch:37 Batch:100 Loss:0.00871\n",
      "Epoch:38 Batch:100 Loss:0.00889\n",
      "Epoch:39 Batch:100 Loss:0.00870\n",
      "Epoch:40 Batch:100 Loss:0.00871\n",
      "Epoch:41 Batch:100 Loss:0.00873\n",
      "Epoch:42 Batch:100 Loss:0.00873\n",
      "Epoch:43 Batch:100 Loss:0.00884\n",
      "Epoch:44 Batch:100 Loss:0.00866\n",
      "Epoch:45 Batch:100 Loss:0.00874\n",
      "Epoch:46 Batch:100 Loss:0.00879\n",
      "Epoch:47 Batch:100 Loss:0.00866\n",
      "Epoch:48 Batch:100 Loss:0.00872\n",
      "Epoch:49 Batch:100 Loss:0.00880\n",
      "Epoch:50 Batch:100 Loss:0.00870\n",
      "Epoch:51 Batch:100 Loss:0.00872\n",
      "Epoch:52 Batch:100 Loss:0.00885\n",
      "Epoch:53 Batch:100 Loss:0.00877\n",
      "Epoch:54 Batch:100 Loss:0.00877\n",
      "Epoch:55 Batch:100 Loss:0.00857\n",
      "Epoch:56 Batch:100 Loss:0.00862\n",
      "Epoch:57 Batch:100 Loss:0.00870\n",
      "Epoch:58 Batch:100 Loss:0.00881\n",
      "Epoch:59 Batch:100 Loss:0.00876\n",
      "Epoch:60 Batch:100 Loss:0.00879\n",
      "Epoch:61 Batch:100 Loss:0.00866\n",
      "Epoch:62 Batch:100 Loss:0.00880\n",
      "Epoch:63 Batch:100 Loss:0.00873\n",
      "Epoch:64 Batch:100 Loss:0.00867\n",
      "Epoch:65 Batch:100 Loss:0.00877\n",
      "Epoch:66 Batch:100 Loss:0.00880\n",
      "Epoch:67 Batch:100 Loss:0.00858\n",
      "Epoch:68 Batch:100 Loss:0.00875\n",
      "Epoch:69 Batch:100 Loss:0.00869\n",
      "Epoch:70 Batch:100 Loss:0.00860\n",
      "Epoch:71 Batch:100 Loss:0.00872\n",
      "Epoch:72 Batch:100 Loss:0.00865\n",
      "Epoch:73 Batch:100 Loss:0.00877\n",
      "Epoch:74 Batch:100 Loss:0.00867\n",
      "Epoch:75 Batch:100 Loss:0.00857\n",
      "Epoch:76 Batch:100 Loss:0.00868\n",
      "Epoch:77 Batch:100 Loss:0.00865\n",
      "Epoch:78 Batch:100 Loss:0.00862\n",
      "Epoch:79 Batch:100 Loss:0.00854\n",
      "Epoch:80 Batch:100 Loss:0.00868\n",
      "Epoch:81 Batch:100 Loss:0.00868\n",
      "Epoch:82 Batch:100 Loss:0.00871\n",
      "Epoch:83 Batch:100 Loss:0.00872\n",
      "Epoch:84 Batch:100 Loss:0.00863\n",
      "Epoch:85 Batch:100 Loss:0.00883\n",
      "Epoch:86 Batch:100 Loss:0.00877\n",
      "Epoch:87 Batch:100 Loss:0.00863\n",
      "Epoch:88 Batch:100 Loss:0.00869\n",
      "Epoch:89 Batch:100 Loss:0.00861\n",
      "Epoch:90 Batch:100 Loss:0.00866\n",
      "Epoch:91 Batch:100 Loss:0.00861\n",
      "Epoch:92 Batch:100 Loss:0.00869\n",
      "Epoch:93 Batch:100 Loss:0.00869\n",
      "Epoch:94 Batch:100 Loss:0.00861\n",
      "Epoch:95 Batch:100 Loss:0.00855\n",
      "Epoch:96 Batch:100 Loss:0.00879\n",
      "Epoch:97 Batch:100 Loss:0.00875\n",
      "Epoch:98 Batch:100 Loss:0.00864\n",
      "Epoch:99 Batch:100 Loss:0.00863\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.015\n",
      "Epoch:10 Batch:10 Loss:0.015\n",
      "Epoch:20 Batch:10 Loss:0.015\n",
      "Epoch:30 Batch:10 Loss:0.016\n",
      "Epoch:40 Batch:10 Loss:0.013\n",
      "Done!\n",
      "######## STEP 15 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 86.26263477351311 setps: 69 count: 69\n",
      "reward: 85.88003414286794 setps: 68 count: 137\n",
      "reward: 90.75490651286529 setps: 75 count: 212\n",
      "reward: 96.94743690548928 setps: 74 count: 286\n",
      "reward: 89.05494447720267 setps: 68 count: 354\n",
      "reward: 78.04886602104814 setps: 65 count: 419\n",
      "reward: 81.52506296937064 setps: 61 count: 480\n",
      "reward: 94.06605980365858 setps: 78 count: 558\n",
      "reward: 83.69876663919422 setps: 65 count: 623\n",
      "reward: 95.00551547599459 setps: 78 count: 701\n",
      "reward: 76.7116526346217 setps: 58 count: 759\n",
      "reward: 94.84983052577589 setps: 78 count: 837\n",
      "reward: 87.15896551087498 setps: 72 count: 909\n",
      "reward: 80.36075467359768 setps: 60 count: 969\n",
      "avg rewards: 87.16610221900534\n",
      "Done! (15000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:100 Loss:0.01319\n",
      "Epoch:1 Batch:100 Loss:0.00810\n",
      "Epoch:2 Batch:100 Loss:0.00813\n",
      "Epoch:3 Batch:100 Loss:0.00813\n",
      "Epoch:4 Batch:100 Loss:0.00812\n",
      "Epoch:5 Batch:100 Loss:0.00821\n",
      "Epoch:6 Batch:100 Loss:0.00821\n",
      "Epoch:7 Batch:100 Loss:0.00832\n",
      "Epoch:8 Batch:100 Loss:0.00837\n",
      "Epoch:9 Batch:100 Loss:0.00836\n",
      "Epoch:10 Batch:100 Loss:0.00835\n",
      "Epoch:11 Batch:100 Loss:0.00836\n",
      "Epoch:12 Batch:100 Loss:0.00826\n",
      "Epoch:13 Batch:100 Loss:0.00840\n",
      "Epoch:14 Batch:100 Loss:0.00830\n",
      "Epoch:15 Batch:100 Loss:0.00833\n",
      "Epoch:16 Batch:100 Loss:0.00845\n",
      "Epoch:17 Batch:100 Loss:0.00829\n",
      "Epoch:18 Batch:100 Loss:0.00833\n",
      "Epoch:19 Batch:100 Loss:0.00840\n",
      "Epoch:20 Batch:100 Loss:0.00851\n",
      "Epoch:21 Batch:100 Loss:0.00862\n",
      "Epoch:22 Batch:100 Loss:0.00829\n",
      "Epoch:23 Batch:100 Loss:0.00839\n",
      "Epoch:24 Batch:100 Loss:0.00833\n",
      "Epoch:25 Batch:100 Loss:0.00844\n",
      "Epoch:26 Batch:100 Loss:0.00832\n",
      "Epoch:27 Batch:100 Loss:0.00833\n",
      "Epoch:28 Batch:100 Loss:0.00834\n",
      "Epoch:29 Batch:100 Loss:0.00845\n",
      "Epoch:30 Batch:100 Loss:0.00836\n",
      "Epoch:31 Batch:100 Loss:0.00844\n",
      "Epoch:32 Batch:100 Loss:0.00837\n",
      "Epoch:33 Batch:100 Loss:0.00839\n",
      "Epoch:34 Batch:100 Loss:0.00839\n",
      "Epoch:35 Batch:100 Loss:0.00845\n",
      "Epoch:36 Batch:100 Loss:0.00843\n",
      "Epoch:37 Batch:100 Loss:0.00825\n",
      "Epoch:38 Batch:100 Loss:0.00847\n",
      "Epoch:39 Batch:100 Loss:0.00833\n",
      "Epoch:40 Batch:100 Loss:0.00829\n",
      "Epoch:41 Batch:100 Loss:0.00839\n",
      "Epoch:42 Batch:100 Loss:0.00833\n",
      "Epoch:43 Batch:100 Loss:0.00845\n",
      "Epoch:44 Batch:100 Loss:0.00829\n",
      "Epoch:45 Batch:100 Loss:0.00827\n",
      "Epoch:46 Batch:100 Loss:0.00841\n",
      "Epoch:47 Batch:100 Loss:0.00840\n",
      "Epoch:48 Batch:100 Loss:0.00835\n",
      "Epoch:49 Batch:100 Loss:0.00836\n",
      "Epoch:50 Batch:100 Loss:0.00843\n",
      "Epoch:51 Batch:100 Loss:0.00822\n",
      "Epoch:52 Batch:100 Loss:0.00834\n",
      "Epoch:53 Batch:100 Loss:0.00838\n",
      "Epoch:54 Batch:100 Loss:0.00864\n",
      "Epoch:55 Batch:100 Loss:0.00831\n",
      "Epoch:56 Batch:100 Loss:0.00837\n",
      "Epoch:57 Batch:100 Loss:0.00830\n",
      "Epoch:58 Batch:100 Loss:0.00834\n",
      "Epoch:59 Batch:100 Loss:0.00858\n",
      "Epoch:60 Batch:100 Loss:0.00834\n",
      "Epoch:61 Batch:100 Loss:0.00833\n",
      "Epoch:62 Batch:100 Loss:0.00833\n",
      "Epoch:63 Batch:100 Loss:0.00837\n",
      "Epoch:64 Batch:100 Loss:0.00834\n",
      "Epoch:65 Batch:100 Loss:0.00830\n",
      "Epoch:66 Batch:100 Loss:0.00830\n",
      "Epoch:67 Batch:100 Loss:0.00832\n",
      "Epoch:68 Batch:100 Loss:0.00842\n",
      "Epoch:69 Batch:100 Loss:0.00833\n",
      "Epoch:70 Batch:100 Loss:0.00836\n",
      "Epoch:71 Batch:100 Loss:0.00829\n",
      "Epoch:72 Batch:100 Loss:0.00827\n",
      "Epoch:73 Batch:100 Loss:0.00836\n",
      "Epoch:74 Batch:100 Loss:0.00823\n",
      "Epoch:75 Batch:100 Loss:0.00825\n",
      "Epoch:76 Batch:100 Loss:0.00826\n",
      "Epoch:77 Batch:100 Loss:0.00831\n",
      "Epoch:78 Batch:100 Loss:0.00833\n",
      "Epoch:79 Batch:100 Loss:0.00838\n",
      "Epoch:80 Batch:100 Loss:0.00825\n",
      "Epoch:81 Batch:100 Loss:0.00828\n",
      "Epoch:82 Batch:100 Loss:0.00829\n",
      "Epoch:83 Batch:100 Loss:0.00835\n",
      "Epoch:84 Batch:100 Loss:0.00836\n",
      "Epoch:85 Batch:100 Loss:0.00839\n",
      "Epoch:86 Batch:100 Loss:0.00827\n",
      "Epoch:87 Batch:100 Loss:0.00845\n",
      "Epoch:88 Batch:100 Loss:0.00824\n",
      "Epoch:89 Batch:100 Loss:0.00823\n",
      "Epoch:90 Batch:100 Loss:0.00829\n",
      "Epoch:91 Batch:100 Loss:0.00838\n",
      "Epoch:92 Batch:100 Loss:0.00833\n",
      "Epoch:93 Batch:100 Loss:0.00823\n",
      "Epoch:94 Batch:100 Loss:0.00825\n",
      "Epoch:95 Batch:100 Loss:0.00832\n",
      "Epoch:96 Batch:100 Loss:0.00830\n",
      "Epoch:97 Batch:100 Loss:0.00829\n",
      "Epoch:98 Batch:100 Loss:0.00838\n",
      "Epoch:99 Batch:100 Loss:0.00835\n",
      "Done!\n",
      "Getting labels for demos....\n",
      "Done!\n",
      "Learning policy....\n",
      "Epoch:0 Batch:10 Loss:0.018\n",
      "Epoch:10 Batch:10 Loss:0.013\n",
      "Epoch:20 Batch:10 Loss:0.013\n",
      "Epoch:30 Batch:10 Loss:0.014\n",
      "Epoch:40 Batch:10 Loss:0.013\n",
      "Done!\n",
      "######## STEP 16 #######\n",
      "Collecting transitions for learning inverse model....\n",
      "reward: 84.5696792464456 setps: 61 count: 61\n",
      "reward: 74.70806734390354 setps: 60 count: 121\n",
      "reward: 81.15782249137555 setps: 63 count: 184\n",
      "reward: 72.37128401664955 setps: 58 count: 242\n",
      "reward: 79.54280044061018 setps: 59 count: 301\n",
      "reward: 96.37686083596637 setps: 73 count: 374\n",
      "reward: 76.80344162162801 setps: 59 count: 433\n",
      "reward: 78.67570061559381 setps: 62 count: 495\n",
      "reward: 78.47626503859065 setps: 58 count: 553\n",
      "reward: 77.1656762928833 setps: 61 count: 614\n",
      "reward: 89.05454233177737 setps: 71 count: 685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 87.85264659883546 setps: 66 count: 751\n",
      "reward: 89.03585727064198 setps: 67 count: 818\n",
      "reward: 84.3591761335061 setps: 64 count: 882\n",
      "reward: 76.64350509285433 setps: 59 count: 941\n",
      "avg rewards: 81.78622169141745\n",
      "Done! (16000, 3)\n",
      "Learning inverse model....\n",
      "Epoch:0 Batch:100 Loss:0.01095\n",
      "Epoch:1 Batch:100 Loss:0.00782\n",
      "Epoch:2 Batch:100 Loss:0.00792\n",
      "Epoch:3 Batch:100 Loss:0.00787\n",
      "Epoch:4 Batch:100 Loss:0.00799\n",
      "Epoch:5 Batch:100 Loss:0.00794\n",
      "Epoch:6 Batch:100 Loss:0.00796\n",
      "Epoch:7 Batch:100 Loss:0.00802\n",
      "Epoch:8 Batch:100 Loss:0.00799\n",
      "Epoch:9 Batch:100 Loss:0.00815\n",
      "Epoch:10 Batch:100 Loss:0.00799\n",
      "Epoch:11 Batch:100 Loss:0.00804\n",
      "Epoch:12 Batch:100 Loss:0.00815\n",
      "Epoch:13 Batch:100 Loss:0.00796\n",
      "Epoch:14 Batch:100 Loss:0.00788\n",
      "Epoch:15 Batch:100 Loss:0.00805\n",
      "Epoch:16 Batch:100 Loss:0.00803\n",
      "Epoch:17 Batch:100 Loss:0.00810\n",
      "Epoch:18 Batch:100 Loss:0.00804\n",
      "Epoch:19 Batch:100 Loss:0.00796\n",
      "Epoch:20 Batch:100 Loss:0.00805\n",
      "Epoch:21 Batch:100 Loss:0.00806\n",
      "Epoch:22 Batch:100 Loss:0.00795\n",
      "Epoch:23 Batch:100 Loss:0.00806\n",
      "Epoch:24 Batch:100 Loss:0.00814\n",
      "Epoch:25 Batch:100 Loss:0.00807\n",
      "Epoch:26 Batch:100 Loss:0.00803\n",
      "Epoch:27 Batch:100 Loss:0.00805\n",
      "Epoch:28 Batch:100 Loss:0.00796\n",
      "Epoch:29 Batch:100 Loss:0.00827\n",
      "Epoch:30 Batch:100 Loss:0.00803\n",
      "Epoch:31 Batch:100 Loss:0.00799\n",
      "Epoch:32 Batch:100 Loss:0.00802\n",
      "Epoch:33 Batch:100 Loss:0.00814\n",
      "Epoch:34 Batch:100 Loss:0.00802\n",
      "Epoch:35 Batch:100 Loss:0.00811\n",
      "Epoch:36 Batch:100 Loss:0.00801\n",
      "Epoch:37 Batch:100 Loss:0.00798\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5958d36320d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     81\u001b[0m                     \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_prime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                     \u001b[0minv_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                     \u001b[0ma_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minv_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_prime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minv_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pwil/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/rl/practical_IL/state_prediction_rl/bco/BCO/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, s, s_prime)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muncertain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pwil/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pwil/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pwil/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env_list = [\"Pendulum-v0\", \"BipedalWalker-v3\", \"Walker2DBulletEnv-v0\", \"HopperBulletEnv-v0\", \"HalfCheetahBulletEnv-v0\", \"AntBulletEnv-v0\", \"HumanoidBulletEnv-v0\"]\n",
    "\n",
    "runs = 20\n",
    "inv_samples = 1000\n",
    "max_steps = 800\n",
    "expert_path='experts/'\n",
    "weight_path=\"weights/\"\n",
    "        \n",
    "test_rewards_envs = []\n",
    "record_folder = \"records/bco/\"\n",
    "init_seeds = [0]\n",
    "itr_per_env = len(init_seeds)\n",
    "\n",
    "for itr_id in range(itr_per_env):\n",
    "    seed = init_seeds[itr_id]\n",
    "    for en in env_list[2:]:\n",
    "        print(\"############# start \"+en+\" training ###################\")\n",
    "\n",
    "        ENV_NAME = en#env_list[3]\n",
    "        env=ENV_NAME\n",
    "        \n",
    "        DEMO_DIR = os.path.join(expert_path, env+'.pkl')\n",
    "        M = inv_samples\n",
    "\n",
    "        record_fn = record_folder + ENV_NAME + str(itr_id) + \".txt\"\n",
    "\n",
    "        \"\"\"load demonstrations\"\"\"\n",
    "        demos = load_demos(DEMO_DIR)\n",
    "\n",
    "        \"\"\"create environments\"\"\"\n",
    "        env = gym.make(ENV_NAME)\n",
    "        obs_dim, act_dim = env.observation_space.shape[0], env.action_space.shape[0]\n",
    "\n",
    "        \"\"\"init random seeds for reproduction\"\"\"\n",
    "        torch.manual_seed(seed)\n",
    "        env.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "\n",
    "        policy = policy_continuous(env.observation_space.shape[0],64,env.action_space.shape[0])#.cuda()\n",
    "        inv_model = inv_dynamics_continuous(env.observation_space.shape[0],100,env.action_space.shape[0])#.cuda()\n",
    "        \n",
    "        \"\"\"start training\"\"\"\n",
    "        inv_dataset_list = []\n",
    "        use_policy = False\n",
    "\n",
    "        transitions = []\n",
    "        for steps in range(runs):\n",
    "            print('######## STEP %d #######'%(steps+1))\n",
    "            ### GET SAMPLES FOR LEARNING INVERSE MODEL\n",
    "            print('Collecting transitions for learning inverse model....')\n",
    "            if steps > 0:\n",
    "                use_policy = True\n",
    "\n",
    "\n",
    "            trans_samples, avg_reward = gen_inv_samples(env, policy.cpu(), M, 'continuous', use_policy,max_steps=max_steps)\n",
    "            #print(np.array(trans_samples).shape, transitions)\n",
    "            transitions = transitions+trans_samples\n",
    "            \n",
    "            f = open(record_fn, \"a+\")\n",
    "            f.write(str(avg_reward) + \"\\n\")\n",
    "            f.close()\n",
    "            \n",
    "            print('Done!', np.array(transitions).shape)\n",
    "\n",
    "            ### LEARN THE INVERSE MODEL\n",
    "            print('Learning inverse model....')\n",
    "            inv_dataset = transition_dataset(transitions)\n",
    "            inv_dataset_list.append(inv_dataset)\n",
    "            inv_dataset_final = ConcatDataset(inv_dataset_list)\n",
    "            inv_loader = DataLoader(inv_dataset_final, batch_size=1024, shuffle=True, num_workers=4)\n",
    "\n",
    "            inv_opt = optim.Adam(inv_model.parameters(), lr=1e-3, weight_decay=0.0001)\n",
    "            inv_loss = nn.MSELoss()\n",
    "            #inv_loss = nn.L1Loss()\n",
    "\n",
    "            for epoch in range(100): \n",
    "                running_loss = 0\n",
    "                for i, data in enumerate(inv_loader):\n",
    "                    s, a, s_prime = data\n",
    "                    inv_opt.zero_grad()\n",
    "                    a_pred = inv_model(s.float(), s_prime.float())\n",
    "                    loss = inv_loss(a_pred, a.float())\n",
    "                    loss.backward()\n",
    "                    running_loss += loss.item()\n",
    "                    if i%100000 == 0:\n",
    "                        print('Epoch:%d Batch:%d Loss:%.5f'%(epoch, i+1, running_loss/100))\n",
    "                        running_loss = 0\n",
    "                    inv_opt.step()\n",
    "                if epoch%10 == 0:\n",
    "                        print('Epoch:%d Batch:%d Loss:%.5f'%(epoch, i+1, loss))\n",
    "            print('Done!')\n",
    "\n",
    "            ### GET ACTIONS FOR DEMOS\n",
    "            inv_model.cpu()\n",
    "            print('Getting labels for demos....')\n",
    "            trajs = get_action_labels(inv_model, demos, 'continuous')\n",
    "            print('Done!')\n",
    "            bc_dataset = imitation_dataset(trajs, type='action')\n",
    "            bc_loader = DataLoader(bc_dataset, batch_size=1024, shuffle=True, num_workers=4)\n",
    "            inv_model\n",
    "\n",
    "            ### PERFORM BEHAVIORAL CLONING\n",
    "            print('Learning policy....')\n",
    "            policy\n",
    "            bc_opt = optim.Adam(policy.parameters(), lr=1e-3, weight_decay=0.0001)\n",
    "            bc_loss = nn.MSELoss()\n",
    "            # bc_loss = nn.L1Loss()\n",
    "\n",
    "            for epoch in range(50):  \n",
    "                running_loss = 0\n",
    "                for i, data in enumerate(bc_loader):\n",
    "                    s, a = data\n",
    "                    bc_opt.zero_grad()\n",
    "                    \"\"\"\n",
    "                    a_mu, a_sigma = policy(s.float())\n",
    "                    a_pred = Normal(loc=a_mu, scale=a_sigma).rsample()\n",
    "                    \"\"\"\n",
    "                    a_pred = policy(s.float())\n",
    "                    loss = bc_loss(a_pred, a)\n",
    "                    running_loss += loss.item()\n",
    "                    loss.backward()\n",
    "                    if i%20 == 19:\n",
    "                        running_loss = 0\n",
    "                    bc_opt.step()\n",
    "                if epoch%10==0:\n",
    "                    print('Epoch:%d Batch:%d Loss:%.3f'%(epoch, i+1, loss))\n",
    "\n",
    "            print('Done!')\n",
    "\n",
    "        torch.save(policy, weight_path+ENV_NAME+str(itr_id)+'_bco.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-stroke",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-arkansas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-distinction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
